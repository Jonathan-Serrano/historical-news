Topic,Count,Name,Representation,Representative_Docs
-1,9388,-1_learning_algorithms_models_deep,"['learning', 'algorithms', 'models', 'deep', 'learn', 'networks', 'optimization', 'neural', 'information', 'data']","['Data-dependent compression of random features for large-scale kernel approximation Kernel methods offer the flexibility to learn complex relationships in modern, large data sets while enjoying strong theoretical guarantees on quality. Unfortunately, these methods typically require cubic running time in the data set size, a prohibitive cost in the large- data setting. Random feature maps (RFMs) and the Nystroöm method both consider low- rank approximations to the kernel matrix as a potential solution. But, in order to achieve desirable theoretical guarantees, the former may require a prohibitively large number of features J+, and the latter may be prohibitively expensive for high-dimensional problems. We propose to combine the simplicity and generality of RFMs with a data-dependent feature selection scheme to achieve desirable theoretical approximation properties of Nyström with just $O(\\log J+)$ features. Our key insight is to begin with a large set of random features, then reduce them to a small number of weighted features in a data-dependent, computationally efficient way, while preserving the statistical guarantees of using the original large set of features. We demonstrate the efficacy of our method with theory and experiments-including on a data set with over 50 million observations. In particular, we show that our method achieves small kernel matrix approximation error and better test set accuracy with provably fewer random features than state-of-the-art methods.', 'Double Nyström Method: An Efficient and Accurate Nyström Scheme for Large-Scale Data Sets The Nyström method has been one of the most effective techniques for kernel-based approach that scales well to large data sets. Since its introduction, there has been a large body of work that improves the approximation accuracy while maintaining computational efficiency. In this paper, we present a novel Nyström method that improves both accuracy and efficiency based on a new theoretical analysis. We first provide a generalized sampling scheme, CAPS, that minimizes a novel error bound based on the subspace distance. We then present our double Nyström method that reduces the size of the decomposition in two stages. We show that our method is highly efficient and accurate compared to other state-of-the-art Nyström methods by evaluating them on a number of real data sets.', 'Goal-Aware Prediction: Learning to Model What Matters Learned dynamics models combined with both planning and policy learning algorithms have shown promise in enabling artificial agents to learn to perform many diverse tasks with limited supervision. However, one of the fundamental challenges in using a learned forward dynamics model is the mismatch between the objective of the learned model (future state reconstruction), and that of the downstream planner or policy (completing a specified task). This issue is exacerbated by vision-based control tasks in diverse real-world environments, where the complexity of the real world dwarfs model capacity. In this paper, we propose to direct prediction towards task relevant information, enabling the model to be aware of the current task and encouraging it to only model relevant quantities of the state space, resulting in a learning objective that more closely matches the downstream task. Further, we do so in an entirely self-supervised manner, without the need for a reward function or image labels. We find that our method more effectively models the relevant parts of the scene conditioned on the goal, and as a result outperforms standard task-agnostic dynamics models and model-free reinforcement learning.']"
0,1,variational autoencoders,"['autoencoder', 'variational', 'lossy', '', '', '', '', '', '', '']",['Variational lossy autoencoder ']
1,2,probabilistic graphical models,"['probabilistic', 'models', 'graphical', 'conference', 'preface', 'on', 'of', 'eighth', '9th', 'proceedings']","['Proceedings of the 9th International Conference on Probabilistic Graphical Models ', 'Proceedings of the Eighth International Conference on Probabilistic Graphical Models Preface']"
2,1,regression models,"['regressions', 'models', '101', '', '', '', '', '', '', '']",['Regressions Models 101 ']
3,1,evolutionary algorithms,"['evolutionary', 'algorithms', 'understanding', '', '', '', '', '', '', '']",['Understanding Evolutionary Algorithms ']
4,3,grammatical inferences,"['inference', 'grammatical', 'conference', 'preface', 'the', 'international', '12th', 'for', 'on', '']","['Preface Preface for the 12th International Conference on Grammatical Inference.', 'International Conference on Grammatical Inference 2016: Preface ', 'International Conference on Grammatical Inference 2018: Preface ']"
5,14,5_preface_abstract_the_,"['preface', 'abstract', 'the', '', '', '', '', '', '', '']","['Preface ', 'Preface ', 'Preface ']"
6,12,6_preface___,"['preface', '', '', '', '', '', '', '', '', '']","['Preface ', 'Preface ', 'Preface ']"
7,60,7_quantum_quantumclassical_qubit_qubits,"['quantum', 'quantumclassical', 'qubit', 'qubits', 'quantuminspired', 'qnns', 'qnn', 'qap', 'qas', 'unitary']","['Quantum Policy Gradient Algorithm with Optimized Action Decoding Quantum machine learning implemented by variational quantum circuits (VQCs) is considered a promising concept for the noisy intermediate-scale quantum computing era. Focusing on applications in quantum reinforcement learning, we propose an action decoding procedure for a quantum policy gradient approach. We introduce a quality measure that enables us to optimize the classical post-processing required for action selection, inspired by local and global quantum measurements. The resulting algorithm demonstrates a significant performance improvement in several benchmark environments. With this technique, we successfully execute a full training routine on a 5-qubit hardware device. Our method introduces only negligible classical overhead and has the potential to improve VQC-based algorithms beyond the field of quantum reinforcement learning.', 'Policy Gradient based Quantum Approximate Optimization Algorithm The quantum approximate optimization algorithm (QAOA), as a hybrid quantum/classical algorithm, has received much interest recently. QAOA can also be viewed as a variational ansatz for quantum control.  However, its direct application to emergent quantum technology encounters additional physical constraints: (i) the states of the quantum system are not observable; (ii) obtaining the derivatives of the objective function can be computationally expensive or even inaccessible in experiments, and (iii) the values of the objective function may be sensitive to various sources of uncertainty, as is the case for noisy intermediate-scale quantum (NISQ) devices. Taking such constraints into account, we show that policy-gradient-based reinforcement learning (RL) algorithms are well suited for optimizing the variational parameters of QAOA in a noise-robust fashion, opening up the way for developing RL techniques for continuous quantum control. This is advantageous to help mitigate and monitor the potentially unknown sources of errors in modern quantum simulators.  We analyze the performance of the algorithm for quantum state transfer problems in single- and multi-qubit systems, subject to various sources of noise such as error terms in the Hamiltonian, or quantum uncertainty in the measurement process. We show that, in noisy setups, it is capable of outperforming state-of-the-art existing optimization algorithms. ', 'Information-theoretic generalization bounds for learning from quantum data Learning tasks play an increasingly prominent role in quantum information and computation. They range from fundamental problems such as state discrimination and metrology over the framework of quantum probably approximately correct (PAC) learning, to the recently proposed shadow variants of state tomography. However, the many directions of quantum learning theory have so far evolved separately. We propose a mathematical formalism for describing quantum learning by training on classical-quantum data and then testing how well the learned hypothesis generalizes to new data. In this framework, we prove bounds on the expected generalization error of a quantum learner in terms of classical and quantum information-theoretic quantities measuring how strongly the learner’s hypothesis depends on the data seen during training. To achieve this, we use tools from quantum optimal transport and quantum concentration inequalities to establish non-commutative versions of decoupling lemmas that underlie classical information-theoretic generalization bounds. Our framework encompasses and gives intuitive generalization bounds for a variety of quantum learning scenarios such as quantum state discrimination, PAC learning quantum states, quantum parameter estimation, and quantumly PAC learning classical functions. Thereby, our work lays a foundation for a unifying quantum information-theoretic perspective on quantum learning.']"
8,166,8_openaiapplechatgptapple_openai_chatgpt_openaiapple,"['openaiapplechatgptapple', 'openai', 'chatgpt', 'openaiapple', 'chatgptenterprise', 'enterpriseaigritlabpaf70chatgpt', 'teamchatgpt', 'enterpriseapi', 'translations', 'healthopenai']","['ChatGPT をすぐに使い始める 利用登録が不要で AI の機能を簡単に体験できるようになります', 'OpenAI におけるエンタープライズプライバシー 信頼とプライバシーは OpenAI の使命の中核を成すものです。ChatGPT Team、ChatGPT Enterprise、API プラットフォームのプライバシーとセキュリティを保護しています。', 'OpenAI における企業プライバシー 信頼とプライバシーは OpenAI の使命の中核を成すものです。ChatGPT Team、ChatGPT Enterprise、API プラットフォームのプライバシーとセキュリティを保護しています。']"
9,11,9_acml_gmml_midl_preface,"['acml', 'gmml', 'midl', 'preface', 'summary', 'proceedings', '2022', 'remarks', '2023', 'fsdm08']","['Preface Preface to MIDL 2022', 'Preface Preface to MIDL 2019', 'Preface Preface to ACML 2014']"
10,36,10_changepoints_changepoint_detection_detect,"['changepoints', 'changepoint', 'detection', 'detect', 'detecting', 'changes', 'bayesian', 'postchange', 'streams', 'change']","['Inductive Conformal Martingales for Change-Point Detection We consider the problem of quickest change-point detection in data streams.\n Classical change-point detection procedures, such as CUSUM, Shiryaev-Roberts and Posterior Probability statistics,\n are optimal only if the change-point model is known, which is an unrealistic assumption in typical applied problems.\n Instead we propose a new method for change-point detection based on Inductive Conformal Martingales,\n which requires only the independence and identical distribution of observations.\n We compare the proposed approach to standard methods,\n as well as to change-point detection oracles, which model a typical practical situation\n when we have only imprecise (albeit parametric) information about pre- and post-change data distributions.\n Results of comparison provide evidence that change-point detection based on Inductive Conformal Martingales is an efficient tool,\n capable to work under quite general conditions unlike traditional approaches.', 'Privately detecting changes in unknown distributions The change-point detection problem seeks to identify distributional changes in streams of data. Increasingly, tools for change-point detection are applied in settings where data may be highly sensitive and formal privacy guarantees are required, such as identifying disease outbreaks based on hospital records, or IoT devices detecting activity within a home. Differential privacy has emerged as a powerful technique for enabling data analysis while preventing information leakage about individuals. Much of the prior work on change-point detection{—}including the only private algorithms for this problem{—}requires complete knowledge of the pre-change and post-change distributions, which is an unrealistic assumption for many practical applications of interest. This work develops differentially private algorithms for solving the change-point detection problem when the data distributions are unknown. Additionally, the data may be sampled from distributions that change smoothly over time, rather than fixed pre-change and post-change distributions. We apply our algorithms to detect changes in the linear trends of such data streams. Finally, we also provide experimental results to empirically validate the performance of our algorithms.', 'Restarted Bayesian Online Change-point Detector achieves Optimal Detection Delay we consider the problem of sequential change-point detection where \tboth the change-points and the distributions before and after the change are assumed to be unknown. For this problem of primary importance in statistical and sequential learning theory, we derive a variant of the Bayesian Online Change Point Detector proposed by \\cite{fearnhead2007line} \twhich is easier to analyze than the original version while keeping its powerful message-passing algorithm. \tWe provide a non-asymptotic analysis of the false-alarm rate and the detection delay that matches the existing lower-bound. We further provide the first explicit high-probability control of the detection delay for such approach. Experiments on synthetic and real-world data show that this proposal outperforms the state-of-art change-point detection strategy, namely the Improved Generalized Likelihood Ratio (Improved GLR) while compares favorably with the original Bayesian Online Change Point Detection strategy.']"
11,219,11_pdes_neural_pde_solvers,"['pdes', 'neural', 'pde', 'solvers', 'learning', 'nonlinear', 'networks', 'models', 'solver', 'modeling']","['Residual-based error bound for physics-informed neural networks Neural networks are universal approximators and are studied for their use in solving differential equations.  However, a major criticism is the lack of error bounds for obtained solutions.  This paper proposes a technique to rigorously evaluate the error bound of Physics-Informed Neural Networks (PINNs) on most linear ordinary differential equations (ODEs), certain nonlinear ODEs, and first-order linear partial differential equations (PDEs).  The error bound is based purely on equation structure and residual information and does not depend on assumptions of how well the networks are trained.  We propose algorithms that bound the error efficiently. Some proposed algorithms provide tighter bounds than others at the cost of longer run time.', 'Semi-supervised learning of partial differential operators and dynamical flows The evolution of many dynamical systems is generically governed by nonlinear partial differential equations (PDEs), whose solution, in a simulation framework, requires vast amounts of computational resources. In this work, we present a novel method that combines a hyper-network solver with a Fourier Neural Operator architecture. Our method treats time and space separately and as a result, it successfully propagates initial conditions in continuous time steps by employing the general composition properties of the partial differential operators. Following previous works, supervision is provided at a specific time point. We test our method on various time evolution PDEs, including nonlinear fluid flows in one, two, or three spatial dimensions. The results show that the new method improves the learning accuracy at the time of the supervision point, and can interpolate the solutions to any intermediate time.', 'PDE-Net: Learning PDEs from Data Partial differential equations (PDEs) play a prominent role in many disciplines of science and engineering. PDEs are commonly derived based on empirical observations. However, with the rapid development of sensors, computational power, and data storage in the past decade, huge quantities of data can be easily collected and efficiently stored. Such vast quantity of data offers new opportunities for data-driven discovery of physical laws. Inspired by the latest development of neural network designs in deep learning, we propose a new feed-forward deep network, called PDE-Net, to fulfill two objectives at the same time: to accurately predict dynamics of complex systems and to uncover the underlying hidden PDE models. Comparing with existing approaches, our approach has the most flexibility by learning both differential operators and the nonlinear response function of the underlying PDE model. A special feature of the proposed PDE-Net is that all filters are properly constrained, which enables us to easily identify the governing PDE models while still maintaining the expressive and predictive power of the network. These constrains are carefully designed by fully exploiting the relation between the orders of differential operators and the orders of sum rules of filters (an important concept originated from wavelet theory). Numerical experiments show that the PDE-Net has the potential to uncover the hidden PDE of the observed dynamics, and predict the dynamical behavior for a relatively long time, even in a noisy environment.']"
12,20,12_learning_theory_learningiwssl2021_educational,"['learning', 'theory', 'learningiwssl2021', 'educational', 'courses', 'students', 'supervised', 'introduction', 'conference', 'workshop']","['Conference on Learning Theory 2018: Preface Preface to the proceedings of the 31st Conference On Learning Theory.', 'Conference on Learning Theory 2019: Preface Preface to the proceedings of the 32nd Conference on Learning Theory.', 'Conference on Learning Theory 2020: Preface Preface to the proceedings of the 32nd Conference on Learning Theory']"
13,27,13_strategic_classification_strategically_strategyproof,"['strategic', 'classification', 'strategically', 'strategyproof', 'classifiers', 'learning', 'classifier', 'decisionmaker', 'learner', 'features']","['Learnability Gaps of Strategic Classification In contrast with standard classification tasks, strategic classification involves agents strategically modifying their features in an effort to receive favorable predictions. For instance, given a classifier determining loan approval based on credit scores, applicants may open or close their credit cards and bank accounts to fool the classifier. The learning goal is to find a classifier robust against strategic manipulations. Various settings, based on what and when information is known, have been explored in strategic classification. In this work, we focus on addressing a fundamental question: the learnability gaps between strategic classification and standard learning.  We essentially show that any learnable class is also strategically learnable: we first consider a fully informative setting, where the manipulation structure (which is modeled by a manipulation graph $G^\\star$) is known and during training time the learner has access to both the pre-manipulation data and post-manipulation data. We provide nearly tight sample complexity and regret bounds, offering significant improvements over prior results. Then, we relax the fully informative setting by introducing two natural types of uncertainty.  First, following Ahmadi et al. (2023), we consider the setting in which the learner only has access to the post-manipulation data. We improve the results of Ahmadi et al. (2023) and close the gap between mistake upper bound and lower bound raised by them.  Our second relaxation of the fully informative setting introduces uncertainty to the manipulation structure. That is, we assume that the manipulation graph is unknown but belongs to a known class of graphs. We provide nearly tight bounds on the learning complexity in various unknown manipulation graph settings. Notably, our algorithm in this setting is of independent interest and can be applied to other problems such as multi-label learning.', 'Generalized Strategic Classification and the Case of Aligned Incentives Strategic classification studies learning in settings where self-interested users can strategically modify their features to obtain favorable predictive outcomes. A key working assumption, however, is that “favorable” always means “positive”; this may be appropriate in some applications (e.g., loan approval), but reduces to a fairly narrow view of what user interests can be. In this work we argue for a broader perspective on what accounts for strategic user behavior, and propose and study a flexible model of generalized strategic classification. Our generalized model subsumes most current models but includes other novel settings; among these, we identify and target one intriguing sub-class of problems in which the interests of users and the system are aligned. This setting reveals a surprising fact: that standard max-margin losses are ill-suited for strategic inputs. Returning to our fully generalized model, we propose a novel max-margin framework for strategic learning that is practical and effective, and which we analyze theoretically. We conclude with a set of experiments that empirically demonstrate the utility of our approach.', 'Strategic Classification Made Practical Strategic classification regards the problem of learning in settings where users can strategically modify their features to improve outcomes. This setting applies broadly, and has received much recent attention. But despite its practical significance, work in this space has so far been predominantly theoretical. In this paper we present a learning framework for strategic classification that is practical. Our approach directly minimizes the “strategic” empirical risk, which we achieve by differentiating through the strategic response of users. This provides flexibility that allows us to extend beyond the original problem formulation and towards more realistic learning scenarios. A series of experiments demonstrates the effectiveness of our approach on various learning settings.']"
14,209,14_fairnessaware_fairness_discrimination_unfairness,"['fairnessaware', 'fairness', 'discrimination', 'unfairness', 'bias', 'classifiers', 'biases', 'unfair', 'classifier', 'classification']","['Achieving Fairness through Separability: A Unified Framework for Fair Representation Learning Fairness is a growing concern in machine learning as state-of-the-art models may amplify social prejudice by making biased predictions against specific demographics such as race and gender. Such discrimination raises issues in various fields such as employment, criminal justice, and trust score evaluation. To address the concerns, we propose learning fair representation through a straightforward yet effective approach to project intrinsic information while filtering sensitive information for downstream tasks. Our model consists of two goals: one is to ensure that the latent data from different demographic groups is non-separable (i.e., make the latent data distribution independent of the sensitive feature to improve fairness); the other is to maximize the separability of latent data from different classes (i.e., maintain the discriminative power of data for the sake of the downstream tasks like classification). Our method adopts a non-zero-sum adversarial game to minimize the distance between data from different demographic groups while maximizing the margin between data from different classes. Moreover, the proposed objective function can be easily generalized to multiple sensitive attributes and multi-class scenarios as it upper bounds popular fairness metrics in these cases. We provide theoretical analysis of the fairness of our model and validate w.r.t. both fairness and predictive performance on benchmark datasets.', 'Fairness in Reinforcement Learning We initiate the study of fairness in reinforcement learning, where the actions of a learning algorithm may affect its environment and future rewards. Our fairness constraint requires that an algorithm never prefers one action over another if the long-term (discounted) reward of choosing the latter action is higher. Our first result is negative: despite the fact that fairness is consistent with the optimal policy, any learning algorithm satisfying fairness must take time exponential in the number of states to achieve non-trivial approximation to the optimal policy. We then provide a provably fair polynomial time algorithm under an approximate notion of fairness, thus establishing an exponential gap between exact and approximate fairness.', 'The cost of fairness in binary classification Binary classifiers are often required to possess fairness in the sense of not overly discriminating with respect to a feature deemed sensitive e.g. race. We study the inherent tradeoffs in learning classifiers with a fairness constraint in the form of two questions: what is the best accuracy we can expect for a given level of fairness?, and what is the nature of these optimal fairness-aware classifiers? To answer these questions, we provide three main contributions. First, we relate two existing fairness measures to cost-sensitive risks. Second, we show that for such cost-sensitive fairness measures, the optimal classifier is an instance-dependent thresholding of the class-probability function. Third, we relate the tradeoff between accuracy and fairness to the alignment between the target and sensitive features’ class-probabilities. A practical implication of our analysis is a simple approach to the fairness-aware problem which involves suitably thresholding class-probability estimates.']"
15,62,15_imputations_imputation_missingness_imputed,"['imputations', 'imputation', 'missingness', 'imputed', 'imputing', 'incomplete', 'datasets', 'impute', 'missing', 'data']","['Transformed Distribution Matching for Missing Value Imputation We study the problem of imputing missing values in a dataset, which has important applications in many domains. The key to missing value imputation is to capture the data distribution with incomplete samples and impute the missing values accordingly. In this paper, by leveraging the fact that any two batches of data with missing values come from the same data distribution, we propose to impute the missing values of two batches of samples by transforming them into a latent space through deep invertible functions and matching them distributionally. To learn the transformations and impute the missing values simultaneously, a simple and well-motivated algorithm is proposed. Our algorithm has fewer hyperparameters to fine-tune and generates high-quality imputations regardless of how missing values are generated. Extensive experiments over a large number of datasets and competing benchmark algorithms show that our method achieves state-of-the-art performance.', 'Probabilistic Imputation for Time-series Classification with Missing Data Multivariate time series data for real-world applications typically contain a significant amount of missing values. The dominant approach for classification with such missing values is to impute them heuristically with specific values (zero, mean, values of adjacent time-steps) or learnable parameters. However, these simple strategies do not take the data generative process into account, and more importantly, do not effectively capture the uncertainty in prediction due to the multiple possibilities for the missing values. In this paper, we propose a novel probabilistic framework for classification with multivariate time series data with missing values. Our model consists of two parts; a deep generative model for missing value imputation and a classifier. Extending the existing deep generative models to better capture structures of time-series data, our deep generative model part is trained to impute the missing values in multiple plausible ways, effectively modeling the uncertainty of the imputation. The classifier part takes the time series data along with the imputed missing values and classifies signals, and is trained to capture the predictive uncertainty due to the multiple possibilities of imputations. Importantly, we show that naïvely combining the generative model and the classifier could result in trivial solutions where the generative model does not produce meaningful imputations. To resolve this, we present a novel regularization technique that can promote the model to produce useful imputation values that help classification. Through extensive experiments on real-world time series data with missing values, we demonstrate the effectiveness of our method.', 'Missing Values and Imputation in Healthcare Data:  Can Interpretable Machine Learning Help? Missing values are a fundamental problem in data science. Many datasets have missing values that must be properly handled because the way missing values are treated can have large impact on the resulting machine learning model. In medical applications, the consequences may affect healthcare decisions. There are many methods in the literature for dealing with missing values, including state-of-the-art methods which often depend on black-box models for imputation. In this work, we show how recent advances in interpretable machine learning provide a new perspective for understanding and tackling the missing value problem. We propose methods based on high-accuracy glass-box Explainable Boosting Machines (EBMs) that can help users (1) gain new insights on missingness mechanisms and better understand the causes of missingness, and (2) detect – or even alleviate – potential risks introduced by imputation algorithms. Experiments on real-world medical datasets illustrate the effectiveness of the proposed methods.']"
16,271,16_optimizing_optimization_optimize_optimisation,"['optimizing', 'optimization', 'optimize', 'optimisation', 'optimum', 'bayesian', 'hyperparameters', 'hyperparameter', 'algorithms', 'benchmark']","['On the role of model uncertainties in Bayesian optimisation Bayesian Optimization (BO) is a popular method for black-box optimization, which relies on uncertainty as part of its decision-making process when deciding which experiment to perform next. However, not much work has addressed the effect of uncertainty on the performance of the BO algorithm and to what extent calibrated uncertainties improve the ability to find the global optimum. In this work, we provide an extensive study of the relationship between the BO performance (regret) and uncertainty calibration for popular surrogate models and acquisition functions, and compare them across both synthetic and real-world experiments. Our results show that Gaussian Processes, and more surprisingly, Deep Ensembles are strong surrogate models. Our results further show a positive association between calibration error and regret, but interestingly, this association disappears  when we control for the type of surrogate model in the analysis. We also study the effect of recalibration and demonstrate that it generally does not lead to improved regret. Finally, we provide theoretical justification for why uncertainty calibration might be difficult to combine with BO due to the small sample sizes commonly used.', 'Amortized Bayesian Optimization over Discrete Spaces Bayesian optimization is a principled approach for globally optimizing expensive, black-box functions by using a surrogate model of the objective. However, each step of Bayesian optimization involves solving an inner optimization problem, in which we maximize an acquisition function  derived from the surrogate model to decide where to query next. This inner problem can be challenging to solve, particularly in discrete spaces, such as protein sequences or molecular graphs, where gradient-based optimization cannot be used. Our key insight is that we can train a generative model to generate candidates that maximize the acquisition function. This is faster than standard model-free local search methods, since we can amortize the cost of learning the model across multiple rounds of Bayesian optimization. We therefore call this Amortized Bayesian Optimization. On several challenging discrete design problems, we show this method generally outperforms other methods at optimizing the inner acquisition function, resulting in more efficient optimization of the outer black-box objective.', 'Computationally Efficient High-Dimensional Bayesian Optimization via Variable Selection Bayesian Optimization (BO) is a widely-used method for the global optimization of black-box functions. While BO has been successfully applied to many scenarios, scaling BO algorithms to high-dimensional domains remains a challenge. Optimizing such functions by vanilla BO is extremely time-consuming. Alternative strategies for high-dimensional BO that are based on the idea of embedding the high-dimensional space to one with low dimensions are sensitive to the choice of the embedding dimension, which needs to be pre-specified. We develop a new computationally efficient high-dimensional BO method that leverages variable selection. We analyze the computational complexity of our algorithm and demonstrate its efficacy on several synthetic and real problems through empirical evaluations.']"
17,13,17_transcriptomics_celltype_genomics_multiomics,"['transcriptomics', 'celltype', 'genomics', 'multiomics', 'cell', 'cells', 'singlecell', 'cellular', 'metagenomic', 'rna']","['Energy-based Modelling for Single-cell Data Annotation Single-cell sequencing has provided profound insights into understanding heterogeneous cellular activities by measuring sequence information at the individual cell resolution. Accurately annotating a single-cell RNA sequencing (scRNA-seq) dataset is a crucial step for the single-cell data analysis pipeline. In particular, previously unobserved cell types and cellular states frequently appear in scRNA-seq experiments and carry valuable information. This highlights the need for reliable annotation tools with out-of-distribution (OOD) detection capability. Recent advances in energy-based modelling have made it possible to design and deploy EBMs for joint discriminative and generative tasks. In this work, we introduced energy-based models (EBMs) for scRNA-seq annotation and investigated generative modelling for OOD detection, which result in more accurate, calibrated, and robust cell-type predictions. Specifically, we developed CLAMS, an EBM instance improved upon the previous joint energy-based model (JEM), for single-cell data hybrid modelling. Our experiments reveal that hybrid modelling with EBMs maintains the strong discriminative power of baseline classifiers and outperforms the state-of-the-art by integrating generative capabilities in data annotation and OOD detection tasks. To the best of our knowledge, we are the first to apply EBMs for single-cell data modelling.', 'LangCell: Language-Cell Pre-training for Cell Identity Understanding Cell identity encompasses various semantic aspects of a cell, including cell type, pathway information, disease information, and more, which are essential for biologists to gain insights into its biological characteristics. Understanding cell identity from the transcriptomic data, such as annotating cell types, has become an important task in bioinformatics. As these semantic aspects are determined by human experts, it is impossible for AI models to effectively carry out cell identity understanding tasks without the supervision signals provided by single-cell and label pairs. The single-cell pre-trained language models (PLMs) currently used for this task are trained only on a single modality, transcriptomics data, lack an understanding of cell identity knowledge. As a result, they have to be fine-tuned for downstream tasks and struggle when lacking labeled data with the desired semantic labels. To address this issue, we propose an innovative solution by constructing a unified representation of single-cell data and natural language during the pre-training phase, allowing the model to directly incorporate insights related to cell identity. More specifically, we introduce <b>LangCell</b>, the first <b>Lang</b>uage-<b>Cell</b> pre-training framework. LangCell utilizes texts enriched with cell identity information to gain a profound comprehension of cross-modal knowledge. Results from experiments conducted on different benchmarks show that LangCell is the only single-cell PLM that can work effectively in zero-shot cell identity understanding scenarios, and also significantly outperforms existing models in few-shot and fine-tuning cell identity understanding scenarios.', 'Investigating RNA splicing as a source of cellular diversity using a binomial mixture model Alternative splicing (AS) contributes significantly to RNA and protein variability yet its role in defining cellular diversity is not fully understood. While Smart-seq2 offers enhanced coverage across transcripts compared to 10X single cell RNA-sequencing (scRNA-seq), current computational methods often miss the full complexity of AS. Most approaches for single cell based differential splicing analysis focus on simple AS events such as exon skipping, and rely on predefined cell type labels or low-dimensional gene expression representations. This limits their ability to detect more complex AS events and makes them dependent on prior knowledge of cell classifications. Here, we present Leaflet, a splice junction centric approach inspired by Leafcutter, our tool for quantifying RNA splicing variation with bulk RNA-seq. Leaflet is a probabilistic mixture model designed to infer AS-driven cell states without the need for cell type labels. We detail Leaflet’s generative model, inference methodology, and its efficiency in detecting differentially spliced junctions. By applying Leaflet to the Tabula Muris brain cell dataset, we highlight cell-state specific splicing patterns, offering a deeper insight into cellular diversity beyond that captured by gene expression alone.']"
18,244,18_molecule_molecular_proteins_molecules,"['molecule', 'molecular', 'proteins', 'molecules', 'ligands', 'models', 'modeling', 'protein', 'peptide', 'representations']","['Reinforcement Learning for Molecular Design Guided by Quantum Mechanics Automating molecular design using deep reinforcement learning (RL) holds the promise of accelerating the discovery of new chemical compounds. Existing approaches work with molecular graphs and thus ignore the location of atoms in space, which restricts them to 1) generating single organic molecules and 2) heuristic reward functions. To address this, we present a novel RL formulation for molecular design in Cartesian coordinates, thereby extending the class of molecules that can be built. Our reward function is directly based on fundamental physical properties such as the energy, which we approximate via fast quantum-chemical methods. To enable progress towards de-novo molecular design, we introduce MolGym, an RL environment comprising several challenging molecular design tasks along with baselines. In our experiments, we show that our agent can efficiently learn to solve these tasks from scratch by working in a translation and rotation invariant state-action space.', 'Learning Subpocket Prototypes for Generalizable Structure-based Drug Design Generating molecules with high binding affinities to target proteins (a.k.a. structure-based drug design) is a fundamental and challenging task in drug discovery. Recently, deep generative models have achieved remarkable success in generating 3D molecules conditioned on the protein pocket. However, most existing methods consider molecular generation for protein pockets independently while neglecting the underlying connections such as subpocket-level similarities. Subpockets are the local protein environments of ligand fragments and pockets with similar subpockets may bind the same molecular fragment (motif) even though their overall structures are different. Therefore, the trained models can hardly generalize to unseen protein pockets in real-world applications. In this paper, we propose a novel method DrugGPS for generalizable structure-based drug design. With the biochemical priors, we propose to learn subpocket prototypes and construct a global interaction graph to model the interactions between subpocket prototypes and molecular motifs. Moreover, a hierarchical graph transformer encoder and motif-based 3D molecule generation scheme are used to improve the model’s performance. The experimental results show that our model consistently outperforms baselines in generating realistic drug candidates with high affinities in challenging out-of-distribution settings.', 'A Latent Diffusion Model for Protein Structure Generation Proteins are complex biomolecules that perform a variety of crucial functions within living organisms. Designing and generating novel proteins can pave the way for many future synthetic biology applications, including drug discovery. However, it remains a challenging computational task due to the large modeling space of protein structures. In this study, we propose a latent diffusion model that can reduce the complexity of protein modeling while flexibly capturing the distribution of natural protein structures in a condensed latent space. Specifically, we propose an equivariant protein autoencoder that embeds proteins into a latent space and then uses an equivariant diffusion model to learn the distribution of the latent protein representations. Experimental results demonstrate that our method can effectively generate novel protein backbone structures with high designability and efficiency. The code will be made publicly available at https://github.com/divelab/AIRS/tree/main/OpenProt/LatentDiff']"
19,102,19_uber_ubers_uber8217s_ubera,"['uber', 'ubers', 'uber8217s', 'ubera', 'ai', 'forecasting', 'engineering', 'transportation', 'machine', 'learning']","['Science at Uber: Powering Machine Learning at Uber <p><i><span style=""font-weight: 400;"">At Uber, we take advanced research work and use it to solve real world problems. In our\xa0 </span></i><a href=""https://eng.uber.com/tag/science-at-uber/"" target=""_blank"" rel=""noopener noreferrer""><i><span style=""font-weight: 400;"">Science at Uber video series</span></i></a><i><span style=""font-weight: 400;"">, Uber employees talk about how we apply data science, artificial intelligence, machine learning, and other innovative technologies </span></i>&#8230;</p>\n<p>The post <a rel=""nofollow"" href=""https://eng.uber.com/uber-science-machine-learning-platform/"">Science at Uber: Powering Machine Learning at Uber</a> appeared first on <a rel=""nofollow"" href=""https://eng.uber.com"">Uber Engineering Blog</a>.</p>\n', 'Engineering Uncertainty Estimation in Neural Networks for Time Series Prediction at Uber <p>Uber Engineering introduces a new Bayesian neural network architecture that more accurately forecasts time series predictions and uncertainty estimations.</p>\n<p>The post <a rel=""nofollow"" href=""https://eng.uber.com/neural-networks-uncertainty-estimation/"">Engineering Uncertainty Estimation in Neural Networks for Time Series Prediction at Uber</a> appeared first on <a rel=""nofollow"" href=""https://eng.uber.com"">Uber Engineering Blog</a>.</p>\n', 'Engineering More Reliable Transportation with Machine Learning and AI at Uber <p>In this article, we highlight how Uber leverages machine learning and artificial intelligence to tackle engineering challenges at scale. </p>\n<p>The post <a rel=""nofollow"" href=""https://eng.uber.com/machine-learning/"">Engineering More Reliable Transportation with Machine Learning and AI at Uber</a> appeared first on <a rel=""nofollow"" href=""https://eng.uber.com"">Uber Engineering Blog</a>.</p>\n']"
20,82,20_nvidia_geforcecom_geforce_nvidias,"['nvidia', 'geforcecom', 'geforce', 'nvidias', 'gpu', 'gpus', 'gtx', 'supercomputer', 'rtx', 'supercomputing']","['Innovation to Impact: How NVIDIA Research Fuels Transformative Work in AI, Graphics and Beyond The roots of many of NVIDIA’s landmark innovations — the foundational technology that powers AI, accelerated computing, real-time ray tracing and seamlessly connected data centers — can be found in the company’s research organization, a global team of around 400 experts in fields including computer architecture, generative AI, graphics and robotics. Established in 2006 and\t<a class=""read-more"" href=""https://blogs.nvidia.com/blog/nvidia-research-ai-graphics/"">\n\t\tRead Article\t\t<span data-icon=""y""></span>\n\t</a>\n\t', 'Gear Up With NVIDIA GeForce At BlizzCon 2011 Calling all Blizzard fans. If you’re going to be at BlizzCon 2011 from October 21 – 22 in Anaheim, Calif., make sure to come by the NVIDIA booth and GeForce LAN in Hall C. After you’ve had your fill of gaming in our LAN area, check out our demo stage to learn how to gear....', 'Accelerating AI Development With NVIDIA RTX PRO Blackwell Series GPUs and NVIDIA NIM Microservices for RTX As generative AI capabilities expand, NVIDIA is equipping developers with the tools to seamlessly integrate AI into creative projects, applications and games to unlock groundbreaking experiences on NVIDIA RTX AI PCs and workstations. At the NVIDIA GTC global AI conference this week, NVIDIA introduced the NVIDIA RTX PRO Blackwell series, a new generation of workstation\t<a class=""read-more"" href=""https://blogs.nvidia.com/blog/rtx-ai-garage-blackwell-pro-nim/"">\n\t\tRead Article\t\t<span data-icon=""y""></span>\n\t</a>\n\t']"
21,17,21_bandits_bandit_causal_reward,"['bandits', 'bandit', 'causal', 'reward', 'optimal', 'interventions', 'regret', 'algorithms', 'prior', 'intervene']","['Additive Causal Bandits with Unknown Graph We explore algorithms to select actions in the causal bandit setting where the learner can choose to intervene on a set of random variables related by a causal graph, and the learner sequentially chooses interventions and observes a sample from the interventional distribution. The learner’s goal is to quickly find the intervention, among all interventions on observable variables, that maximizes the expectation of an outcome variable. We depart from previous literature by assuming no knowledge of the causal graph except that latent confounders between the outcome and its ancestors are not present. We first show that the unknown graph problem can be exponentially hard in the parents of the outcome. To remedy this, we adopt an additional additive assumption on the outcome which allows us to solve the problem by casting it as an additive combinatorial linear bandit problem with full-bandit feedback. We propose a novel action-elimination algorithm for this setting, show how to apply this algorithm to the causal bandit problem, provide sample complexity bounds, and empirically validate our findings on a suite of randomly generated causal models, effectively showing that one does not need to explicitly learn the parents of the outcome to identify the best intervention.', ' Budgeted and Non-Budgeted Causal Bandits   Learning good interventions in a causal graph can be modelled as a stochastic multi-armed bandit problem with side-information. First, we study this problem when interventions are more expensive than observations and a budget is specified. If there are no backdoor paths from the intervenable nodes to the reward node then we propose an algorithm to minimize simple regret that optimally trades-off observations and interventions based on the cost of intervention. We also propose an algorithm that accounts for the cost of interventions, utilizes causal side-information, and minimizes the expected cumulative regret without exceeding the budget. Our algorithm performs better than standard algorithms that do not take side-information into account. Finally, we study the problem of learning best interventions without budget constraint in general graphs and give an algorithm that achieves constant expected cumulative regret in terms of the instance parameters when the parent distribution of the reward variable for each intervention is known. Our results are experimentally validated and compared to the best-known bounds in the current literature. ', 'Regret Analysis of Bandit Problems with Causal Background Knowledge We study how to learn optimal interventions sequentially given causal information represented as a causal graph along with associated conditional distributions. Causal modeling is useful in real world problems like online advertisement where complex causal mechanisms underlie the relationship between interventions and outcomes. We propose two algorithms, causal upper confidence bound (C-UCB) and causal Thompson Sampling (C-TS), that enjoy improved cumulative regret bounds compared with algorithms that do not use causal information. We thus resolve an open problem posed by Lattimore et al. (2016). Further, we extend C-UCB and C-TS to the linear bandit setting and propose causal linear UCB (CL-UCB) and causal linear TS (CL-TS) algorithms. These algorithms enjoy a cumulative regret bound that only scales with the feature dimension. Our experiments show the benefit of using causal information. For example, we observe that even with a few hundreds of iterations, the regret of causal algorithms is less than that of standard algorithms by a factor of three. We also show that under certain causal structures, our algorithms scale better than the standard bandit algorithms as the number of interventions increases.']"
22,729,22_causal_causality_inference_unobserved,"['causal', 'causality', 'inference', 'unobserved', 'observational', 'confounders', 'models', 'discovery', 'estimating', 'identifiable']","['Causal Effect Identifiability under Partial-Observability Causal effect identifiability is concerned with establishing the effect of intervening on a set of variables on another set of variables from observational or interventional distributions under causal assumptions that are usually encoded in the form of a causal graph. Most of the results of this literature implicitly assume that every variable modeled in the graph is measured in the available distributions. In practice, however, the data collections of the different studies considered do not measure the same variables, consistently. In this paper, we study the causal effect identifiability problem when the available distributions encompass different sets of variables, which we refer to as identification under partial-observability. We study a number of properties of the factors that comprise a causal effect under various levels of abstraction, and then characterize the relationship between them with respect to their status relative to the identification of a targeted intervention. We establish a sufficient graphical criterion for determining whether the effects are identifiable from partially-observed distributions. Finally, building on these graphical properties, we develop an algorithm that returns a formula for a causal effect in terms of the available distributions.', 'Estimating Possible Causal Effects with Latent Variables via Adjustment Causal effect identification is a fundamental task in artificial intelligence. A most ideal scenario for causal effect identification is that there is a directed acyclic graph as a prior causal graph encoding the causal relations of all relevant variables. In real tasks, however, the prior causal graph is usually not available, and some relevant variables may be latent as well. With observational data, we can only learn a partial ancestral graph (PAG), which contains some indeterminate causal relations. Since many causal graphs can correspond to one PAG, they are possibly associated with different causal effects. The aim of this paper is to estimate these possible causal effects via covariate adjustment given a PAG. This task is challenging because the number of causal graphs corresponding to a PAG grows super-exponentially with the number of variables. We propose a new graphical characterization for possible adjustment sets, and based on this, we develop the first method to determine the set of possible causal effects that are consistent with the given PAG without enumerating any causal graphs. Our method can output the same set as the enumeration method with super-exponentially less complexity. Experiments validate the effectiveness and tremendous efficiency improvement of the proposed method.', 'Causal Discovery in Linear Structural Causal Models with Deterministic Relations Linear structural causal models (SCMs)– in which each observed variable is generated by a subset of the other observed variables as well as a subset of the exogenous sources– are pervasive in causal inference and casual discovery. However, for the task of causal discovery, existing work almost exclusively focus on the submodel where each observed variable is associated with a distinct source with non-zero variance. This results in the restriction that no observed variable can deterministically depend on other observed variables or latent confounders. In this paper, we extend the results on structure learning by focusing on a subclass of linear SCMs which do not have this property, i.e., models in which observed variables can be causally affected by any subset of the sources, and are allowed to be a deterministic function of other observed variables or latent confounders. This allows for a more realistic modeling of influence or information propagation in systems. We focus on the task of causal discovery form observational data generated from a member of this subclass. We derive a set of necessary and sufficient conditions for unique identifiability of the causal structure. To the best of our knowledge, this is the first work that gives identifiability results for causal discovery under both latent confounding and deterministic relationships. Further, we propose an algorithm for recovering the underlying causal structure when the aforementioned conditions are satisfied. We validate our theoretical results both on synthetic and real datasets.']"
23,182,23_algorithms_learning_python_optimizer,"['algorithms', 'learning', 'python', 'optimizer', 'machine', 'apis', 'automl', 'pythona', 'hyperparameter', 'optimization']","['Automated Machine Learning (AutoML) Libraries for Python <p>AutoML provides tools to automatically discover good machine learning model pipelines for a dataset with very little user intervention. It is ideal for domain experts new to machine learning or machine learning practitioners looking to get good results quickly for a predictive modeling task. Open-source libraries are available for using AutoML methods with popular machine [&#8230;]</p>\n<p>The post <a rel=""nofollow"" href=""https://machinelearningmastery.com/automl-libraries-for-python/"">Automated Machine Learning (AutoML) Libraries for Python</a> appeared first on <a rel=""nofollow"" href=""https://machinelearningmastery.com"">Machine Learning Mastery</a>.</p>\n', 'Ray: A Distributed System for AI <p>As machine learning algorithms and techniques have advanced, more and more machine learning applications require multiple machines and must exploit parallelism.\nHowever, the infrastructure for doing machine learning on clusters remains ad-hoc. While good solutions for specific use cases (e.g., parameter servers or hyperparameter search) and high-quality distributed systems outside of AI do exist (e.g., Hadoop or Spark), practitioners developing algorithms at the frontier often build their own systems infrastructure from scratch. This amounts to a lot of redundant effort.</p>\n\n<p>As an example, take a conceptually simple algorithm like <a href=""https://arxiv.org/abs/1703.03864"">Evolution Strategies for reinforcement learning</a>. The algorithm is about a dozen lines of pseudocode, and its Python implementation doesn’t take much more than that. However, running the algorithm efficiently on a larger machine or cluster requires significantly more software engineering. The authors’ implementation involves thousands of lines of code and must define communication protocols, message serialization and deserialization strategies, and various data handling strategies.</p>\n\n<p>One of <a href=""https://github.com/ray-project/ray"">Ray</a>’s goals is to enable practitioners to turn a prototype algorithm that runs on a laptop into a high-performance distributed application that runs efficiently on a cluster (or on a single multi-core machine) with relatively few additional lines of code. Such a framework should include the performance benefits of a hand-optimized system without requiring the user to reason about scheduling, data transfers, and machine failures.</p>\n\n<!--more-->\n\n<h1 id=""an-open-source-framework-for-ai"">An Open-Source Framework for AI</h1>\n\n<p><strong>Relation to deep learning frameworks:</strong> Ray is fully compatible with deep learning frameworks like TensorFlow, PyTorch, and MXNet, and it is natural to use one or more deep learning frameworks along with Ray in many applications (for example, our reinforcement learning libraries use TensorFlow and PyTorch heavily).</p>\n\n<p><strong>Relation to other distributed systems:</strong> Many popular distributed systems are used today, but most of them were not built with AI applications in mind and lack the required performance for supporting and the APIs for expressing AI applications. The following features are missing (in various combinations) from today’s distributed systems:</p>\n\n<ul>\n  <li>Support for millisecond level tasks and millions of tasks per second</li>\n  <li>Nested parallelism (parallelizing tasks within tasks, e.g., parallel simulations inside of a hyperparameter search) (see the figure below)</li>\n  <li>Arbitrary task dependencies determined dynamically at runtime (e.g., to avoid waiting for slow workers)</li>\n  <li>Tasks operating on shared mutable state (e.g., neural net weights or a simulator)</li>\n  <li>Support for heterogeneous resources (CPUs, GPUs, etc)</li>\n</ul>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/static/blog/ray/task_hier.png"" alt=""nested_parallelism"" width=""600"" /><br />\n<i>\nA simple example of nested parallelism. One application runs two parallel experiments (each of which is a long-running task), and each experiment runs a number of parallel simulations (each of which is also a task).\n</i>\n</p>\n\n<p>There are two main ways of using Ray: through its lower-level APIs and higher-level libraries.\nThe higher-level libraries are built on top of the lower-level APIs.\nCurrently these include <a href=""http://ray.readthedocs.io/en/latest/rllib.html"">Ray RLlib</a>, a scalable reinforcement learning library and <a href=""http://ray.readthedocs.io/en/latest/tune.html"">Ray.tune</a>, an efficient distributed hyperparameter search library.</p>\n\n<h1 id=""ray-lower-level-apis"">Ray Lower-Level APIs</h1>\n<p>The goal of the Ray API is to make it natural to express very general computational patterns and applications without being restricted to fixed patterns like MapReduce.</p>\n\n<h2 id=""dynamic-task-graphs"">Dynamic Task Graphs</h2>\n<p>The underlying primitive in a Ray application or job is a <em>dynamic task graph</em>. This is very different from the computation graph in TensorFlow. Whereas in TensorFlow, a computation graph represents a neural network and is executed many times in a single application, in Ray, the task graph represents the entire application and is only executed a single time. The task graph is not known up front. It is constructed dynamically as an application runs, and the execution of one task may trigger the creation of more tasks.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/static/blog/ray/task_graph.png"" alt=""dynamic_task_graph"" width=""300"" /><br />\n<i>\nAn example computation graph. The white ovals refer to tasks, and the blue boxes refer to objects. Arrows indicate that a task depends on an object or that a task creates an object.\n</i>\n</p>\n\n<p>Arbitrary Python functions can be executed as tasks, and they can depend arbitrarily on the outputs of other tasks.\nThis is illustrated in the example below.</p>\n<div class=""language-python highlighter-rouge""><div class=""highlight""><pre class=""highlight""><code><span class=""c""># Define two remote functions. Invocations of these functions create tasks</span>\n<span class=""c""># that are executed remotely.</span>\n\n<span class=""nd"">@ray.remote</span>\n<span class=""k"">def</span> <span class=""nf"">multiply</span><span class=""p"">(</span><span class=""n"">x</span><span class=""p"">,</span> <span class=""n"">y</span><span class=""p"">):</span>\n    <span class=""k"">return</span> <span class=""n"">np</span><span class=""o"">.</span><span class=""n"">dot</span><span class=""p"">(</span><span class=""n"">x</span><span class=""p"">,</span> <span class=""n"">y</span><span class=""p"">)</span>\n\n<span class=""nd"">@ray.remote</span>\n<span class=""k"">def</span> <span class=""nf"">zeros</span><span class=""p"">(</span><span class=""n"">size</span><span class=""p"">):</span>\n    <span class=""k"">return</span> <span class=""n"">np</span><span class=""o"">.</span><span class=""n"">zeros</span><span class=""p"">(</span><span class=""n"">size</span><span class=""p"">)</span>\n\n<span class=""c""># Start two tasks in parallel. These immediately return futures and the</span>\n<span class=""c""># tasks are executed in the background.</span>\n<span class=""n"">x_id</span> <span class=""o"">=</span> <span class=""n"">zeros</span><span class=""o"">.</span><span class=""n"">remote</span><span class=""p"">((</span><span class=""mi"">100</span><span class=""p"">,</span> <span class=""mi"">100</span><span class=""p"">))</span>\n<span class=""n"">y_id</span> <span class=""o"">=</span> <span class=""n"">zeros</span><span class=""o"">.</span><span class=""n"">remote</span><span class=""p"">((</span><span class=""mi"">100</span><span class=""p"">,</span> <span class=""mi"">100</span><span class=""p"">))</span>\n\n<span class=""c""># Start a third task. This will not be scheduled until the first two</span>\n<span class=""c""># tasks have completed.</span>\n<span class=""n"">z_id</span> <span class=""o"">=</span> <span class=""n"">multiply</span><span class=""o"">.</span><span class=""n"">remote</span><span class=""p"">(</span><span class=""n"">x_id</span><span class=""p"">,</span> <span class=""n"">y_id</span><span class=""p"">)</span>\n\n<span class=""c""># Get the result. This will block until the third task completes.</span>\n<span class=""n"">z</span> <span class=""o"">=</span> <span class=""n"">ray</span><span class=""o"">.</span><span class=""n"">get</span><span class=""p"">(</span><span class=""n"">z_id</span><span class=""p"">)</span>\n</code></pre></div></div>\n\n<h2 id=""actors"">Actors</h2>\n<p>One thing that can’t be done with just the remote functions and tasks described above is to have multiple tasks operating on the same shared mutable state.\nThis comes up in multiple contexts in machine learning where the shared state may be the state of a simulator, the weights of a neural network, or something else entirely.\nRay uses an actor abstraction to encapsulate mutable state shared between multiple tasks.\nHere’s a toy example of how to do this with an Atari simulator.</p>\n\n<div class=""language-python highlighter-rouge""><div class=""highlight""><pre class=""highlight""><code><span class=""kn"">import</span> <span class=""nn"">gym</span>\n\n<span class=""nd"">@ray.remote</span>\n<span class=""k"">class</span> <span class=""nc"">Simulator</span><span class=""p"">(</span><span class=""nb"">object</span><span class=""p"">):</span>\n    <span class=""k"">def</span> <span class=""nf"">__init__</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""p"">):</span>\n        <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">env</span> <span class=""o"">=</span> <span class=""n"">gym</span><span class=""o"">.</span><span class=""n"">make</span><span class=""p"">(</span><span class=""s"">""Pong-v0""</span><span class=""p"">)</span>\n        <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">env</span><span class=""o"">.</span><span class=""n"">reset</span><span class=""p"">()</span>\n\n    <span class=""k"">def</span> <span class=""nf"">step</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""p"">,</span> <span class=""n"">action</span><span class=""p"">):</span>\n        <span class=""k"">return</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">env</span><span class=""o"">.</span><span class=""n"">step</span><span class=""p"">(</span><span class=""n"">action</span><span class=""p"">)</span>\n\n<span class=""c""># Create a simulator, this will start a remote process that will run</span>\n<span class=""c""># all methods for this actor.</span>\n<span class=""n"">simulator</span> <span class=""o"">=</span> <span class=""n"">Simulator</span><span class=""o"">.</span><span class=""n"">remote</span><span class=""p"">()</span>\n\n<span class=""n"">observations</span> <span class=""o"">=</span> <span class=""p"">[]</span>\n<span class=""k"">for</span> <span class=""n"">_</span> <span class=""ow"">in</span> <span class=""nb"">range</span><span class=""p"">(</span><span class=""mi"">4</span><span class=""p"">):</span>\n    <span class=""c""># Take action 0 in the simulator. This call does not block and</span>\n    <span class=""c""># it returns a future.</span>\n    <span class=""n"">observations</span><span class=""o"">.</span><span class=""n"">append</span><span class=""p"">(</span><span class=""n"">simulator</span><span class=""o"">.</span><span class=""n"">step</span><span class=""o"">.</span><span class=""n"">remote</span><span class=""p"">(</span><span class=""mi"">0</span><span class=""p"">))</span>\n</code></pre></div></div>\n\n<p>Though simple, an actor can be used in very flexible ways.\nFor example, and actor can encapsulate a simulator or a neural network policy, and it can be used for distributed training (as with a parameter server) or for policy serving in a live application.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/static/blog/ray/policy_actor.png"" alt=""policy_actor"" width=""200"" style=""max-width:28%;margin-right:20px;"" />\n<img src=""http://bair.berkeley.edu/static/blog/ray/param_actor.png"" alt=""param_actor"" width=""400"" style=""max-width:58%;margin-left:20px;"" /><br />\n<i>\n<b>Left:</b> An actor serving predictions/actions to a number of client processes.\n<b>Right:</b> Multiple parameter server actors performing distributed training with multiple worker processes.\n</i>\n</p>\n\n<h2 id=""parameter-server-example"">Parameter server example</h2>\n<p>A parameter server can be implemented as a Ray actor as follows:</p>\n<div class=""language-python highlighter-rouge""><div class=""highlight""><pre class=""highlight""><code><span class=""nd"">@ray.remote</span>\n<span class=""k"">class</span> <span class=""nc"">ParameterServer</span><span class=""p"">(</span><span class=""nb"">object</span><span class=""p"">):</span>\n    <span class=""k"">def</span> <span class=""nf"">__init__</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""p"">,</span> <span class=""n"">keys</span><span class=""p"">,</span> <span class=""n"">values</span><span class=""p"">):</span>\n        <span class=""c""># These values will be mutated, so we must create a local copy.</span>\n        <span class=""n"">values</span> <span class=""o"">=</span> <span class=""p"">[</span><span class=""n"">value</span><span class=""o"">.</span><span class=""n"">copy</span><span class=""p"">()</span> <span class=""k"">for</span> <span class=""n"">value</span> <span class=""ow"">in</span> <span class=""n"">values</span><span class=""p"">]</span>\n        <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">parameters</span> <span class=""o"">=</span> <span class=""nb"">dict</span><span class=""p"">(</span><span class=""nb"">zip</span><span class=""p"">(</span><span class=""n"">keys</span><span class=""p"">,</span> <span class=""n"">values</span><span class=""p"">))</span>\n\n    <span class=""k"">def</span> <span class=""nf"">get</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""p"">,</span> <span class=""n"">keys</span><span class=""p"">):</span>\n        <span class=""k"">return</span> <span class=""p"">[</span><span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">parameters</span><span class=""p"">[</span><span class=""n"">key</span><span class=""p"">]</span> <span class=""k"">for</span> <span class=""n"">key</span> <span class=""ow"">in</span> <span class=""n"">keys</span><span class=""p"">]</span>\n\n    <span class=""k"">def</span> <span class=""nf"">update</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""p"">,</span> <span class=""n"">keys</span><span class=""p"">,</span> <span class=""n"">values</span><span class=""p"">):</span>\n        <span class=""c""># This update function adds to the existing values, but the update</span>\n        <span class=""c""># function can be defined arbitrarily.</span>\n        <span class=""k"">for</span> <span class=""n"">key</span><span class=""p"">,</span> <span class=""n"">value</span> <span class=""ow"">in</span> <span class=""nb"">zip</span><span class=""p"">(</span><span class=""n"">keys</span><span class=""p"">,</span> <span class=""n"">values</span><span class=""p"">):</span>\n            <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">parameters</span><span class=""p"">[</span><span class=""n"">key</span><span class=""p"">]</span> <span class=""o"">+=</span> <span class=""n"">value</span>\n</code></pre></div></div>\n\n<p>See a <a href=""http://ray.readthedocs.io/en/latest/example-parameter-server.html"">more complete example</a>.</p>\n\n<p>To instantiate the parameter server, do the following.</p>\n<div class=""language-python highlighter-rouge""><div class=""highlight""><pre class=""highlight""><code><span class=""n"">parameter_server</span> <span class=""o"">=</span> <span class=""n"">ParameterServer</span><span class=""o"">.</span><span class=""n"">remote</span><span class=""p"">(</span><span class=""n"">initial_keys</span><span class=""p"">,</span> <span class=""n"">initial_values</span><span class=""p"">)</span>\n</code></pre></div></div>\n<p>To create four long-running workers that continuously retrieve and update the parameters, do the following.</p>\n\n<div class=""language-python highlighter-rouge""><div class=""highlight""><pre class=""highlight""><code><span class=""nd"">@ray.remote</span>\n<span class=""k"">def</span> <span class=""nf"">worker_task</span><span class=""p"">(</span><span class=""n"">parameter_server</span><span class=""p"">):</span>\n    <span class=""k"">while</span> <span class=""bp"">True</span><span class=""p"">:</span>\n        <span class=""n"">keys</span> <span class=""o"">=</span> <span class=""p"">[</span><span class=""s"">\'key1\'</span><span class=""p"">,</span> <span class=""s"">\'key2\'</span><span class=""p"">,</span> <span class=""s"">\'key3\'</span><span class=""p"">]</span>\n        <span class=""c""># Get the latest parameters.</span>\n        <span class=""n"">values</span> <span class=""o"">=</span> <span class=""n"">ray</span><span class=""o"">.</span><span class=""n"">get</span><span class=""p"">(</span><span class=""n"">parameter_server</span><span class=""o"">.</span><span class=""n"">get</span><span class=""o"">.</span><span class=""n"">remote</span><span class=""p"">(</span><span class=""n"">keys</span><span class=""p"">))</span>\n        <span class=""c""># Compute some parameter updates.</span>\n        <span class=""n"">updates</span> <span class=""o"">=</span> <span class=""err"">…</span>\n        <span class=""c""># Update the parameters.</span>\n        <span class=""n"">parameter_server</span><span class=""o"">.</span><span class=""n"">update</span><span class=""o"">.</span><span class=""n"">remote</span><span class=""p"">(</span><span class=""n"">keys</span><span class=""p"">,</span> <span class=""n"">updates</span><span class=""p"">)</span>\n\n<span class=""c""># Start 4 long-running tasks.</span>\n<span class=""k"">for</span> <span class=""n"">_</span> <span class=""ow"">in</span> <span class=""nb"">range</span><span class=""p"">(</span><span class=""mi"">4</span><span class=""p"">):</span>\n    <span class=""n"">worker_task</span><span class=""o"">.</span><span class=""n"">remote</span><span class=""p"">(</span><span class=""n"">parameter_server</span><span class=""p"">)</span>\n</code></pre></div></div>\n\n<h1 id=""ray-high-level-libraries"">Ray High-Level Libraries</h1>\n<p><a href=""http://ray.readthedocs.io/en/latest/rllib.html"">Ray RLlib</a> is a scalable reinforcement learning library built to run on many machines.\nIt can be used through example training scripts as well as through a Python API.\nIt currently includes implementations of:</p>\n<ul>\n  <li>A3C</li>\n  <li>DQN</li>\n  <li>Evolution Strategies</li>\n  <li>PPO</li>\n</ul>\n\n<p>We are working on adding more algorithms.\nRLlib is fully compatible with the <a href=""https://gym.openai.com/docs/"">OpenAI gym</a>.</p>\n\n<p><a href=""http://ray.readthedocs.io/en/latest/tune.html"">Ray.tune</a> is an efficient distributed hyperparameter search library.\nIt provides a Python API for use with deep learning, reinforcement learning, and other compute-intensive tasks.\nHere is a toy example illustrating usage:</p>\n\n<div class=""language-python highlighter-rouge""><div class=""highlight""><pre class=""highlight""><code><span class=""kn"">from</span> <span class=""nn"">ray.tune</span> <span class=""kn"">import</span> <span class=""n"">register_trainable</span><span class=""p"">,</span> <span class=""n"">grid_search</span><span class=""p"">,</span> <span class=""n"">run_experiments</span>\n\n<span class=""c""># The function to optimize. The hyperparameters are in the config</span>\n<span class=""c""># argument.</span>\n<span class=""k"">def</span> <span class=""nf"">my_func</span><span class=""p"">(</span><span class=""n"">config</span><span class=""p"">,</span> <span class=""n"">reporter</span><span class=""p"">):</span>\n    <span class=""kn"">import</span> <span class=""nn"">time</span><span class=""p"">,</span> <span class=""n"">numpy</span> <span class=""k"">as</span> <span class=""n"">np</span>\n    <span class=""n"">i</span> <span class=""o"">=</span> <span class=""mi"">0</span>\n    <span class=""k"">while</span> <span class=""bp"">True</span><span class=""p"">:</span>\n        <span class=""n"">reporter</span><span class=""p"">(</span><span class=""n"">timesteps_total</span><span class=""o"">=</span><span class=""n"">i</span><span class=""p"">,</span> <span class=""n"">mean_accuracy</span><span class=""o"">=</span><span class=""p"">(</span><span class=""n"">i</span> <span class=""o"">**</span> <span class=""n"">config</span><span class=""p"">[</span><span class=""s"">\'alpha\'</span><span class=""p"">]))</span>\n        <span class=""n"">i</span> <span class=""o"">+=</span> <span class=""n"">config</span><span class=""p"">[</span><span class=""s"">\'beta\'</span><span class=""p"">]</span>\n        <span class=""n"">time</span><span class=""o"">.</span><span class=""n"">sleep</span><span class=""p"">(</span><span class=""mf"">0.01</span><span class=""p"">)</span>\n\n<span class=""n"">register_trainable</span><span class=""p"">(</span><span class=""s"">\'my_func\'</span><span class=""p"">,</span> <span class=""n"">my_func</span><span class=""p"">)</span>\n\n<span class=""n"">run_experiments</span><span class=""p"">({</span>\n    <span class=""s"">\'my_experiment\'</span><span class=""p"">:</span> <span class=""p"">{</span>\n        <span class=""s"">\'run\'</span><span class=""p"">:</span> <span class=""s"">\'my_func\'</span><span class=""p"">,</span>\n        <span class=""s"">\'resources\'</span><span class=""p"">:</span> <span class=""p"">{</span><span class=""s"">\'cpu\'</span><span class=""p"">:</span> <span class=""mi"">1</span><span class=""p"">,</span> <span class=""s"">\'gpu\'</span><span class=""p"">:</span> <span class=""mi"">0</span><span class=""p"">},</span>\n        <span class=""s"">\'stop\'</span><span class=""p"">:</span> <span class=""p"">{</span><span class=""s"">\'mean_accuracy\'</span><span class=""p"">:</span> <span class=""mi"">100</span><span class=""p"">},</span>\n        <span class=""s"">\'config\'</span><span class=""p"">:</span> <span class=""p"">{</span>\n            <span class=""s"">\'alpha\'</span><span class=""p"">:</span> <span class=""n"">grid_search</span><span class=""p"">([</span><span class=""mf"">0.2</span><span class=""p"">,</span> <span class=""mf"">0.4</span><span class=""p"">,</span> <span class=""mf"">0.6</span><span class=""p"">]),</span>\n            <span class=""s"">\'beta\'</span><span class=""p"">:</span> <span class=""n"">grid_search</span><span class=""p"">([</span><span class=""mi"">1</span><span class=""p"">,</span> <span class=""mi"">2</span><span class=""p"">]),</span>\n        <span class=""p"">},</span>\n    <span class=""p"">}</span>\n<span class=""p"">})</span>\n</code></pre></div></div>\n\n<p>In-progress results can be visualized live using tools such as Tensorboard and rllab’s VisKit (or you can read the JSON logs directly).\nRay.tune supports grid search, random search, and more sophisticated early stopping algorithms like HyperBand.</p>\n\n<h1 id=""more-information"">More Information</h1>\n\n<p>For more information about Ray, please take a look at the following links.</p>\n<ul>\n  <li><a href=""https://arxiv.org/abs/1712.05889"">Our paper</a></li>\n  <li><a href=""https://github.com/ray-project/ray"">Our codebase</a></li>\n  <li><a href=""http://ray.readthedocs.io/en/latest/index.html"">Our documentation</a></li>\n</ul>\n\n<p>Ray can be installed with <code class=""highlighter-rouge"">pip install ray</code>.\nWe encourage you to try out Ray and see what you think.\nIf you have any feedback for us, please let us know, e.g., through our mailing list <a href=""mailto:ray-dev@googlegroups.com"">ray-dev@googlegroups.com</a>.</p>\n\n', 'Ray: A Distributed System for AI <p>As machine learning algorithms and techniques have advanced, more and more machine learning applications require multiple machines and must exploit parallelism.\nHowever, the infrastructure for doing machine learning on clusters remains ad-hoc. While good solutions for specific use cases (e.g., parameter servers or hyperparameter search) and high-quality distributed systems outside of AI do exist (e.g., Hadoop or Spark), practitioners developing algorithms at the frontier often build their own systems infrastructure from scratch. This amounts to a lot of redundant effort.</p>\n\n<p>As an example, take a conceptually simple algorithm like <a href=""https://arxiv.org/abs/1703.03864"">Evolution Strategies for reinforcement learning</a>. The algorithm is about a dozen lines of pseudocode, and its Python implementation doesn’t take much more than that. However, running the algorithm efficiently on a larger machine or cluster requires significantly more software engineering. The authors’ implementation involves thousands of lines of code and must define communication protocols, message serialization and deserialization strategies, and various data handling strategies.</p>\n\n<p>One of <a href=""https://github.com/ray-project/ray"">Ray</a>’s goals is to enable practitioners to turn a prototype algorithm that runs on a laptop into a high-performance distributed application that runs efficiently on a cluster (or on a single multi-core machine) with relatively few additional lines of code. Such a framework should include the performance benefits of a hand-optimized system without requiring the user to reason about scheduling, data transfers, and machine failures.</p>\n\n<!--more-->\n\n<h1 id=""an-open-source-framework-for-ai"">An Open-Source Framework for AI</h1>\n\n<p><strong>Relation to deep learning frameworks:</strong> Ray is fully compatible with deep learning frameworks like TensorFlow, PyTorch, and MXNet, and it is natural to use one or more deep learning frameworks along with Ray in many applications (for example, our reinforcement learning libraries use TensorFlow and PyTorch heavily).</p>\n\n<p><strong>Relation to other distributed systems:</strong> Many popular distributed systems are used today, but most of them were not built with AI applications in mind and lack the required performance for supporting and the APIs for expressing AI applications. The following features are missing (in various combinations) from today’s distributed systems:</p>\n\n<ul>\n  <li>Support for millisecond level tasks and millions of tasks per second</li>\n  <li>Nested parallelism (parallelizing tasks within tasks, e.g., parallel simulations inside of a hyperparameter search) (see the figure below)</li>\n  <li>Arbitrary task dependencies determined dynamically at runtime (e.g., to avoid waiting for slow workers)</li>\n  <li>Tasks operating on shared mutable state (e.g., neural net weights or a simulator)</li>\n  <li>Support for heterogeneous resources (CPUs, GPUs, etc)</li>\n</ul>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/static/blog/ray/task_hier.png"" alt=""nested_parallelism"" width=""600"" /><br />\n<i>\nA simple example of nested parallelism. One application runs two parallel experiments (each of which is a long-running task), and each experiment runs a number of parallel simulations (each of which is also a task).\n</i>\n</p>\n\n<p>There are two main ways of using Ray: through its lower-level APIs and higher-level libraries.\nThe higher-level libraries are built on top of the lower-level APIs.\nCurrently these include <a href=""http://ray.readthedocs.io/en/latest/rllib.html"">Ray RLlib</a>, a scalable reinforcement learning library and <a href=""http://ray.readthedocs.io/en/latest/tune.html"">Ray.tune</a>, an efficient distributed hyperparameter search library.</p>\n\n<h1 id=""ray-lower-level-apis"">Ray Lower-Level APIs</h1>\n<p>The goal of the Ray API is to make it natural to express very general computational patterns and applications without being restricted to fixed patterns like MapReduce.</p>\n\n<h2 id=""dynamic-task-graphs"">Dynamic Task Graphs</h2>\n<p>The underlying primitive in a Ray application or job is a <em>dynamic task graph</em>. This is very different from the computation graph in TensorFlow. Whereas in TensorFlow, a computation graph represents a neural network and is executed many times in a single application, in Ray, the task graph represents the entire application and is only executed a single time. The task graph is not known up front. It is constructed dynamically as an application runs, and the execution of one task may trigger the creation of more tasks.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/static/blog/ray/task_graph.png"" alt=""dynamic_task_graph"" width=""300"" /><br />\n<i>\nAn example computation graph. The white ovals refer to tasks, and the blue boxes refer to objects. Arrows indicate that a task depends on an object or that a task creates an object.\n</i>\n</p>\n\n<p>Arbitrary Python functions can be executed as tasks, and they can depend arbitrarily on the outputs of other tasks.\nThis is illustrated in the example below.</p>\n<div class=""language-python highlighter-rouge""><div class=""highlight""><pre class=""highlight""><code><span class=""c""># Define two remote functions. Invocations of these functions create tasks</span>\n<span class=""c""># that are executed remotely.</span>\n\n<span class=""nd"">@ray.remote</span>\n<span class=""k"">def</span> <span class=""nf"">multiply</span><span class=""p"">(</span><span class=""n"">x</span><span class=""p"">,</span> <span class=""n"">y</span><span class=""p"">):</span>\n    <span class=""k"">return</span> <span class=""n"">np</span><span class=""o"">.</span><span class=""n"">dot</span><span class=""p"">(</span><span class=""n"">x</span><span class=""p"">,</span> <span class=""n"">y</span><span class=""p"">)</span>\n\n<span class=""nd"">@ray.remote</span>\n<span class=""k"">def</span> <span class=""nf"">zeros</span><span class=""p"">(</span><span class=""n"">size</span><span class=""p"">):</span>\n    <span class=""k"">return</span> <span class=""n"">np</span><span class=""o"">.</span><span class=""n"">zeros</span><span class=""p"">(</span><span class=""n"">size</span><span class=""p"">)</span>\n\n<span class=""c""># Start two tasks in parallel. These immediately return futures and the</span>\n<span class=""c""># tasks are executed in the background.</span>\n<span class=""n"">x_id</span> <span class=""o"">=</span> <span class=""n"">zeros</span><span class=""o"">.</span><span class=""n"">remote</span><span class=""p"">((</span><span class=""mi"">100</span><span class=""p"">,</span> <span class=""mi"">100</span><span class=""p"">))</span>\n<span class=""n"">y_id</span> <span class=""o"">=</span> <span class=""n"">zeros</span><span class=""o"">.</span><span class=""n"">remote</span><span class=""p"">((</span><span class=""mi"">100</span><span class=""p"">,</span> <span class=""mi"">100</span><span class=""p"">))</span>\n\n<span class=""c""># Start a third task. This will not be scheduled until the first two</span>\n<span class=""c""># tasks have completed.</span>\n<span class=""n"">z_id</span> <span class=""o"">=</span> <span class=""n"">multiply</span><span class=""o"">.</span><span class=""n"">remote</span><span class=""p"">(</span><span class=""n"">x_id</span><span class=""p"">,</span> <span class=""n"">y_id</span><span class=""p"">)</span>\n\n<span class=""c""># Get the result. This will block until the third task completes.</span>\n<span class=""n"">z</span> <span class=""o"">=</span> <span class=""n"">ray</span><span class=""o"">.</span><span class=""n"">get</span><span class=""p"">(</span><span class=""n"">z_id</span><span class=""p"">)</span>\n</code></pre></div></div>\n\n<h2 id=""actors"">Actors</h2>\n<p>One thing that can’t be done with just the remote functions and tasks described above is to have multiple tasks operating on the same shared mutable state.\nThis comes up in multiple contexts in machine learning where the shared state may be the state of a simulator, the weights of a neural network, or something else entirely.\nRay uses an actor abstraction to encapsulate mutable state shared between multiple tasks.\nHere’s a toy example of how to do this with an Atari simulator.</p>\n\n<div class=""language-python highlighter-rouge""><div class=""highlight""><pre class=""highlight""><code><span class=""kn"">import</span> <span class=""nn"">gym</span>\n\n<span class=""nd"">@ray.remote</span>\n<span class=""k"">class</span> <span class=""nc"">Simulator</span><span class=""p"">(</span><span class=""nb"">object</span><span class=""p"">):</span>\n    <span class=""k"">def</span> <span class=""nf"">__init__</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""p"">):</span>\n        <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">env</span> <span class=""o"">=</span> <span class=""n"">gym</span><span class=""o"">.</span><span class=""n"">make</span><span class=""p"">(</span><span class=""s"">""Pong-v0""</span><span class=""p"">)</span>\n        <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">env</span><span class=""o"">.</span><span class=""n"">reset</span><span class=""p"">()</span>\n\n    <span class=""k"">def</span> <span class=""nf"">step</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""p"">,</span> <span class=""n"">action</span><span class=""p"">):</span>\n        <span class=""k"">return</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">env</span><span class=""o"">.</span><span class=""n"">step</span><span class=""p"">(</span><span class=""n"">action</span><span class=""p"">)</span>\n\n<span class=""c""># Create a simulator, this will start a remote process that will run</span>\n<span class=""c""># all methods for this actor.</span>\n<span class=""n"">simulator</span> <span class=""o"">=</span> <span class=""n"">Simulator</span><span class=""o"">.</span><span class=""n"">remote</span><span class=""p"">()</span>\n\n<span class=""n"">observations</span> <span class=""o"">=</span> <span class=""p"">[]</span>\n<span class=""k"">for</span> <span class=""n"">_</span> <span class=""ow"">in</span> <span class=""nb"">range</span><span class=""p"">(</span><span class=""mi"">4</span><span class=""p"">):</span>\n    <span class=""c""># Take action 0 in the simulator. This call does not block and</span>\n    <span class=""c""># it returns a future.</span>\n    <span class=""n"">observations</span><span class=""o"">.</span><span class=""n"">append</span><span class=""p"">(</span><span class=""n"">simulator</span><span class=""o"">.</span><span class=""n"">step</span><span class=""o"">.</span><span class=""n"">remote</span><span class=""p"">(</span><span class=""mi"">0</span><span class=""p"">))</span>\n</code></pre></div></div>\n\n<p>Though simple, an actor can be used in very flexible ways.\nFor example, and actor can encapsulate a simulator or a neural network policy, and it can be used for distributed training (as with a parameter server) or for policy serving in a live application.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/static/blog/ray/policy_actor.png"" alt=""policy_actor"" width=""200"" style=""max-width:28%;margin-right:20px;"" />\n<img src=""http://bair.berkeley.edu/static/blog/ray/param_actor.png"" alt=""param_actor"" width=""400"" style=""max-width:58%;margin-left:20px;"" /><br />\n<i>\n<b>Left:</b> An actor serving predictions/actions to a number of client processes.\n<b>Right:</b> Multiple parameter server actors performing distributed training with multiple worker processes.\n</i>\n</p>\n\n<h2 id=""parameter-server-example"">Parameter server example</h2>\n<p>A parameter server can be implemented as a Ray actor as follows:</p>\n<div class=""language-python highlighter-rouge""><div class=""highlight""><pre class=""highlight""><code><span class=""nd"">@ray.remote</span>\n<span class=""k"">class</span> <span class=""nc"">ParameterServer</span><span class=""p"">(</span><span class=""nb"">object</span><span class=""p"">):</span>\n    <span class=""k"">def</span> <span class=""nf"">__init__</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""p"">,</span> <span class=""n"">keys</span><span class=""p"">,</span> <span class=""n"">values</span><span class=""p"">):</span>\n        <span class=""c""># These values will be mutated, so we must create a local copy.</span>\n        <span class=""n"">values</span> <span class=""o"">=</span> <span class=""p"">[</span><span class=""n"">value</span><span class=""o"">.</span><span class=""n"">copy</span><span class=""p"">()</span> <span class=""k"">for</span> <span class=""n"">value</span> <span class=""ow"">in</span> <span class=""n"">values</span><span class=""p"">]</span>\n        <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">parameters</span> <span class=""o"">=</span> <span class=""nb"">dict</span><span class=""p"">(</span><span class=""nb"">zip</span><span class=""p"">(</span><span class=""n"">keys</span><span class=""p"">,</span> <span class=""n"">values</span><span class=""p"">))</span>\n\n    <span class=""k"">def</span> <span class=""nf"">get</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""p"">,</span> <span class=""n"">keys</span><span class=""p"">):</span>\n        <span class=""k"">return</span> <span class=""p"">[</span><span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">parameters</span><span class=""p"">[</span><span class=""n"">key</span><span class=""p"">]</span> <span class=""k"">for</span> <span class=""n"">key</span> <span class=""ow"">in</span> <span class=""n"">keys</span><span class=""p"">]</span>\n\n    <span class=""k"">def</span> <span class=""nf"">update</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""p"">,</span> <span class=""n"">keys</span><span class=""p"">,</span> <span class=""n"">values</span><span class=""p"">):</span>\n        <span class=""c""># This update function adds to the existing values, but the update</span>\n        <span class=""c""># function can be defined arbitrarily.</span>\n        <span class=""k"">for</span> <span class=""n"">key</span><span class=""p"">,</span> <span class=""n"">value</span> <span class=""ow"">in</span> <span class=""nb"">zip</span><span class=""p"">(</span><span class=""n"">keys</span><span class=""p"">,</span> <span class=""n"">values</span><span class=""p"">):</span>\n            <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">parameters</span><span class=""p"">[</span><span class=""n"">key</span><span class=""p"">]</span> <span class=""o"">+=</span> <span class=""n"">value</span>\n</code></pre></div></div>\n\n<p>See a <a href=""http://ray.readthedocs.io/en/latest/example-parameter-server.html"">more complete example</a>.</p>\n\n<p>To instantiate the parameter server, do the following.</p>\n<div class=""language-python highlighter-rouge""><div class=""highlight""><pre class=""highlight""><code><span class=""n"">parameter_server</span> <span class=""o"">=</span> <span class=""n"">ParameterServer</span><span class=""o"">.</span><span class=""n"">remote</span><span class=""p"">(</span><span class=""n"">initial_keys</span><span class=""p"">,</span> <span class=""n"">initial_values</span><span class=""p"">)</span>\n</code></pre></div></div>\n<p>To create four long-running workers that continuously retrieve and update the parameters, do the following.</p>\n\n<div class=""language-python highlighter-rouge""><div class=""highlight""><pre class=""highlight""><code><span class=""nd"">@ray.remote</span>\n<span class=""k"">def</span> <span class=""nf"">worker_task</span><span class=""p"">(</span><span class=""n"">parameter_server</span><span class=""p"">):</span>\n    <span class=""k"">while</span> <span class=""bp"">True</span><span class=""p"">:</span>\n        <span class=""n"">keys</span> <span class=""o"">=</span> <span class=""p"">[</span><span class=""s"">\'key1\'</span><span class=""p"">,</span> <span class=""s"">\'key2\'</span><span class=""p"">,</span> <span class=""s"">\'key3\'</span><span class=""p"">]</span>\n        <span class=""c""># Get the latest parameters.</span>\n        <span class=""n"">values</span> <span class=""o"">=</span> <span class=""n"">ray</span><span class=""o"">.</span><span class=""n"">get</span><span class=""p"">(</span><span class=""n"">parameter_server</span><span class=""o"">.</span><span class=""n"">get</span><span class=""o"">.</span><span class=""n"">remote</span><span class=""p"">(</span><span class=""n"">keys</span><span class=""p"">))</span>\n        <span class=""c""># Compute some parameter updates.</span>\n        <span class=""n"">updates</span> <span class=""o"">=</span> <span class=""err"">…</span>\n        <span class=""c""># Update the parameters.</span>\n        <span class=""n"">parameter_server</span><span class=""o"">.</span><span class=""n"">update</span><span class=""o"">.</span><span class=""n"">remote</span><span class=""p"">(</span><span class=""n"">keys</span><span class=""p"">,</span> <span class=""n"">updates</span><span class=""p"">)</span>\n\n<span class=""c""># Start 4 long-running tasks.</span>\n<span class=""k"">for</span> <span class=""n"">_</span> <span class=""ow"">in</span> <span class=""nb"">range</span><span class=""p"">(</span><span class=""mi"">4</span><span class=""p"">):</span>\n    <span class=""n"">worker_task</span><span class=""o"">.</span><span class=""n"">remote</span><span class=""p"">(</span><span class=""n"">parameter_server</span><span class=""p"">)</span>\n</code></pre></div></div>\n\n<h1 id=""ray-high-level-libraries"">Ray High-Level Libraries</h1>\n<p><a href=""http://ray.readthedocs.io/en/latest/rllib.html"">Ray RLlib</a> is a scalable reinforcement learning library built to run on many machines.\nIt can be used through example training scripts as well as through a Python API.\nIt currently includes implementations of:</p>\n<ul>\n  <li>A3C</li>\n  <li>DQN</li>\n  <li>Evolution Strategies</li>\n  <li>PPO</li>\n</ul>\n\n<p>We are working on adding more algorithms.\nRLlib is fully compatible with the <a href=""https://gym.openai.com/docs/"">OpenAI gym</a>.</p>\n\n<p><a href=""http://ray.readthedocs.io/en/latest/tune.html"">Ray.tune</a> is an efficient distributed hyperparameter search library.\nIt provides a Python API for use with deep learning, reinforcement learning, and other compute-intensive tasks.\nHere is a toy example illustrating usage:</p>\n\n<div class=""language-python highlighter-rouge""><div class=""highlight""><pre class=""highlight""><code><span class=""kn"">from</span> <span class=""nn"">ray.tune</span> <span class=""kn"">import</span> <span class=""n"">register_trainable</span><span class=""p"">,</span> <span class=""n"">grid_search</span><span class=""p"">,</span> <span class=""n"">run_experiments</span>\n\n<span class=""c""># The function to optimize. The hyperparameters are in the config</span>\n<span class=""c""># argument.</span>\n<span class=""k"">def</span> <span class=""nf"">my_func</span><span class=""p"">(</span><span class=""n"">config</span><span class=""p"">,</span> <span class=""n"">reporter</span><span class=""p"">):</span>\n    <span class=""kn"">import</span> <span class=""nn"">time</span><span class=""p"">,</span> <span class=""n"">numpy</span> <span class=""k"">as</span> <span class=""n"">np</span>\n    <span class=""n"">i</span> <span class=""o"">=</span> <span class=""mi"">0</span>\n    <span class=""k"">while</span> <span class=""bp"">True</span><span class=""p"">:</span>\n        <span class=""n"">reporter</span><span class=""p"">(</span><span class=""n"">timesteps_total</span><span class=""o"">=</span><span class=""n"">i</span><span class=""p"">,</span> <span class=""n"">mean_accuracy</span><span class=""o"">=</span><span class=""p"">(</span><span class=""n"">i</span> <span class=""o"">**</span> <span class=""n"">config</span><span class=""p"">[</span><span class=""s"">\'alpha\'</span><span class=""p"">]))</span>\n        <span class=""n"">i</span> <span class=""o"">+=</span> <span class=""n"">config</span><span class=""p"">[</span><span class=""s"">\'beta\'</span><span class=""p"">]</span>\n        <span class=""n"">time</span><span class=""o"">.</span><span class=""n"">sleep</span><span class=""p"">(</span><span class=""mf"">0.01</span><span class=""p"">)</span>\n\n<span class=""n"">register_trainable</span><span class=""p"">(</span><span class=""s"">\'my_func\'</span><span class=""p"">,</span> <span class=""n"">my_func</span><span class=""p"">)</span>\n\n<span class=""n"">run_experiments</span><span class=""p"">({</span>\n    <span class=""s"">\'my_experiment\'</span><span class=""p"">:</span> <span class=""p"">{</span>\n        <span class=""s"">\'run\'</span><span class=""p"">:</span> <span class=""s"">\'my_func\'</span><span class=""p"">,</span>\n        <span class=""s"">\'resources\'</span><span class=""p"">:</span> <span class=""p"">{</span><span class=""s"">\'cpu\'</span><span class=""p"">:</span> <span class=""mi"">1</span><span class=""p"">,</span> <span class=""s"">\'gpu\'</span><span class=""p"">:</span> <span class=""mi"">0</span><span class=""p"">},</span>\n        <span class=""s"">\'stop\'</span><span class=""p"">:</span> <span class=""p"">{</span><span class=""s"">\'mean_accuracy\'</span><span class=""p"">:</span> <span class=""mi"">100</span><span class=""p"">},</span>\n        <span class=""s"">\'config\'</span><span class=""p"">:</span> <span class=""p"">{</span>\n            <span class=""s"">\'alpha\'</span><span class=""p"">:</span> <span class=""n"">grid_search</span><span class=""p"">([</span><span class=""mf"">0.2</span><span class=""p"">,</span> <span class=""mf"">0.4</span><span class=""p"">,</span> <span class=""mf"">0.6</span><span class=""p"">]),</span>\n            <span class=""s"">\'beta\'</span><span class=""p"">:</span> <span class=""n"">grid_search</span><span class=""p"">([</span><span class=""mi"">1</span><span class=""p"">,</span> <span class=""mi"">2</span><span class=""p"">]),</span>\n        <span class=""p"">},</span>\n    <span class=""p"">}</span>\n<span class=""p"">})</span>\n</code></pre></div></div>\n\n<p>In-progress results can be visualized live using tools such as Tensorboard and rllab’s VisKit (or you can read the JSON logs directly).\nRay.tune supports grid search, random search, and more sophisticated early stopping algorithms like HyperBand.</p>\n\n<h1 id=""more-information"">More Information</h1>\n\n<p>For more information about Ray, please take a look at the following links.</p>\n<ul>\n  <li><a href=""https://arxiv.org/abs/1712.05889"">Our paper</a></li>\n  <li><a href=""https://github.com/ray-project/ray"">Our codebase</a></li>\n  <li><a href=""http://ray.readthedocs.io/en/latest/index.html"">Our documentation</a></li>\n</ul>\n\n<p>Ray can be installed with <code class=""highlighter-rouge"">pip install ray</code>.\nWe encourage you to try out Ray and see what you think.\nIf you have any feedback for us, please let us know, e.g., through our mailing list <a href=""mailto:ray-dev@googlegroups.com"">ray-dev@googlegroups.com</a>.</p>\n\n']"
24,33,24_ml4h_health_healths_healthcare,"['ml4h', 'health', 'healths', 'healthcare', 'machinelearning', 'learning', 'biomedical', 'medical', 'mldqa', 'ai']","['Machine Learning for Health (ML4H) 2020: Advancing Healthcare for All ', 'ML4H Auditing: From Paper to Practice Healthcare systems are currently adapting to digital technologies, producing large quantities of novel data. Based on these data, machine-learning algorithms have been developed to support practitioners in labor-intensive workflows such as diagnosis, prognosis, triage or treatment of disease. However, their translation into medical practice is often hampered by a lack of careful evaluation in different settings. Efforts have started worldwide to establish guidelines for evaluating machine learning for health (ML4H) tools, highlighting the necessity to evaluate models for bias, interpretability, robustness, and possible failure modes. However, testing and adopting these guidelines in practice remains an open challenge. In this work, we target the paper-to-practice gap by applying an ML4H audit framework proposed by the ITU/WHO Focus Group on Artificial Intelligence for Health (FG-AI4H) to three use cases: diagnostic prediction of diabetic retinopathy, diagnostic prediction of Alzheimer’s disease, and cytomorphologic classification for leukemia diagnostics. The assessment comprises dimensions such as bias, interpretability, and robustness. Our results highlight the importance of fine-grained and caseadapted quality assessment, provide support for incorporating proposed quality assessment considerations of ML4H during the entire development life cycle, and suggest improvements for future ML4H reference evaluation frameworks.', 'Machine Learning for Health (ML4H) 2022 ']"
25,33,25_sagemaker_aws_amazon_mlflow,"['sagemaker', 'aws', 'amazon', 'mlflow', 'tensorflow', 'ec2', 'instances', 'amis', 'machine', 'personalize']","['Simulate quantum systems on Amazon SageMaker Amazon SageMaker is a fully-managed service that enables developers and data scientists to quickly and easily build, train, and deploy machine learning models at any scale. But besides streamlining the machine learning (ML) workflow, Amazon SageMaker also provides a serverless, powerful, and easy-to-use compute environment to execute and parallelize a large spectrum of scientific computing […]', 'Amazon Pinpoint campaigns driven by machine learning on Amazon SageMaker In this blog post, I want to continue the theme of demonstrating agility, cost efficiency, and how AWS can help you innovate through your customer analytics practice. Many of you are exploring how AI can enhance their customer 360o initiatives. I’ll demonstrate how targeted campaigns can be driven by machine learning (ML) through solutions that leverage Amazon SageMaker and Amazon Pinpoint.', 'Using R with Amazon SageMaker This blog post describes how to train, deploy, and retrieve predictions from a machine learning (ML) model using&nbsp;Amazon SageMaker&nbsp;and R. The model predicts abalone age as measured by the number of rings in the shell. The reticulate package will be used as an R interface to&nbsp;Amazon SageMaker Python SDK to make API calls to Amazon […]']"
26,38,26_privacy_private_privacypreserving_bandits,"['privacy', 'private', 'privacypreserving', 'bandits', 'optimal', 'reinforcement', 'guarantees', 'bandit', 'shuffle', 'privatize']","['Differentially Private No-regret Exploration in Adversarial Markov Decision Processes We study learning adversarial Markov decision process (MDP) in the episodic setting under the constraint of differential privacy (DP). This is motivated by the widespread applications of reinforcement learning (RL) in non-stationary and even adversarial scenarios, where protecting users’ sensitive information is vital. We first propose two efficient frameworks for adversarial MDPs, spanning full-information and bandit settings. Within each framework, we consider both Joint DP (JDP), where a central agent is trusted to protect the sensitive data, and Local DP (LDP), where the information is protected directly on the user side. Then, we design novel privacy mechanisms to privatize the stochastic transition and adversarial losses. By instantiating such privacy mechanisms to satisfy JDP and LDP requirements, we obtain near-optimal regret guarantees for both frameworks. To our knowledge, these are the first algorithms to tackle the challenge of private learning in adversarial MDPs.', 'Improved Regret for Differentially Private Exploration in Linear MDP We study privacy-preserving exploration in sequential decision-making for environments that rely on sensitive data such as medical records. In particular, we focus on solving the problem of reinforcement learning (RL) subject to the constraint of (joint) differential privacy in the linear MDP setting, where both dynamics and rewards are given by linear functions. Prior work on this problem due to (Luyo et al., 2021) achieves a regret rate that has a dependence of O(K^{3/5}) on the number of episodes K. We provide a private algorithm with an improved regret rate with an optimal dependence of O($\\sqrt{}$K) on the number of episodes. The key recipe for our stronger regret guarantee is the adaptivity in the policy update schedule, in which an update only occurs when sufficient changes in the data are detected. As a result, our algorithm benefits from low switching cost and only performs O(log(K)) updates, which greatly reduces the amount of privacy noise. Finally, in the most prevalent privacy regimes where the privacy parameter \\epsilon is a constant, our algorithm incurs negligible privacy cost{—}in comparison with the existing non-private regret bounds, the additional regret due to privacy appears in lower-order terms.', 'Privacy Amplification via Shuffling for Linear Contextual Bandits Contextual bandit algorithms are widely used in domains where it is desirable to provide a personalized service by leveraging contextual information, that may contain sensitive information that needs to be protected. Inspired by this scenario, we study the contextual linear bandit problem with differential privacy (DP) constraints. While the literature has focused on either centralized (joint DP) or local (local DP) privacy, we consider the shuffle model of privacy and we show that it is possible to achieve a privacy/utility trade-off between JDP and LDP. By leveraging shuffling from privacy and batching from bandits, we present an algorithm with regret bound $\\widetilde{\\mathcal{O}}(T^{2/3}/\\varepsilon^{1/3})$, while guaranteeing both central (joint) and local privacy. Our result shows that it is possible to obtain a trade-off between JDP and LDP by leveraging the shuffle model while preserving local privacy.']"
27,151,27_distributed_communicationefficient_sgd_bottleneck,"['distributed', 'communicationefficient', 'sgd', 'bottleneck', 'learning', 'optimization', 'stochastic', 'gradient', 'communications', 'nodes']","['DAve-QN: A Distributed Averaged Quasi-Newton Method with Local Superlinear Convergence Rate In this paper, we consider distributed algorithms for solving the empirical risk minimization problem under the master/worker communication model. We develop a distributed asynchronous quasi-Newton algorithm that can achieve superlinear convergence. To our knowledge, this is the first distributed asynchronous algorithm with superlinear convergence guarantees. Our algorithm is communication-efficient in the sense that at every iteration the master node and workers communicate vectors of size $O(p)$, where $p$ is the dimension of the decision variable.  The proposed method is based on a distributed asynchronous averaging scheme of decision vectors and gradients in a way to effectively capture the local Hessian information of the objective function. Our convergence theory supports asynchronous computations subject to both bounded delays and unbounded delays with a bounded time-average. Unlike in the majority of asynchronous optimization literature, we do not require choosing smaller stepsize when delays are huge. We provide numerical experiments that match our theoretical results and showcase significant improvement comparing to state-of-the-art distributed algorithms.', 'On the Linear Speedup Analysis of Communication Efficient Momentum SGD for Distributed Non-Convex Optimization Recent developments on large-scale distributed machine learning applications, e.g., deep neural networks, benefit enormously from the advances in distributed non-convex optimization techniques, e.g., distributed Stochastic Gradient Descent (SGD). A series of recent works study the linear speedup property of distributed SGD variants with reduced communication. The linear speedup property enables us to scale out the computing capability by adding more computing nodes into our system. The reduced communication complexity is desirable since communication overhead is often the performance bottleneck in distributed systems. Recently, momentum methods are more and more widely adopted by practitioners to train machine learning models since they can often converge faster and generalize better. However, it remains unclear whether any distributed momentum SGD possesses the same linear speedup property as distributed SGD and has reduced communication complexity. This paper fills the gap by considering a distributed communication efficient momentum SGD method and proving its linear speedup property.', ' Communication-Compressed Adaptive Gradient Method for Distributed Nonconvex Optimization   Due to the explosion in the size of the training datasets, distributed learning has received growing interest in recent years. One of the major bottlenecks is the large communication cost between the central server and the local workers. While error feedback compression has been proven to be successful in reducing communication costs with stochastic gradient descent (SGD), there are much fewer attempts in building communication-efficient adaptive gradient methods with provable guarantees, which are widely used in training large-scale machine learning models. In this paper, we propose a new communication-compressed AMSGrad for distributed nonconvex optimization problem, which is provably efficient. Our proposed distributed learning framework features an effective gradient compression strategy and a worker-side model update design. We prove that the proposed communication-efficient distributed adaptive gradient method converges to the first-order stationary point with the same iteration complexity as uncompressed vanilla AMSGrad in the stochastic nonconvex optimization setting. Experiments on various benchmarks back up our theory. ']"
28,18,28_optimization_algorithmic_learningbased_constraints,"['optimization', 'algorithmic', 'learningbased', 'constraints', 'constraint', 'mpnns', 'solvers', 'programming', 'optimal', 'neural']","['CombOptNet: Fit the Right NP-Hard Problem by Learning Integer Programming Constraints Bridging logical and algorithmic reasoning with modern machine learning techniques is a fundamental challenge with potentially transformative impact. On the algorithmic side, many NP-hard problems can be expressed as integer programs, in which the constraints play the role of their ’combinatorial specification’. In this work, we aim to integrate integer programming solvers into neural network architectures as layers capable of learning both the cost terms and the constraints. The resulting end-to-end trainable architectures jointly extract features from raw data and solve a suitable (learned) combinatorial problem with state-of-the-art integer programming solvers. We demonstrate the potential of such layers with an extensive performance analysis on synthetic data and with a demonstration on a competitive computer vision keypoint matching benchmark.', 'Predicting Lagrangian Multipliers for Mixed Integer Linear Programs Lagrangian Relaxation stands among the most efficient approaches for solving Mixed Integer Linear Programs (MILPs) with difficult constraints. Given any duals for these constraints, called Lagrangian Multipliers (LMs), it returns a bound on the optimal value of the MILP, and Lagrangian methods seek the LMs giving the best such bound. But these methods generally rely on iterative algorithms resembling gradient descent to maximize the concave piecewise linear dual function: the computational burden grows quickly with the number of relaxed constraints. We introduce a deep learning approach that bypasses the descent, effectively amortizing per instance optimization. A probabilistic encoder based on a graph neural network computes, given a MILP instance and its Continuous Relaxation (CR) solution, high-dimensional representations of relaxed constraints, which are turned into LMs by a decoder. We train the encoder and the decoder jointly by directly optimizing the bound obtained from the predicted multipliers. Our method is applicable to any problem with a compact MILP formulation, and to any Lagrangian Relaxation providing a tighter bound than CR. Experiments on two widely known problems, Multi-Commodity Network Design and Generalized Assignment, show that our approach closes up to 85% of the gap between the continuous relaxation and the best Lagrangian bound, and provides a high-quality warm-start for descent-based Lagrangian methods.', 'Contrastive Predict-and-Search for Mixed Integer Linear Programs Mixed integer linear programs (MILP) are flexible and powerful tools for modeling and solving many difficult real-world combinatorial optimization problems. In this paper, we propose a novel machine learning (ML)-based framework ConPaS that learns to predict solutions to MILPs with contrastive learning. For training, we collect high-quality solutions as positive samples. We also collect low-quality or infeasible solutions as negative samples using novel optimization-based or sampling approaches. We then learn to make discriminative predictions by contrasting the positive and negative samples. During testing, we predict and fix the assignments for a subset of integer variables and then solve the resulting reduced MILP to find high-quality solutions. Empirically, ConPaS achieves state-of-the-art results compared to other ML-based approaches in terms of the quality of and the speed at which solutions are found.']"
29,418,29_privacy_privacypreserving_private_privately,"['privacy', 'privacypreserving', 'private', 'privately', 'public', 'dataset', 'differentially', 'algorithms', 'data', 'attacks']","['Private Convex Empirical Risk Minimization and High-dimensional Regression We consider \\emphdifferentially private algorithms for convex empirical risk minimization (ERM). Differential privacy (Dwork et al., 2006b) is a recently introduced notion of privacy which guarantees that an algorithm’s output does not depend on the data of any individual in the dataset. This is crucial in fields that handle sensitive data, such as genomics, collaborative filtering, and economics. Our motivation is the design of private algorithms for sparse learning problems, in which one aims to find solutions (e.g., regression parameters) with few non-zero coefficients. To this end: (a) We significantly extend the analysis of the “objective perturbation” algorithm of Chaudhuri et al. (2011) for convex ERM problems. We show that their method can be modified to use less noise (be more accurate), and to apply to problems with hard constraints and non-differentiable regularizers. We also give a tighter, data-dependent analysis of the additional error introduced by their method. A key tool in our analysis is a new nontrivial limit theorem for differential privacy which is of independent interest: if a sequence of differentially private algorithms converges, in a \\emphweak sense, then the limit algorithm is also differentially private. In particular, our methods give the best known algorithms for differentially private linear regression. These methods work in settings where the number of parameters p is less than the number of samples n. (b) We give the first two private algorithms for \\emphsparse regression problems in high-dimensional settings, where p is much larger than n. We analyze their performance for linear regression: under standard assumptions on the data, our algorithms have vanishing empirical risk for n = poly(s, \\log p) when there exists a good regression vector with s nonzero coefficients. Our algorithms demonstrate that randomized algorithms for sparse regression problems can be both stable and accurate - a combination which is impossible for deterministic algorithms.', 'Differentially Private Sum-Product Networks Differentially private ML approaches seek to learn models which may be publicly released while guaranteeing that the input data is kept private. One issue with this construction is that further model releases based on the same training data (e.g. for a new task) incur a further privacy budget cost. Privacy-preserving synthetic data generation is one possible solution to this conundrum. However, models trained on synthetic private data struggle to approach the performance of private, ad-hoc models. In this paper, we present a novel method based on sum-product networks that is able to perform both privacy-preserving classification and privacy-preserving data generation with a single model. To the best of our knowledge, ours is the first approach that provides both discriminative and generative capabilities to differentially private ML. We show that our approach outperforms the state of the art in terms of stability (i.e. number of training runs required for convergence) and utility of the generated data.', 'Label differential privacy and private training data release We study differentially private mechanisms for sharing training data in machine learning settings. Our goal is to enable learning of an accurate predictive model while protecting the privacy of each user’s label. Previous work established privacy guarantees that assumed the features are public and given exogenously, a setting known as label differential privacy. In some scenarios, this can be a strong assumption that removes the interplay between features and labels from the privacy analysis. We relax this approach and instead assume the features are drawn from a distribution that depends on the private labels. We first show that simply adding noise to the label, as in previous work, can lead to an arbitrarily weak privacy guarantee, and also present methods for estimating this privacy loss from data. We then present a new mechanism that replaces some training examples with synthetically generated data, and show that our mechanism has a much better privacy-utility tradeoff if the synthetic data is ‘realistic’, in a certain quantifiable sense. Finally, we empirically validate our theoretical analysis.']"
30,11,30_adversarial_fedredefense_malicious_backdoors,"['adversarial', 'fedredefense', 'malicious', 'backdoors', 'evasion', 'adversaries', 'attacks', 'backdoor', 'defenses', 'federated']","['CRFL: Certifiably Robust Federated Learning against Backdoor Attacks Federated Learning (FL) as a distributed learning paradigm that aggregates information from diverse clients to train a shared global model, has demonstrated great success. However, malicious clients can perform poisoning attacks and model replacement to introduce backdoors into the trained global model. Although there have been intensive studies designing robust aggregation methods and empirical robust federated training protocols against backdoors, existing approaches lack robustness certification. This paper provides the first general framework, Certifiably Robust Federated Learning (CRFL), to train certifiably robust FL models against backdoors. Our method exploits clipping and smoothing on model parameters to control the global model smoothness, which yields a sample-wise robustness certification on backdoors with limited magnitude. Our certification also specifies the relation to federated learning parameters, such as poisoning ratio on instance level, number of attackers, and training iterations. Practically, we conduct comprehensive experiments across a range of federated datasets, and provide the first benchmark for certified robustness against backdoor attacks in federated learning. Our code is publicaly available at https://github.com/AI-secure/CRFL.', 'FedREDefense: Defending against Model Poisoning Attacks for Federated Learning using Model Update Reconstruction Error Federated Learning (FL) faces threats from model poisoning attacks. Existing defenses, typically relying on cross-client/global information to mitigate these attacks, fall short when faced with non-IID data distributions and/or a large number of malicious clients. To address these challenges, we present FedREDefense. Unlike existing methods, it doesn’t hinge on similar distributions across clients or a predominant presence of benign clients. Instead, it assesses the likelihood that a client’s model update is a product of genuine training, solely based on the characteristics of the model update itself. Our key finding is that model updates stemming from genuine training can be approximately reconstructed with some distilled local knowledge, while those from deliberate handcrafted model poisoning attacks cannot. Drawing on this distinction, FedREDefense identifies and filters out malicious clients based on the discrepancies in their model update Reconstruction Errors. Empirical tests on three benchmark datasets confirm that FedREDefense successfully filters model poisoning attacks in FL—even in scenarios with high non-IID degrees and large numbers of malicious clients.', 'Invariant Aggregator for Defending against Federated Backdoor Attacks Federated learning enables training high-utility models across several clients without directly sharing their private data. As a downside, the federated setting makes the model vulnerable to various adversarial attacks in the presence of malicious clients. Despite the theoretical and empirical success in defending against attacks that aim to degrade models’ utility, defense against backdoor attacks that increase model accuracy on backdoor samples exclusively without hurting the utility on other samples remains challenging. To this end, we first analyze the failure modes of existing defenses over a flat loss landscape, which is common for well-designed neural networks such as Resnet (He et al., 2015) but is often overlooked by previous works. Then, we propose an invariant aggregator that redirects the aggregated update to invariant directions that are generally useful via selectively masking out the update elements that favor few and possibly malicious clients. Theoretical results suggest that our approach provably mitigates backdoor attacks and remains effective over flat loss landscapes. Empirical results on three datasets with different modalities and varying numbers of clients further demonstrate that our approach mitigates a broad class of backdoor attacks with a negligible cost on the model utility.']"
31,39,31_privacy_federated_privacypreserving_privacysensitive,"['privacy', 'federated', 'privacypreserving', 'privacysensitive', 'private', 'distributed', 'decentralized', 'trusted', 'sharing', 'secret']","['Active Membership Inference Attack under Local Differential Privacy in Federated Learning Federated learning (FL) was originally regarded as a framework for collaborative learning among clients with data privacy protection through a coordinating server. In this paper, we propose a new active membership inference (AMI) attack carried out by a dishonest server in FL. In AMI attacks, the server crafts and embeds malicious parameters into global models to effectively infer whether a target data sample is included in a client’s private training data or not. By exploiting the correlation among data features through a non-linear decision boundary, AMI attacks with a certified guarantee of success can achieve severely high success rates under rigorous local differential privacy (LDP) protection; thereby exposing clients’ training data to significant privacy risk. Theoretical and experimental results on several benchmark datasets show that adding sufficient privacy-preserving noise to prevent our attack would significantly damage FL’s model utility.', 'Decentralized Federated Learning Algorithm Based on Federated Groups and Secure Multiparty Computation To solve the problem that the centralized federal learning based on privacy protection relies on trusted central servers, has low resistance to malicious attacks, and is prone to privacy leakage, this paper proposes a decentralized federated learning algorithm based on federated groups and secure multiparty computation. By establishing a federated group mechanism based on model relevance, each client has its own federated group, and model parameters are only transmitted among fed- erated group members, members outside the group are unable to access parameter information. Secret owners utilize secret sharing algorithms to split their model parameters into several secret shares, which are then transmitted to federated group members through secure channels. Federated group members then aggregate all transmitted secret shares by weighted averaging, and the secret owner receives the aggregated secret shares passed back from all federated group members, and then uses the secret recovery algorithms to recover secret, and obtains the updated parameter model. In the federated group, while a member becomes a Byzantine node, it is removed from the federated group, and another client is selected to join the group based on model relevance. So, each client participating in federated learning serves as both a data node and a computing node, federated learning eliminates reliance on servers and achieves decentralization. The good privacy performance of the proposed algorithm model is theoretically analyzed, and experiments on the FedML platform demonstrated that the algorithm has stronger resistance to attacks.', ' Federated f-Differential Privacy   Federated learning (FL) is a training paradigm where the clients collaboratively learn models by repeatedly sharing information without compromising much on the privacy of their local sensitive data. In this paper, we introduce \\emph{federated $f$-differential privacy}, a new notion specifically tailored to the federated setting, based on the framework of Gaussian differential privacy. Federated $f$-differential privacy operates on \\emph{record level}: it provides the privacy guarantee on each individual record of one client’s data against adversaries. We then propose a generic private federated learning framework \\fedsync that accommodates a large family of state-of-the-art FL algorithms, which provably achieves {federated $f$-differential privacy}. Finally, we empirically demonstrate the trade-off between privacy guarantee and prediction performance for models trained by \\fedsync in computer vision tasks. ']"
32,19,32_microsoft_researchers_research_researchap,"['microsoft', 'researchers', 'research', 'researchap', 'azure', 'technologies', 'innovation', 'tech', 'linkedin', 'computing']","['Microsoft Research Faculty Summit 2016 includes live streamed sessions with Satya Nadella and Bill Gates <p>By Harold Javid, General Chair, Faculty Summit This year’s invitation-only Microsoft Research Faculty Summit (July 13–15, 2016, in Redmond, WA) is promising to be the most exciting in the event’s 17 years. Next week, more than 400 attendees will have the opportunity to hear Satya Nadella, CEO of Microsoft, on Wednesday, July 13, and Bill [&#8230;]</p>\n<p>The post <a rel=""nofollow"" href=""https://www.microsoft.com/en-us/research/microsoft-research-faculty-summit-2016-includes-live-streamed-sessions-satya-nadella-bill-gates/"">Microsoft Research Faculty Summit 2016 includes live streamed sessions with Satya Nadella and Bill Gates</a> appeared first on <a rel=""nofollow"" href=""https://www.microsoft.com/en-us/research"">Microsoft Research</a>.</p>\n', 'Microsoft Research brings summer school to Russia’s emergent tech hub <p>By John Kaiser, Writer, Microsoft Microsoft Research recently concluded \xa0its eighth annual summer school in Kazan, Russia, challenging students to conceptualize and build new applications from the sensors and devices emerging from the Internet of Things (IoT). “Our summer schools are an opportunity to expose students to the latest technologies in computer science,” said Judith [&#8230;]</p>\n<p>The post <a rel=""nofollow"" href=""https://www.microsoft.com/en-us/research/microsoft-research-brings-summer-school-russias-emergent-tech-hub/"">Microsoft Research brings summer school to Russia’s emergent tech hub</a> appeared first on <a rel=""nofollow"" href=""https://www.microsoft.com/en-us/research"">Microsoft Research</a>.</p>\n', 'Microsoft Research Faculty Summit opens <p>By\xa0Harold Javid,\xa0General Chair, Faculty Summit Microsoft Research’s annual Faculty Summit opened Wednesday with a series of talks about how technological innovation can benefit both business and society. In a fireside chat with Jeannette M. Wing, Microsoft&#8217;s corporate vice president in charge of the company&#8217;s basic research labs, Microsoft CEO Satya Nadella underscored the key role [&#8230;]</p>\n<p>The post <a rel=""nofollow"" href=""https://www.microsoft.com/en-us/research/microsoft-research-faculty-summit-opens/"">Microsoft Research Faculty Summit opens</a> appeared first on <a rel=""nofollow"" href=""https://www.microsoft.com/en-us/research"">Microsoft Research</a>.</p>\n']"
33,159,33_federated_learning_distributed_collaboratively,"['federated', 'learning', 'distributed', 'collaboratively', 'generalization', 'unlearning', 'fedavg', 'personalized', 'datasets', 'client']","['Personalized Federated Learning through Local Memorization Federated learning allows clients to collaboratively learn statistical models while keeping their data local. Federated learning was originally used to train a unique global model to be served to all clients, but this approach might be sub-optimal when clients’ local data distributions are heterogeneous. In order to tackle this limitation, recent personalized federated learning methods train a separate model for each client while still leveraging the knowledge available at other clients. In this work, we exploit the ability of deep neural networks to extract high quality vectorial representations (embeddings) from non-tabular data, e.g., images and text, to propose a personalization mechanism based on local memorization. Personalization is obtained by interpolating a collectively trained global model with a local $k$-nearest neighbors (kNN) model based on the shared representation provided by the global model. We provide generalization bounds for the proposed approach in the case of binary classification, and we show on a suite of federated datasets that this approach achieves significantly higher accuracy and fairness than state-of-the-art methods.', 'Communication-Efficient Federated Learning With Data and Client Heterogeneity Federated Learning (FL) enables large-scale distributed training of machine learning models, while still allowing individual nodes to maintain data locally. However, executing FL at scale comes with inherent practical challenges: 1) heterogeneity of the local node data distributions, 2) heterogeneity of node computational speeds (asynchrony), but also 3) constraints in the amount of communication between the clients and the server. In this work, we present the first variant of the classic federated averaging (FedAvg) algorithm which, at the same time, supports data heterogeneity, partial client asynchrony, and communication compression. Our algorithm comes with a novel, rigorous analysis showing that, in spite of these system relaxations, it can provide similar convergence to FedAvg in interesting parameter regimes. Experimental results in the rigorous LEAF benchmark on setups of up to $300$ nodes show that our algorithm ensures fast convergence for standard federated tasks, improving upon prior quantized and asynchronous approaches.', 'Exploiting Shared Representations for Personalized Federated Learning Deep neural networks have shown the ability to extract universal feature representations from data such as images and text that have been useful for a variety of learning tasks. However, the fruits of representation learning have yet to be fully-realized in federated settings. Although data in federated settings is often non-i.i.d. across clients, the success of centralized deep learning suggests that data often shares a global {\\em feature representation}, while the statistical heterogeneity across clients or tasks is concentrated in the {\\em labels}. Based on this intuition, we propose a novel federated learning framework and algorithm for learning a shared data representation across clients and unique local heads for each client. Our algorithm harnesses the distributed computational power across clients to perform many local-updates with respect to the low-dimensional local parameters for every update of the representation. We prove that this method obtains linear convergence to the ground-truth representation with near-optimal sample complexity in a linear setting, demonstrating that it can efficiently reduce the problem dimension for each client. Further, we provide extensive experimental results demonstrating the improvement of our method over alternative personalized federated learning approaches in heterogeneous settings.']"
34,14,34_byzantinepgd_byzantineresilient_byzantinetolerant_byzantine,"['byzantinepgd', 'byzantineresilient', 'byzantinetolerant', 'byzantine', 'sgd', 'byzantinerobust', 'distributed', 'robustness', 'robust', 'learning']","['Dynamic Byzantine-Robust Learning: Adapting to Switching Byzantine Workers Byzantine-robust learning has emerged as a prominent fault-tolerant distributed machine learning framework. However, most techniques focus on the <em>static</em> setting, wherein the identity of Byzantine workers remains unchanged throughout the learning process. This assumption fails to capture real-world <em>dynamic</em> Byzantine behaviors, which may include intermittent malfunctions or targeted, time-limited attacks. Addressing this limitation, we propose DynaBRO – a new method capable of withstanding any sub-linear number of identity changes across rounds. Specifically, when the number of such changes is $\\mathcal{O}(\\sqrt{T})$ (where $T$ is the total number of training rounds), DynaBRO nearly matches the state-of-the-art asymptotic convergence rate of the static setting. Our method utilizes a multi-level Monte Carlo (MLMC) gradient estimation technique applied at the server to robustly aggregated worker updates. By additionally leveraging an adaptive learning rate, we circumvent the need for prior knowledge of the fraction of Byzantine workers.', 'Asynchronous Byzantine Machine Learning (the case of SGD) Asynchronous distributed machine learning solutions have proven very effective so far, but always assuming perfectly functioning workers. In practice, some of the workers can however exhibit Byzantine behavior, caused by hardware failures, software bugs, corrupt data, or even malicious attacks. We introduce Kardam, the first distributed asynchronous stochastic gradient descent (SGD) algorithm that copes with Byzantine workers. Kardam consists of two complementary components: a filtering and a dampening component. The first is scalar-based and ensures resilience against 1/3 Byzantine workers. Essentially, this filter leverages the Lipschitzness of cost functions and acts as a self-stabilizer against Byzantine workers that would attempt to corrupt the progress of SGD. The dampening component bounds the convergence rate by adjusting to stale information through a generic gradient weighting scheme. We prove that Kardam guarantees almost sure convergence in the presence of asynchrony and Byzantine behavior, and we derive its convergence rate. We evaluate Kardam on the CIFAR100 and EMNIST datasets and measure its overhead with respect to non Byzantine-resilient solutions. We empirically show that Kardam does not introduce additional noise to the learning procedure but does induce a slowdown (the cost of Byzantine resilience) that we both theoretically and empirically show to be less than f/n, where f is the number of Byzantine failures tolerated and n the total number of workers. Interestingly, we also empirically observe that the dampening component is interesting in its own right for it enables to build an SGD algorithm that outperforms alternative staleness-aware asynchronous competitors in environments with honest workers.', 'Fixing by Mixing: A Recipe for Optimal Byzantine ML under Heterogeneity Byzantine machine learning (ML) aims to ensure the resilience of distributed learning algorithms to misbehaving (or Byzantine) machines. Although this problem received significant attention, prior works often assume the data held by the machines to be homogeneous, which is seldom true in practical settings. Data heterogeneity makes Byzantine ML considerably more challenging, since a Byzantine machine can hardly be distinguished from a non-Byzantine outlier. A few solutions have been proposed to tackle this issue, but these provide suboptimal probabilistic guarantees and fare poorly in practice. This paper closes the theoretical gap, achieving optimality and inducing good empirical results. In fact, we show how to automatically adapt existing solutions for (homogeneous) Byzantine ML to the heterogeneous setting through a powerful mechanism, we call nearest neighbor mixing (NNM), which boosts any standard robust distributed gradient descent variant to yield optimal Byzantine resilience under heterogeneity. We obtain similar guarantees (in expectation) by plugging NNM in the distributed stochastic heavy ball method, a practical substitute to distributed gradient descent. We obtain empirical results that significantly outperform state-of-the-art Byzantine ML solutions.']"
35,13,35_randomness_randomization_martingaletheoretic_forecasters,"['randomness', 'randomization', 'martingaletheoretic', 'forecasters', 'forecasts', 'forecasting', 'forecast', 'forecaster', 'random', 'prediction']","['Faster online calibration without randomization: interval forecasts and the power of two choices We study the problem of making calibrated probabilistic forecasts for a binary sequence generated by an adversarial nature. Following the seminal paper of Foster and Vohra (1998), nature is often modeled as an adaptive adversary who sees all activity of the forecaster except the randomization that the forecaster may deploy. A number of papers have proposed randomized forecasting strategies that achieve an $\\epsilon$-calibration error rate of $O(1/\\sqrt{T})$, which we prove is tight in general. On the other hand, it is well known that it is not possible to be calibrated without randomization, or if nature also sees the forecaster’s randomization; in both cases the calibration error could be $\\Omega(1)$. Inspired by the equally seminal works on the power of two choices and imprecise probability theory, we study a small variant of the standard online calibration problem. The adversary gives the forecaster the option of making two nearby probabilistic forecasts, or equivalently an interval forecast of small width, and the endpoint closest to the revealed outcome is used to judge calibration. This power of two choices, or imprecise forecast, accords the forecaster with significant power—we show that a faster $\\epsilon$-calibration rate of $O(1/T)$ can be achieved even without deploying any randomization.', 'Computable Randomness is Inherently Imprecise We use the martingale-theoretic approach of game-theoretic probability to incorporate imprecision into the study of randomness. In particular, we define a notion of computable randomness associated with interval, rather than precise, forecasting systems, and study its properties. The richer mathematical structure that thus arises lets us better understand and place existing results for the precise limit. When we focus on constant interval forecasts, we find that every infinite sequence of zeroes and ones has an associated filter of intervals with respect to which it is computably random. It may happen that none of these intervals is precise, which justifies the title of this paper. We illustrate this by showing that computable randomness associated with non-stationary precise forecasting systems can be captured by a stationary interval forecast, which must then be less precise: a gain in model simplicity is thus paid for by a loss in precision.', 'Randomness and Imprecision: A Discussion of Recent Results We discuss our recent work on incorporating imprecision in the field of algorithmic randomness, based on the martingale-theoretic approach of game-theoretic probability. We consider several notions of randomness associated with interval, rather than precise, forecasting systems. We study their properties and argue that there are quite a number of reasons for wanting to do so. First, the richer mathematical structure in this generalisation provides a useful backdrop for a better understanding of precise randomness. Second, randomness associated with non-stationary precise forecasting systems can be captured by a constant but less precise interval forecast: greater model simplicity requires more imprecision. Third, imprecise randomness can’t always be explained away as a result of (over)simplification: there are sequences that are random for a constant interval forecast, but never random for any computable (more) precise forecasting system. Incorporating imprecision into randomness therefore allows us to do more than was hitherto possible. Finally, the random sequences for a non-vacuous interval forecast constitute a meagre set, as they do for precise forecasts: imprecise and precise random sequences are equally rare from a topological point of view, and are, in that sense, equally interesting.']"
36,13,36_markovian_markov_markovianity_stochastic,"['markovian', 'markov', 'markovianity', 'stochastic', 'discretetime', 'ergodicity', 'expectation', 'ergodic', 'inferences', 'expectations']","['Hitting times for continuous-time imprecise-Markov chains We study the problem of characterizing the expected hitting times for a robust generalization of continuous-time Markov chains. This generalization is based on the theory of imprecise probabilities, and the models with which we work essentially constitute sets of stochastic processes. Their inferences are tight lower- and upper bounds with respect to variation within these sets.  We consider three distinct types of these models, corresponding to different levels of generality and structural independence assumptions on the constituent processes.  Our main results are twofold; first, we demonstrate that the hitting times for all three types are equivalent. Moreover, we show that these inferences are described by a straightforward generalization of a well-known linear system of equations that characterizes expected hitting times for traditional time-homogeneous continuous-time Markov chains.', 'Efficient Computation of Updated Lower Expectations for Imprecise Continuous-Time Hidden Markov Chains We consider the problem of performing inference with imprecise continuous-time hidden Markov chains, that is, imprecise continuous-time Markov chains that are augmented with random output variables whose distribution depends on the hidden state of the chain. The prefix ‘imprecise’ refers to the fact that we do not consider a classical continuous-time Markov chain, but replace it with a robust extension that allows us to represent various types of model uncertainty, using the theory of imprecise probabilities. The inference problem amounts to computing lower expectations of functions on the state-space of the chain, given observations of the output variables. We develop and investigate this problem with very few assumptions on the output variables in particular, they can be chosen to be either discrete or continuous random variables. Our main result is a polynomial runtime algorithm to compute the lower expectation of functions on the state-space at any given time-point, given a collection of observations of the output variables.', 'Hitting Times and Probabilities for Imprecise Markov Chains We consider the problem of characterising expected hitting times and hitting probabilities for imprecise Markov chains. To this end, we consider three distinct ways in which imprecise Markov chains have been defined in the literature: as sets of homogeneous Markov chains, as sets of more general stochastic processes, and as game-theoretic probability models. Our first contribution is that all these different types of imprecise Markov chains have the same lower and upper expected hitting times, and similarly the hitting probabilities are the same for these three types. Moreover, we provide a characterisation of these quantities that directly generalises a similar characterisation for precise, homogeneous Markov chains.']"
37,87,37_probabilistic_probabilities_possibilistic_inferential,"['probabilistic', 'probabilities', 'possibilistic', 'inferential', 'epistemic', 'uncertainty', 'imprecisely', 'imprecise', 'belief', 'models']","['Empirical Interpretation of Imprecise Probabilities This paper investigates the possibility of a frequentist interpretation of imprecise probabilities, by generalizing the approach of Bernoulli’s Ars Conjectandi. That is, by studying, in the case of games of chance, under which assumptions imprecise probabilities can be satisfactorily estimated from data. In fact, estimability on the basis of finite amounts of data is a necessary condition for imprecise probabilities in order to have a clear empirical meaning. Unfortunately, imprecise probabilities can be estimated arbitrarily well from data only in very limited settings.', 'Aggregating Belief Models This paper has two goals. The first goal is to say something about how one might combine different agents’ imprecise probabilities to generate an aggregate imprecise probability. The second goal is to champion the very general theory of “belief models” (de Cooman “Belief models: an order theoretic investigation” Annals of Mathematics and AI 2005) which, I think, deserves more attention. The belief models framework is interesting partly because many other formal models of reasoning appear as special cases of belief models (for example, propositional logic, ranking functions, imprecise probability).', 'A Study of the Pari-Mutuel Model from the Point of View of Imprecise Probabilities The Pari-Mutuel model is a distortion model that has its origin in horse racing. Since then, it has been applied in many different fields, such as finance or risk analysis. In this paper we investigate the properties of the Pari-Mutuel model within the framework of Imprecise Probabilities. Since a Pari-Mutuel model induces (2-monotone) coherent lower and upper probabilities, we investigate its connections with other relevant models within this theory, such as probability intervals and belief functions. We also determine the number of extreme points of the credal set induced by the Pari-Mutuel model and study how to combine the information given by multiple Pari-Mutuel models.']"
38,11,38_learning_boosting_fields_prediction,"['learning', 'boosting', 'fields', 'prediction', 'models', 'field', 'estimationbased', 'inferences', 'svm', 'structured']","['Parameter Learning and Convergent Inference for Dense Random Fields Dense random fields are models in which all pairs of variables are directly connected by pairwise potentials. It has recently been shown that mean field inference in dense random fields can be performed efficiently and that these models enable significant accuracy gains in computer vision applications. However, parameter estimation for dense random fields is still poorly understood. In this paper, we present an efficient algorithm for learning parameters in dense random fields. All parameters are estimated jointly, thus capturing dependencies between them. We show that gradients of a variety of loss functions over the mean field marginals can be computed efficiently. The resulting algorithm learns parameters that directly optimize the performance of mean field inference in the model.  As a supporting result, we present an efficient inference algorithm for dense random fields that is guaranteed to converge.', 'Hidden-Unit Conditional Random Fields The paper explores a generalization of conditional random fields (CRFs) in which binary stochastic hidden units appear between the data and the labels. Hidden-unit CRFs are potentially more powerful than standard CRFs because they can represent nonlinear dependencies at each frame. The hidden units in these models also learn to discover latent distributed structure in the data that improves classification. We derive efficient algorithms for inference and learning in these models by observing that the hidden units are conditionally independent given the data and the labels. Finally, we show that hidden-unit CRFs perform well in experiments on a range of tasks, including optical character recognition, text classification, protein structure prediction, and part-of-speech tagging.', 'Efficient Second-Order Gradient Boosting for Conditional Random Fields Conditional random fields (CRFs) are an important class of models for accurate structured prediction, but effective design of the feature functions is a major challenge when applying CRF models to real world data. Gradient boosting, which is used to automatically induce and select feature functions, is a natural candidate solution to the problem.  However, it is non-trivial to derive gradient boosting algorithms for CRFs due to the dense Hessian matrices introduced by variable dependencies. Existing approaches thus use only first-order information when optimizing likelihood, and hence face convergence issues. We incorporate second-order information by deriving a Markov Chain mixing rate bound to quantify the dependencies, and introduce a gradient boosting algorithm that iteratively optimizes an adaptive upper bound of the objective function.  The resulting algorithm induces and selects features for CRFs via functional space optimization, with provable convergence guarantees.  Experimental results on three real world datasets demonstrate that the mixing rate based upper bound is effective for learning CRFs with non-linear potentials.']"
39,26,39_reinforcement_policies_learning_policy,"['reinforcement', 'policies', 'learning', 'policy', 'healthcare', 'plan', 'outcomes', 'sequential', 'regimens', 'representations']","['An Empirical Study of Representation Learning for Reinforcement Learning in Healthcare Reinforcement Learning (RL) has recently been applied to sequential estimation and prediction problems identifying and developing hypothetical treatment strategies for septic patients, with a particular focus on offline learning with observational data. In practice, successful RL relies on informative latent states derived from sequential observations to develop optimal treatment strategies. To date, how best to construct such states in a healthcare setting is an open question. In this paper, we perform an empirical study of several information encoding architectures using data from septic patients in the MIMIC-III dataset to form representations of a patient state. We evaluate the impact of representation dimension, correlations with established acuity scores, and the treatment policies derived from them. We find that sequentially formed state representations facilitate effective policy learning in batch settings, validating a more thoughtful approach to representation learning that remains faithful to the sequential and partial nature of healthcare data.', 'How Should We Represent History in Interpretable Models of Clinical Policies? Modeling policies for sequential clinical decision-making based on observational data is useful for describing treatment practices, standardizing frequent patterns in treatment, and evaluating alternative policies. For each task, it is essential that the policy model is interpretable. Learning accurate models requires effectively capturing a patient’s state, either through sequence representation learning or carefully crafted summaries of their medical history. While recent work has favored the former, it remains a question as to how histories should best be represented for interpretable policy modeling. Focused on model fit, we systematically compare diverse approaches to summarizing patient history for interpretable modeling of clinical policies across four sequential decision-making tasks. We illustrate differences in the policies learned using various representations by breaking down evaluations by patient subgroups, critical states, and stages of treatment, highlighting challenges specific to common use cases. We find that interpretable sequence models using learned representations perform on par with black-box models across all tasks. Interpretable models using hand-crafted representations perform substantially worse when ignoring history entirely, but are made competitive by incorporating only a few aggregated and recent elements of patient history. The added benefits of using a richer representation are pronounced for subgroups and in specific use cases. This underscores the importance of evaluating policy models in the context of their intended use.', 'Continuous State-Space Models for Optimal Sepsis Treatment: a Deep Reinforcement Learning Approach Sepsis is a leading cause of mortality in intensive care units (ICUs) and costs hospitals billions annually. Treating a septic patient is highly challenging, because individual patients respond very differently to medical interventions and there is no universally agreed-upon treatment for sepsis. Understanding more about a patient’s physiological state at a given time could hold the key to effective treatment policies. In this work, we propose a new approach to deduce optimal treatment policies for septic patients by using continuous state-space models and deep reinforcement learning. Learning treatment policies over continuous state-spaces is important, because doing so allows us to retain more of the patient’s physiological information. Our model is able to learn clinically interpretable treatment policies, similar in important aspects to the treatment policies of physicians. Evaluating our algorithm on past ICU patient data, we find that our model could reduce absolute patient mortality in the hospital by up to 3.6% over observed clinical policies. The learned treatment policies could be used to aid intensive care clinicians in medical decision making and improve the likelihood of patient survival.']"
40,108,40_submodularity_submodular_maximizing_maximization,"['submodularity', 'submodular', 'maximizing', 'maximization', 'optimization', 'optimal', 'minimization', 'algorithms', 'supermodular', 'knapsack']","['Streaming Submodular Maximization under a k-Set System Constraint In this paper, we propose a novel framework that converts streaming algorithms for monotone submodular maximization into streaming algorithms for non-monotone submodular maximization. This reduction readily leads to the currently tightest deterministic approximation ratio for submodular maximization subject to a $k$-matchoid constraint. Moreover, we propose the first streaming algorithm for monotone submodular maximization subject to $k$-extendible and $k$-set system constraints. Together with our proposed reduction, we obtain $O(k\\log k)$ and $O(k^2\\log k)$ approximation ratio for submodular maximization subject to the above constraints, respectively. We extensively evaluate the empirical performance of our algorithm against the existing work in a series of experiments including finding the maximum independent set in randomly generated graphs, maximizing linear functions over social networks, movie recommendation, Yelp location summarization, and Twitter data summarization.', 'Instance Specific Approximations for Submodular Maximization The predominant measure for the performance of an algorithm is its worst-case approximation guarantee. While worst-case approximations give desirable robustness guarantees, they can differ significantly from the performance of an algorithm in practice. For the problem of monotone submodular maximization under a cardinality constraint, the greedy algorithm is known to obtain a 1-1/e approximation guarantee, which is optimal for a polynomial-time algorithm. However, very little is known about the approximation achieved by greedy and other submodular maximization algorithms on real instances. We develop an algorithm that gives an instance-specific approximation for any solution of an instance of monotone submodular maximization under a cardinality constraint. This algorithm uses a novel dual approach to submodular maximization. In particular, it relies on the construction of a lower bound to the dual objective that can also be exactly minimized. We use this algorithm to show that on a wide variety of real-world datasets and objectives, greedy and other algorithms find solutions that approximate the optimal solution significantly better than the 1-1/e \xa0 0.63 worst-case approximation guarantee, often exceeding 0.9.', 'Maximization of Monotone $k$-Submodular Functions with Bounded Curvature and Non-$k$-Submodular Functions The concept of $k$-submodularity is an extension of submodularity, of which maximization has various applications, such as influence maximization and sensor placement. In such situations, to model complicated real problems, we want to deal with multiple factors, such as, more detailed parameter representing a property of a given function or a constraint which should be imposed for a given function, simultaneously. Besides, it is preferable that an algorithm for the modeling problem is simple. In this paper, for both monotone $k$-submodular function maximization with bounded curvature and monotone weakly $k$-submodular function maximization, we give approximation ratio analysis on greedy-type algorithms on the problem with the matroid constraint and that with the individual size constraint. Furthermore, we give an approximation ratio analysis on another type of the relaxation of $k$-submodular functions, approximately $k$-submodular functions, with the matroid constraint.']"
41,96,41_recommender_factorization_recommendation_recommendations,"['recommender', 'factorization', 'recommendation', 'recommendations', 'embeddings', 'personalized', 'embedding', 'ratings', 'collaborative', 'predicting']","['Multi-output Gaussian Processes for uncertainty-aware recommender systems Recommender systems are often designed based on a collaborative filtering approach, where user preferences are predicted by modelling interactions between users and items. Many common approaches to solve the collaborative filtering task are based on learning representations of users and items, including simple matrix factorization, Gaussian process latent variable models, and neural-network based embeddings. While matrix factorization approaches fail to model nonlinear relations, neural networks can potentially capture such complex relations with unprecedented predictive power and are highly scalable. However, neither of them is able to model predictive uncertainties. In contrast, Gaussian Process based models can generate a predictive distribution, but cannot scale to large amounts of data. In this manuscript, we propose a novel approach combining the representation learning paradigm of collaborative filtering with multi-output Gaussian processes in a joint framework to generate uncertainty-aware recommendations. We introduce an efficient strategy for model training and inference, resulting in a model that scales to very large and sparse datasets and achieves competitive performance in terms of classical metrics quantifying the reconstruction error. In addition to accurately predicting user preferences, our model also provides meaningful uncertainty estimates about that prediction.', 'Fast Collaborative Filtering from Implicit Feedback with Provable Guarantees Building recommendation algorithm is one of the most challenging tasks in Machine Learning. Although most of the recommendation systems are built on explicit feedback available from the users in terms of rating or text, a majority of the applications do not receive such feedback. Here we consider the recommendation task where the only available data is the records of user-item interaction over web applications over time, in terms of subscription or purchase of items; this is known as implicit feedback recommendation. There is usually a massive amount of such user-item interaction available for any web applications. Algorithms like PLSI or Matrix Factorization runs several iterations through the dataset and may prove very expensive for large datasets. Here we propose a recommendation algorithm based on Method of Moment, which involves factorization of second and third order moments of the dataset. Our algorithm can be proven to be globally convergent using PAC learning theory. Further, we show how to extract the parameters using only three passes through the entire dataset. This results in a highly scalable algorithm that scales up to million of users even on a machine with a single-core processor and 8 GB RAM and produces competitive performance in comparison with existing algorithms.', 'Towards Open-World Recommendation: An Inductive Model-based Collaborative Filtering Approach Recommendation models can effectively estimate underlying user interests and predict one’s future behaviors by factorizing an observed user-item rating matrix into products of two sets of latent factors. However, the user-specific embedding factors can only be learned in a transductive way, making it difficult to handle new users on-the-fly. In this paper, we propose an inductive collaborative filtering framework that contains two representation models. The first model follows conventional matrix factorization which factorizes a group of key users’ rating matrix to obtain meta latents. The second model resorts to attention-based structure learning that estimates hidden relations from query to key users and learns to leverage meta latents to inductively compute embeddings for query users via neural message passing. Our model enables inductive representation learning for users and meanwhile guarantees equivalent representation capacity as matrix factorization. Experiments demonstrate that our model achieves promising results for recommendation on few-shot users with limited training ratings and new unseen users which are commonly encountered in open-world recommender systems.']"
42,44,42_spiking_neuron_neurons_neural,"['spiking', 'neuron', 'neurons', 'neural', 'neuronal', 'neuromorphic', 'snn', 'spike', 'synaptic', 'spikelm']","['Training a General Spiking Neural Network with Improved Efficiency and Minimum Latency Spiking Neural Networks (SNNs) that operate in an event-driven manner and employ binary spike representation have recently emerged as promising candidates for energy-efficient computing. However, a cost bottleneck arises in obtaining high-performance SNNs: training a SNN model requires a large number of time steps in addition to the usual learning iterations, hence this limits their energy efficiency. This paper proposes a general training framework that enhances feature learning and activation efficiency within a limited time step, providing a new solution for more energy-efficient SNNs.  Our framework allows SNN neurons to learn robust spike feature from different receptive fields and update neuron states by utilizing both current stimuli and recurrence information transmitted from other neurons. This setting continuously complements information within a single time step. Additionally, we propose a projection function to merge these two stimuli to smoothly optimize neuron weights (spike firing threshold and activation). We evaluate the proposal for both convolution and recurrent models. Our experimental results indicate state-of-the-art visual classification tasks, including CIFAR10, CIFAR100, and TinyImageNet. Our framework achieves 72.41% and 72.31% top-1 accuracy with only 1 time step on CIFAR100 for CNNs and RNNs, respectively. Our method reduces 10X and 3X joule energy than a standard ANN and SNN, respectively, on CIFAR10, without additional time steps.', 'Temporal Spiking Neural Networks with Synaptic Delay for Graph Reasoning Spiking neural networks (SNNs) are investigated as biologically inspired models of neural computation, distinguished by their computational capability and energy efficiency due to precise spiking times and sparse spikes with event-driven computation. A significant question is how SNNs can emulate human-like graph-based reasoning of concepts and relations, especially leveraging the temporal domain optimally. This paper reveals that SNNs, when amalgamated with synaptic delay and temporal coding, are proficient in executing (knowledge) graph reasoning. It is elucidated that spiking time can function as an additional dimension to encode relation properties via a neural-generalized path formulation. Empirical results highlight the efficacy of temporal delay in relation processing and showcase exemplary performance in diverse graph reasoning tasks. The spiking model is theoretically estimated to achieve $20\\times$ energy savings compared to non-spiking counterparts, deepening insights into the capabilities and potential of biologically inspired SNNs for efficient reasoning. The code is available at https://github.com/pkuxmq/GRSNN.', 'Efficient and Effective Time-Series Forecasting with Spiking Neural Networks Spiking neural networks (SNNs), inspired by the spiking behavior of biological neurons, provide a unique pathway for capturing the intricacies of temporal data. However, applying SNNs to time-series forecasting is challenging due to difficulties in effective temporal alignment, complexities in encoding processes, and the absence of standardized guidelines for model selection. In this paper, we propose a framework for SNNs in time-series forecasting tasks, leveraging the efficiency of spiking neurons in processing temporal information. Through a series of experiments, we demonstrate that our proposed SNN-based approaches achieve comparable or superior results to traditional time-series forecasting methods on diverse benchmarks with much less energy consumption. Furthermore, we conduct detailed analysis experiments to assess the SNN’s capacity to capture temporal dependencies within time-series data, offering valuable insights into its nuanced strengths and effectiveness in modeling the intricate dynamics of temporal data. Our study contributes to the expanding field of SNNs and offers a promising alternative for time-series forecasting tasks, presenting a pathway for the development of more biologically inspired and temporally aware forecasting models. Our code is available at https://github.com/microsoft/SeqSNN.']"
43,192,43_forgetting_continual_memorization_learning,"['forgetting', 'continual', 'memorization', 'learning', 'neural', 'memory', 'learned', 'forget', 'memories', 'continually']","['Learn to Grow: A Continual Structure Learning Framework for Overcoming Catastrophic Forgetting Addressing catastrophic forgetting is one of the key challenges in continual learning where machine learning systems are trained with sequential or streaming tasks. Despite recent remarkable progress in state-of-the-art deep learning, deep neural networks (DNNs) are still plagued with the catastrophic forgetting problem. This paper presents a conceptually simple yet general and effective framework for handling catastrophic forgetting in continual learning with DNNs. The proposed method consists of two components: a neural structure optimization component and a parameter learning and/or fine-tuning component. By separating the explicit neural structure learning and the parameter estimation, not only is the proposed method capable of evolving neural structures in an intuitively meaningful way, but also shows strong capabilities of alleviating catastrophic forgetting in experiments. Furthermore, the proposed method outperforms all other baselines on the permuted MNIST dataset, the split CIFAR100 dataset and the Visual Domain Decathlon dataset in continual learning setting.', 'Wide Neural Networks Forget Less Catastrophically A primary focus area in continual learning research is alleviating the ""catastrophic forgetting"" problem in neural networks by designing new algorithms that are more robust to the distribution shifts. While the recent progress in continual learning literature is encouraging, our understanding of what properties of neural networks contribute to catastrophic forgetting is still limited. To address this, instead of focusing on continual learning algorithms, in this work, we focus on the model itself and study the impact of ""width"" of the neural network architecture on catastrophic forgetting, and show that width has a surprisingly significant effect on forgetting. To explain this effect, we study the learning dynamics of the network from various perspectives such as gradient orthogonality, sparsity, and lazy training regime. We provide potential explanations that are consistent with the empirical results across different architectures and continual learning benchmarks.', 'On the Convergence of Continual Learning with Adaptive Methods One of the objectives of continual learning is to prevent catastrophic forgetting in learning multiple tasks sequentially, and the existing solutions have been driven by the conceptualization of the plasticity-stability dilemma. However, the convergence of continual learning for each sequential task is less studied so far. In this paper, we provide a convergence analysis of memory-based continual learning with stochastic gradient descent and empirical evidence that training current tasks causes the cumulative degradation of previous tasks. We propose an adaptive method for nonconvex continual learning (NCCL), which adjusts step sizes of both previous and current tasks with the gradients. The proposed method can achieve the same convergence rate as the SGD method when the catastrophic forgetting term which we define in the paper is suppressed at each iteration. Further, we demonstrate that the proposed algorithm improves the performance of continual learning over existing methods for several image classification tasks.']"
44,16,44_batchrank_bandits_bandit_ranking,"['batchrank', 'bandits', 'bandit', 'ranking', 'clicks', 'rankings', 'relevancedriven', 'rank', 'retrieval', 'ranked']","['UniRank: Unimodal Bandit Algorithms for Online Ranking We tackle, in the multiple-play bandit setting, the online ranking problem of assigning L items to K predefined positions on a web page in order to maximize the number of user clicks. We propose a generic algorithm, UniRank, that tackles state-of-the-art click models. The regret bound of this algorithm is a direct consequence of the pseudo-unimodality property of the bandit setting with respect to a graph where nodes are ordered sets of indistinguishable items. The main contribution of UniRank is its O(L/$\\Delta$ logT) regret for T consecutive assignments, where $\\Delta$ relates to the reward-gap between two items. This regret bound is based on the usually implicit condition that two items may not have the same attractiveness. Experiments against state-of-the-art learning algorithms specialized or not for different click models, show that our method has better regret performance than other generic algorithms on real life and synthetic datasets.', 'Cascading Linear Submodular Bandits: Accounting for Position Bias and Diversity in Online Learning to Rank Online learning, position bias, and diversified retrieval are three crucial aspects in designing ranking systems based on user clicks. One simple click model which explains the position bias is the cascade model. Many online learning variants of the cascade model have been proposed, but none so far captures diversity. Similarly, online learning to rank methods which capture diversity do not handle position bias. Motivated by these limitations, we propose a novel click model, and the associated online learning variant to address both position bias and diversified retrieval in a unified framework. Despite the objective function not being a standard submodular set function, we prove that a greedy-approach is a reasonable approximation. We then propose, CascadeLSB, an algorithm whose goal is to recommend K most attractive items from a large set of L items with the attractiveness of items being modeled as a submodular utility function. We analyze the algorithm and prove a gap-free upper bound on its n-step regret. We comprehensively evaluate CascadeLSB on synthetic and real datasets, where it  outperforms all the baselines.', 'DCM Bandits: Learning to Rank with Multiple Clicks A search engine recommends to the user a list of web pages. The user examines this list, from the first page to the last, and clicks on all attractive pages until the user is satisfied. This behavior of the user can be described by the dependent click model (DCM). We propose DCM bandits, an online learning variant of the DCM where the goal is to maximize the probability of recommending satisfactory items, such as web pages. The main challenge of our learning problem is that we do not observe which attractive item is satisfactory. We propose a computationally-efficient learning algorithm for solving our problem, dcmKL-UCB; derive gap-dependent upper bounds on its regret under reasonable assumptions; and also prove a matching lower bound up to logarithmic factors. We evaluate our algorithm on synthetic and real-world problems, and show that it performs well even when our model is misspecified. This work presents the first practical and regret-optimal online algorithm for learning to rank with multiple clicks in a cascade-like click model.']"
45,96,45_ranking_rankings_ranked_ranks,"['ranking', 'rankings', 'ranked', 'ranks', 'rank', 'sorting', 'comparisons', 'aggregation', 'retrieval', 'algorithms']","['A Statistical Convergence Perspective of Algorithms for Rank Aggregation from Pairwise Data There has been much interest recently in the problem of rank aggregation from pairwise data. A natural question that arises is: under what sorts of statistical assumptions do various rank aggregation algorithms converge to an ‘optimal’ ranking? In this paper, we consider this question in a natural setting where pairwise comparisons are drawn randomly and independently from some underlying probability distribution. We first show that, under a ‘time-reversibility’ or Bradley-Terry-Luce (BTL) condition on the distribution generating the outcomes of the pairwise comparisons, the rank centrality (PageRank) and least squares (HodgeRank) algorithms both converge to an optimal ranking. Next, we show that a matrix version of the Borda count algorithm, and more surprisingly, an algorithm which performs maximal likelihood estimation under a BTL assumption, both converge to an optimal ranking under a ‘low-noise’ condition that is strictly more general than BTL. Finally, we propose a new SVM-based algorithm for rank aggregation from pairwise data, and show that this converges to an optimal ranking under an even more general condition that we term ‘generalized low-noise’. In all cases, we provide explicit sample complexity bounds for exact recovery of an optimal ranking. Our experiments confirm our theoretical findings and help to shed light on the statistical behavior of various rank aggregation algorithms.', 'Efficient Ranking from Pairwise Comparisons The ranking of n objects based on pairwise comparisons is a core machine learning problem, arising in recommender systems, ad placement, player ranking, biological applications and others. In many practical situations the true pairwise comparisons cannot be actively measured, but a subset of all n(n-1)/2 comparisons is passively and noisily observed. Optimization algorithms (e.g., the SVM) could be used to predict a ranking with fixed expected Kendall tau distance, while achieving an Ω(n) lower bound on the corresponding sample complexity. However, due to their centralized structure they are difficult to extend to online or distributed settings. In this paper we show that much simpler algorithms can match the same Ω(n) lower bound in expectation. Furthermore, if an average of O(n\\log(n)) binary comparisons are measured, then one algorithm recovers the true ranking in a uniform sense, while the other predicts the ranking more accurately near the top than the bottom. We discuss extensions to online and distributed ranking, with benefits over traditional alternatives.  ', 'Approximate Ranking from Pairwise Comparisons A common problem in machine learning is to rank a set of n items based on pairwise comparison. Here, ranking refers to partitioning the items into sets of pre-specified sizes according to theirs scores, which includes identification of the top-k items as the most prominent special case.  The score of a given item is defined as the probability that it beats a randomly chosen other item.  In practice, in particular when n is large, finding an exact ranking typically requires a prohibitively large number of comparisons. What comes to our rescue here is that in practice, one is usually content with finding an approximate ranking. In this paper we consider the problem of finding approximate rankings from pairwise comparisons. We analyze an active ranking algorithm that counts the number of comparisons won, and decides whether to stop or which pair of items to compare next, based on confidence intervals computed from the data collected in previous steps. We show that this algorithm succeeds in recovering approximate rankings using a number of comparisons that is close to optimal up to logarithmic factors. We also present numerical results, showing that in practice, approximation can drastically reduce the number of comparisons required to estimate a ranking. ']"
46,82,46_gpt3_gpt4_gpt_gpt4o,"['gpt3', 'gpt4', 'gpt', 'gpt4o', 'gpt35', 'api', 'searchgpt', 'agente', 'turbo', 'cliente']","['Automatizar los agentes de atención al cliente MavenAGI es una empresa de software nacida en la era de la IA. Aprovechando la flexibilidad de GPT-4, acaba de lanzar un agente de atención al cliente con IA, que varias empresas como Tripadvisor, Clickup y Rho ya usan para ganar tiempo y brindar una atención mejor a sus clientes. ', 'Automatización de los agentes de atención al cliente MavenAGI es una nueva empresa de software para la era de la IA. Recientemente lanzaron un agente de atención al cliente con IA, construido sobre la flexibilidad de GPT-4. Hoy en día, un gran número de empresas como Tripadvisor, Clickup y Rho ya utilizan este agente para ahorrar tiempo y ofrecer un mejor servicio a sus clientes. ', 'La disponibilidad general de la API de GPT-4 y el desuso de los modelos anteriores en la API de Completions Las API de GPT-3.5 Turbo, DALL·E y Whisper también están disponibles de forma general, y estamos lanzando un plan de desuso para los modelos anteriores de la API de Completions, que se retirarán a principios de 2024.']"
47,22,47_chatgpt_chatgpts_chat_chats,"['chatgpt', 'chatgpts', 'chat', 'chats', 'conversations', 'conversation', 'talking', 'messaging', 'gpt', 'gpts']","['ChatGPT can now see, hear, and speak We are beginning to roll out new voice and image capabilities in ChatGPT. They offer a new, more intuitive type of interface by allowing you to have a voice conversation or show ChatGPT what you’re talking about.', 'New ways to manage your data in ChatGPT ChatGPT users can now turn off chat history, allowing you to choose which conversations can be used to train our models.', 'Introducing ChatGPT Team We’re launching a new ChatGPT plan for teams of all sizes, which provides a secure, collaborative workspace to get the most out of ChatGPT at work.']"
48,100,48_openai_contenido_contenidos_tratamientos,"['openai', 'contenido', 'contenidos', 'tratamientos', 'trabajo', 'por', 'en', 'es', 'como', 'plataforma']","['Respuesta de OpenAI para mitigar los riesgos de la IA de frontera Actualización sobre la Cumbre de Seguridad de la IA del Reino Unido', 'OpenAI colabora con Stack\xa0Overflow en la integración de una nueva API  OpenAI colabora con Stack\xa0Overflow en la integración de una nueva API \n\nStack Overflow y OpenAI anunciaron la integración de una nueva API para facilitar a los desarrolladores las bondades colectivas de la plataforma de conocimientos técnicos líder en su sector y los LLM más populares para crear soluciones basadas en IA. ', 'Programa de investigación para la evaluación de las repercusiones económicas de los modelos de generación de código ']"
49,30,49_conversaciones_diferencias_fuente_creatividad,"['conversaciones', 'diferencias', 'fuente', 'creatividad', 'trabajamos', 'contenido', 'como', 'calidad', 'aprendemos', 'chatgpt']","['ChatGPT muestra noticias de calidad gracias a The Atlantic  The Atlantic dará a conocer una colaboración estratégica con OpenAI para desarrollar contenido y productos. El periódico no solo contará con un lugar privilegiado como fuente de noticias de primera categoría en el ecosistema de OpenAI, ya que se podrá acceder a sus artículos en ChatGPT; sino que también tendrá una participación determinante en cómo las noticias se muestran y presentan en futuros productos con contenido destacado en tiempo real. ', 'Nuestro enfoque en cuanto a datos y la IA Poco más de un año después del lanzamiento de ChatGPT, la IA está cambiando la forma en la que vivimos, trabajamos y aprendemos. Además, ha suscitado importantes conversaciones sobre los datos en la era de la IA. En este artículo explicamos en mayor profundidad nuestro enfoque, el nuevo Media Manager para creadores y propietarios de contenido, y hacia dónde nos dirigimos.', 'Mejorar la calidad de las noticias de ChatGPT con The Atlantic  The Atlantic anuncia una colaboración estratégica con OpenAI vinculada con productos y contenido que posiciona a la publicación como fuente periodística de referencia en el ecosistema de OpenAI. Los usuarios no solo podrán consultar los artículos de The Atlantic en ChatGPT, sino que la revista digital, en su calidad de socio, ayudará a definir la forma en la que se muestran y se presentan las noticias en futuros productos de búsqueda en tiempo real. ']"
50,525,50_adversarial_adversarially_adversary_adversaries,"['adversarial', 'adversarially', 'adversary', 'adversaries', 'robustness', 'robust', 'defenses', 'attacks', 'classifiers', 'defense']","['Defending against Whitebox Adversarial Attacks via Randomized Discretization Adversarial perturbations dramatically decrease the accuracy of state-of-the-art image classifiers. In this paper, we propose and analyze a simple and computationally efficient defense strategy: inject random Gaussian noise, discretize each pixel, and then feed the result into any pre-trained classifier. Theoretically, we show that our randomized discretization strategy reduces the KL divergence between original and adversarial inputs, leading to a lower bound on the classification accuracy of any classifier against any (potentially whitebox) $L_{\\infty}$-bounded adversarial attack. Empirically, we evaluate our defense on adversarial examples generated by a strong iterative PGD attack. On ImageNet, our defense is more robust than adversarially-trained networks and the winning defenses of the NIPS 2017 Adversarial Attacks & Defenses competition.', ' Adversarially Robust Estimate and Risk Analysis in Linear Regression   Adversarial robust learning aims to design algorithms that are robust to small adversarial perturbations on input variables. Beyond the existing studies on the predictive performance to adversarial samples, our goal is to understand statistical properties of adversarial robust estimates and analyze adversarial risk in the setup of linear regression models. By discovering the statistical minimax rate of convergence of adversarial robust estimators, we emphasize the importance of incorporating model information, e.g., sparsity, in adversarial robust learning. Further, we reveal an explicit connection of adversarial and standard estimates, and propose a straightforward two-stage adversarial training framework, which facilitates to utilize model structure information to improve adversarial robustness. In theory, the consistency of the adversarial robust estimator is proven and its Bahadur representation is also developed for the statistical inference purpose. The proposed estimator converges in a sharp rate under either low-dimensional or sparse scenario. Moreover, our theory confirms two phenomena in adversarial robust learning: adversarial robustness hurts generalization, and unlabeled data help improve the generalization. In the end, we conduct numerical simulations to verify our theory. ', 'Improving Adversarial Robustness by Putting More Regularizations on Less Robust Samples Adversarial training, which is to enhance robustness against adversarial attacks, has received much attention because it is easy to generate human-imperceptible perturbations of data to deceive a given deep neural network. In this paper, we propose a new adversarial training algorithm that is theoretically well motivated and empirically superior to other existing algorithms. A novel feature of the proposed algorithm is to apply more regularization to data vulnerable to adversarial attacks than other existing regularization algorithms do. Theoretically, we show that our algorithm can be understood as an algorithm of minimizing a newly derived upper bound of the robust risk. Numerical experiments illustrate that our proposed algorithm improves the generalization (accuracy on examples) and robustness (accuracy on adversarial attacks) simultaneously to achieve the state-of-the-art performance.']"
51,237,51_explanations_explainability_interpretability_ai,"['explanations', 'explainability', 'interpretability', 'ai', 'interpretable', 'explaining', 'attributions', 'counterfactuals', 'models', 'prediction']","['Generating Deep Networks Explanations with Robust Attribution Alignment Attribution methods play a key role in generating post-hoc explanations on pre-trained models, however it has been shown that existing methods yield unfaithful and noisy explanations. In this paper, we propose a new paradigm of attribution method: we treat the model’s explanations as a part of network’s outputs then generate attribution maps from the underlying deep network. The generated attribution maps are up-sampled from the last convolutional layer of the network to obtain localization information about the target to be explained. Inspired by recent studies that showed adversarially robust models’ saliency map aligns well with human perception, we utilize attribution maps from the robust model to supervise the learned attributions. Our proposed method can produce visually plausible explanations along with the prediction in inference phase. Experiments on real datasets show that our proposed method yields more faithful explanations than post-hoc attribution methods with lighter computational costs.', 'Problems with Shapley-value-based explanations as feature importance measures Game-theoretic formulations of feature importance have become popular as a way to ""explain"" machine learning models. These methods define a cooperative game between the features of a model and distribute influence among these input elements using some form of the game’s unique Shapley values. Justification for these methods rests on two pillars: their desirable mathematical properties, and their applicability to specific motivations for explanations. We show that mathematical problems arise when Shapley values are used for feature importance and that the solutions to mitigate these necessarily induce further complexity, such as the need for causal reasoning. We also draw on additional literature to argue that Shapley values do not provide explanations which suit human-centric goals of explainability.', ' Exploring Counterfactual Explanations Through the Lens of Adversarial Examples: A Theoretical and Empirical Analysis   As machine learning (ML) models becomemore widely deployed in high-stakes applications, counterfactual explanations have emerged as key tools for providing actionable model explanations in practice. Despite the growing popularity of counterfactual explanations, the theoretical understanding of these explanations is still lacking behind. In this work, we systematically analyze counterfactual explanations through the lens of adversarial examples. We do so by formalizing the similarities between popular counterfactual explanation and adversarial example generation methods identifying conditions when they are equivalent. We then derive upper bounds between the solutions output by counterfactual explanation and adversarial example generation methods, which we validate on several real world data sets. By establishing these theoretical and empirical similarities between counterfactual explanations and adversarial examples, our work raises fundamental questions about the design and development of existing counterfactual explanation algorithms. ']"
52,39,52_mit_institute_initiative_intelligence,"['mit', 'institute', 'initiative', 'intelligence', 'ai', 'consortium', 'institutewide', 'mitnano', 'academia', 'innovative']","['MIT announces leadership of its Quest for Intelligence Faculty from across the Institute tapped to lead new initiative in human and machine intelligence. ', 'MIT announces leadership of its Quest for Intelligence Faculty from across the Institute tapped to lead new initiative in human and machine intelligence. ', 'MIT and SenseTime announce effort to advance artificial intelligence research Alliance will be part of new MIT Intelligence Quest.']"
53,24,53_mit_mits_campus_schwarzman,"['mit', 'mits', 'campus', 'schwarzman', 'schwarzmans', 'faculty', 'alumni', 'college', 'stephen', 'alumnus']","['Provost&#039;s letter to the faculty about the MIT Stephen A. Schwarzman College of Computing ', 'Letter to the MIT community regarding the MIT Stephen A. Schwarzman College of Computing ', 'Letter to the MIT community regarding the MIT Stephen A. Schwarzman College of Computing ']"
54,78,54_auction_auctions_bidding_bids,"['auction', 'auctions', 'bidding', 'bids', 'bid', 'pricing', 'bidders', 'revenuemaximizing', 'optimal', 'valuations']","['Reserve Pricing in Repeated Second-Price Auctions with Strategic Bidders We study revenue optimization learning algorithms for repeated second-price auctions with reserve where a seller interacts with multiple strategic bidders each of which holds a fixed private valuation for a good and seeks to maximize his expected future cumulative discounted surplus.\t We propose a novel algorithm that has strategic regret upper bound of $O(\\log\\log T)$ for worst-case valuations. This pricing is based on our novel transformation that upgrades an algorithm designed for the setup with a single buyer to the multi-buyer case. We provide theoretical guarantees on the ability of a transformed algorithm to learn the valuation of a strategic buyer, which has uncertainty about the future due to the presence of rivals.', 'Incentive-aware Contextual Pricing with Non-parametric Market Noise We consider a dynamic pricing problem for repeated contextual second-price auctions with multiple strategic buyers who aim to maximize their long-term time discounted utility. The seller has limited information on buyers’ overall demand curves which depends on a non-parametric market-noise distribution, and buyers may potentially submit corrupted bids (relative to true valuations) to manipulate the seller’s pricing policy for more favorable reserve prices in the future. We focus on designing the seller’s learning policy to set contextual reserve prices where the seller’s goal is to minimize regret compared to the revenue of a benchmark clairvoyant policy that has full information of buyers’ demand. We propose a policy with a phased-structure that incorporates randomized “isolation” periods, during which a buyer is randomly chosen to solely participate in the auction. We show that this design allows the seller to control the number of periods in which buyers significantly corrupt their bids. We then prove that our policy enjoys a T-period regret of $O(\\sqrt{T})$ facing strategic buyers. Finally, we conduct numerical simulations to compare our proposed algorithm to standard pricing policies. Our numerical results show that our algorithm outperforms these policies under various buyer bidding behavior.', 'Robust Pricing in Dynamic Mechanism Design Motivated by the repeated sale of online ads via auctions, optimal pricing in repeated auctions has attracted a large body of research. While dynamic mechanisms offer powerful techniques to improve on both revenue and efficiency by optimizing auctions across different items, their reliance on exact distributional information of buyers’ valuations (present and future) limits their use in practice. In this paper, we propose robust dynamic mechanism design. We develop a new framework to design dynamic mechanisms that are robust to both estimation errors in value distributions and strategic behavior. We apply the framework in learning environments, leading to the first policy that achieves provably low regret against the optimal dynamic mechanism in contextual auctions, where the dynamic benchmark has full and accurate distributional information.']"
55,28,55_electroencephalogram_electroencephalography_seizures_epileptic,"['electroencephalogram', 'electroencephalography', 'seizures', 'epileptic', 'epilepsy', 'eegs', 'seizure', 'eeg', 'bci', 'braincomputer']","['Detecting seizures in EEG recordings using conformal prediction This study examines the use of the Conformal Prediction (CP) framework for the provision of confidence information in the detection of seizures in electroencephalograph (EEG) recordings. The detection of seizures is an important task since EEG recordings of seizures are of primary interest in the evaluation of epileptic patients. However, manual review of long-term EEG recordings for detecting and analyzing seizures that may have occurred is a time-consuming process. Therefore a technique for automatic detection of seizures in such recordings is highly beneficial since it can be used to significantly reduce the amount of data in need of manual review. Additionally, due to the infrequent and unpredictable occurrence of seizures, having high sensitivity is crucial for seizure detection systems. This is the main motivation for this study, since CP can be used for controlling the error rate of predictions and therefore guaranteeing an upper bound on the frequency of false negatives.', 'Attention-Based Network for Weak Labels in Neonatal Seizure Detection Seizures are a common emergency in the neonatal intesive care unit (NICU) among newborns receiving therapeutic hypothermia for hypoxic ischemic encephalopathy. The high incidence of seizures in this patient population necessitates continuous electroencephalographic (EEG) monitoring to detect and treat them. Due to EEG recordings being reviewed intermittently throughout the day, inevitable delays to seizure identification and treatment arise. In recent years, work on neonatal seizure detection using deep learning algorithms has started gaining momentum. These algorithms face numerous challenges: first, the training data for such algorithms comes from individual patients, each with varying levels of label imbalance since the seizure burden in NICU patients differs by several orders of magnitude. Second, seizures in neonates are usually localized in a subset of EEG channels, and performing annotations per channel is very time-consuming. Hence models which make use of labels only per time periods, and not per channels, are preferable. In this work we assess how different deep learning models and data balancing methods influence learning in neonatal seizure detection in EEGs. We propose a model which provides a level of importance to each of the EEG channels - a proxy to whether a channel exhibits seizure activity or not, and we provide a quantitative assessment of how well this mechanism works. The model is portable to EEG devices with differing layouts without retraining, facilitating its potential deployment across different medical centers. We also provide a first assessment of how a deep learning model for neonatal seizure detection agrees with human rater decisions - an important milestone for deployment to clinical practice. We show that high AUC values in a deep learning model do not necessarily correspond to agreement with a human expert, and there is still a need to further refine such algorithms for optimal seizure discrimination.', 'Real-Time Seizure Detection using EEG: A Comprehensive Comparison of Recent Approaches under a Realistic Setting Electroencephalogram (EEG) is an important diagnostic test that physicians use to record brain activity and detect seizures by monitoring the signals. There have been several attempts to detect seizures and abnormalities in EEG signals with modern deep learning models to reduce the clinical burden. However, they cannot be fairly compared against each other as they were tested in distinct experimental settings. Also, some of them are not trained in real-time seizure detection tasks, making it hard for on-device applications. In this work, for the first time, we extensively compare multiple state-of-the-art models and signal feature extractors in a real-time seizure detection framework suitable for real-world application, using various evaluation metrics including a new one we propose to evaluate more practical aspects of seizure detection models.']"
56,102,56_annotations_text_nlp_notes,"['annotations', 'text', 'nlp', 'notes', 'retrieval', 'biomedical', 'records', 'medical', 'patients', 'record']","['Conceptualizing Machine Learning for Dynamic Information Retrieval of Electronic Health Record Notes The large amount of time clinicians spend sifting through patient notes and documenting in electronic health records (EHRs) is a leading cause of clinician burnout. By proactively and dynamically retrieving relevant notes during the documentation process, we can reduce the effort required to find relevant patient history. In this work, we conceptualize the use of EHR audit logs for machine learning as a source of supervision of note relevance in a specific clinical context, at a particular point in time. Our evaluation focuses on the dynamic retrieval in the emergency department, a high acuity setting with unique patterns of information retrieval and note writing. We show that our methods can achieve an AUC of 0.963 for predicting which notes will be read in an individual note writing session. We additionally conduct a user study with several clinicians and find that our framework can help clinicians retrieve relevant information more efficiently. Demonstrating that our framework and methods can perform well in this demanding setting is a promising proof of concept that they will translate to other clinical settings and data modalities (e.g., labs, medications, imaging).', 'Deep EHR: Chronic Disease Prediction Using Medical Notes Early detection of preventable diseases is important for better disease management, improved interventions, and more efficient health-care resource allocation. Various machine learning approaches have been developed to exploit information in Electronic Health Record (EHR) for this task. Majority of previous attempts, however, focus on structured fields and lose the vast amount of information in the unstructured notes.  In this work we propose a general multi-task framework for disease onset prediction that combines both free-text medical notes and structured information. We compare performance of different deep learning architectures including CNN, LSTM and hierarchical models. In contrast to traditional text-based prediction models, our approach does not require disease specific feature engineering, and can handle negations and numerical values that exist in the text. Our results on a cohort of about 1 million patients show that models using text outperform models using just structured data, and that models capable of using numerical values and negations in the text, in addition to the raw text, further improve performance. Additionally, we compare different visualization methods for medical professionals to interpret model predictions. ', 'Enriching Unsupervised User Embedding via Medical Concepts Clinical notes in Electronic Health Records (EHR) present rich documented information of patients to inference phenotype for disease diagnosis and study patient characteristics for cohort selection. Unsupervised user embedding aims to encode patients into fixed-length vectors without human supervisions. Medical concepts extracted from the clinical notes contain rich connections between patients and their clinical categories. However, existing \\textit{unsupervised} approaches of user embeddings from clinical notes do not explicitly incorporate medical concepts. In this study, we propose a concept-aware unsupervised user embedding that jointly leverages text documents and medical concepts from two clinical corpora, MIMIC-III and Diabetes. We evaluate user embeddings on both extrinsic and intrinsic tasks, including phenotype classification, in-hospital mortality prediction, patient retrieval, and patient relatedness. Experiments on the two clinical corpora show our approach exceeds unsupervised baselines, and incorporating medical concepts can significantly improve the baseline performance.']"
57,92,57_grammars_grammar_syntactic_linguistic,"['grammars', 'grammar', 'syntactic', 'linguistic', 'languages', 'learnability', 'automata', 'language', 'strings', 'algorithms']","['Evaluation of selection in context-free grammar learning systems Grammatical inference deals with learning of grammars describing languages.  Formal grammatical inference aims at identifying families of languages that have a shared property, which can be used to prove efficient learnability of the families formally.  In contrast, in empirical grammatical inference research, practical systems are developed that are applied to languages.  The effectiveness of these systems is measured by comparing the learned grammar against a Gold standard which indicates the ground truth. From successful empirical learnability results, either shared properties may be identified, leading to further formal learnability results, or modifications to the systems may be made, improving practical results.  Proper evaluation of empirical systems is, therefore, essential.  Here, we evaluate and compare existing state-of-the-art context-free grammar learning systems (and novel systems based on combinations of existing phases) in a standardized evaluation environment (on a corpus of plain natural language sentences), illustrating future directions for empirical grammatical inference research.', 'Testing Distributional Properties of Context-Free Grammars Recent algorithms for distributional learning of context-free grammars can learn all languages defined by grammars that have certain distributional properties: the finite kernel property (FKP) and the finite context property (FCP). In this paper we present some algorithms for approximately determining whether a given grammar has one of these properties. We then present the results of some experiments that indicate that with randomly generated context-free grammars in Chomsky normal form, which generate infinite languages and are derivationally sparse, nearly all grammars have the finite kernel property, whereas the finite context property is much less common.', 'Grammatical Inference of some Probabilistic Context-Free Grammars from Positive Data using Minimum Satisfiability Recently, different theoretical learning results have been found for a variety of context-free grammar subclasses through the use of distributional learning (Clark, 2010b). However, these results are still not extended to probabilistic grammars. In this work, we give a practical algorithm, with some proven properties, that learns a subclass of probabilistic grammars from positive data. A minimum satisfiability solver is used to direct the search towards small grammars. Experiments on typical context-free languages and artificial natural language grammars give positive results.']"
58,19,58_influenza_outbreak_pandemic_vaccine,"['influenza', 'outbreak', 'pandemic', 'vaccine', 'vaccination', 'epidemic', 'epidemics', 'coronavirus', 'vaccines', 'flu']","['Machine Learning-Powered Mitigation Policy Optimization in Epidemiological Models A crucial aspect of managing a public health crisis is to effectively balance prevention and mitigation strategies, while taking their socio-economic impact into account. In particular, determining the influence of different non-pharmaceutical interventions (NPIs) on the effective use of public resources is an important problem, given the uncertainties on when a vaccine will be made available. In this paper, we propose a new approach for obtaining optimal policy recommendations based on Epicast models, which can characterize the disease progression under different interventions, and a look-ahead reward optimization strategy to choose the suitable NPI at different stages of an epidemic. Given the time delay inherent in any Epicast model and the exponential nature especially of an unmanaged epidemic, we find that such a look-ahead strategy infers non-trivial policies that adhere well to the constraints specified. Using two different Epicast models, namely SEIR and EpiCast, we evaluate the proposed algorithm to determine the optimal NPI policy, under a constraint on the number of daily new cases and the primary reward being the absence of restrictions.', 'Characterizing and Understanding Temporal Effects in COVID-19 Data Since the global outbreak of the coronavirus 2019 pandemic, hundreds of works have been published, analyzing and modeling multiple aspects of the disease. Several of them venture into predictive and modeling tasks, such as mortality prediction and patient severity scoring, using  machine-learning (ML) algorithms. An important limitation for most of these works is the fact that they do not consider the multiple temporal aspects of this pandemic, especially regarding disease profile and distributional changes over the months. Such temporal effects are mostly due to multiple interactions between different and novel viral strains, combined with mass vaccination campaigns targeting different groups or patterns (e.g., prioritizing older individuals and those with comorbidity first) and availability of different vaccines. These temporal effects result in impaired model effectiveness and classification errors. In this paper, using a large dataset with over 10,000 patients from 39 hospitals in Brazil admitted during a period of more than 20 months, we provide an overview of the multiple forms of temporal drift that happened during the pandemic and the magnitude of their effects on model effectiveness. Our analyses encompass changes in the severely ill patients’ profile as well as how mortality rates have changed over time. We also investigate how the importance of different predictive variables change and shift over time.', 'Characterizing and Understanding Temporal Effects in COVID-19 Data Since the global outbreak of the coronavirus 2019 pandemic, hundreds of works have been published, analyzing and modeling multiple aspects of the disease. Several of them venture into predictive and modeling tasks, such as mortality prediction and patient severity scoring, using  machine-learning (ML) algorithms. An important limitation for most of these works is the fact that they do not consider the multiple temporal aspects of this pandemic, especially regarding disease profile and distributional changes over the months. Such temporal effects are mostly due to multiple interactions between different and novel viral strains, combined with mass vaccination campaigns targeting different groups or patterns (e.g., prioritizing older individuals and those with comorbidity first) and availability of different vaccines. These temporal effects result in impaired model effectiveness and classification errors. In this paper, using a large dataset with over 10,000 patients from 39 hospitals in Brazil admitted during a period of more than 20 months, we provide an overview of the multiple forms of temporal drift that happened during the pandemic and the magnitude of their effects on model effectiveness. Our analyses encompass changes in the severely ill patients’ profile as well as how mortality rates have changed over time. We also investigate how the importance of different predictive variables change and shift over time.']"
59,266,59_survival_mortality_predicting_prediction,"['survival', 'mortality', 'predicting', 'prediction', 'predict', 'outcomes', 'predictive', 'datasets', 'dataset', 'models']","['A Transductive Approach to Survival Ranking for Cancer Risk Stratification How can we stratify patients into subgroups based on their expected survival in a purely data-driven manner? Identifying cancer patients at higher risk is crucial in planning personalized treatment to improve patient survival outcomes. The main challenge with existing approaches is the underlying complexity of handling censoring in the survival data and manually setting a precise threshold to stratify patients into risk groups. In this paper, a Transductive Survival Ranking (TSR) model for patient risk stratification is proposed. The model handles samples in pairs to make use of instances with censored survival information. It incorporates unlabeled test samples in the training process to maximize the margin between their predicted survival scores resulting in automatic patient stratification into subgroups without the need for any additional post-processing or manual threshold selection. The model was evaluated on several datasets with varying sets of covariates, and all stratification were significant ($p < 0.05$) with high concordance indices of up to 0.78 in Disease Specific Survival and 0.75 in Overall Survival.', 'Interpretable Survival Analysis for Heart Failure Risk Prediction Survival analysis, or time-to-event analysis, is an important and widespread problem in healthcare research. Medical research has traditionally relied on Cox models for survival analysis, due to their simplicity and interpretability. Cox models assume a log-linear hazard function as well as proportional hazards over time, and can perform poorly when these assumptions fail. Newer survival models based on machine learning avoid these assumptions and offer improved accuracy, yet sometimes at the expense of model interpretability, which is vital for clinical use. We propose a novel survival analysis pipeline that is both interpretable and competitive with state-of-the-art survival models. Specifically, we use an improved version of survival stacking to transform a survival analysis problem to a classification problem, ControlBurn to perform feature selection, and Explainable Boosting Machines to generate interpretable predictions. To evaluate our pipeline, we predict risk of heart failure using a large-scale EHR database. Our pipeline achieves state-of-the-art performance and provides interesting and novel insights about risk factors for heart failure.', 'Boosting Transfer Learning with Survival Data from Heterogeneous Domains Survival models derived from health care data are an important support to inform critical screening and therapeutic decisions. Most models however, do not generalize to populations outside the marginal and conditional distribution assumptions for which they were derived. This presents a significant barrier to the deployment of machine learning techniques into wider clinical practice as most medical studies are data scarce, especially for the analysis of time-to-event outcomes. In this work we propose a survival prediction model that is able to improve predictions on a small data domain of interest - such as a local hospital - by leveraging related data from other domains - such as data from other hospitals. We construct an ensemble of weak survival predictors which iteratively adapt the marginal distributions of the source and target data such that similar source patients contribute to the fit and ultimately improve predictions on target patients of interest. This represents the first boosting-based transfer learning algorithm in the survival analysis literature. We demonstrate the performance and utility of our algorithm on synthetic and real healthcare data collected at various locations.']"
60,26,60_discovery_hypotheses_hypothesis_testing,"['discovery', 'hypotheses', 'hypothesis', 'testing', 'tests', 'discoveries', 'algorithms', 'adaptively', 'selective', 'predetermined']","['Online multiple testing with e-values A scientist tests a continuous stream of hypotheses over time in the course of her investigation — she does not test a predetermined, fixed number of hypotheses. The scientist wishes to make as many discoveries as possible while ensuring the number of false discoveries is controlled — a well recognized way for accomplishing this is to control the false discovery rate (FDR). Prior methods for FDR control in the online setting have focused on formulating algorithms when specific dependency structures are assumed to exist between the test statistics of each hypothesis. However, in practice, these dependencies often cannot be known beforehand or tested after the fact. Our algorithm, e-LOND, provides FDR control under arbitrary, possibly unknown, dependence. We show that our method is more powerful than existing approaches to this problem through simulations. We also formulate extensions of this algorithm to utilize randomization for increased power and for constructing confidence intervals in online selective inference.', 'PAPRIKA: Private Online False Discovery Rate Control In hypothesis testing, a \\emph{false discovery} occurs when a hypothesis is incorrectly rejected due to noise in the sample. When adaptively testing multiple hypotheses, the probability of a false discovery increases as more tests are performed. Thus the problem of \\emph{False Discovery Rate (FDR) control} is to find a procedure for testing multiple hypotheses that accounts for this effect in determining the set of hypotheses to reject. The goal is to minimize the number (or fraction) of false discoveries, while maintaining a high true positive rate (i.e., correct discoveries). In this work, we study False Discovery Rate (FDR) control in multiple hypothesis testing under the constraint of differential privacy for the sample. Unlike previous work in this direction, we focus on the \\emph{online setting}, meaning that a decision about each hypothesis must be made immediately after the test is performed, rather than waiting for the output of all tests as in the offline setting. We provide new private algorithms based on state-of-the-art results in non-private online FDR control. Our algorithms have strong provable guarantees for privacy and statistical performance as measured by FDR and power. We also provide experimental results to demonstrate the efficacy of our algorithms in a variety of data environments.', 'Contextual Online False Discovery Rate Control Multiple hypothesis testing, a situation when we wish to consider many hypotheses, is a core problem in statistical inference that arises in almost every scientific field. In this setting, controlling the false discovery rate (FDR), which is the expected proportion of type I error, is an important challenge for making meaningful inferences. In this paper, we consider a setting where an ordered (possibly infinite) sequence of hypotheses arrives in a stream, and for each hypothesis we observe a p-value along with a set of features specific to that hypothesis. The decision whether or not to reject the current hypothesis must be made immediately at each time step, before the next hypothesis is observed. This model provides a general way of leveraging the side (contextual) information in the data to help maximize the number of discoveries while controlling the FDR.We propose a new class of powerful online testing procedures, where the rejection thresholds are learned sequentially by incorporating contextual information and previous results. We prove that any rule in this class controls online FDR under some standard assumptions. We then focus on a subclass of these procedures, based on weighting the rejection thresholds, to derive a practical algorithm that learns a parametric weight function in an online fashion to gain more discoveries. We also theoretically prove that our proposed procedures, under some easily verifiable assumptions, would lead to an increase of statistical power over a popular online testing procedure proposed by Javanmard and Montanari (2018). Finally, we demonstrate the superior performance of our procedure, by comparing it to state-of-the-art online multiple testing procedures, on both synthetic data and real data generated from different applications.']"
61,13,61_openai_partnership_chatgpt_partnering,"['openai', 'partnership', 'chatgpt', 'partnering', 'journalism', 'partners', 'news', 'media', 'enterprise', 'openais']","['OpenAI partners with Schibsted Media Group  OpenAI and Schibsted Media Group announce content partnership to bring Guardian news and archive content to  ChatGPT.', 'Enhancing news in ChatGPT with The Atlantic  The Atlantic is announcing a strategic content and product partnership with OpenAI, which positions The Atlantic as a premium news source within OpenAI. The Atlantic’s articles will be discoverable within OpenAI’s products, including ChatGPT, and as a partner, The Atlantic will help to shape how news is surfaced and presented in future real-time discovery products. ', 'OpenAI and GEDI partner for Italian news content  OpenAI and GEDI announce strategic partnership to bring Italian-language news content to ChatGPT.']"
62,33,62_bandits_bandit_rkhs_kernelized,"['bandits', 'bandit', 'rkhs', 'kernelized', 'optimization', 'kernelised', 'regret', 'kernels', 'rkhss', 'algorithms']","['Open Problem: Tight Online Confidence Intervals for RKHS Elements Confidence intervals are a crucial building block in the analysis of various online learning problems. The analysis of kernel-based bandit and reinforcement learning problems utilize confidence intervals applicable to the elements of a reproducing kernel Hilbert space (RKHS).  However, the existing confidence bounds do not appear to be tight, resulting in suboptimal regret bounds.  In fact, the existing regret bounds for several kernelized bandit algorithms (e.g., GP-UCB, GP-TS, and their variants) may fail to even be sublinear. It is unclear whether the suboptimal regret bound is a fundamental shortcoming of these algorithms or an artifact of the proof, and the main challenge seems to stem from the online (sequential) nature of the observation points. We formalize the question of online confidence intervals in the RKHS setting and overview the existing results. ', 'Approximation Theory Based Methods for RKHS Bandits The RKHS bandit problem (also called kernelized multi-armed bandit problem) is an online optimization problem of non-linear functions with noisy feedback. Although the problem has been extensively studied, there are unsatisfactory results for some problems compared to the well-studied linear bandit case. Specifically, there is no general algorithm for the adversarial RKHS bandit problem. In addition, high computational complexity of existing algorithms hinders practical application. We address these issues by considering a novel amalgamation of approximation theory and the misspecified linear bandit problem. Using an approximation method, we propose efficient algorithms for the stochastic RKHS bandit problem and the first general algorithm for the adversarial RKHS bandit problem. Furthermore, we empirically show that one of our proposed methods has comparable cumulative regret to IGP-UCB and its running time is much shorter.', ' On Information Gain and Regret Bounds in Gaussian Process Bandits   Consider the sequential optimization of an expensive to evaluate and possibly non-convex objective function $f$ from noisy feedback, that can be considered as a continuum-armed bandit problem. Upper bounds on the regret performance of several learning algorithms (GP-UCB, GP-TS, and their variants) are known under both a Bayesian (when $f$ is a sample from a Gaussian process (GP)) and a frequentist (when $f$ lives in a reproducing kernel Hilbert space) setting. The regret bounds often rely on the maximal information gain $\\gamma_T$ between $T$ observations and the underlying GP (surrogate) model. We provide general bounds on $\\gamma_T$ based on the decay rate of the eigenvalues of the GP kernel, whose specialisation for commonly used kernels improves the existing bounds on $\\gamma_T$, and subsequently the regret bounds relying on $\\gamma_T$ under numerous settings. For the Matérn family of kernels, where the lower bounds on $\\gamma_T$, and regret under the frequentist setting, are known, our results close a huge polynomial in $T$ gap between the upper and lower bounds (up to logarithmic in $T$ factors). ']"
63,1144,63_bandits_bandit_optimal_adversarial,"['bandits', 'bandit', 'optimal', 'adversarial', 'reward', 'guarantees', 'optimization', 'minimax', 'stochastic', 'regret']","['Nonstochastic Bandits with Composite Anonymous Feedback We investigate a nonstochastic bandit setting in which the loss of an action is not immediately charged to the player, but rather spread over at most d consecutive steps in an adversarial way. This implies that the instantaneous loss observed by the player at the end of each round is a sum of as many as d loss components of previously played actions. Hence, unlike the standard bandit setting with delayed feedback, here the player cannot observe the individual delayed losses, but only their sum. Our main contribution is a general reduction transforming a standard bandit algorithm into one that can operate in this harder setting. We also show how the regret of the transformed algorithm can be bounded in terms of the regret of the original algorithm. Our reduction cannot be improved in general: we prove a lower bound on the regret of any bandit algorithm in this setting that matches (up to log factors) the upper bound obtained via our reduction. Finally, we show how our reduction can be extended to more complex bandit settings, such as combinatorial linear bandits and online bandit convex optimization.', 'Contextual Bandits with Smooth Regret: Efficient Learning in Continuous Action Spaces Designing efficient general-purpose contextual bandit algorithms that work with large—or even infinite—action spaces would facilitate application to important scenarios such as information retrieval, recommendation systems, and continuous control. While obtaining standard regret guarantees can be hopeless, alternative regret notions have been proposed to tackle the large action setting. We propose a smooth regret notion for contextual bandits, which dominates previously proposed alternatives. We design a statistically and computationally efficient algorithm—for the proposed smooth regret—that works with general function approximation under standard supervised oracles. We also present an adaptive algorithm that automatically adapts to any smoothness level. Our algorithms can be used to recover the previous minimax/Pareto optimal guarantees under the standard regret, e.g., in bandit problems with multiple best arms and Lipschitz/H{ö}lder bandits. We conduct large-scale empirical evaluations demonstrating the efficacy of our proposed algorithms.', 'Contexts can be Cheap: Solving Stochastic Contextual Bandits with Linear Bandit Algorithms    In this paper, we address the stochastic contextual linear bandit problem, where a decision maker is provided a context (a random set of actions drawn from a distribution). The expected reward of each action is specified by the inner product of the action  and an unknown parameter. The goal is to design an algorithm that learns to play as close as possible  to the unknown optimal policy after a number of action plays. This problem is considered more challenging than the linear bandit problem, which can be viewed as a contextual bandit problem with a \\emph{fixed} context. Surprisingly, in this paper, we show that the stochastic contextual problem can be solved as if it is a linear bandit problem. In particular, we establish a novel reduction framework that converts every stochastic contextual linear bandit instance to a linear bandit instance, when the context distribution is known. When the context distribution is unknown, we establish an algorithm that reduces the stochastic contextual instance to a sequence of linear bandit instances with small misspecifications and achieves nearly the same worst-case regret bound as the algorithm that solves the misspecified linear bandit instances.       As a consequence, our results imply a $O(d\\sqrt{T\\log T})$ high-probability regret bound for contextual linear bandits, making progress in resolving an open problem in Li et al., 2019b, 2021.      Our reduction framework opens up a new way to approach stochastic contextual linear bandit problems, and enables  improved regret bounds in a number of instances including the batch setting, contextual bandits with misspecifications, contextual bandits with sparse unknown parameters, and contextual bandits with adversarial corruption.']"
64,46,64_ai_aipowered_cybersecurity_security,"['ai', 'aipowered', 'cybersecurity', 'security', 'infrastructure', 'intelligence', 'protecting', 'capabilities', 'safety', 'governance']","['Reimagining secure infrastructure for advanced AI Securing advanced AI systems will require an evolution in infrastructure security. We’re calling for research and investment in six security measures that we believe will play key roles in protecting advanced AI.\n\nProtecting, exploring, and applying advanced artificial intelligence (AI) is our strategic imperative. OpenAI’s mission is to deliver positive impact of advanced AI to everything from healthcare to science to education – and yes, even to cybersecurity. That work begins with building secure, trustworthy AI systems and protecting the underlying technologies from those who seek to subvert our work to cause harm.\n', 'Position: Near to Mid-term Risks and Opportunities of Open-Source Generative AI In the next few years, applications of Generative AI are expected to revolutionize a number of different areas, ranging from science & medicine to education. The potential for these seismic changes has triggered a lively debate about potential risks and resulted in calls for tighter regulation, in particular from some of the major tech companies who are leading in AI development. While regulation is important, it is key that it does not put at risk the budding field of open-source Generative AI. We argue for the responsible open sourcing of generative AI models in the near and medium term. To set the stage, we first introduce an AI openness taxonomy system and apply it to 40 current large language models. We then outline differential benefits and risks of open versus closed source AI and present potential risk mitigation, ranging from best practices to calls for technical and scientific contributions. We hope that this report will add a much needed missing voice to the current public discourse on near to mid-term AI safety and other societal impact.', 'Position: AI-Powered Autonomous Weapons Risk Geopolitical Instability and Threaten AI Research The recent embrace of machine learning (ML) in the development of autonomous weapons systems (AWS) creates serious risks to geopolitical stability and the free exchange of ideas in AI research. This topic has received comparatively little attention of late compared to risks stemming from superintelligent artificial general intelligence (AGI), but requires fewer assumptions about the course of technological development and is thus a nearer-future issue. ML is already enabling the substitution of AWS for human soldiers in many battlefield roles, reducing the upfront human cost, and thus political cost, of waging offensive war. In the case of peer adversaries, this increases the likelihood of ""low intensity"" conflicts which risk escalation to broader warfare. In the case of non-peer adversaries, it reduces the domestic blowback to wars of aggression. This effect can occur regardless of other ethical issues around the use of military AI such as the risk of civilian casualties, and does not require any superhuman AI capabilities. Further, the military value of AWS raises the specter of an AI-powered arms race and the misguided imposition of national security restrictions on AI research. Our goal in this paper is to raise awareness among the public and ML researchers on the near-future risks posed by full or near-full autonomy in military technology, and we provide regulatory suggestions to mitigate these risks. We call upon AI policy experts and the defense AI community in particular to embrace transparency and caution in their development and deployment of AWS to avoid the negative effects on global stability and AI research that we highlight here.']"
65,101,65_openai_ofopenai_openais_api,"['openai', 'ofopenai', 'openais', 'api', 'agi', 'o1', 'latest', 'enterpriseready', 'o3mini', 'ai']","['OpenAI Scholars 2018: Final projects Our first cohort of\xa0OpenAI Scholars\xa0has now completed the program.', 'OpenAI API We’re releasing an API for accessing new AI models developed by OpenAI.', 'OpenAI Five Benchmark The OpenAI Five Benchmark match is now over!']"
66,162,66_regularization_transport_optimal_regularized,"['regularization', 'transport', 'optimal', 'regularized', 'transportation', 'wasserstein', 'optimization', 'metrics', 'distances', 'metric']","['Wasserstein Wormhole: Scalable Optimal Transport Distance with Transformer Optimal transport (OT) and the related Wasserstein metric ($W$) are powerful and ubiquitous tools for comparing distributions. However, computing pairwise Wasserstein distances rapidly becomes intractable as cohort size grows. An attractive alternative would be to find an embedding space in which pairwise Euclidean distances map to OT distances, akin to standard multidimensional scaling (MDS). We present Wasserstein Wormhole, a transformer-based autoencoder that embeds empirical distributions into a latent space wherein Euclidean distances approximate OT distances. Extending MDS theory, we show that our objective function implies a bound on the error incurred when embedding non-Euclidean distances. Empirically, distances between Wormhole embeddings closely match Wasserstein distances, enabling linear time computation of OT distances. Along with an encoder that maps distributions to embeddings, Wasserstein Wormhole includes a decoder that maps embeddings back to distributions, allowing for operations in the embedding space to generalize to OT spaces, such as Wasserstein barycenter estimation and OT interpolation. By lending scalability and interpretability to OT approaches, Wasserstein Wormhole unlocks new avenues for data analysis in the fields of computational geometry and single-cell biology.', 'Improving approximate optimal transport distances using quantization Optimal transport (OT) is a popular tool in machine learning to compare probability measures geometrically, but it comes with substantial computational burden. Linear programming algorithms for computing OT distances scale cubically in the size of the input, making OT impractical in the large-sample regime. We introduce a practical algorithm, which relies on a quantization step, to estimate OT distances between measures given cheap sample access. We also provide a variant of our algorithm to improve the performance of approximate solvers, focusing on those for entropy-regularized transport. We give theoretical guarantees on the benefits of this quantization step and display experiments showing that it behaves well in practice, providing a practical approximation algorithm that can be used as a drop-in replacement for existing OT estimators.', 'Gaussian-Smoothed Optimal Transport: Metric Structure and Statistical Efficiency Optimal transport (OT), and in particular the Wasserstein distance, has seen a surge of interest and applications in machine learning. However, empirical approximation under Wasserstein distances suffers from a severe curse of dimensionality, rendering them impractical in high dimensions. As a result, entropically regularized OT has become a popular workaround. However, while it enjoys fast algorithms and better statistical properties, it looses the metric structure that Wasserstein distances enjoy. This work proposes a novel Gaussian-smoothed OT (GOT) framework, that achieves the best of both worlds: preserving the 1-Wasserstein metric structure while alleviating the empirical approximation curse of dimensionality. Furthermore, as the Gaussian-smoothing parameter shrinks to zero, GOT $\\Gamma$-converges towards classic OT (with convergence of optimizers), thus serving as a natural extension. An empirical study that validates the theoretical results is provided, promoting Gaussian-smoothed OT as a powerful alternative to entropic OT.']"
67,15,67_similaritybased_embeddings_embedding_metric,"['similaritybased', 'embeddings', 'embedding', 'metric', 'metrics', 'deep', 'similarity', 'classifiers', 'learning', 'benchmark']","['Revisiting Training Strategies and Generalization Performance in Deep Metric Learning Deep Metric Learning (DML) is arguably one of the most influential lines of research for learning visual similarities with many proposed approaches every year. Although the field benefits from the rapid progress, the divergence in training protocols, architectures, and parameter choices make an unbiased comparison difficult. To provide a consistent reference point, we revisit the most widely used DML objective functions and conduct a study of the crucial parameter choices as well as the commonly neglected mini-batch sampling process. Under consistent comparison, DML objectives show much higher saturation than indicated by literature. Further based on our analysis, we uncover a correlation between the embedding space density and compression to the generalization performance of DML models. Exploiting these insights, we propose a simple, yet effective, training regularization to reliably boost the performance of ranking-based DML models on various standard benchmark datasets. Code and a publicly accessible WandB-repo are available at https://github.com/Confusezius/Revisiting_Deep_Metric_Learning_PyTorch.', 'Distance Metric Learning with Joint Representation Diversification Distance metric learning (DML) is to learn a representation space equipped with a metric, such that similar examples are closer than dissimilar examples concerning the metric. The recent success of DNNs motivates many DML losses that encourage the intra-class compactness and inter-class separability. The trade-off between inter-class compactness and inter-class separability shapes the DML representation space by determining how much information of the original inputs to retain. In this paper, we propose a Distance Metric Learning with Joint Representation Diversification (JRD) that allows a better balancing point between intra-class compactness and inter-class separability. Specifically, we propose a Joint Representation Similarity regularizer that captures different abstract levels of invariant features and diversifies the joint distributions of representations across multiple layers. Experiments on three deep DML benchmark datasets demonstrate the effectiveness of the proposed approach.', 'How Classification Baseline Works for Deep Metric Learning: A Perspective of Metric Space Deep Metric Learning (DML) stands as a powerful technique utilized for training models to capture semantic similarities between data points across various domains, including computer vision, natural language processing, and recommendation systems. Current approaches in DML often prioritize the development of novel network structures or loss functions while overlooking metric properties and the intricate relationship between classification and metric learning. This oversight results in significant time overhead, particularly when the number of categories increases. To address this challenge, we propose extending the loss function used in classification to function as a metric, thereby imposing constraints on the distances between training samples based on the triangle inequality. This approach is akin to proxy-based methods and aims to enhance the efficiency of DML. Drawing inspiration from metrically convex metrics, we introduce the concept of a ""weak-metric"" to overcome the limitations associated with certain loss functions that cannot be straightforwardly extended to full metrics. This ensures the effectiveness of DML under various circumstances. Furthermore, we extend the Cross Entropy loss function to function as a weak-metric and introduce a novel metric loss derived from Cross Entropy for experimental comparisons with other methods. The results underscore the credibility and reliability of our proposal, showcasing its superiority over state-of-the-art techniques. Notably, our approach also exhibits significantly faster training times as the number of categories increases, making it a compelling choice for large-scale datasets.']"
68,64,68_hashing_hash_hashes_hashed,"['hashing', 'hash', 'hashes', 'hashed', 'search', 'nearest', 'indexing', 'similarity', 'locality', 'algorithms']","['Approximate Nearest Neighbor Search with Window Filters We define and investigate the problem of <em>c-approximate window search</em>: approximate nearest neighbor search where each point in the dataset has a numeric label, and the goal is to find nearest neighbors to queries within arbitrary label ranges. Many semantic search problems, such as image and document search with timestamp filters, or product search with cost filters, are natural examples of this problem. We propose and theoretically analyze a modular tree-based framework for transforming an index that solves the traditional c-approximate nearest neighbor problem into a data structure that solves window search. On standard nearest neighbor benchmark datasets equipped with random label values, adversarially constructed embeddings, and image search embeddings with real timestamps, we obtain up to a $75\\times$ speedup over existing solutions at the same level of recall.', 'Improved nearest neighbor search using auxiliary information and priority functions Nearest neighbor search using random projection trees has recently been shown to achieve superior performance, in terms of better accuracy while retrieving less number of data points, compared to locality sensitive hashing based methods. However, to achieve acceptable nearest neighbor search accuracy for large scale applications, where number of data points and/or number of features can be very large, it requires users to maintain, store and search through large number of such independent random projection trees, which may be undesirable for many practical applications. To address this issue, in this paper we present different search strategies to improve nearest neighbor search performance of a single random projection tree. Our approach exploits properties of single and multiple random projections, which allows us to store meaningful auxiliary information at internal nodes of a random projection tree as well as to design priority functions to guide the search process that results in improved nearest neighbor search performance. Empirical results on multiple real world datasets show that our proposed method improves the search accuracy of a single tree compared to baseline methods.', 'Densifying One Permutation Hashing via Rotation for Fast Near Neighbor Search The query complexity of \\em locality sensitive hashing (LSH) based similarity search is dominated by the number of hash evaluations, and this number grows with the data size\xa0\\citeProc:Indyk_STOC98. In industrial applications such as search where the data are often high-dimensional and binary (e.g., text n-grams),  \\em minwise hashing is widely adopted, which requires applying a large number  of permutations on the data. This is  costly in computation and energy-consumption.    In this paper, we propose a  hashing technique which generates all the necessary hash evaluations needed for similarity search, using  one single permutation.  The heart of the proposed hash function is a  “rotation” scheme which densifies the sparse sketches of \\em one permutation hashing\xa0\\citeProc:Li_Owen_Zhang_NIPS12 in an unbiased fashion thereby maintaining the LSH property. This makes the obtained sketches suitable for hash table construction. This idea of rotation presented in this paper could be of independent  interest  for densifying  other types of sparse sketches.     Using our proposed hashing method, the  query time of a (K,L)-parameterized LSH is reduced from  the typical O(dKL) complexity to merely O(KL+dL), where d is the  number of nonzeros of the data vector, K is the number of hashes in each hash table, and L is the number of hash tables.  Our experimental evaluation on real data confirms that the proposed scheme significantly reduces the query processing time over minwise hashing without loss in retrieval accuracies.']"
69,21,69_chess_ai_neurips_competitions,"['chess', 'ai', 'neurips', 'competitions', 'ais', 'tournament', 'reconnaissance', 'compete', 'games', 'competition']","['The First International Competition in Machine Reconnaissance Blind Chess Reconnaissance blind chess (RBC) is a chess variant in which a player cannot see her opponent’s pieces but can learn about them through private, explicit sensing actions.  The game presents numerous research challenges, and was the focus of a competition held in conjunction with of the 2019 Conference on Neural Information Processing Systems (NeurIPS).  The 22 bots that played in the tournament leveraged a diverse set of algorithms, including variations of multi-state tracking, piece-wise probability estimation, Gibbs sampling, bandit algorithms, tree search, counterfactual regret minimization (CFR), deep learning, and others.  None of the algorithms of which we are aware converges to an optimal strategy. Top algorithms generally incorporated sensing strategies that successfully minimized uncertainty (as measured in the number of possible opponent states).  The top two approaches reduced this raw uncertainty metric less than some others.  Successful strategies sometimes defied conventional wisdom in chess, as evidenced by deviations between win rate and aggregate move strength as assessed by the leading available chess engine.', 'The Second NeurIPS Tournament of Reconnaissance Blind Chess Reconnaissance Blind Chess is an imperfect-information variant of chess with significant private information that challenges state-of-the-art algorithms. The Johns Hopkins University Applied Physics Laboratory and several organizing partners held the second NeurIPS machine Reconnaissance Blind Chess competition in 2021. 18 bots competed in 9,180 games, revealing a dominant champion with 91% wins. The top four bots in the tournament matched or exceeded the performance of the inaugural tournament’s winner. However, none of the algorithms converge to an optimal, unexploitable strategy or appear to have addressed the core research challenges associated with Reconnaissance Blind Chess.', 'The Machine Reconnaissance Blind Chess Tournament of NeurIPS 2022 Reconnaissance Blind Chess is a game that plays like regular chess but rather than continuously observing the entire board, each player can only momentarily and privately observe selected board regions.  It has imperfect information and little common knowledge.  The Johns Hopkins University Applied Physics Laboratory (the game’s creator) and several partners organized the third NeurIPS machine Reconnaissance Blind Chess competition in 2022 to bring people together to attempt to tackle research challenges presented by the game.  18 bots played each other in 9,180 games (60 matches per bot pair) over 4 days.  The top bot exceeded the performance of all of last year’s bots yet a practical, sound (unexploitable) algorithm remains unknown.']"
70,23,70_routes_route_metaheuristic_routing,"['routes', 'route', 'metaheuristic', 'routing', 'heuristics', 'heuristic', 'vrp', 'vrps', 'learning', 'vehicles']","['Learning 3-opt heuristics for traveling salesman problem via deep reinforcement learning Traveling salesman problem (TSP) is a classical combinatorial optimization problem. As it represents a large number of important practical problems, it has received extensive studies and a great variety of algorithms have been proposed to solve it, including exact and heuristic algorithms. The success of heuristic algorithms relies heavily on the design of powerful heuristic rules, and most of the existing heuristic rules were manually designed by experienced experts to model their insights and observations on TSP instances and solutions. Recent studies have shown an alternative promising design strategy that directly learns heuristic rules from TSP instances without any manual interference. Here, we report an iterative improvement approach (called Neural-3-OPT) that solves TSP through automatically learning effective 3-opt heuristics via deep reinforcement learning. In the proposed approach, we adopt a pointer network to select 3 links from the current tour,and a feature-wise linear modulation network to select an appropriate way to reconnect the segments after removing the selected 3 links. We demonstrate that our approach achieves state-of-the-art performance on both real TSP instances and randomly-generated instances than, to the best of our knowledge, the existing neural network-based approaches.', 'The EURO Meets NeurIPS 2022 Vehicle Routing Competition Solving vehicle routing problems (VRPs) is an essential task for many industrial applications. Although VRPs have been traditionally studied in the operations research (OR) domain, they have lately been the subject of extensive work in the machine learning (ML) community. Both the OR and ML communities have begun to integrate ML into their methods, but in vastly different ways. While the OR community primarily relies on simplistic ML methods, the ML community generally uses deep learning, but fails to outperform OR baselines. To address this gap, the <em>EURO Meets NeurIPS 2022 Vehicle Routing Competition</em> brought together the OR and ML communities as a joint effort of several previous competitions to solve a challenging VRP variant on real-world data provided by ORTEC, a leading provider of vehicle routing software. The challenge focuses on both a ""classic"" deterministic VRP with time windows (VRPTW) and a dynamic version in which new orders arrive over the course of a day. Over 50 teams submitted solutions over a 13-week submission period, battling for not only the best performance on the competition problems, but also for the longest dominance of the leaderboard. The goals of the competition were achieved, with both state-of-the-art techniques in OR and ML playing a significant role in several of the winning submissions.', 'Towards Omni-generalizable Neural Methods for Vehicle Routing Problems Learning heuristics for vehicle routing problems (VRPs) has gained much attention due to the less reliance on hand-crafted rules. However, existing methods are typically trained and tested on the same task with a fixed size and distribution (of nodes), and hence suffer from limited generalization performance. This paper studies a challenging yet realistic setting, which considers generalization across both size and distribution in VRPs. We propose a generic meta-learning framework, which enables effective training of an initialized model with the capability of fast adaptation to new tasks during inference. We further develop a simple yet efficient approximation method to reduce the training overhead. Extensive experiments on both synthetic and benchmark instances of the traveling salesman problem (TSP) and capacitated vehicle routing problem (CVRP) demonstrate the effectiveness of our method. The code is available at: https://github.com/RoyalSkye/Omni-VRP.']"
71,13,71_timeseries_warping_regularization_dtw,"['timeseries', 'warping', 'regularization', 'dtw', 'spatiotemporal', 'dtwbased', 'divergence', 'divergences', 'dtan', 'alignments']","['Soft-DTW: a Differentiable Loss Function for Time-Series We propose in this paper a differentiable learning loss between time series, building upon the celebrated dynamic time warping (DTW) discrepancy. Unlike the Euclidean distance, DTW can compare time series of variable size and is robust to shifts or dilatations across the time dimension. To compute DTW, one typically solves a minimal-cost alignment problem between two time series using dynamic programming. Our work takes advantage of a smoothed formulation of DTW, called soft-DTW, that computes the soft-minimum of all alignment costs. We show in this paper that soft-DTW is a <em>differentiable</em> loss function, and that both its value and gradient can be computed with quadratic time/space complexity (DTW has quadratic time but linear space complexity). We show that this regularization is particularly well suited to average and cluster time series under the DTW geometry, a task for which our proposal significantly outperforms existing baselines (Petitjean et al., 2011). Next, we propose to tune the parameters of a machine that outputs time series by minimizing its fit with ground-truth labels in a soft-DTW sense. Source code is available at https://github.com/mblondel/soft-dtw', 'Random Warping Series: A Random Features Method for Time-Series Embedding Time series data analytics has been a problem of substantial interests for decades, and Dynamic Time Warping (DTW) has been the most widely adopted technique to measure dissimilarity between time series. A number of global-alignment kernels have since been proposed in the spirit of DTW to extend its use to kernel-based estimation method such as support vector machine. However, those kernels suffer from diagonal dominance of the Gram matrix and a quadratic complexity w.r.t. the sample size. In this work, we study a family of alignment-aware positive definite (p.d.) kernels, with its feature embedding given by a distribution of Random Warping Series (RWS). The proposed kernel does not suffer from the issue of diagonal dominance while naturally enjoys a Random Features (RF) approximation, which reduces the computational complexity of existing DTW-based techniques from quadratic to linear in terms of both the number and the length of time-series. We also study the convergence of the RF approximation for the domain of time series of unbounded length. Our extensive experiments on 16 benchmark datasets demonstrate that RWS outperforms or matches state-of-the-art classification and clustering methods in both accuracy and computational time.', 'Spatio-temporal alignments: Optimal transport through space and time     Comparing data defined over space and time is notoriously hard. It involves quantifying both spatial and temporal variability while taking into account the chronological structure of the data. Dynamic Time Warping (DTW) computes a minimal cost alignment between time series that preserves the chronological order but is inherently blind to spatio-temporal shifts. In this paper, we propose Spatio-Temporal Alignments (STA), a new differentiable formulation of DTW that captures spatial and temporal variability. Spatial differences between time samples are captured using regularized Optimal transport. While temporal alignment cost exploits a smooth variant of DTW called soft-DTW. We show how smoothing DTW leads to alignment costs that increase quadratically with time shifts. The costs are expressed using an unbalanced Wasserstein distance to cope with observations that are not probabilities. Experiments on handwritten letters and brain imaging data confirm our theoretical findings and illustrate the effectiveness of STA as a dissimilarity for spatio-temporal data.']"
72,26,72_quantile_quantiles_prediction_forecasting,"['quantile', 'quantiles', 'prediction', 'forecasting', 'forecasts', 'regression', 'predictions', 'estimation', 'predictive', 'estimates']","['Relaxed Quantile Regression: Prediction Intervals for Asymmetric Noise Constructing valid prediction intervals rather than point estimates is a well-established approach for uncertainty quantification in the regression setting. Models equipped with this capacity output an interval of values in which the ground truth target will fall with some prespecified probability. This is an essential requirement in many real-world applications where simple point predictions’ inability to convey the magnitude and frequency of errors renders them insufficient for high-stakes decisions. Quantile regression is a leading approach for obtaining such intervals via the empirical estimation of quantiles in the (non-parametric) distribution of outputs. This method is simple, computationally inexpensive, interpretable, assumption-free, and effective. However, it does require that the specific quantiles being learned are chosen a priori. This results in (a) intervals that are arbitrarily symmetric around the median which is sub-optimal for realistic skewed distributions, or (b) learning an excessive number of intervals. In this work, we propose Relaxed Quantile Regression (RQR), a direct alternative to quantile regression based interval construction that removes this arbitrary constraint whilst maintaining its strengths. We demonstrate that this added flexibility results in intervals with an improvement in desirable qualities (e.g. mean width) whilst retaining the essential coverage guarantees of quantile regression.', 'High-Dimensional Structured Quantile Regression Quantile regression aims at modeling the conditional median and quantiles of a response variable given certain predictor variables. In this work we consider the problem of linear quantile regression in high dimensions where the number of predictor variables is much higher than the number of samples available for parameter estimation. We assume the true parameter to have some structure characterized as having a small value according to some atomic norm R(.) and consider the norm regularized quantile regression estimator. We characterize the sample complexity for consistent recovery and give non-asymptotic bounds on the estimation error. While this problem has been previously considered, our analysis reveals geometric and statistical characteristics of the problem not available in prior literature. We perform experiments on synthetic data which support the theoretical results.', ' Learning Quantile Functions without Quantile Crossing for Distribution-free Time Series Forecasting   Quantile regression is an effective technique to quantify uncertainty, fit challenging underlying distributions, and often provide full probabilistic predictions through joint learnings over multiple quantile levels. A common drawback of these joint quantile regressions, however, is quantile crossing, which violates the desirable monotone property of the conditional quantile function. In this work, we propose the Incremental (Spline) Quantile Functions I(S)QF, a flexible and efficient distribution-free quantile estimation framework that resolves quantile crossing with a simple neural network layer. Moreover, I(S)QF inter/extrapolate to predict arbitrary quantile levels that differ from the underlying training ones. Equipped with the analytical evaluation of the continuous ranked probability score of I(S)QF representations, we apply our methods to NN-based times series forecasting cases, where the savings of the expensive re-training costs for non-trained quantile levels is particularly significant. We also provide a generalization error analysis of our proposed approaches under the sequence-to-sequence setting. Lastly, extensive experiments demonstrate the improvement of consistency and accuracy errors over other baselines. ']"
73,10,73_forecast_forecasts_forecasting_weather4cast,"['forecast', 'forecasts', 'forecasting', 'weather4cast', 'meteorological', 'weather2k', 'weather', 'precipitation', 'climate', 'prediction']","['ClimaX: A foundation model for weather and climate Recent data-driven approaches based on machine learning aim to directly solve a downstream forecasting or projection task by learning a data-driven functional mapping using deep neural networks. However, these networks are trained using curated and homogeneous climate datasets for specific spatiotemporal tasks, and thus lack the generality of currently used computationally intensive physics-informed numerical models for weather and climate modeling. We develop and demonstrate ClimaX, a flexible and generalizable deep learning model for weather and climate science that can be trained using heterogeneous datasets spanning different variables, spatio-temporal coverage, and physical groundings. ClimaX extends the Transformer architecture with novel encoding and aggregation blocks that allow effective use of available compute and data while maintaining general utility. ClimaX is pretrained with a self-supervised learning objective on climate datasets derived from CMIP6. The pretrained ClimaX can then be fine-tuned to address a breadth of climate and weather tasks, including those that involve atmospheric variables and spatio-temporal scales unseen during pretraining. Compared to existing data-driven baselines, we show that this generality in ClimaX results in superior performance on benchmarks for weather forecasting and climate projections, even when pretrained at lower resolutions and compute budgets. Our source code is available at https://github.com/microsoft/ClimaX.', 'Weather2K: A Multivariate Spatio-Temporal Benchmark Dataset for Meteorological Forecasting Based on Real-Time Observation Data from Ground Weather Stations Weather forecasting is one of the cornerstones of meteorological work. In this paper, we present a new benchmark dataset named Weather2K, which aims to make up for the deficiencies of existing weather forecasting datasets in terms of real-time, reliability, and diversity, as well as the key bottleneck of data quality. To be specific, our Weather2K is featured from the following aspects: 1) Reliable and real-time data. The data is hourly collected from 2,130 ground weather stations covering an area of 6 million square kilometers. 2) Multivariate meteorological variables. 20 meteorological factors and 3 constants for position information are provided with a length of 40,896 time steps. 3) Applicable to diverse tasks. We conduct a set of baseline tests on time series forecasting and spatio-temporal forecasting. To the best of our knowledge, our Weather2K is the first attempt to tackle weather forecasting task by taking full advantage of the strengths of observation data from ground weather stations. Based on Weather2K, we further propose Meteorological Factors based Multi-Graph Convolution Network (MFMGCN), which can effectively construct the intrinsic correlation among geographic locations based on meteorological factors. Sufficient experiments show that MFMGCN improves both the forecasting performance and temporal robustness. We hope our Weather2K can significantly motivate researchers to develop efficient and accurate algorithms to advance the task of weather forecasting. The dataset can be available at https://github.com/bycnfz/weather2k/.', ""GenCast: Diffusion-based ensemble forecasting for medium-range weather arXiv:2312.15796v2 Announce Type: replace \nAbstract: Weather forecasts are fundamentally uncertain, so predicting the range of probable weather scenarios is crucial for important decisions, from warning the public about hazardous weather, to planning renewable energy use. Here, we introduce GenCast, a probabilistic weather model with greater skill and speed than the top operational medium-range weather forecast in the world, the European Centre for Medium-Range Forecasts (ECMWF)'s ensemble forecast, ENS. Unlike traditional approaches, which are based on numerical weather prediction (NWP), GenCast is a machine learning weather prediction (MLWP) method, trained on decades of reanalysis data. GenCast generates an ensemble of stochastic 15-day global forecasts, at 12-hour steps and 0.25 degree latitude-longitude resolution, for over 80 surface and atmospheric variables, in 8 minutes. It has greater skill than ENS on 97.4% of 1320 targets we evaluated, and better predicts extreme weather, tropical cyclones, and wind power production. This work helps open the next chapter in operational weather forecasting, where critical weather-dependent decisions are made with greater accuracy and efficiency.""]"
74,53,74_forecasting_forecast_forecasts_forecaster,"['forecasting', 'forecast', 'forecasts', 'forecaster', 'timeseries', 'prediction', 'predictions', 'lstm', 'models', 'autogluontimeseries']","['Foundations of Sequence-to-Sequence Modeling for Time Series The availability of large amounts of time series data, paired with the performance of deep-learning algorithms on a broad class of problems, has recently led to significant interest in the use of sequence-to-sequence models for time series forecasting. We provide the first theoretical analysis of this time series forecasting framework. We include a comparison of sequence-to-sequence modeling to classical time series models, and as such  our theory can serve as a quantitative guide for practitioners choosing between different modeling methodologies.', 'Unified Training of Universal Time Series Forecasting Transformers Deep learning for time series forecasting has traditionally operated within a one-model-per-dataset framework, limiting its potential to leverage the game-changing impact of large pre-trained models. The concept of <em>universal forecasting</em>, emerging from pre-training on a vast collection of time series datasets, envisions a single Large Time Series Model capable of addressing diverse downstream forecasting tasks. However, constructing such a model poses unique challenges specific to time series data: (i) cross-frequency learning, (ii) accommodating an arbitrary number of variates for multivariate time series, and (iii) addressing the varying distributional properties inherent in large-scale data. To address these challenges, we present novel enhancements to the conventional time series Transformer architecture, resulting in our proposed <b>M</b>asked Enc<b>o</b>der-based Un<b>i</b>ve<b>r</b>s<b>a</b>l T<b>i</b>me Series Forecasting Transformer (<b>Moirai</b>). Trained on our newly introduced Large-scale Open Time Series Archive (LOTSA) featuring over 27B observations across nine domains, Moirai achieves competitive or superior performance as a zero-shot forecaster when compared to full-shot models. Code, data, and model weights can be found at https://github.com/SalesforceAIResearch/uni2ts.', 'Coherent Probabilistic Forecasts for Hierarchical Time Series Many applications require forecasts for a hierarchy comprising a set of time series along with aggregates of subsets of these series. Hierarchical forecasting require not only good prediction accuracy at each level of the hierarchy, but also the coherency between different levels — the property that forecasts add up appropriately across the hierarchy. A fundamental limitation of prior research is the focus on forecasting the mean of each time series. We consider the situation where probabilistic forecasts are needed for each series in the hierarchy, and propose an algorithm to compute predictive distributions rather than mean forecasts only. Our algorithm has the advantage of synthesizing information from different levels in the hierarchy through a sparse forecast combination and a probabilistic hierarchical aggregation. We evaluate the accuracy of our forecasting algorithm on both simulated data and large-scale electricity smart meter data. The results show consistent performance gains compared to state-of-the art methods.']"
75,11,75_forecasting_forecasts_prediction_predicting,"['forecasting', 'forecasts', 'prediction', 'predicting', 'predictive', 'estimations', 'regression', 'schedule', 'estimate', 'future']","['Application of conformal prediction interval estimations to market makers’ net positions  In this study we focus on the application of Conformal Prediction (CP) interval estimations to provide financial Market Makers (MMs) with some “meaningful” forecasts relating to their future short-term position in a given financial market. The idea is that using these market position forecasts, MMs can deploy proactive risk management strategies with a given degree of confidence. We make use of a novel financial time series dataset that comprises the net positions of a given MM over a three year period for trades pertaining to the top-traded Foreign Exchange (FX) symbols. This dataset – \\nolinebreak{NetPositionTimeSeries} – is noisy and complex. The net positions within it are generated from the trades of tens of thousands of clients trading in different directions (buy or sell) and over many different time horizons. We approached the problem of predicting future net position not as one that required an accurate point estimate as this is impossible. Rather we sought to gain a meaningful range of possible position bounds which would nonetheless be invaluable. In this study we tested a range of predictive Machine Learning (ML) techniques. We compared the CP framework to benchmark methods like moving average (MA) and quantile regression (QR). We demonstrate how application of the CP framework gives well calibrated region bounds on the MM net position forecasts.', 'Robust Gas Demand Forecasting With Conformal Prediction  Predicting the future trends of customer gas demand as precisely as possible is vital for securing the supply chain from production to distribution. The operations at Air Liquide require the predictions of a Machine Learning forecaster to be coupled with rigorous Uncertainty Quantification (UQ), building trustworthy and informative prediction intervals. To address these industrial needs, we propose to apply Conformal Prediction (CP), a framework that can provide probabilistic guarantees for any underlying predictive model. The problem is formulated as time series forecasting, which may counter the CP hypothesis of data exchangeability. Nevertheless, our experiments show that CP methods enhance the predictive coverage of the tested UQ approaches. We also test EnbPI, a conformal method designed specifically for time series, and propose a locally adaptive variant. To carry out our experiments with prediction intervals using multiple regression models, we introduce our new python library PUNCC and a novel dataset (around 10k observations) provided by Air Liquide which leverages over 7 years of data of weekly gas consumption.', 'Evaluation of conformal-based probabilistic\n forecasting methods for short-term wind speed\n forecasting We apply Conformal Predictive Distribution Systems\n (CPDS) and a non-exchangeable version of the\n traditional Conformal Prediction (NECP) method to\n short-term wind speed forecasting to generate\n probabilistic forecasts. These are compared to the\n more traditional Quantile Regression Forest (QRF)\n method. A short-term forecast is available from a\n few hours before the forecasted time period and is\n only extended a couple days into the future.  The\n methods are supplied ensemble forecasts as input and\n additionally the Conformal methods are supplied with\n post-processed point forecasts for generating the\n probability distributions. In the NECP case we\n propose a method of producing probability\n distributions by creating sequentially larger\n prediction intervals. The methods are compared\n through a teaching schedule, to mimic a real-world\n setting. For each model update in the teaching\n schedule a grid-search approach is applied to select\n each method’s optimal hyperparameters, respectively.\n The methods are tested out of the box with tweaks to\n few hyperparameters. We also introduce a normalized\n nonconformity score and use it with the conformal\n method that handles data that violates the\n exchangeability assumption. The resulting\n probability distributions are compared to actual\n wind measurements through Continuous Ranked\n Probability Scores (CRPS) as well as their validity\n and efficiency of certain prediction intervals.  Our\n results suggest that the conformal based methods,\n with the pre-trained underlying model, produce\n slightly more conservative but more efficient\n probability distributions than QRF at a lower\n computational cost. We further propose how the\n conformal-based methods could be improved for the\n application to real-world scenarios. ']"
76,102,76_prediction_predictors_predictive_conformal,"['prediction', 'predictors', 'predictive', 'conformal', 'predictor', 'classifiers', 'predictions', 'classification', 'regression', 'confidence']","['Confident Object Detection via Conformal Prediction and Conformal Risk Control: an Application to Railway Signaling Deploying deep learning models in real-world certified systems requires the ability to provide\n confidence estimates that accurately reflect their\n uncertainty. In this paper, we demonstrate the use\n of the conformal prediction framework to construct\n reliable and trustworthy predictors for detecting\n railway signals. Our approach is based on a novel\n dataset that includes images taken from the\n perspective of a train operator and state-of-the-art\n object detectors. We test several conformal\n approaches and introduce a new method based on\n conformal risk control. Our findings demonstrate\n the potential of the conformal prediction framework\n to evaluate model performance and provide practical\n guidance for achieving formally guaranteed\n uncertainty bounds.', 'On the Calibration of Aggregated Conformal Predictors Conformal prediction is a learning framework that produces models that associate with each of their predictions a measure of statistically valid confidence.\n These models are typically constructed on top of traditional machine learning algorithms.\n An important result of conformal prediction theory is that the models produced are provably valid under relatively weak assumptions—in particular,\n their validity is independent of the specific underlying learning algorithm on which they are based.\n Since validity is automatic, much research on conformal predictors has been focused on improving their informational and computational efficiency.\n As part of the efforts in constructing efficient conformal predictors, aggregated conformal predictors were developed,\n drawing inspiration from the field of classification and regression ensembles.\n Unlike early definitions of conformal prediction procedures, the validity of aggregated conformal predictors is not fully understood—while it has been shown\n that they might attain empirical exact validity under certain circumstances,\n their theoretical validity is conditional on additional assumptions that require further clarification.\n In this paper, we show why validity is not automatic for aggregated conformal predictors,\n and provide a revised definition of aggregated conformal predictors that gains approximate validity\n conditional on properties of the underlying learning algorithm.', 'Cross-conformal predictive distributions Conformal predictive systems are a recent modification of conformal predictors that output, in regression problems, probability distributions for labels of test observations rather than set predictions. The extra information provided by conformal predictive systems may be useful, e.g., in decision making problems. Conformal predictive systems inherit the relative computational inefficiency of conformal predictors. In this paper we discuss two computationally efficient versions of conformal predictive systems, which we call split conformal predictive systems and cross-conformal predictive systems, and discuss their advantages and limitations.']"
77,10,77_reinforcement_causal_causalities_causality,"['reinforcement', 'causal', 'causalities', 'causality', 'learning', 'reward', 'learned', 'discovering', 'exploration', 'abstraction']","[' GalilAI: Out-of-Task Distribution Detection using Causal Active Experimentation for Safe Transfer RL   Out-of-distribution (OOD) detection is a well-studied topic in supervised learning. Extending the successes in supervised learning methods to the reinforcement learning (RL) setting, however, is difficult due to the data generating process - RL agents actively query their environment for data and this data is a function of the policy followed by the agent. Thus, an agent could neglect a shift in the environment if its policy did not lead it to explore the aspect of the environment that shifted. Therefore, to achieve safe and robust generalization in RL, there exists an unmet need for OOD detection through active experimentation. Here, we attempt to bridge this lacuna by first - defining a causal framework for OOD scenarios or environments encountered by RL agents in the wild. Then, we propose a novel task - that of Out-of-Task Distribution (OOTD) detection. We introduce an RL agent which actively experiments in a test environment and subsequently concludes whether it is OOTD or not. We name our method GalilAI, in honor of Galileo Galilei, as it also discovers, among other causal processes, that gravitational acceleration is independent of the mass of a body. Finally, we propose a simple probabilistic neural network baseline for comparison, which extends extant Model-Based RL. We find that our method outperforms the baseline significantly. ', 'Causal Discovery and Reinforcement Learning: A Synergistic Integration Both Reinforcement Learning (RL) and Causal Modeling (CM) are indispensable parts in the road for general artificial intelligence, however, they are usually treated separately, despite the fact that both areas can effectively complement each other in problem solving. On one hand, the interventional nature of the data generating process in RL favors the discovery of the underlying causal structure. On the other hand, if an agent knows the possible consequences of its actions, given by causal models, it can make better selections of them, reducing exploration and, therefore, accelerating the learning process. Also, ensuring that such an agent maintains a causal model for the world it operates in, improves interpretability and transfer learning, among other benefits. In this article, we propose a combination strategy to provide an intelligent agent with the ability to simultaneously learn and use causal models in the context of reinforcement learning. The proposed method learns a Causal Dynamic Bayesian Network for each of the agent actions and uses those models to improve the action selection process.  To test our algorithm, experiments were performed on a simple synthetic scenario called the “coffee-task"". Our method achieves better results in policy learning than a traditional model-free algorithm (Q-Learning), and it also learns the underlying causal models. We believe that the results obtained reveal several interesting and challenging directions for future work.', 'Learning Causal Overhypotheses through Exploration in Children and Computational Models Despite recent progress in reinforcement learning (RL), RL algorithms for exploration still remain an active area of research. Existing methods often focus on state-based metrics, which do not con-sider the underlying causal structures of the environment, and while recent research has begun to explore RL environments for causal learning, these environments primarily leverage causal information through causal inference or induction rather than exploration. In contrast, human children—some of the most proficient explorers—have been shown to use causal information to great benefit.In this work, we introduce a novel RL environment designed with a controllable causal structure, which allows us to evaluate exploration strategies used by both agents and children in a unified environment.  In addition, through experimentation on both computation models and children, we demonstrate that there are significant differences between information-gain optimal RL exploration in causal environments and the exploration of children in the same environments. We leverage this new insight to lay the groundwork for future research into efficient exploration and disambiguation of causal structures for RL algorithms.']"
78,68,78_topological_topologically_topology_homology,"['topological', 'topologically', 'topology', 'homology', 'persistent', 'persistence', 'networks', 'manifold', 'homological', 'graphs']","['Optimizing persistent homology based functions Solving optimization tasks based on functions and losses with a topological flavor is a very active and growing field of research in data science and Topological Data Analysis, with applications in non-convex optimization, statistics and machine learning. However, the approaches proposed in the literature are usually anchored to a specific application and/or topological construction, and do not come with theoretical guarantees. To address this issue, we study the differentiability of a general map associated with the most common topological construction, that is, the persistence map. Building on real analytic geometry arguments, we propose a general framework that allows us to define and compute gradients for persistence-based functions in a very simple way. We also provide a simple, explicit and sufficient condition for convergence of stochastic subgradient methods for such functions. This result encompasses all the constructions and applications of topological optimization in the literature. Finally, we provide associated code, that is easy to handle and to mix with other non-topological methods and constraints, as well as some experiments showcasing the versatility of our approach.', 'Convergence rates for persistence diagram estimation in Topological Data Analysis Computational topology  has recently seen an important development toward data analysis, giving birth to Topological Data Analysis. Persistent homology appears as a fundamental tool in this field. We show that  the use of persistent homology can be naturally considered in general statistical frameworks. We establish convergence rates of persistence diagrams associated to data randomly sampled from any compact metric space to a well defined limit diagram encoding the topological features of the support of the measure from which the data have been sampled. Our approach relies on a recent and deep stability result for persistence that allows to relate our problem to support estimation problems (with respect to the Gromov-Hausdorff distance). Some numerical experiments are performed in various contexts to illustrate our results.', 'Rethinking Persistent Homology For Visual Recognition Persistent topological properties of an image serve as an additional descriptor providing an insight that might not be discovered by traditional neural networks. The existing research in this area focuses primarily on efficiently integrating topological properties of the data in the learning process in order to enhance the performance. However, there is no existing study to demonstrate all possible scenarios where introducing topological properties can boost or harm the performance. This paper performs a detailed analysis of the effectiveness of topological properties for image classification in various training scenarios, defined by: the number of training samples, the complexity of the training data and the complexity of the backbone network. We identify the scenarios that benefit the most from topological features, e.g., training simple networks on small datasets. Additionally, we discuss the problem of topological consistency of the datasets which is one of the major bottlenecks for using topological features for classification. We further demonstrate how the topological inconsistency can harm the performance for certain scenarios.']"
79,15,79_reinforcement_markov_games_agents,"['reinforcement', 'markov', 'games', 'agents', 'agent', 'multiagent', 'multiagency', 'policies', 'stochastic', 'multiagents']","['Policy Gradient Play with Networked Agents in Markov Potential Games We introduce a distributed policy gradient play algorithm with networked agents playing Markov potential games. Agents have rewards at each stage of the game, that depend on the joint actions of agents given a common dynamic state. Agents implement parameterized and differentiable policies to take actions against each other. Markov potential assumes the existence of potential value functions. In a differentiable Markov potential game, partial gradients of a potential function are equal to the local gradients with respect to the individual parameters. In this work, agents receive information on other agents’ parameters via a communication network in addition to rewards. Agents then use stochastic gradients with respect to local estimates of joint policy parameters to update their policy parameters. We show that agents’ joint policy converges to a first-order stationary point of Markov potential value function with any type of function approximation, state and action spaces. Numerical experiments confirm the convergence result in the lake game, a Markov potential game.', 'The Complexity of Markov Equilibrium in Stochastic Games We show that computing approximate stationary Markov coarse correlated equilibria (CCE) in general-sum stochastic games is PPAD-hard, even when there are two players, the game is turn-based, the discount factor is an absolute constant, and the approximation is an absolute constant. Our intractability results stand in sharp contrast to the results in normal-form games, where exact CCEs are efficiently computable. A fortiori, our results imply that, in the setting of multi-agent reinforcement learning (MARL), it is computationally hard to learn stationary Markov CCE policies in stochastic games, even when the interaction is two-player and turn-based, and both the discount factor and the desired approximation of the learned policies is an absolute constant. In turn, these results stand in sharp contrast to single-agent reinforcement learning (RL) where near-optimal stationary Markov policies can be computationally efficiently learned. Complementing our intractability results for stationary Markov CCEs, we provide a decentralized algorithm (assuming shared randomness among players) for learning a nonstationary Markov CCE policy with polynomial time and sample complexity in all problem parameters. Previous work for learning Markov CCE policies all required exponential time and sample complexity in the number of players. In balance, our work advocates for the use of nonstationary Markov CCE policies as a computationally and statistically tractable solution concept in MARL, advancing an important and outstanding frontier in machine learning.', 'Breaking the Curse of Multiagents in a Large State Space: RL  in Markov Games with Independent  Linear Function Approximation We propose a new model, \\emph{independent linear Markov game}, for multi-agent reinforcement learning with a large state space and a large number of agents.This is a class of Markov games with \\emph{independent} linear function approximation, where each agent has its own function approximation for the state-action value functions that are {\\it marginalized} by other players’ policies. We design new algorithms for learning the Markov coarse correlated equilibria (CCE) and Markov correlated equilibria (CE) with sample complexity bounds that only scale polynomially with \\emph{each agent’s own function class complexity}, thus breaking the curse of multiagents. In contrast, existing works for Markov games with function approximation have sample complexity bounds scale with the size of the \\emph{joint action space} when specialized to the canonical tabular Markov game setting, which is exponentially large in the number of agents. Our algorithms rely on two key technical innovations:  (1) utilizing policy replay to tackle {\\it non-stationarity} incurred by multiple agents and the use of function approximation; (2) separating learning Markov equilibria and exploration in the Markov games, which allows us to use the full-information no-regret learning oracle instead of the stronger bandit-feedback no-regret learning oracle used in the tabular setting.  Furthermore, we propose an iterative-best-response type algorithm that can learn pure Markov Nash  equilibria in independent linear Markov potential games, with applications in learning in congestion games.In the tabular case, by adapting the  policy replay mechanism for independent linear Markov games, we propose an algorithm with $\\widetilde{O}(\\epsilon^{-2})$ sample complexity to learn Markov CCE, which  improves the state-of-the-art result $\\widetilde{O}(\\epsilon^{-3})$ in \\citep{daskalakis2022complexity}, where $\\epsilon$ is the desired accuracy, and also significantly improves other problem parameters. Furthermore, we design  the first provably efficient  algorithm for learning Markov CE that breaks the curse of multiagents. ']"
80,60,80_reinforcement_agents_agent_multiagent,"['reinforcement', 'agents', 'agent', 'multiagent', 'cooperation', 'rewards', 'reward', 'learning', 'coordination', 'cooperative']","['Learning policy representations in multiagent systems ', 'UneVEn: Universal Value Exploration for Multi-Agent Reinforcement Learning VDN and QMIX are two popular value-based algorithms for cooperative MARL that learn a centralized action value function as a monotonic mixing of per-agent utilities. While this enables easy decentralization of the learned policy, the restricted joint action value function can prevent them from solving tasks that require significant coordination between agents at a given timestep. We show that this problem can be overcome by improving the joint exploration of all agents during training. Specifically, we propose a novel MARL approach called Universal Value Exploration (UneVEn) that learns a set of related tasks simultaneously with a linear decomposition of universal successor features. With the policies of already solved related tasks, the joint exploration process of all agents can be improved to help them achieve better coordination. Empirical results on a set of exploration games, challenging cooperative predator-prey tasks requiring significant coordination among agents, and StarCraft II micromanagement benchmarks show that UneVEn can solve tasks where other state-of-the-art MARL methods fail.', 'Q-value Path Decomposition for Deep Multiagent Reinforcement Learning Recently, deep multiagent reinforcement learning (MARL) has become a highly active research area as many real-world problems can be inherently viewed as multiagent systems. A particularly interesting and widely applicable class of problems is the partially observable cooperative multiagent setting, in which a team of agents learns to coordinate their behaviors conditioning on their private observations and commonly shared global reward signals. One natural solution is to resort to the centralized training and decentralized execution paradigm and during centralized training, one key challenge is the multiagent credit assignment: how to allocate the global rewards for individual agent policies for better coordination towards maximizing system-level’s benefits. In this paper, we propose a new method called Q-value Path Decomposition (QPD) to decompose the system’s global Q-values into individual agents’ Q-values. Unlike previous works which restrict the representation relation of the individual Q-values and the global one, we leverage the integrated gradient attribution technique into deep MARL to directly decompose global Q-values along trajectory paths to assign credits for agents. We evaluate QPD on the challenging StarCraft II micromanagement tasks and show that QPD achieves the state-of-the-art performance in both homogeneous and heterogeneous multiagent scenarios compared with existing cooperative MARL algorithms.']"
81,13,81_agents_agent_cooperative_reinforcement,"['agents', 'agent', 'cooperative', 'reinforcement', 'cooperation', 'multiagent', 'centralized', 'learning', 'decentralized', 'optimal']","['Decentralized Multi-Agents by Imitation of a Centralized Controller We consider a multi-agent reinforcement learning problem where each agent seeks to maximize a shared reward while interacting with other agents, and they may or may not be able to communicate. Typically the agents do not have access to other agent policies and thus each agent is situated in a non-stationary and partially-observable environment. In order to obtain multi-agents that act in a decentralized manner, we introduce a novel algorithm under the popular framework of centralized training, but decentralized execution. This training framework first obtains solutions to a multi- agent problem with a single centralized joint-space learner, which is then used to guide imitation learning for independent decentralized multi-agents. This framework has the flexibility to use any reinforcement learning algorithm to obtain the expert as well as any imitation learning algorithm to obtain the decentralized agents. This is in contrast to other multi-agent learning algorithms that, for example, can require more specific structures. We present some theoretical bounds for our method, and we show that one can obtain decentralized solutions to a multi-agent problem through imitation learning. ', 'Sample and Communication-Efficient Decentralized Actor-Critic Algorithms with Finite-Time Analysis Actor-critic (AC) algorithms have been widely used in decentralized multi-agent systems to learn the optimal joint control policy. However, existing decentralized AC algorithms either need to share agents’ sensitive information or lack communication-efficiency. In this work, we develop decentralized AC and natural AC (NAC) algorithms that avoid sharing agents’ local information and are sample and communication-efficient. In both algorithms, agents share only noisy rewards and use mini-batch local policy gradient updates to ensure high sample and communication efficiency. Particularly for decentralized NAC, we develop a decentralized Markovian SGD algorithm with an adaptive mini-batch size to efficiently compute the natural policy gradient. Under Markovian sampling and linear function approximation, we prove that the proposed decentralized AC and NAC algorithms achieve the state-of-the-art sample complexities $\\mathcal{O}(\\epsilon^{-2}\\ln\\epsilon^{-1})$ and $\\mathcal{O}(\\epsilon^{-3}\\ln\\epsilon^{-1})$, respectively, and achieve an improved communication complexity $\\mathcal{O}(\\epsilon^{-1}\\ln\\epsilon^{-1})$. Numerical experiments demonstrate that the proposed algorithms achieve lower sample and communication complexities than the existing decentralized AC algorithms.', 'Dealing With Non-stationarity in Decentralized Cooperative Multi-Agent Deep Reinforcement Learning via Multi-Timescale Learning Decentralized cooperative multi-agent deep reinforcement learning (MARL) can be a versatile learning framework, particularly in scenarios where centralized training is either not possible or not practical. One of the critical challenges in decentralized deep MARL is the non-stationarity of the learning environment when multiple agents are learning concurrently. A commonly used and efficient scheme for decentralized MARL is independent learning in which agents concurrently update their policies independently of each other. We first show that independent learning does not always converge, while sequential learning where agents update their policies one after another in a sequence is guaranteed to converge to an agent-by-agent optimal solution. In sequential learning, when one agent updates its policy, all other agent’s policies are kept fixed, alleviating the challenge of non-stationarity due to simultaneous updates in other agents’ policies. However, it can be slow because only one agent is learning at any time. Therefore it might also not always be practical. In this work, we propose a decentralized cooperative MARL algorithm based on multi-timescale learning. In multi-timescale learning, all agents learn simultaneously, but at different learning rates. In our proposed method, when one agent updates its policy, other agents are allowed to update their policies as well, but at a slower rate. This speeds up sequential learning, while also minimizing non-stationarity caused by other agents updating concurrently. Multi-timescale learning outperforms state-of-the-art decentralized learning methods on a set of challenging multi-agent cooperative tasks in the epymarl benchmark. This can be seen as a first step towards more general decentralized cooperative deep MARL methods based on multi-timescale learning.']"
82,99,82_games_strategies_strategy_nash,"['games', 'strategies', 'strategy', 'nash', 'play', 'poker', 'equilibrium', 'subgame', 'equilibria', 'game']","['Learning in Multi-Player Stochastic Games We consider the problem of simultaneous learning in stochastic games with many players in the finite-horizon setting. While the typical target solution for a stochastic game is a Nash equilibrium, this is intractable with many players. We instead focus on variants of <em> correlated equilibria</em>, such as those studied for extensive-form games. We begin with a hardness result for the adversarial MDP problem: even for a horizon of 3, obtaining sublinear regret against the best non-stationary policy is NP-hard when both rewards and transitions are adversarial. This implies that convergence to even the weakest natural solution concept—normal-form coarse correlated equilibrium—is not possible via black-box reduction to a no-regret algorithm even in stochastic games with constant horizon (unless $NP\\subseteqBPP$). Instead, we turn to a different target: algorithms which <em> generate</em> an equilibrium when they are used by all players. Our main result is algorithm which generates an <em> extensive-form</em> correlated equilibrium, whose runtime is exponential in the horizon but polynomial in all other parameters. We give a similar algorithm which is polynomial in all parameters for “fast-mixing” stochastic games. We also show a method for efficiently reaching normal-form coarse correlated equilibria in “single-controller” stochastic games which follows the traditional no-regret approach. When shared randomness is available, the two generative algorithms can be extended to give simultaneous regret bounds and converge in the traditional sense.', 'Learning Zero-Sum Simultaneous-Move Markov Games Using Function Approximation and Correlated Equilibrium  In this work, we develop provably efficient reinforcement learning algorithms for two-player zero-sum Markov games with  simultaneous moves. We consider a family of Markov games where the reward function and transition kernel possess a linear structure. Two settings are studied: In the offline setting, we control both players and the goal is to find the Nash Equilibrium efficiently by minimizing the worst-case duality gap.  In the online setting, we control a single player and  play against an arbitrary opponent; the goal is to minimize the regret. For both settings, we propose an optimistic variant of the least-squares minimax value iteration algorithm. We show that our algorithm is computationally efficient and provably achieves an $\\tilde O(\\sqrt{d^3 H^3 T} )$ upper bound on the duality gap and regret, without requiring additional assumptions on the sampling model. We highlight that our setting requires overcoming several new challenges that are absent in MDPs or turn-based Markov games. In particular,  to  achieve optimism under the simultaneous-move games,  we construct both upper and lower confidence bounds of the value function, and then derive the optimistic policy by solving a general-sum matrix game with these bounds as the payoff matrices. As finding the Nash Equilibrium of this general-sum game is computationally hard, our algorithm instead solves for a Coarse Correlated Equilibrium (CCE), which can be obtained efficiently via linear programming. To our best knowledge, such a CCE-based mechanism for implementing optimism has not appeared in the literature and might be of interest in its own right.', 'Learning $\\epsilon$-Nash equilibrium stationary policies in stochastic games with unknown independent chains using online mirror descent We study a subclass of n-player stochastic games, namely, stochastic games with independent chains and unknown transition matrices. In this class of games, players control their own internal Markov chains whose transitions do not depend on the states/actions of other players. However, players’ decisions are coupled through their payoff functions. We assume players can receive only realizations of their payoffs, and that the players can not observe the states and actions of other players, nor do they know the transition probability matrices of their own Markov chain. Relying on a compact dual formulation of the game based on occupancy measures and the technique of confidence set to maintain high-probability estimates of the unknown transition matrices, we propose a fully decentralized mirror descent algorithm to learn an $\\epsilon$-Nash equilibrium stationary policy for this class of games. The proposed algorithm has the desired properties of independence and convergence. Specifically, assuming the existence of a variationally stable Nash equilibrium policy, we show that the proposed algorithm in which players make their decisions independently and in a decentralized fashion converges asymptotically to the stable $\\epsilon$-Nash equilibrium stationary policy with arbitrarily high probability.']"
83,14,83_meanfield_reinforcement_mfgs_agents,"['meanfield', 'reinforcement', 'mfgs', 'agents', 'multiagent', 'mfg', 'agent', 'games', 'mfgsc', 'learning']","['Oracle-free Reinforcement Learning in Mean-Field Games along a Single Sample Path We consider online reinforcement learning in Mean-Field Games (MFGs). Unlike traditional approaches, we alleviate the need for a mean-field oracle by developing an algorithm that approximates the Mean-Field Equilibrium (MFE) using the single sample path of the generic agent. We call this Sandbox Learning, as it can be used as a warm-start for any agent learning in a multi-agent non-cooperative setting. We adopt a two time-scale approach in which an online fixed-point recursion for the mean-field operates on a slower time-scale, in tandem with a control policy update on a faster time-scale for the generic agent. Given that the underlying Markov Decision Process (MDP) of the agent is communicating, we provide finite sample convergence guarantees in terms of convergence of the mean-field and control policy to the mean-field equilibrium. The sample complexity of the Sandbox learning algorithm is $O(\\epsilon^{-4})$ where $\\epsilon$ is the MFE approximation error. This is similar to works which assume access to oracle. Finally, we empirically demonstrate the effectiveness of the sandbox learning algorithm in diverse scenarios, including those where the MDP does not necessarily have a single communicating class.', 'Major-Minor Mean Field Multi-Agent Reinforcement Learning Multi-agent reinforcement learning (MARL) remains difficult to scale to many agents. Recent MARL using Mean Field Control (MFC) provides a tractable and rigorous approach to otherwise difficult cooperative MARL. However, the strict MFC assumption of many independent, weakly-interacting agents is too inflexible in practice. We generalize MFC to instead simultaneously model many similar and few complex agents – as Major-Minor Mean Field Control (M3FC). Theoretically, we give approximation results for finite agent control, and verify the sufficiency of stationary policies for optimality together with a dynamic programming principle. Algorithmically, we propose Major-Minor Mean Field MARL (M3FMARL) for finite agent systems instead of the limiting system. The algorithm is shown to approximate the policy gradient of the underlying M3FC MDP. Finally, we demonstrate its capabilities experimentally in various scenarios. We observe a strong performance in comparison to state-of-the-art policy gradient MARL methods.', 'Global Convergence of Policy Gradient for Linear-Quadratic Mean-Field Control/Game in Continuous Time Recent years have witnessed the success of multi-agent reinforcement learning, which has motivated new research directions for mean-field control (MFC) and mean-field game (MFG), as the multi-agent system can be well approximated by a mean-field problem when the number of agents grows to be very large. In this paper, we study the policy gradient (PG) method for the linear-quadratic mean-field control and game, where we assume each agent has identical linear state transitions and quadratic cost functions. While most recent works on policy gradient for MFC and MFG are based on discrete-time models, we focus on a continuous-time model where some of our analyzing techniques could be valuable to the interested readers. For both the MFC and the MFG, we provide PG update and show that it converges to the optimal solution at a linear rate, which is verified by a synthetic simulation. For the MFG, we also provide sufficient conditions for the existence and uniqueness of the Nash equilibrium.']"
84,172,84_classifiers_classification_labeling_learning,"['classifiers', 'classification', 'labeling', 'learning', 'labeled', 'classifier', 'actively', 'datasets', 'labels', 'label']","['Active Testing: Sample-Efficient Model Evaluation We introduce a new framework for sample-efficient model evaluation that we call active testing. While approaches like active learning reduce the number of labels needed for model training, existing literature largely ignores the cost of labeling test data, typically unrealistically assuming large test sets for model evaluation. This creates a disconnect to real applications, where test labels are important and just as expensive, e.g. for optimizing hyperparameters. Active testing addresses this by carefully selecting the test points to label, ensuring model evaluation is sample-efficient. To this end, we derive theoretically-grounded and intuitive acquisition strategies that are specifically tailored to the goals of active testing, noting these are distinct to those of active learning. As actively selecting labels introduces a bias; we further show how to remove this bias while reducing the variance of the estimator at the same time. Active testing is easy to implement and can be applied to any supervised machine learning method. We demonstrate its effectiveness on models including WideResNets and Gaussian processes on datasets including Fashion-MNIST and CIFAR-100.', 'A Robust Zero-Sum Game Framework for Pool-based Active Learning In this paper, we present a novel robust zero- sum game framework for pool-based active learning grounded on advanced statistical learning theory. Pool-based active learning usually consists of two components, namely, learning of a classifier given labeled data and querying of unlabeled data for labeling. Most previous studies on active learning consider these as two separate tasks and propose various heuristics for selecting important unlabeled data for labeling, which may render the selection of unlabeled examples sub-optimal for minimizing the classification error. In contrast, the present work formulates active learning as a unified optimization framework for learning the classifier, i.e., the querying of labels and the learning of models are unified to minimize a common objective for statistical learning. In addition, the proposed method avoids the issues of many previous algorithms such as inefficiency, sampling bias and sensitivity to imbalanced data distribution. Besides theoretical analysis, we conduct extensive experiments on benchmark datasets and demonstrate the superior performance of the proposed active learning method compared with the state-of-the-art methods.', 'Negative Results for Active Learning with Convex Losses We study the problem of active learning with convex loss functions.  We prove that even under bounded noise constraints, the minimax rates for proper active learning are often no better than passive learning.']"
85,31,85_drift_drifts_drifting_adaptive,"['drift', 'drifts', 'drifting', 'adaptive', 'streams', 'classifiers', 'ensemble', 'classification', 'learning', 'streaming']","['Federated Learning under Distributed Concept Drift Federated Learning (FL) under distributed concept drift is a largely unexplored area. Although concept drift is itself a well-studied phenomenon, it poses particular challenges for FL, because drifts arise staggered in time and space (across clients). Our work is the first to explicitly study data heterogeneity in both dimensions. We first demonstrate that prior solutions to drift adaptation, with their single global model, are ill-suited to staggered drifts, necessitating multiple-model solutions. We identify the problem of drift adaptation as a time-varying clustering problem, and we propose two new clustering algorithms for reacting to drifts based on local drift detection and hierarchical clustering. Empirical evaluation shows that our solutions achieve significantly higher accuracy than existing baselines, and are comparable to an idealized algorithm with oracle knowledge of the ground-truth clustering of clients to concepts at each time step.', 'DriftSurf: Stable-State / Reactive-State Learning under Concept Drift When learning from streaming data, a change in the data distribution, also known as concept drift, can render a previously-learned model inaccurate and require training a new model. We present an adaptive learning algorithm that extends previous drift-detection-based methods by incorporating drift detection into a broader stable-state/reactive-state process. The advantage of our approach is that we can use aggressive drift detection in the stable state to achieve a high detection rate, but mitigate the false positive rate of standalone drift detection via a reactive state that reacts quickly to true drifts while eliminating most false positives. The algorithm is generic in its base learner and can be applied across a variety of supervised learning problems. Our theoretical analysis shows that the risk of the algorithm is (i) statistically better than standalone drift detection and (ii) competitive to an algorithm with oracle knowledge of when (abrupt) drifts occur. Experiments on synthetic and real datasets with concept drifts confirm our theoretical analysis.', 'Towards Non-Parametric Drift Detection via Dynamic Adapting Window Independence Drift Detection (DAWIDD) The notion of concept drift refers to the phenomenon that the distribution, which is underlying the observed data, changes over time; as a consequence machine learning models may become inaccurate and need adjustment. Many online learning schemes include drift detection to actively detect and react to observed changes. Yet, reliable drift detection constitutes a challenging problem in particular in the context of high dimensional data, varying drift characteristics, and the absence of a parametric model such as a classification scheme which reflects the drift. In this paper we present a novel concept drift detection method, Dynamic Adapting Window Independence Drift Detection (DAWIDD), which aims for non-parametric drift detection of diverse drift characteristics. For this purpose, we establish a mathematical equivalence of the presence of drift to the dependency of specific random variables in an according drift process. This allows us to rely on independence tests rather than parametric models or the classification loss, resulting in a fairly robust scheme to universally detect different types of drift, as it is also confirmed in experiments.']"
86,11,86_traffic_traffic4cast_forecasting_prediction,"['traffic', 'traffic4cast', 'forecasting', 'prediction', 'predict', 'gps', 'spatiotemporal', 'networks', 'graphresnets', 'graphresnet']","['Traffic4cast at NeurIPS 2021 - Temporal and Spatial Few-Shot Transfer Learning in Gridded Geo-Spatial Processes The IARAI Traffic4cast competitions at NeurIPS 2019 and 2020 showed that neural networks can successfully predict future traffic conditions 1 hour into the future on simply aggregated GPS probe data in time and space bins. We thus reinterpreted the challenge of forecasting traffic conditions as a movie completion task. U-Nets proved to be the winning architecture, demonstrating an ability to extract relevant features in this complex real-world geo-spatial process. Building on the previous competitions, Traffic4cast 2021 now focuses on the question of model robustness and generalizability across time and space. Moving from one city to an entirely different city, or moving from pre-COVID times to times after COVID hit the world thus introduces a clear domain shift. We thus, for the first time, release data featuring such domain shifts. The competition now covers ten cities over 2 years, providing data compiled from over $10^{12}$ GPS probe data. Winning solutions captured traffic dynamics sufficiently well to even cope with these complex domain shifts. Surprisingly, this seemed to require only the previous 1h traffic dynamic history and static road graph as input. ', 'The surprising efficiency of framing geo-spatial time series forecasting as a video prediction task – Insights from the IARAI Traffic4cast Competition at NeurIPS 2019 Deep Neural Networks models are state-of-the-art solutions in accurately forecasting future video frames in a movie.  A successful video prediction model needs to extract and encode semantic features that describe the complex spatio-temporal correlations within image sequences of the real world.  The IARAI Traffic4cast Challenge of the NeurIPS Competition Track 2019 for the first time introduced the novel argument that this is also highly relevant for urban traffic. By framing traffic prediction as a movie completion task, the challenge requires models to take advantage of complex geo-spatial and temporal patterns of the underlying process. We here report on the success and insights obtained in a first Traffic Map Movie forecasting challenge. Although short-term traffic prediction is considered hard, this novel approach allowed several research groups to successfully predict future traffic states in a purely data-driven manner from pixel space. We here expand on the original rationale, summarize key findings, and discuss promising future directions of the t4c competition at NeurIPS.', 'Traffic4cast at NeurIPS 2020 - yet more on the unreasonable effectiveness of gridded geo-spatial processes The IARAI Traffic4cast competition at NeurIPS 2019 showed that neural networks can successfully predict future traffic conditions 15 minutes into the future on simply aggregated GPS probe data  in time and space bins, thus interpreting the challenge of forecasting traffic conditions as a movie completion task. U-nets proved to be the winning architecture then, demonstrating an ability  to extract relevant features in the complex, real-world, geo-spatial process that is traffic derived from a large data set. The IARAI Traffic4cast challenge at NeurIPS 2020 build on the insights of the previous year and sought to both challenge some assumptions inherent in our 2019 competition design and explore how far this neural network technique can be pushed. We found that the  prediction horizon can be extended successfully to 60 minutes into the future, that there is further evidence that traffic depends more on recent dynamics than on the additional static or dynamic location specific data provided and that a reasonable starting point when exploring a general aggregated geo-spatial process in time and space is a U-net architecture.']"
87,128,87_highway_traffic_autonomous_driving,"['highway', 'traffic', 'autonomous', 'driving', 'racing', 'vehicles', 'road', 'planning', 'vehicle', 'cars']","['Driving Policy Transfer via Modularity and Abstraction End-to-end approaches to autonomous driving have high sample complexity and are difficult to scale to realistic urban driving. Simulation can help end-to-end driving systems by providing a cheap, safe, and diverse training environment. Yet training driving policies in simulation brings up the problem of transferring such policies to the real world. We present an approach to transferring driving policies from simulation to reality via modularity and abstraction. Our approach is inspired by classic driving systems and aims to combine the benefits of modular architectures and end-to-end deep learning approaches. The key idea is to encapsulate the driving policy such that it is not directly exposed to raw perceptual input or low-level vehicle dynamics. We evaluate the presented approach in simulated urban environments and in the real world. In particular, we transfer a driving policy trained in simulation to a 1/5-scale robotic truck that is deployed in a variety of conditions, with no finetuning, on two continents.', 'Learning Realistic Traffic Agents in Closed-loop Realistic traffic simulation is crucial for developing self-driving software in a safe and scalable manner prior to real-world deployment. Typically, imitation learning (IL) is used to learn human-like traffic agents directly from real-world observations collected offline, but without explicit specification of traffic rules, agents trained from IL alone frequently display unrealistic infractions like collisions and driving off the road. This problem is exacerbated in out-of-distribution and long-tail scenarios. On the other hand, reinforcement learning (RL) can train traffic agents to avoid infractions, but using RL alone results in unhuman-like driving behaviors. We propose Reinforcing Traffic Rules (RTR), a holistic closed-loop learning objective to match expert demonstrations under a traffic compliance constraint, which naturally gives rise to a joint IL + RL approach, obtaining the best of both worlds. Our method learns in closed-loop simulations of both nominal scenarios from real-world datasets as well as procedurally generated long-tail scenarios. Our experiments show that RTR learns more realistic and generalizable traffic simulation policies, achieving significantly better tradeoffs between human-like driving and traffic compliance in both nominal and long-tail scenarios. Moreover, when used as a data generation tool for training prediction models, our learned traffic policy leads to considerably improved downstream prediction metrics compared to baseline traffic agents.', 'Deep Traffic Benchmark: Aerial Perception and Driven Behavior Dataset Predicting human driving behavior has always been an important area of autonomous driving research. Existing data on autonomous driving in this area is limited in both perspective and duration. For example, vehicles may block each other on the road, although data from vehicles behind them is useful for research. In addition, driving in this area is constrained by the road environment, and the host vehicle cannot observe the designated area for an extended period of time. To investigate the potential relationship between human driving behavior and traffic conditions, we provide a drone-collected video dataset, Deep Traffic, that includes: (1) aerial footage from a vertical perspective, (2) image and annotation capture for training vehicle destination detection and semantic segmentation model, (3) high-definition map data of the captured area, (4) development scripts for various features. Deep Traffic is the largest and most comprehensive dataset to date, covering both urban and high-speed areas. We believe that this benchmark dataset will greatly facilitate the development of drones to monitor traffic flow and study human driver behavior, and that the capacity of the traffic system is of great importance. All datasets and pre-training results can be downloaded from github project.']"
88,280,88_safety_robust_safetycritical_control,"['safety', 'robust', 'safetycritical', 'control', 'controllers', 'unsafe', 'lyapunov', 'controller', 'adaptive', 'optimization']","['Barrier Bayesian Linear Regression: Online Learning of Control Barrier Conditions for Safety-Critical Control of Uncertain Systems In this work, we consider the problem of designing a safety filter for a nonlinear uncertain control system. Our goal is to augment an arbitrary controller with a safety filter such that the overall closed-loop system is guaranteed to stay within a given state constraint set, referred to as being safe. For systems with known dynamics, control barrier functions (CBFs) provide a scalar condition for determining if a system is safe. For uncertain systems, robust or adaptive CBF certification approaches have been proposed. However, these approaches can be conservative or require the system to have a particular parametric structure. For more generic uncertain systems,   machine learning approaches have been used to approximate the CBF condition. These works typically assume that the learning module is sufficiently trained prior to deployment. Safety during learning is not guaranteed.  We propose a barrier Bayesian linear regression (BBLR) approach that  guarantees safe online learning of the CBF condition for the true, uncertain system. We assume that the error between the  nominal system and the true system is bounded and exploit the structure of the CBF condition. We show that our approach can safely expand the set of certifiable control inputs despite system and learning uncertainties. The effectiveness of our approach is demonstrated in simulation using a two-dimensional pendulum stabilization task.', 'Safe Nonlinear Control Using Robust Neural Lyapunov-Barrier Functions Safety and stability are common requirements for robotic control systems; however, designing safe, stable controllers remains difficult for nonlinear and uncertain models. We develop a model-based learning approach to synthesize robust feedback controllers with safety and stability guarantees. We take inspiration from robust convex optimization and Lyapunov theory to define robust control Lyapunov barrier functions that generalize despite model uncertainty. We demonstrate our approach in simulation on problems including car trajectory tracking, nonlinear control with obstacle avoidance, satellite rendezvous with safety constraints, and flight control with a learned ground effect model. Simulation results show that our approach yields controllers that match or exceed the capabilities of robust MPC while reducing computational costs by an order of magnitude. We provide source code at github.com/dawsonc/neural_clbf/.', ' Online Robust Control of Nonlinear Systems with Large Uncertainty   Robust control is a core approach for controlling systems with performance guarantees that are robust to modeling error, and is widely used in real-world systems. However, current robust control approaches can only handle small system uncertainty, and thus require significant effort in system identification prior to controller design. We present an online approach that robustly controls a nonlinear system under large model uncertainty. Our approach is based on decomposing the problem into two sub-problems, “robust control design” (which assumes small model uncertainty) and “chasing consistent models”, which can be solved using existing tools from control theory and online learning, respectively. We provide a learning convergence analysis that yields a finite mistake bound on the number of times performance requirements are not met and can provide strong safety guarantees, by bounding the worst-case state deviation. To the best of our knowledge, this is the first approach for online robust control of nonlinear systems with such learning theoretic and safety guarantees. We also show how to instantiate this framework for general robotic systems, demonstrating the practicality of our approach. ']"
89,65,89_hawkes_events_event_spatiotemporal,"['hawkes', 'events', 'event', 'spatiotemporal', 'temporal', 'models', 'networks', 'modeling', 'probabilistic', 'prediction']","['A mutually exciting latent space Hawkes process model for continuous-time networks Networks and temporal point processes serve as fundamental building blocks for modeling complex dynamic relational data in various domains. We propose the latent space Hawkes (LSH) model, a novel generative model for continuous-time networks of relational events, using a latent space representation for nodes. We model relational events between nodes using mutually exciting Hawkes processes with baseline intensities dependent upon the distances between the nodes in the latent space and sender and receiver specific effects. We demonstrate that our proposed LSH model can replicate many features observed in real temporal networks including reciprocity and transitivity, while also achieving superior prediction accuracy and providing more interpretable fits than existing models.', 'The Hawkes Edge Partition Model for Continuous-time Event-based Temporal Networks We propose a novel probabilistic framework to model continuously generated interaction events data. Our goal is to infer the \\emph{implicit} community structure underlying the temporal interactions among entities, and also to exploit how the latent structure influence their interaction dynamics. To this end, we model the reciprocating interactions between individuals using mutually-exciting Hawkes processes. The base rate of the Hawkes process for each pair of individuals is built upon the latent representations inferred using the hierarchical gamma process edge partition model (HGaP-EPM). In particular, our model allows the interaction dynamics between each pair of individuals to be modulated by their respective affiliated communities.Moreover, our model can flexibly incorporate the auxiliary individuals’ attributes, or covariates associated with interaction events. Efficient Gibbs sampling and Expectation-Maximization algorithms are developed to perform inference via Pólya-Gamma data augmentation strategy. Experimental results on real-world datasets demonstrate that our model not only achieves competitive performance compared with state-of-the-art methods, but also discovers interpretable latent structure behind the observed temporal interactions.', 'Isotonic Hawkes Processes Hawkes processes are powerful tools for modeling the mutual-excitation phenomena commonly observed in event data from a variety of domains, such as social networks, quantitative finance and healthcare records. The intensity function of a Hawkes process is typically assumed to be linear in the sum of triggering kernels, rendering it inadequate to capture nonlinear effects present in real-world data. To address this shortcoming, we propose an Isotonic-Hawkes process whose intensity function is modulated by an additional nonlinear link function. We also developed a novel iterative algorithm which learns both the nonlinear link function and other parameters provably. We showed that Isotonic-Hawkes processes can fit a variety of nonlinear patterns which cannot be captured by conventional Hawkes processes, and achieve superior empirical performance in real world applications.']"
90,11,90_autoencoders_mri_autoencoder_neuroimaging,"['autoencoders', 'mri', 'autoencoder', 'neuroimaging', 'anomaly', 'denoising', 'outlier', 'imagewise', 'imaging', 'segmentation']","['Patched Diffusion Models for Unsupervised Anomaly Detection in Brain MRI The use of supervised deep learning techniques to detect pathologies in brain MRI scans can be challenging due to the diversity of brain anatomy and the need for annotated data sets. An alternative approach is to use unsupervised anomaly detection, which only requires sample-level labels of healthy brains to create a reference representation. This reference representation can then be compared to unhealthy brain anatomy in a pixel-wise manner to identify abnormalities. To accomplish this, generative models are needed to create anatomically consistent MRI scans of healthy brains. While recent diffusion models have shown promise in this task, accurately generating the complex structure of the human brain remains a challenge. In this paper, we propose a method that reformulates the generation task of diffusion models as a patch-based estimation of healthy brain anatomy, using spatial context to guide and improve reconstruction. We evaluate our approach on data of tumors and multiple sclerosis lesions and demonstrate a relative improvement of 25.1% compared to existing baselines.', 'Unsupervised Brain Anomaly Detection and Segmentation with Transformers Pathological brain appearances may be so heterogeneous as to be intelligible only as anomalies, defined by their deviation from normality rather than any specific pathological characteristic. Amongst the hardest tasks in medical imaging, detecting such anomalies requires models of the normal brain that combine compactness with the expressivity of the complex, long-range interactions that characterise its structural organisation. These are requirements transformers have arguably greater potential to satisfy than other current candidate architectures, but their application has been inhibited by their demands on data and computational resource. Here we combine the latent representation of vector quantised variational autoencoders with an ensemble of autoregressive transformers to enable unsupervised anomaly detection and segmentation defined by deviation from healthy brain imaging data, achievable at low computational cost, within relative modest data regimes. We compare our method to current state-of-the-art approaches across a series of experiments involving synthetic and real pathological lesions. On real lesions, we train our models on 15,000 radiologically normal participants from UK Biobank, and evaluate performance on four different brain MR datasets with small vessel disease, demyelinating lesions, and tumours. We demonstrate superior anomaly detection performance both image-wise and pixel-wise, achievable without post-processing. These results draw attention to the potential of transformers in this most challenging of imaging tasks.', 'Denoising Autoencoders for Unsupervised Anomaly Detection in Brain MRI Pathological brain lesions exhibit diverse appearance in brain images, making it difficult to train supervised detection solutions due to the lack of comprehensive data and annotations. Thus, in this work we tackle unsupervised anomaly detection, using only healthy data for training with the aim of detecting unseen anomalies at test time. Many current approaches employ autoencoders with restrictive architectures (i.e. containing information bottlenecks) that tend to give poor reconstructions of not only the anomalous but also the normal parts of the brain. Instead, we investigate classical denoising autoencoder models that do not require bottlenecks and can employ skip connections to give high resolution fidelity. We design a simple noise generation method of upscaling low-resolution noise that enables high-quality reconstructions. We find that with appropriate noise generation, denoising autoencoder reconstruction errors generalize to hyperintense lesion segmentation and reach state of the art performance for unsupervised tumor detection in brain MRI data, beating more complex methods such as variational autoencoders. We believe this provides a strong and easy-to-implement baseline for further research into unsupervised anomaly detection.']"
91,44,91_neural_fmri_neuroimaging_neuroimage,"['neural', 'fmri', 'neuroimaging', 'neuroimage', 'brain', 'neuronal', 'neuroscience', 'networks', 'connectomics', 'eeg']","['Bayesian Structure Learning for Dynamic Brain Connectivity Human brain activity as measured by fMRI exhibits strong correlations between brain regions which are believed to vary over time. Importantly, dynamic connectivity has been linked to individual differences in physiology, psychology and behavior, and has shown promise as a biomarker for disease. The state of the art in computational neuroimaging is to estimate the brain networks as relatively short sliding window covariance matrices, which leads to high variance estimates, thereby resulting in high overall error.  This manuscript proposes a novel Bayesian model for dynamic brain connectivity. Motivated by the underlying neuroscience, the model estimates covariances which vary smoothly over time, with an instantaneous decomposition into a collection of spatially sparse components – resulting in parsimonious and highly interpretable estimates of dynamic brain connectivity. Simulated results are presented to illustrate the performance of the model even when it is mis-specified. For real brain imaging data with unknown ground truth, in addition to qualitative evaluation, we devise a simple classification task which suggests that the estimated brain networks better capture the underlying structure.', 'FBNETGEN: Task-aware GNN-based fMRI Analysis via Functional Brain Network Generation Functional magnetic resonance imaging (fMRI) is one of the most common imaging modalities to investigate brain functions. Recent studies in neuroscience stress the great potential of functional brain networks constructed from fMRI data for clinical predictions. Traditional functional brain networks, however, are noisy and unaware of downstream prediction tasks, while also incompatible with the deep graph neural network (GNN) models. In order to fully unleash the power of GNNs in network-based fMRI analysis, we develop FBNETGEN, a task-aware and interpretable fMRI analysis framework via deep brain network generation. In particular, we formulate (1) prominent region of interest (ROI) features extraction, (2) brain networks generation, and (3) clinical predictions with GNNs, in an end-to-end trainable model under the guidance of particular prediction tasks. Along with the process, the key novel component is the graph generator which learns to transform raw time-series features into task-oriented brain networks. Our learnable graphs also provide unique interpretations by highlighting prediction-related brain regions. Comprehensive experiments on two datasets, i.e., the recently released and currently largest publicly available fMRI dataset Adolescent Brain Cognitive Development (ABCD), and the widely-used fMRI dataset PNC, prove the superior effectiveness and interpretability of FBNETGEN. The implementation is available at https://github.com/Wayfear/FBNETGEN.', 'DBGSL: Dynamic Brain Graph Structure Learning Recently, graph neural networks (GNNs) have shown success at learning representations of brain graphs derived from functional magnetic resonance imaging (fMRI) data. The majority of existing GNN methods, however, assume brain graphs are static over time and the graph adjacency matrix is known prior to model training. These assumptions are at odds with neuroscientific evidence that brain graphs are time-varying with a connectivity structure that depends on the choice of functional connectivity measure. Noisy brain graphs that do not truly represent the underling fMRI data can have a detrimental impact on the performance of GNNs. As a solution, we propose Dynamic Brain Graph Structure Learning (DBGSL), a novel method for learning the optimal time-varying dependency structure of fMRI data induced by a downstream prediction task. Experiments demonstrate DBGSL achieves state-of-the-art performance for sex classification using real-world resting-state and task fMRI data. Moreover, analysis of the learnt dynamic graphs highlights prediction-related brain regions which align with existing neuroscience literature.']"
92,540,92_segmentation_cnn_microscopy_deep,"['segmentation', 'cnn', 'microscopy', 'deep', 'imaging', 'images', 'dataset', 'datasets', 'convolutional', 'classification']","['Unsupervised Domain Adaptation through Shape Modeling for Medical Image Segmentation Shape information is a strong and valuable prior in segmenting organs in medical images. However, most current deep learning based segmentation algorithms have not taken shape information into consideration, which can lead to bias towards texture. We aim at modeling shape explicitly and using it to help medical image segmentation. Previous methods proposed Variational Autoencoder (VAE) based models to learn the distribution of shape for a particular organ and used it to automatically evaluate the quality of a segmentation prediction by fitting it into the learned shape distribution. Based on which we aim at incorporating VAE into current segmentation pipelines. Specifically, we propose a new unsupervised domain adaptation pipeline based on a pseudo loss and a VAE reconstruction loss under a teacher-student learning paradigm. Both losses are optimized simultaneously and, in return, boost the segmentation task performance. Extensive experiments on three public Pancreas segmentation datasets as well as two in-house Pancreas segmentation datasets show consistent improvements with at least 2.8 points gain in the Dice score, demonstrating the effectiveness of our method in challenging unsupervised domain adaptation scenarios for medical image segmentation. We hope this work will advance shape analysis and geometric learning in medical imaging.', 'Upscaling Prostate Cancer MRI Images to Cell-level Resolution Using Self-supervised Learning Magnetic Resonance Imaging (MRI) plays a pivotal role in medical imaging, particularly in the diagnosis and treatment of cancers via radiography. However, one of the limitations of MRI is its low spatial resolution, which can hinder the accurate detection and characterization of cancerous lesions, especially those that are small or subtle in nature. There is a growing need for advancements in MRI technology to improve the resolution of MRI, particularly in the field of oncology, where precise detection and segmentation of tumors are crucial for effective treatment planning and optimal patient outcomes. In this paper, we proposed a self-supervised deep learning technique to upscale cancer MRI images to cell-level resolution with pathology Whole Slide Imaging (WSI). By integrating information from pathology WSIs with MRI images, this approach aims to create hybrid images that offer a more detailed and comprehensive view of cancer tissue structures. We evaluated our techniques using prostate lesions both on the similarity metrics and downstream segmentation tasks. For the similarity, our reconstructed fusion images can achieve an average 0.933 in structural similarity index. We improved lesion segmentation dice score from 57.3% to 64.0% on the test cases. Such fusion of the two imaging modalities shows promise for improving the accuracy and reliability of cancer diagnosis, guiding treatment decisions, and ultimately improving patient outcomes.', 'Diffusion Models for Implicit Image Segmentation Ensembles Diffusion models have shown impressive performance for generative modelling of images. In this paper, we present a novel semantic segmentation method based on diffusion models. By modifying the training and sampling scheme, we show that diffusion models can perform lesion segmentation of medical images. To generate an image-specific segmentation, we train the model on the ground truth segmentation, and use the image as a prior during training and in every step during the sampling process. With the given stochastic sampling process, we can generate a distribution of segmentation masks. This property allows us to compute pixel-wise uncertainty maps of the segmentation, and allows an implicit ensemble of segmentations that increases the segmentation performance. We evaluate our method on the BRATS2020 dataset for brain tumor segmentation. Compared to state-of-the-art segmentation models, our approach yields good segmentation results and, additionally, detailed uncertainty maps.']"
93,67,93_grasping_grasp_grasps_tactile,"['grasping', 'grasp', 'grasps', 'tactile', 'gripper', 'visuotactile', 'grippers', 'touch', 'robotic', 'sensing']","['Towards Learning to Detect and Predict Contact Events on Vision-based Tactile Sensors In essence, successful grasp boils down to correct responses to multiple contact events between fingertips and objects. In most scenarios, tactile sensing is adequate to distinguish contact events. Due to the nature of high dimensionality of tactile information, classifying spatiotemporal tactile signals using conventional model-based methods is difficult. In this work, we propose to predict and classify tactile signal using deep learning methods, seeking to enhance the adaptability of the robotic grasp system to external event changes that may lead to grasping failure. We develop a deep learning framework and collect 6650 tactile image sequences with a vision-based tactile sensor, and the neural network is integrated into a contact-event-based robotic grasping system. In grasping experiments, we achieved 52% increase in terms of object lifting success rate with contact detection, significantly higher robustness under unexpected loads with slip prediction compared with open-loop grasps, demonstrating that integration of the proposed framework into robotic grasping system substantially improves picking success rate and capability to withstand external disturbances.', 'Releasing the Dexterity Network (Dex-Net) 2.0 Dataset for Deep Grasping <p>Reliable robot grasping across many objects is challenging due to sensor\nnoise and occlusions that lead to uncertainty about the precise shape, position, and mass of objects.\nThe Dexterity Network (Dex-Net) 2.0 is a project centered on using physics-based models of robust robot grasping to generate massive datasets of parallel-jaw grasps across thousands of 3D CAD object models.\nThese datasets are used to train deep neural networks to plan grasps from a point clouds on a physical robot that can lift and transport a wide variety of objects.</p>\n\n<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>\n<div class=""embed-container"">    <iframe title=""YouTube video player"" width=""640"" height=""390"" src=""//www.youtube.com/embed/i6K3GI2_EgU"" frameborder=""0"" allowfullscreen=""""></iframe></div>\n\n<p>To facilitate reproducibility and future research, this blog post announces the release of the:</p>\n\n<ol>\n  <li><strong>Dexterity Network (Dex-Net) 2.0 dataset:</strong> 6.7 million pairs of synthetic point clouds and grasps with robustness labels. [<a href=""http://bit.ly/2rIM7Jk"">link to data folder</a>]</li>\n  <li><strong>Grasp Quality CNN (GQ-CNN) model:</strong> 18 million parameters trained on the Dex-Net 2.0 dataset. [<a href=""http://bit.ly/2tAFMko"">link to our models</a>]</li>\n  <li><strong>GQ-CNN Python Package:</strong> Code to replicate our GQ-CNN training results on synthetic data (note System Requirements below). [<a href=""https://berkeleyautomation.github.io/gqcnn/"">link to code</a>].</li>\n</ol>\n\n<p>In the post, we also summarize the methods behind Dex-Net 2.0 (1), our experimental results on a real robot, and details on the datasets, models, and code.</p>\n\n<p>Research papers and additional information on the Dexterity Network can be found on the project website: <a href=""https://berkeleyautomation.github.io/dex-net"">https://berkeleyautomation.github.io/dex-net</a>.</p>\n\n<p>Dex-Net is a project in the <a href=""http://autolab.berkeley.edu/"">AUTOLAB</a> at UC Berkeley that is advised by <a href=""http://goldberg.berkeley.edu/"">Prof. Ken Goldberg</a>.</p>\n\n<!--more-->\n\n<h1 id=""background-on-grasping"">Background on Grasping</h1>\n<p>Robot grasping across many objects is difficult due to sensor\nnoise and occlusions, which make it challenging to precisely infer physical\nproperties such as object shape, pose, material properties, mass, and the\nlocations of contact points between the fingers and object. Recent results\nsuggest that deep neural networks trained on large datasets of human grasp\nlabels (2) or trials of grasping on a physical system (3) can be used to plan\nsuccessful grasps across a wide variety of objects directly from images (4) with\nno explicit modeling of physics, similar to generalization results seen in\ncomputer vision. However, the training datasets may be time consuming to generate.</p>\n\n<p>To reduce training time, one alternative is to use Cloud Computing to rapidly compute grasps across a\nlarge dataset of object mesh models (5) using physics-based models of grasping\n(6). These methods rank grasps by a quantity called the <em>grasp robustness</em>, which is the probability of grasp success predicted by models from mechanics, such as\nwhether or not the grasp can resist arbitrary forces and torques\naccording to probability distributions over properties such as object position and surface friction (7).\nHowever, these methods make the strong assumption of a perception system that estimates these properties either\nperfectly or according to known Gaussian distributions. In practice, these\nperception systems are slow, prone to errors, and may not generalize well to new\nobjects. Despite over 30 years of research, in practice it is common to\nplan grasps using heuristics such as detecting cylinders in applications such as\nhome decluttering (8) and the Amazon Picking Challenge (9).</p>\n\n<!--more-->\n\n<h1 id=""the-dexterity-network-dex-net-20"">The Dexterity Network (Dex-Net) 2.0</h1>\n\n<p>Rather than attempt to estimate 3D object shape and\npose from images, Dex-Net 2.0 uses a probabilistic model to\ngenerate synthetic point clouds, grasps, and grasp robustness labels from datasets\nof 3D object meshes (10) using physics-based models of grasping, image rendering, and camera noise.\nThe main insight behind the method is that robust parallel-jaw grasps of an object are strongly correlated with the shape of the object.\nThese geometric affordances for grasping, such as handles and cylinders,\nare visible in partial point clouds and their correlation with grasping will evident in samples from the model.\nWe hypothesize that Deep CNNs are able to learn these correlations using a hierarchical set of filters that recognize geometric primitives, similar to the Gabor-like\nfilters learned by CNNs for image classification (11).</p>\n\n<p>We formalize and study this approach in our paper, <a href=""http://berkeleyautomation.github.io/dex-net/#dexnet_2"">“Dex-Net 2.0: Deep Learning\nto Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp\nMetrics.”</a> In the paper we detail the\nDexterity Network (Dex-Net) 2.0, a dataset of 6.7 million robust grasps and\npoint clouds with synthetic noise generated from our probabilistic model of\ngrasping rigid objects on a tabletop with a parallel-jaw gripper. We develop a\ndeep Grasp Quality Convolutional Neural Network (GQ-CNN) model and train it on\nDex-Net 2.0 to estimate grasp robustness from a candidate grasp and point cloud.\nWe use the GQ-CNN to plan grasps on a physical\nrobot by sampling a set of grasp candidates from an input point cloud with edge\ndetection and executing the most robust grasp estimated by the GQ-CNN:</p>\n\n<p><img src=""http://i.imgur.com/ChTDZKW.png"" alt=""grasping_policy"" title=""Dex-Net 2.0 Grasping Policy"" /></p>\n\n<p>When trained on Dex-Net 2.0, the GQ-CNN learns a set of low-level filters that\nappear to detect image gradients at various scales. Filters can be organized\ninto two classes: coarse oriented gradient filters that may be useful for\nestimating collisions between the gripper and object and fine vertical filters\nthat may be useful for estimating surface normals at the locations of contact\nbetween the fingers and object:</p>\n\n<p><img src=""http://i.imgur.com/OelLCuA.png"" alt=""grasp_quality_cnn"" title=""Grasp Quality Convolutional Neural Network"" /></p>\n\n<h1 id=""experiments-with-the-abb-yumi"">Experiments with the ABB YuMi</h1>\n\n<p>To evaluate GQ-CNN-based grasp planning on a physical robot, we ran over 1,000\ntrials of grasping on an <a href=""http://new.abb.com/products/robotics/industrial-robots/yumi"">ABB YuMi</a> to investigate:</p>\n\n<ol>\n  <li><strong>Model Performance:</strong> Can a GQ-CNN trained entirely on synthetic data for a\nset of known objects be used to successfully grasp the objects on a physical\nrobot?</li>\n  <li><strong>Generalization:</strong> Can the GQ-CNN be used to successfully grasp novel\nobjects that were not seen in training?</li>\n</ol>\n\n<h3 id=""model-performance"">Model Performance</h3>\n\n<p>We first measured the ability of our method to plan grasps that could maintain a\ngrasp on the object while lifting the object, transporting it, and shaking it\nwithin the gripper. We used a set of eight 3D printed objects with known\nshape, center of mass, and frictional properties to highlight differences\nbetween our physical models and grasping on the physical robot. To explore failure modes, we\nchose objects with <em>adversarial</em> geometry for two-finger grippers such as smooth,\ncurved surfaces and narrow openings.</p>\n\n<p>We found that the Dex-Net 2.0 grasp planner could achieve up to 93% success on\nthe physical robot and was 3x faster than a method that matched the exact object\nshape to the point cloud. The results suggest that our physics-based model is a\nuseful proxy for grasp outcomes on a physical robot when object properties are\nknown and that the GQ-CNN can be used to plan highly precise grasps. Here’s an\nexample:</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/blog/assets/dexnet2/dex-net.gif"" alt=""dexnet_gif"" />\n</p>\n\n<h3 id=""generalization"">Generalization</h3>\n\n<p>We also evaluated the ability to generalize to previously unseen objects by\ntesting on a set of 40 novel objects including objects with moving parts and deformation, such\nas a can opener and a washcloth. After analyzing the\ndata further we found a surprising result: the GQ-CNN had just one false\npositive out of 69 grasps it predicted to succeed. This 99% precision score\nis important because it suggests that the robot could anticipate failures based on its confidence labels and\nperform recovery actions such as poking objects or asking a human for help.</p>\n\n<h1 id=""limitations"">Limitations</h1>\n\n<p>The results of grasp planning with Dex-Net 2.0 suggest that it is possible to\nachieve highly reliable grasping across a wide variety of objects by training\nneural networks on only synthetic data that is generated using physical models of\ngrasping and image formation. However, there are several limitations of the\ncurrent method:</p>\n\n<ol>\n  <li><strong>Sensor Capabilities.</strong> Some sources of noise on the physical\ndepth camera, such as missing data, are not accounted for by the Dex-Net\n2.0 model. Furthermore, depth cameras cannot see objects that are transparent or\nflat on a table.</li>\n  <li><strong>Model Limitations.</strong> The physical model of grasping used by Dex-Net 2.0\nconsiders fingertip grasps of rigid objects. We do not account for grasping\nstrategies such as pinching a flat piece of paper into the gripper or hooking an\nobject with a finger.</li>\n  <li><strong>Single Objects.</strong> The method is designed to only grasp objects in\nisolation. We are currently working on extending the Dex-Net 2.0 model to\ngrasping objects from a pile.</li>\n  <li><strong>Task-Independence.</strong> The method plans grasps that can be used to robustly\nlift and transport an object but does not consider use cases of an object such\nas exact placement, stacking, or connecting it to another object in assembly\nwhich may require more precise grasps. We are researching possible extensions with\ntask-based grasp quality metrics, dynamic simulation, and learning from\ndemonstration.</li>\n</ol>\n\n<h1 id=""dataset-and-code-release"">Dataset and Code Release</h1>\n\n<p>Over summer 2017, we are releasing a subset of our code, datasets, and the trained GQ-CNN weights  which we hope will facilitate further research and comparisons.</p>\n\n<p>Today we’re releasing the <strong><a href=""https://berkeleyautomation.github.io/gqcnn/"">Dex-Net 2.0 Training Dataset and Code</a></strong>, which includes the Dex-Net 2.0 dataset with 6.7 million synthetic datapoints, pretrained GQ-CNN models from the paper, and the <em>gqcnn</em> Python package for replicating our experiments on classifying robust grasps on synthetic data with GQ-CNNs.\nWe hope this will facilitate development of new GQ-CNN architectures and training methods that perform better on both synthetic datasets and datasets collected with our robot.\nYou can access the release with these links: [<a href=""http://bit.ly/2rIM7Jk"">datasets</a>] [<a href=""http://bit.ly/2tAFMko"">models</a>] [<a href=""https://berkeleyautomation.github.io/gqcnn/"">code</a>]</p>\n\n<h3 id=""system-requirements"">System Requirements</h3>\n<p>Please note that strong performance on this particular dataset may not be indicative of performance on other robots because the dataset is specific to:\n1) The ABB YuMi gripper due to collision geometry.\n2) A Primesense Carmine 1.08 sensor due to camera parameters used in rendering.\n3) The set of poses of the camera relative to the table: 50-70 centimeters directly above a table looking straight down.</p>\n\n<p>Nonetheless, the algorithms behind the dataset can be used to generate datasets for other two-finger grippers, cameras, and camera poses relative to the robot.\nWe hypothesize that GQ-CNN-based grasp planning will perform best if the training datasets are generated using the gripper geometry, camera intrinsics, and camera location specific to the hardware setup.</p>\n\n<h3 id=""abb-yumi-benchmark"">ABB YuMi Benchmark</h3>\n<p>We plan to keep <a href=""https://berkeleyautomation.github.io/gqcnn/benchmarks/benchmarks.html"">a leaderboard of performance</a> on the Dex-Net 2.0 dataset to\ninvestigate improvements to the GQ-CNN architecture, since our best models\nachieve only 93% classification accuracy on synthetic data.\nSince datasets are specific to a hardware setup, we volunteer to benchmark performance on the physical robot for models that we deed <em>signficantly</em>\noutperform other methods on synthetic data.\nWe invite researchers from any discipline or background to participate.</p>\n\n<h3 id=""python-package"">Python Package</h3>\n<p>To aid in training GQ-CNNs, we developed <a href=""https://berkeleyautomation.github.io/gqcnn/"">the <em>gqcnn</em> Python package</a>.\nUsing <em>gqcnn</em>, you can quickly get started training GQ-CNNs on datasets generated with Dex-Net 2.0.\nThere are tutorials to replicate the results from our RSS paper, and we invite researchers to try to improve classification performance on synthetic datasets as well as datasets of grasps collected with our physical ABB YuMi robot.</p>\n\n<p>We’re also working a ROS service for grasp planning with GQ-CNNs.\nThe ROS package will enable users to see the results of grasp planning with GQ-CNNs on custom point clouds.\nWe encourage interested parties to set up a Primesense Carmine 1.08 or Microsoft Kinect for Xbox 360 roughly 50-70 cm above a table and attempt grasps planned by a GQ-CNN-based grasp planner.\nWhile our dataset may not generalize to other hardware setups as noted above, we hope that with further research it may be possible to use GQ-CNNs for lifting and transporting objects with other robots.\nIf you are interested in a research collaboration on such a project, please email <a href=""http://www.jeff-mahler.com"">Jeff Mahler</a> (jmahler@berkeley.edu).</p>\n\n<h3 id=""future-releases"">Future Releases</h3>\n<p>We are also aiming for the following releases and dates of additional data and functionality from Dex-Net over summer and fall 2017:</p>\n<ul>\n  <li><strong>Dex-Net Object Mesh Dataset v1.1:</strong> The subset of 1,500 3D object models from Dex-Net 1.0 used in the RSS paper, labeled with Parallel-Jaw grasps for the ABB YuMi (14). <em>July 12, 2017.</em></li>\n  <li><strong>Dex-Net as a Service:</strong> HTTP web API to create new databases with custom 3D models and compute grasp robustness metrics. <em>Fall 2017.</em></li>\n</ul>\n\n<h1 id=""contact"">Contact</h1>\n<p>See <a href=""http://berkeleyautomation.github.io/dex-net/"">the project website</a> for updates and progress.</p>\n\n<p>For more information please contact <a href=""http://www.jeff-mahler.com"">Jeff Mahler</a> or <a href=""http://goldberg.berkeley.edu/"">Prof. Ken Goldberg</a>\nof <a href=""http://autolab.berkeley.edu/"">the Berkeley AUTOLAB</a>.</p>\n\n<h3 id=""acknowledgments"">Acknowledgments</h3>\n\n<p>This research was performed at the <a href=""http://autolab.berkeley.edu/"">AUTOLAB</a> at UC\nBerkeley in affiliation with the Berkeley AI Research (BAIR) Lab, the Real-Time\nIntelligent Secure Execution (RISE) Lab, and the CITRIS People and Robots (CPAR)\nInitiative. The authors were supported in part by the U.S. National Science\nFoundation under NRI Award IIS-1227536: Multilateral Manipulation by Human-Robot\nCollaborative Systems, the Department of Defense (DoD) through the National\nDefense Science &amp; Engineering Graduate Fellowship (NDSEG) Program, the Berkeley\nDeep Drive (BDD) Program, and by donations from Siemens, Google, Cisco,\nAutodesk, IBM, Amazon Robotics, and Toyota Robotics Institute. Any opinions,\nfindings, and conclusions or recommendations expressed in this material are\nthose of the author(s) and do not necessarily reflect the views of the Sponsors.</p>\n\n<h3 id=""references"">References</h3>\n\n<p>(1): Mahler, Jeffrey, Jacky Liang, Sherdil Niyaz, Michael Laskey, Richard Doan, Xinyu Liu, Juan Aparicio Ojea, and Ken Goldberg. “Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics.” arXiv preprint arXiv:1703.09312 (2017). <a href=""https://arxiv.org/abs/1703.09312"">(Paper)</a> <a href=""http://berkeleyautomation.github.io/dex-net/"">(Website)</a></p>\n\n<p>(2): Kappler, Daniel, Jeannette Bohg, and Stefan Schaal. “Leveraging Big Data for Grasp Planning.” In Robotics and Automation (ICRA), 2015 IEEE International Conference on, pp. 4304-4311. IEEE, 2015.</p>\n\n<p>(3): Levine, Sergey, Peter Pastor, Alex Krizhevsky, and Deirdre Quillen. “Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection.” arXiv preprint arXiv:1603.02199 (2016).</p>\n\n<p>(4): Johns, Edward, Stefan Leutenegger, and Andrew J. Davison. “Deep Learning a Grasp Function for Grasping under Gripper Pose Uncertainty.” In Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference on, pp. 4461-4468. IEEE, 2016.</p>\n\n<p>(5): Goldfeder, Corey, Matei Ciocarlie, Hao Dang, and Peter K. Allen. “The Columbia Grasp Database.” In Robotics and Automation, 2009. ICRA’09. IEEE International Conference on, pp. 1710-1716. IEEE, 2009.</p>\n\n<p>(6): Prattichizzo, Domenico, and Jeffrey C. Trinkle. “Grasping.” In Springer Handbook of Robotics, pp. 955-988. Springer International Publishing, 2016.</p>\n\n<p>(7): Weisz, Jonathan, and Peter K. Allen. “Pose Error Robust Grasping from Contact Wrench Space Metrics.” In Robotics and Automation (ICRA), 2012 IEEE International Conference on, pp. 557-562. IEEE, 2012.</p>\n\n<p>(8): Ciocarlie, Matei, Kaijen Hsiao, Edward Gil Jones, Sachin Chitta, Radu Bogdan Rusu, and Ioan A. Şucan. “Towards Reliable Grasping and Manipulation in Household Environments.” In Experimental Robotics, pp. 241-252. Springer Berlin Heidelberg, 2014.</p>\n\n<p>(9): Hernandez, Carlos, Mukunda Bharatheesha, Wilson Ko, Hans Gaiser, Jethro Tan, Kanter van Deurzen, Maarten de Vries et al. “Team Delft’s Robot Winner of the Amazon Picking Challenge 2016.” arXiv preprint arXiv:1610.05514 (2016).</p>\n\n<p>(10): Mahler, Jeffrey, Florian T. Pokorny, Brian Hou, Melrose Roderick, Michael Laskey, Mathieu Aubry, Kai Kohlhoff, Torsten Kröger, James Kuffner, and Ken Goldberg. “Dex-Net 1.0: A Cloud-Based Network of 3D Objects for Robust Grasp Planning using a Multi-Armed Bandit Model with Correlated Rewards.” In Robotics and Automation (ICRA), 2016 IEEE International Conference on, pp. 1957-1964. IEEE, 2016.</p>\n\n<p>(11): Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. “Imagenet Classification with Deep Convolutional Neural Networks.” In Advances in Neural Information Processing Systems, pp. 1097-1105. 2012.</p>\n\n', 'Releasing the Dexterity Network (Dex-Net) 2.0 Dataset for Deep Grasping <p>Reliable robot grasping across many objects is challenging due to sensor\nnoise and occlusions that lead to uncertainty about the precise shape, position, and mass of objects.\nThe Dexterity Network (Dex-Net) 2.0 is a project centered on using physics-based models of robust robot grasping to generate massive datasets of parallel-jaw grasps across thousands of 3D CAD object models.\nThese datasets are used to train deep neural networks to plan grasps from a point clouds on a physical robot that can lift and transport a wide variety of objects.</p>\n\n<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>\n<div class=""embed-container"">    <iframe title=""YouTube video player"" width=""640"" height=""390"" src=""//www.youtube.com/embed/i6K3GI2_EgU"" frameborder=""0"" allowfullscreen=""""></iframe></div>\n\n<p>To facilitate reproducibility and future research, this blog post announces the release of the:</p>\n\n<ol>\n  <li><strong>Dexterity Network (Dex-Net) 2.0 dataset:</strong> 6.7 million pairs of synthetic point clouds and grasps with robustness labels. [<a href=""http://bit.ly/2rIM7Jk"">link to data folder</a>]</li>\n  <li><strong>Grasp Quality CNN (GQ-CNN) model:</strong> 18 million parameters trained on the Dex-Net 2.0 dataset. [<a href=""http://bit.ly/2tAFMko"">link to our models</a>]</li>\n  <li><strong>GQ-CNN Python Package:</strong> Code to replicate our GQ-CNN training results on synthetic data (note System Requirements below). [<a href=""https://berkeleyautomation.github.io/gqcnn/"">link to code</a>].</li>\n</ol>\n\n<p>In the post, we also summarize the methods behind Dex-Net 2.0 (1), our experimental results on a real robot, and details on the datasets, models, and code.</p>\n\n<p>Research papers and additional information on the Dexterity Network can be found on the project website: <a href=""https://berkeleyautomation.github.io/dex-net"">https://berkeleyautomation.github.io/dex-net</a>.</p>\n\n<p>Dex-Net is a project in the <a href=""http://autolab.berkeley.edu/"">AUTOLAB</a> at UC Berkeley that is advised by <a href=""http://goldberg.berkeley.edu/"">Prof. Ken Goldberg</a>.</p>\n\n<!--more-->\n\n<h1 id=""background-on-grasping"">Background on Grasping</h1>\n<p>Robot grasping across many objects is difficult due to sensor\nnoise and occlusions, which make it challenging to precisely infer physical\nproperties such as object shape, pose, material properties, mass, and the\nlocations of contact points between the fingers and object. Recent results\nsuggest that deep neural networks trained on large datasets of human grasp\nlabels (2) or trials of grasping on a physical system (3) can be used to plan\nsuccessful grasps across a wide variety of objects directly from images (4) with\nno explicit modeling of physics, similar to generalization results seen in\ncomputer vision. However, the training datasets may be time consuming to generate.</p>\n\n<p>To reduce training time, one alternative is to use Cloud Computing to rapidly compute grasps across a\nlarge dataset of object mesh models (5) using physics-based models of grasping\n(6). These methods rank grasps by a quantity called the <em>grasp robustness</em>, which is the probability of grasp success predicted by models from mechanics, such as\nwhether or not the grasp can resist arbitrary forces and torques\naccording to probability distributions over properties such as object position and surface friction (7).\nHowever, these methods make the strong assumption of a perception system that estimates these properties either\nperfectly or according to known Gaussian distributions. In practice, these\nperception systems are slow, prone to errors, and may not generalize well to new\nobjects. Despite over 30 years of research, in practice it is common to\nplan grasps using heuristics such as detecting cylinders in applications such as\nhome decluttering (8) and the Amazon Picking Challenge (9).</p>\n\n<!--more-->\n\n<h1 id=""the-dexterity-network-dex-net-20"">The Dexterity Network (Dex-Net) 2.0</h1>\n\n<p>Rather than attempt to estimate 3D object shape and\npose from images, Dex-Net 2.0 uses a probabilistic model to\ngenerate synthetic point clouds, grasps, and grasp robustness labels from datasets\nof 3D object meshes (10) using physics-based models of grasping, image rendering, and camera noise.\nThe main insight behind the method is that robust parallel-jaw grasps of an object are strongly correlated with the shape of the object.\nThese geometric affordances for grasping, such as handles and cylinders,\nare visible in partial point clouds and their correlation with grasping will evident in samples from the model.\nWe hypothesize that Deep CNNs are able to learn these correlations using a hierarchical set of filters that recognize geometric primitives, similar to the Gabor-like\nfilters learned by CNNs for image classification (11).</p>\n\n<p>We formalize and study this approach in our paper, <a href=""http://berkeleyautomation.github.io/dex-net/#dexnet_2"">“Dex-Net 2.0: Deep Learning\nto Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp\nMetrics.”</a> In the paper we detail the\nDexterity Network (Dex-Net) 2.0, a dataset of 6.7 million robust grasps and\npoint clouds with synthetic noise generated from our probabilistic model of\ngrasping rigid objects on a tabletop with a parallel-jaw gripper. We develop a\ndeep Grasp Quality Convolutional Neural Network (GQ-CNN) model and train it on\nDex-Net 2.0 to estimate grasp robustness from a candidate grasp and point cloud.\nWe use the GQ-CNN to plan grasps on a physical\nrobot by sampling a set of grasp candidates from an input point cloud with edge\ndetection and executing the most robust grasp estimated by the GQ-CNN:</p>\n\n<p><img src=""http://i.imgur.com/ChTDZKW.png"" alt=""grasping_policy"" title=""Dex-Net 2.0 Grasping Policy"" /></p>\n\n<p>When trained on Dex-Net 2.0, the GQ-CNN learns a set of low-level filters that\nappear to detect image gradients at various scales. Filters can be organized\ninto two classes: coarse oriented gradient filters that may be useful for\nestimating collisions between the gripper and object and fine vertical filters\nthat may be useful for estimating surface normals at the locations of contact\nbetween the fingers and object:</p>\n\n<p><img src=""http://i.imgur.com/OelLCuA.png"" alt=""grasp_quality_cnn"" title=""Grasp Quality Convolutional Neural Network"" /></p>\n\n<h1 id=""experiments-with-the-abb-yumi"">Experiments with the ABB YuMi</h1>\n\n<p>To evaluate GQ-CNN-based grasp planning on a physical robot, we ran over 1,000\ntrials of grasping on an <a href=""http://new.abb.com/products/robotics/industrial-robots/yumi"">ABB YuMi</a> to investigate:</p>\n\n<ol>\n  <li><strong>Model Performance:</strong> Can a GQ-CNN trained entirely on synthetic data for a\nset of known objects be used to successfully grasp the objects on a physical\nrobot?</li>\n  <li><strong>Generalization:</strong> Can the GQ-CNN be used to successfully grasp novel\nobjects that were not seen in training?</li>\n</ol>\n\n<h3 id=""model-performance"">Model Performance</h3>\n\n<p>We first measured the ability of our method to plan grasps that could maintain a\ngrasp on the object while lifting the object, transporting it, and shaking it\nwithin the gripper. We used a set of eight 3D printed objects with known\nshape, center of mass, and frictional properties to highlight differences\nbetween our physical models and grasping on the physical robot. To explore failure modes, we\nchose objects with <em>adversarial</em> geometry for two-finger grippers such as smooth,\ncurved surfaces and narrow openings.</p>\n\n<p>We found that the Dex-Net 2.0 grasp planner could achieve up to 93% success on\nthe physical robot and was 3x faster than a method that matched the exact object\nshape to the point cloud. The results suggest that our physics-based model is a\nuseful proxy for grasp outcomes on a physical robot when object properties are\nknown and that the GQ-CNN can be used to plan highly precise grasps. Here’s an\nexample:</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/blog/assets/dexnet2/dex-net.gif"" alt=""dexnet_gif"" />\n</p>\n\n<h3 id=""generalization"">Generalization</h3>\n\n<p>We also evaluated the ability to generalize to previously unseen objects by\ntesting on a set of 40 novel objects including objects with moving parts and deformation, such\nas a can opener and a washcloth. After analyzing the\ndata further we found a surprising result: the GQ-CNN had just one false\npositive out of 69 grasps it predicted to succeed. This 99% precision score\nis important because it suggests that the robot could anticipate failures based on its confidence labels and\nperform recovery actions such as poking objects or asking a human for help.</p>\n\n<h1 id=""limitations"">Limitations</h1>\n\n<p>The results of grasp planning with Dex-Net 2.0 suggest that it is possible to\nachieve highly reliable grasping across a wide variety of objects by training\nneural networks on only synthetic data that is generated using physical models of\ngrasping and image formation. However, there are several limitations of the\ncurrent method:</p>\n\n<ol>\n  <li><strong>Sensor Capabilities.</strong> Some sources of noise on the physical\ndepth camera, such as missing data, are not accounted for by the Dex-Net\n2.0 model. Furthermore, depth cameras cannot see objects that are transparent or\nflat on a table.</li>\n  <li><strong>Model Limitations.</strong> The physical model of grasping used by Dex-Net 2.0\nconsiders fingertip grasps of rigid objects. We do not account for grasping\nstrategies such as pinching a flat piece of paper into the gripper or hooking an\nobject with a finger.</li>\n  <li><strong>Single Objects.</strong> The method is designed to only grasp objects in\nisolation. We are currently working on extending the Dex-Net 2.0 model to\ngrasping objects from a pile.</li>\n  <li><strong>Task-Independence.</strong> The method plans grasps that can be used to robustly\nlift and transport an object but does not consider use cases of an object such\nas exact placement, stacking, or connecting it to another object in assembly\nwhich may require more precise grasps. We are researching possible extensions with\ntask-based grasp quality metrics, dynamic simulation, and learning from\ndemonstration.</li>\n</ol>\n\n<h1 id=""dataset-and-code-release"">Dataset and Code Release</h1>\n\n<p>Over summer 2017, we are releasing a subset of our code, datasets, and the trained GQ-CNN weights  which we hope will facilitate further research and comparisons.</p>\n\n<p>Today we’re releasing the <strong><a href=""https://berkeleyautomation.github.io/gqcnn/"">Dex-Net 2.0 Training Dataset and Code</a></strong>, which includes the Dex-Net 2.0 dataset with 6.7 million synthetic datapoints, pretrained GQ-CNN models from the paper, and the <em>gqcnn</em> Python package for replicating our experiments on classifying robust grasps on synthetic data with GQ-CNNs.\nWe hope this will facilitate development of new GQ-CNN architectures and training methods that perform better on both synthetic datasets and datasets collected with our robot.\nYou can access the release with these links: [<a href=""http://bit.ly/2rIM7Jk"">datasets</a>] [<a href=""http://bit.ly/2tAFMko"">models</a>] [<a href=""https://berkeleyautomation.github.io/gqcnn/"">code</a>]</p>\n\n<h3 id=""system-requirements"">System Requirements</h3>\n<p>Please note that strong performance on this particular dataset may not be indicative of performance on other robots because the dataset is specific to:\n1) The ABB YuMi gripper due to collision geometry.\n2) A Primesense Carmine 1.08 sensor due to camera parameters used in rendering.\n3) The set of poses of the camera relative to the table: 50-70 centimeters directly above a table looking straight down.</p>\n\n<p>Nonetheless, the algorithms behind the dataset can be used to generate datasets for other two-finger grippers, cameras, and camera poses relative to the robot.\nWe hypothesize that GQ-CNN-based grasp planning will perform best if the training datasets are generated using the gripper geometry, camera intrinsics, and camera location specific to the hardware setup.</p>\n\n<h3 id=""abb-yumi-benchmark"">ABB YuMi Benchmark</h3>\n<p>We plan to keep <a href=""https://berkeleyautomation.github.io/gqcnn/benchmarks/benchmarks.html"">a leaderboard of performance</a> on the Dex-Net 2.0 dataset to\ninvestigate improvements to the GQ-CNN architecture, since our best models\nachieve only 93% classification accuracy on synthetic data.\nSince datasets are specific to a hardware setup, we volunteer to benchmark performance on the physical robot for models that we deed <em>signficantly</em>\noutperform other methods on synthetic data.\nWe invite researchers from any discipline or background to participate.</p>\n\n<h3 id=""python-package"">Python Package</h3>\n<p>To aid in training GQ-CNNs, we developed <a href=""https://berkeleyautomation.github.io/gqcnn/"">the <em>gqcnn</em> Python package</a>.\nUsing <em>gqcnn</em>, you can quickly get started training GQ-CNNs on datasets generated with Dex-Net 2.0.\nThere are tutorials to replicate the results from our RSS paper, and we invite researchers to try to improve classification performance on synthetic datasets as well as datasets of grasps collected with our physical ABB YuMi robot.</p>\n\n<p>We’re also working a ROS service for grasp planning with GQ-CNNs.\nThe ROS package will enable users to see the results of grasp planning with GQ-CNNs on custom point clouds.\nWe encourage interested parties to set up a Primesense Carmine 1.08 or Microsoft Kinect for Xbox 360 roughly 50-70 cm above a table and attempt grasps planned by a GQ-CNN-based grasp planner.\nWhile our dataset may not generalize to other hardware setups as noted above, we hope that with further research it may be possible to use GQ-CNNs for lifting and transporting objects with other robots.\nIf you are interested in a research collaboration on such a project, please email <a href=""http://www.jeff-mahler.com"">Jeff Mahler</a> (jmahler@berkeley.edu).</p>\n\n<h3 id=""future-releases"">Future Releases</h3>\n<p>We are also aiming for the following releases and dates of additional data and functionality from Dex-Net over summer and fall 2017:</p>\n<ul>\n  <li><strong>Dex-Net Object Mesh Dataset v1.1:</strong> The subset of 1,500 3D object models from Dex-Net 1.0 used in the RSS paper, labeled with Parallel-Jaw grasps for the ABB YuMi (14). <em>July 12, 2017.</em></li>\n  <li><strong>Dex-Net as a Service:</strong> HTTP web API to create new databases with custom 3D models and compute grasp robustness metrics. <em>Fall 2017.</em></li>\n</ul>\n\n<h1 id=""contact"">Contact</h1>\n<p>See <a href=""http://berkeleyautomation.github.io/dex-net/"">the project website</a> for updates and progress.</p>\n\n<p>For more information please contact <a href=""http://www.jeff-mahler.com"">Jeff Mahler</a> or <a href=""http://goldberg.berkeley.edu/"">Prof. Ken Goldberg</a>\nof <a href=""http://autolab.berkeley.edu/"">the Berkeley AUTOLAB</a>.</p>\n\n<h3 id=""acknowledgments"">Acknowledgments</h3>\n\n<p>This research was performed at the <a href=""http://autolab.berkeley.edu/"">AUTOLAB</a> at UC\nBerkeley in affiliation with the Berkeley AI Research (BAIR) Lab, the Real-Time\nIntelligent Secure Execution (RISE) Lab, and the CITRIS People and Robots (CPAR)\nInitiative. The authors were supported in part by the U.S. National Science\nFoundation under NRI Award IIS-1227536: Multilateral Manipulation by Human-Robot\nCollaborative Systems, the Department of Defense (DoD) through the National\nDefense Science &amp; Engineering Graduate Fellowship (NDSEG) Program, the Berkeley\nDeep Drive (BDD) Program, and by donations from Siemens, Google, Cisco,\nAutodesk, IBM, Amazon Robotics, and Toyota Robotics Institute. Any opinions,\nfindings, and conclusions or recommendations expressed in this material are\nthose of the author(s) and do not necessarily reflect the views of the Sponsors.</p>\n\n<h3 id=""references"">References</h3>\n\n<p>(1): Mahler, Jeffrey, Jacky Liang, Sherdil Niyaz, Michael Laskey, Richard Doan, Xinyu Liu, Juan Aparicio Ojea, and Ken Goldberg. “Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics.” arXiv preprint arXiv:1703.09312 (2017). <a href=""https://arxiv.org/abs/1703.09312"">(Paper)</a> <a href=""http://berkeleyautomation.github.io/dex-net/"">(Website)</a></p>\n\n<p>(2): Kappler, Daniel, Jeannette Bohg, and Stefan Schaal. “Leveraging Big Data for Grasp Planning.” In Robotics and Automation (ICRA), 2015 IEEE International Conference on, pp. 4304-4311. IEEE, 2015.</p>\n\n<p>(3): Levine, Sergey, Peter Pastor, Alex Krizhevsky, and Deirdre Quillen. “Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection.” arXiv preprint arXiv:1603.02199 (2016).</p>\n\n<p>(4): Johns, Edward, Stefan Leutenegger, and Andrew J. Davison. “Deep Learning a Grasp Function for Grasping under Gripper Pose Uncertainty.” In Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference on, pp. 4461-4468. IEEE, 2016.</p>\n\n<p>(5): Goldfeder, Corey, Matei Ciocarlie, Hao Dang, and Peter K. Allen. “The Columbia Grasp Database.” In Robotics and Automation, 2009. ICRA’09. IEEE International Conference on, pp. 1710-1716. IEEE, 2009.</p>\n\n<p>(6): Prattichizzo, Domenico, and Jeffrey C. Trinkle. “Grasping.” In Springer Handbook of Robotics, pp. 955-988. Springer International Publishing, 2016.</p>\n\n<p>(7): Weisz, Jonathan, and Peter K. Allen. “Pose Error Robust Grasping from Contact Wrench Space Metrics.” In Robotics and Automation (ICRA), 2012 IEEE International Conference on, pp. 557-562. IEEE, 2012.</p>\n\n<p>(8): Ciocarlie, Matei, Kaijen Hsiao, Edward Gil Jones, Sachin Chitta, Radu Bogdan Rusu, and Ioan A. Şucan. “Towards Reliable Grasping and Manipulation in Household Environments.” In Experimental Robotics, pp. 241-252. Springer Berlin Heidelberg, 2014.</p>\n\n<p>(9): Hernandez, Carlos, Mukunda Bharatheesha, Wilson Ko, Hans Gaiser, Jethro Tan, Kanter van Deurzen, Maarten de Vries et al. “Team Delft’s Robot Winner of the Amazon Picking Challenge 2016.” arXiv preprint arXiv:1610.05514 (2016).</p>\n\n<p>(10): Mahler, Jeffrey, Florian T. Pokorny, Brian Hou, Melrose Roderick, Michael Laskey, Mathieu Aubry, Kai Kohlhoff, Torsten Kröger, James Kuffner, and Ken Goldberg. “Dex-Net 1.0: A Cloud-Based Network of 3D Objects for Robust Grasp Planning using a Multi-Armed Bandit Model with Correlated Rewards.” In Robotics and Automation (ICRA), 2016 IEEE International Conference on, pp. 1957-1964. IEEE, 2016.</p>\n\n<p>(11): Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. “Imagenet Classification with Deep Convolutional Neural Networks.” In Advances in Neural Information Processing Systems, pp. 1097-1105. 2012.</p>\n\n']"
94,49,94_neural_neuralsymbolic_symbolic_semantics,"['neural', 'neuralsymbolic', 'symbolic', 'semantics', 'learning', 'semantic', 'reasoning', 'knowledge', 'logic', 'representations']","['Learning Continuous Semantic Representations of Symbolic Expressions Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. As a step in this direction, we propose a new architecture, called neural equivalence network, for the problem of learning continuous semantic representations of algebraic and logical expressions. These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different. The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures. We perform an exhaustive evaluation on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures.', 'Analysis for Abductive Learning and Neural-Symbolic Reasoning Shortcuts Abductive learning models (ABL) and neural-symbolic predictive models (NeSy) have been recently shown effective, as they allow us to infer labels that are consistent with some prior knowledge by reasoning over high-level concepts extracted from sub-symbolic inputs. However, their generalization ability is affected by reasoning shortcuts: high accuracy on given targets but leveraging intermediate concepts with unintended semantics. Although there have been techniques to alleviate reasoning shortcuts, theoretical efforts on this issue remain to be limited. This paper proposes a simple and effective analysis to quantify harm caused by it and how can mitigate it. We quantify three main factors in how NeSy algorithms are affected by reasoning shortcuts: the complexity of the knowledge base, the sample size, and the hypothesis space. In addition, we demonstrate that ABL can reduce shortcut risk by selecting specific distance functions in consistency optimization, thereby demonstrating its potential and approach to solving shortcut problems. Empirical studies demonstrate the rationality of the analysis. Moreover, the proposal is suitable for many ABL and NeSy algorithms and can be easily extended to handle other cases of reasoning shortcuts.', 'Closed Loop Neural-Symbolic Learning via Integrating Neural Perception, Grammar Parsing, and Symbolic Reasoning The goal of neural-symbolic computation is to integrate the connectionist and symbolist paradigms. Prior methods learn the neural-symbolic models using reinforcement learning (RL) approaches, which ignore the error propagation in the symbolic reasoning module and thus converge slowly with sparse rewards. In this paper, we address these issues and close the loop of neural-symbolic learning by (1) introducing the grammar model as a symbolic prior to bridge neural perception and symbolic reasoning, and (2) proposing a novel back-search algorithm which mimics the top-down human-like learning procedure to propagate the error through the symbolic reasoning module efficiently. We further interpret the proposed learning framework as maximum likelihood estimation using Markov chain Monte Carlo sampling and the back-search algorithm as a Metropolis-Hastings sampler. The experiments are conducted on two weakly-supervised neural-symbolic tasks: (1) handwritten formula recognition on the newly introduced HWF dataset; (2) visual question answering on the CLEVR dataset. The results show that our approach significantly outperforms the RL methods in terms of performance, converging speed, and data efficiency. Our code and data are released at https://liqing-ustc.github.io/NGS.']"
95,24,95_gaze_gazeformermd_eyetracking_gazebased,"['gaze', 'gazeformermd', 'eyetracking', 'gazebased', 'gazetransformer', 'gazeclr', 'eye', 'eyeclosure', 'attention', 'visual']","['Cracking the Code of Live Human Social Interactions in Autism: A Review of the Eye-Tracking Literature Human social interaction involves a complex, dynamic exchange of verbal and non-verbal information. Over the last decade, eye-tracking technology has afforded unique insight into the way eye gaze information, including both holding gaze and shifting gaze, organizes live human interactions. For example, while playing a social game together, speakers end their turn by directing gaze at the listener, who begins to speak with averted gaze (Ho et al., 2015). These findings reflect how eye gaze can be used to signal important turn-taking transitions in social interactions. Deficits in conversational turn-taking is a core feature of autism spectrum disorders. Individuals on the autism spectrum also have notable difficulties processing eye gaze information (Griffin {&} Scherf, 2020). A central hypothesis in the literature is that the difficulties in processing eye gaze information are foundational to the social communication deficits that make social interactions so challenging for individuals on the autism spectrum. Although eye-tracking technology has been used extensively to assess the way individuals on the spectrum attend to stimuli presented on computer screens (for review see Papagiannopoulou et al., 2014), it has rarely been used to evaluate the critical question regarding whether and how autistic individuals process non-verbal social cues from their partners during live social interactions. Here, we review this emerging literature with a focus on characterizing the experimental paradigms and eye-tracking procedures to understand the scope (and limitations) of research questions and findings. We discuss the theoretical implications of the findings from this review and provide recommendations for future work that will be essential to understand whether and how fundamental difficulties in perceiving and processing information about eye gaze cues interfere with social communication skills in autism.', 'Integrating eye gaze into machine learning using fractal curves Eye gaze tracking has traditionally employed a camera to capture a participant’s eye move- ments and characterise their visual fixations. However, gaze pattern recognition is still challenging. This is due to both gaze point sparsity, and a seemingly random approach participants take to viewing unfamiliar stimuli without a set task. Our paper proposes a method for integrating eye gaze into machine learning by con- verting a fixation’s two dimensional (x, y) coordinate into a one dimensional Hilbert curve distance metric, making it well suited for implementation into machine learning. We will compare this approach to a traditional grid-based string substitution technique, with an example implementation demonstrated in a Support Vector Machine and Convolutional Neural Network. Finally, a comparison will be made to examine what method performs better. Results have shown that this method can be both useful to dynamically quantise scan- paths for tuning statistical significance in large datasets, and to investigate the nuances of similarity found in shared bottom-up processing when participants observe unfamiliar stimuli in a free viewing experiment. Real world applications can include expertise-related eye gaze prediction, medical screening, and image saliency identification. Keywords: Neuroscience, eye tracking, fractals, support vector machine, convolutional neural network.', 'Interaction-aware Dynamic 3D Gaze Estimation in Videos Human gaze in in-the-wild and outdoor human activities is a continuous and dynamic process that is driven by the anatomical eye movements such as fixations, saccades and smooth pursuit. However, learning gaze dynamics in videos remains as a challenging task as annotating human gaze in videos is labor-expensive. In this paper, we propose a novel method for dynamic 3D gaze estimation in videos by utilizing the human interaction labels. Our model contains a temporal gaze estimator which is built upon Autoregressive Transformer structures. Besides, our model learns the spatial relationship of gaze among multiple subjects, by constructing a Human Interaction Graph from predicted gaze and update the gaze feature with a structure-aware Transformer. Our model predict future gaze conditioned on historical gaze and the gaze interactions in an autoregressive manner. We propose a multi-state training algorithm to alternately update the Interaction module and dynamic gaze estimation module, when training on a mixture of labeled and unlabeled sequences. We show significant improvements in both within-domain gaze estimation accuracy and cross-domain generalization on the physically-unconstrained gaze estimation benchmark.']"
96,10,96_diffusionmap_manifolds_diffusion_manifold,"['diffusionmap', 'manifolds', 'diffusion', 'manifold', 'dimensionality', 'embedding', 'lowdimensional', 'riemannian', 'models', 'dimensional']","['Generative Modeling on Manifolds Through Mixture of Riemannian Diffusion Processes Learning the distribution of data on Riemannian manifolds is crucial for modeling data from non-Euclidean space, which is required by many applications in diverse scientific fields. Yet, existing generative models on manifolds suffer from expensive divergence computation or rely on approximations of heat kernel. These limitations restrict their applicability to simple geometries and hinder scalability to high dimensions. In this work, we introduce the Riemannian Diffusion Mixture, a principled framework for building a generative diffusion process on manifolds. Instead of following the denoising approach of previous diffusion models, we construct a diffusion process using a mixture of bridge processes derived on general manifolds without requiring heat kernel estimations. We develop a geometric understanding of the mixture process, deriving the drift as a weighted mean of tangent directions to the data points that guides the process toward the data distribution. We further propose a scalable training objective for learning the mixture process that readily applies to general manifolds. Our method achieves superior performance on diverse manifolds with dramatically reduced number of in-training simulation steps for general manifolds.', 'Diffusion Models Encode the Intrinsic Dimension of Data Manifolds In this work, we provide a mathematical proof that diffusion models encode data manifolds by approximating their normal bundles. Based on this observation we propose a novel method for extracting the intrinsic dimension of the data manifold from a trained diffusion model. Our insights are based on the fact that a diffusion model approximates the score function i.e. the gradient of the log density of a noise-corrupted version of the target distribution for varying levels of corruption. We prove that as the level of corruption decreases, the score function points towards the manifold, as this direction becomes the direction of maximal likelihood increase. Therefore, at low noise levels, the diffusion model provides us with an approximation of the manifold’s normal bundle, allowing for an estimation of the manifold’s intrinsic dimension. To the best of our knowledge our method is the first estimator of intrinsic dimension based on diffusion models and it outperforms well established estimators in controlled experiments on both Euclidean and image data.', 'A Geometric Insight into Equivariant Message Passing Neural Networks on Riemannian Manifolds This work proposes a geometric insight into equivariant message passing on Riemannian manifolds. As previously proposed, numerical features on Riemannian manifolds are represented as coordinate-independent feature fields on the manifold. To any coordinate-independent feature field on a manifold comes attached an equivariant embedding of the principal bundle to the space of numerical features. We argue that the metric this embedding induces on the numerical feature space should optimally preserve the principal bundle’s original metric. This optimality criterion leads to the minimization of a twisted form of the Polyakov action with respect to the graph of this embedding, yielding an equivariant diffusion process on the associated vector bundle. We obtain a message passing scheme on the manifold by discretizing the diffusion equation flow for a fixed time step. We propose a higher- order equivariant diffusion process equivalent to diffusion on the cartesian product of the base manifold. The discretization of the higher-order diffusion process on a graph yields a new general class of equivariant GNN, generalizing the ACE and MACE formalism to data on Riemannian manifolds.']"
97,53,97_contrastive_learning_supervised_classification,"['contrastive', 'learning', 'supervised', 'classification', 'embedding', 'negatives', 'learned', 'representations', 'datasets', 'learn']","['On the Surrogate Gap between Contrastive and Supervised Losses Contrastive representation learning encourages data representation to make semantically similar pairs closer than randomly drawn negative samples, which has been successful in various domains such as vision, language, and graphs. Recent theoretical studies have attempted to explain the benefit of the large negative sample size by upper-bounding the downstream classification loss with the contrastive loss. However, the previous surrogate bounds have two drawbacks: they are only legitimate for a limited range of negative sample sizes and prohibitively large even within that range. Due to these drawbacks, there still does not exist a consensus on how negative sample size theoretically correlates with downstream classification performance. Following the simplified setting where positive pairs are drawn from the true distribution (not generated by data augmentation; as supposed in previous studies), this study establishes surrogate upper and lower bounds for the downstream classification loss for all negative sample sizes that best explain the empirical observations on the negative sample size in the earlier studies. Our bounds suggest that the contrastive loss can be viewed as a surrogate objective of the downstream loss and larger negative sample sizes improve downstream classification because the surrogate gap between contrastive and supervised losses decays. We verify that our theory is consistent with experiments on synthetic, vision, and language datasets.', 'Revisiting Contrastive Learning through the Lens of Neighborhood Component Analysis: an Integrated Framework As a seminal tool in self-supervised representation learning, contrastive learning has gained unprecedented attention in recent years. In essence, contrastive learning aims to leverage pairs of positive and negative samples for representation learning, which relates to exploiting neighborhood information in a feature space. By investigating the connection between contrastive learning and neighborhood component analysis (NCA), we provide a novel stochastic nearest neighbor viewpoint of contrastive learning and subsequently propose a series of contrastive losses that outperform the existing ones. Under our proposed framework, we show a new methodology to design integrated contrastive losses that could simultaneously achieve good accuracy and robustness on downstream tasks. With the integrated framework, we achieve up to 6% improvement on the standard accuracy and 17% improvement on the robust accuracy.', 'Do More Negative Samples Necessarily Hurt In Contrastive Learning? Recent investigations in noise contrastive estimation suggest, both empirically as well as theoretically, that while having more “negative samples” in the contrastive loss improves downstream classification performance initially, beyond a threshold, it hurts downstream performance due to a “collision-coverage” trade-off. But is such a phenomenon inherent in contrastive learning? We show in a simple theoretical setting, where positive pairs are generated by sampling from the underlying latent class (introduced by Saunshi et al. (ICML 2019)), that the downstream performance of the representation optimizing the (population) contrastive loss in fact does not degrade with the number of negative samples. Along the way, we give a structural characterization of the optimal representation in our framework, for noise contrastive estimation. We also provide empirical support for our theoretical results on CIFAR-10 and CIFAR-100 datasets.']"
98,25,98_pomdps_pomdp_reinforcement_planning,"['pomdps', 'pomdp', 'reinforcement', 'planning', 'markov', 'learning', 'observable', 'bapomdps', 'mdp', 'decisionmaking']","['Provable Representation with Efficient Planning for Partially Observable Reinforcement Learning In most real-world reinforcement learning applications, state information is only partially observable, which breaks the Markov decision process assumption and leads to inferior performance for algorithms that conflate observations with state. Partially Observable Markov Decision Processes (POMDPs), on the other hand, provide a general framework that allows for partial observability to be accounted for in <em>learning, exploration and planning</em>, but presents significant computational and statistical challenges. To address these difficulties, we develop a representation-based perspective that leads to a coherent framework and tractable algorithmic approach for practical reinforcement learning from partial observations. We provide a theoretical analysis for justifying the statistical efficiency of the proposed algorithm, and also empirically demonstrate the proposed algorithm can surpass state-of-the-art performance with partial observations across various benchmarks, advancing reliable reinforcement learning towards more practical applications.', 'A PAC RL Algorithm for Episodic POMDPs Many interesting real world domains involve reinforcement learning (RL) in partially observable environments. Efficient learning in such domains is important, but existing sample complexity bounds for partially observable RL are at least exponential in the episode length. We give, to our knowledge, the first partially observable RL algorithm with a polynomial bound on the number of episodes on which the algorithm may not achieve near-optimal performance. Our algorithm is suitable for an important class of episodic POMDPs. Our approach builds on recent advances in the method of moments for latent variable model estimation.', 'When Is Partially Observable Reinforcement Learning Not Scary? Partial observability is ubiquitous in applications of Reinforcement Learning (RL), in which agents learn to make a sequence of decisions despite lacking complete information about the latent states of the controlled system. Partially observable RL is notoriously difficult in theory—well-known complexity-theoretic results show that learning partially observable Markov decision processes (POMDPs) requires an exponential number of samples in the worst case.  Yet, this does not rule out the possible existence of interesting subclasses of POMDPs, which include a large set of partial observable applications in practice while being tractable. In this paper we identify a rich family of tractable POMDPs, which we call weakly revealing POMDPs. This family rules out the pathological instances of POMDPs with non-informative observations. We prove that for weakly revealing POMDPs, a simple algorithm combining optimism and Maximum Likelihood Estimation (MLE) is sufficient to guarantee a polynomial sample complexity. To the best of our knowledge, this is the first provably sample-efficient result for learning in overcomplete POMDPs—where the number of latent states can be larger than the number of observations—in settings where exploration is necessary.']"
99,118,99_representations_cnns_invariances_equivariant,"['representations', 'cnns', 'invariances', 'equivariant', 'invariance', 'equivariance', 'convolutions', 'symmetries', 'representation', 'neural']","['Learning Symmetric Embeddings for Equivariant World Models Incorporating symmetries can lead to highly data-efficient and generalizable models by defining equivalence classes of data samples related by transformations. However, characterizing how transformations act on input data is often difficult, limiting the applicability of equivariant models. We propose learning symmetric embedding networks (SENs) that encode an input space (e.g. images), where we do not know the effect of transformations (e.g. rotations), to a feature space that transforms in a known manner under these operations. This network can be trained end-to-end with an equivariant task network to learn an explicitly symmetric representation. We validate this approach in the context of equivariant transition models with 3 distinct forms of symmetry. Our experiments demonstrate that SENs facilitate the application of equivariant networks to data with complex symmetry representations. Moreover, doing so can yield improvements in accuracy and generalization relative to both fully-equivariant and non-equivariant baselines.', 'Computing representations for Lie algebraic networks Recent work has constructed neural networks that are equivariant to continuous symmetry groups such as 2D and 3D rotations. This is accomplished using explicit {\\it Lie group representations} to derive the equivariant kernels and nonlinearities. We present three contributions motivated by frontier applications of equivariance beyond rotations and translations. First, we relax the requirement for explicit Lie group representations with a novel algorithm that finds representations of arbitrary Lie groups given only the {\\it structure constants} of the associated Lie algebra. Second, we provide a self-contained method and software for building Lie group-equivariant neural networks using these representations. Third, we contribute a novel benchmark dataset for classifying objects from relativistic point clouds, and apply our methods to construct the first object-tracking model equivariant to the Poincaré group.', 'Equivariant Representation Learning via Class-Pose Decomposition We introduce a general method for learning representations that are equivariant to symmetries of data. Our central idea is to decompose the latent space into an invariant factor and the symmetry group itself. The components semantically correspond to intrinsic data classes and poses respectively. The learner is trained on a loss encouraging equivariance based on supervision from relative symmetry information. The approach is motivated by theoretical results from group theory and guarantees representations that are lossless, interpretable and disentangled. We provide an empirical investigation via experiments involving datasets with a variety of symmetries. Results show that our representations capture the geometry of data and outperform other equivariant representation learning frameworks.']"
100,29,100_estimators_unbiased_estimating_estimates,"['estimators', 'unbiased', 'estimating', 'estimates', 'estimation', 'policies', 'estimator', 'sampling', 'policy', 'policys']","['From Importance Sampling to Doubly Robust Policy Gradient We show that on-policy policy gradient (PG) and its variance reduction variants can be derived by taking finite-difference of function evaluations supplied by estimators from the importance sampling (IS) family for off-policy evaluation (OPE). Starting from the doubly robust (DR) estimator (Jiang & Li, 2016), we provide a simple derivation of a very general and flexible form of PG, which subsumes the state-of-the-art variance reduction technique (Cheng et al., 2019) as its special case and immediately hints at further variance reduction opportunities overlooked by existing literature. We analyze the variance of the new DR-PG estimator, compare it to existing methods as well as the Cramer-Rao lower bound of policy gradient, and empirically show its effectiveness.', 'Understanding the Curse of Horizon in Off-Policy Evaluation via Conditional Importance Sampling Off-policy policy estimators that use importance sampling (IS) can suffer from high variance in long-horizon domains, and there has been particular excitement over new IS methods that leverage the structure of Markov decision processes. We analyze the variance of the most popular approaches through the viewpoint of conditional Monte Carlo. Surprisingly, we find that in finite horizon MDPs there is no strict variance reduction of per-decision importance sampling or marginalized importance sampling, comparing with vanilla importance sampling. We then provide sufficient conditions under which the per-decision or marginalized estimators will provably reduce the variance over importance sampling with finite horizons. For the asymptotic (in terms of horizon $T$) case, we develop upper and lower bounds on the variance of those estimators which yields sufficient conditions under which there exists an exponential v.s. polynomial gap between the variance of importance sampling and that of the per-decision or stationary/marginalized estimators. These results help advance our understanding of if and when new types of IS estimators will improve the accuracy of off-policy estimation.', 'Importance Sampling Policy Evaluation with an Estimated Behavior Policy We consider the problem of off-policy evaluation in Markov decision processes. Off-policy evaluation is the task of evaluating the expected return of one policy with data generated by a different, behavior policy. Importance sampling is a technique for off-policy evaluation that re-weights off-policy returns to account for differences in the likelihood of the returns between the two policies. In this paper, we study importance sampling with an estimated behavior policy where the behavior policy estimate comes from the same set of data used to compute the importance sampling estimate. We find that this estimator often lowers the mean squared error of off-policy evaluation compared to importance sampling with the true behavior policy or using a behavior policy that is estimated from a separate data set. Intuitively, estimating the behavior policy in this way corrects for error due to sampling in the action-space. Our empirical results also extend to other popular variants of importance sampling and show that estimating a non-Markovian behavior policy can further lower large-sample mean squared error even when the true behavior policy is Markovian.']"
101,10,101_learning_learnt_learnable_curriculum,"['learning', 'learnt', 'learnable', 'curriculum', 'learners', 'learner', 'iterative', 'training', 'datasets', 'benchmark']","['Iterative Machine Teaching In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner. We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers. We also validate our theoretical findings with extensive experiments on different data distribution and real image datasets.', 'CurBench: Curriculum Learning Benchmark Curriculum learning is a training paradigm where machine learning models are trained in a meaningful order, inspired by the way humans learn curricula. Due to its capability to improve model generalization and convergence, curriculum learning has gained considerable attention and has been widely applied to various research domains. Nevertheless, as new curriculum learning methods continue to emerge, it remains an open issue to benchmark them fairly. Therefore, we develop CurBench, the first benchmark that supports systematic evaluations for curriculum learning. Specifically, it consists of 15 datasets spanning 3 research domains: computer vision, natural language processing, and graph machine learning, along with 3 settings: standard, noise, and imbalance. To facilitate a comprehensive comparison, we establish the evaluation from 2 dimensions: performance and complexity. CurBench also provides a unified toolkit that plugs automatic curricula into general machine learning processes, enabling the implementation of 15 core curriculum learning methods. On the basis of this benchmark, we conduct comparative experiments and make empirical analyses of existing methods. CurBench is open-source and publicly available at https://github.com/THUMNLab/CurBench.', 'Curriculum Learning by Transfer Learning: Theory and Experiments with Deep Networks We provide theoretical investigation of curriculum learning in the context of stochastic gradient descent when optimizing the convex linear regression loss. We prove that the rate of convergence of an ideal curriculum learning method is monotonically increasing with the difficulty of the examples. Moreover, among all equally difficult points, convergence is faster when using points which incur higher loss with respect to the current hypothesis. We then analyze curriculum learning in the context of training a CNN. We describe a method which infers the curriculum by way of transfer learning from another network, pre-trained on a different task. While this approach can only approximate the ideal curriculum, we observe empirically similar behavior to the one predicted by the theory, namely, a significant boost in convergence speed at the beginning of training. When the task is made more difficult, improvement in generalization performance is also observed. Finally, curriculum learning exhibits robustness against unfavorable conditions such as excessive regularization.']"
102,14,102_reinforcement_rewardconditioned_reward_rewards,"['reinforcement', 'rewardconditioned', 'reward', 'rewards', 'rewardsincontext', 'preference', 'ai', 'regularization', 'towards', 'models']","['MaxMin-RLHF: Alignment with Diverse Human Preferences Reinforcement Learning from Human Feedback (RLHF) aligns language models to human preferences by employing a singular reward model derived from preference data. However, the single reward model overlooks the rich diversity of human preferences inherent in data collected from multiple users. In this work, we first derive an impossibility result of alignment with single reward RLHF, thereby highlighting its insufficiency in representing diverse human preferences. Next, we propose to learn a mixture of reward models via an expectation-maximization algorithm and solve a MaxMin alignment objective inspired by the Egalitarian principle in social choice theory to better honor diverse human preferences. We present comprehensive experimental results on small-scale (GPT-2) and large-scale language (with Tulu2-7B)) and show the efficacy of the proposed approach in the presence of diversity among human preferences. We remark that our findings in this work are not only limited to language models but also extend to reinforcement learning in general.', 'Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-constraint This paper studies the theoretical framework of the alignment process of generative models with Reinforcement Learning from Human Feedback (RLHF). We consider a standard mathematical formulation, the reverse-KL regularized contextual bandit for RLHF. Despite its widespread practical application, a rigorous theoretical analysis of this formulation remains open. We investigate its behavior in three distinct settings—offline, online, and hybrid—and propose efficient algorithms with finite-sample theoretical guarantees. Moving towards practical applications, our framework, with a robust approximation of the information-theoretical policy improvement oracle, naturally gives rise to several novel RLHF algorithms. This includes an iterative version of the Direct Preference Optimization (DPO) algorithm for online settings, and a multi-step rejection sampling strategy for offline scenarios. Our empirical evaluations on real-world alignment experiment of large language model demonstrate that these proposed methods significantly surpass existing strong baselines, such as DPO and Rejection Sampling Optimization (RSO), showcasing the connections between solid theoretical foundations and their potent practical implementations.', 'Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint arXiv:2312.11456v4 Announce Type: replace \nAbstract: This paper studies the alignment process of generative models with Reinforcement Learning from Human Feedback (RLHF). We first identify the primary challenges of existing popular methods like offline PPO and offline DPO as lacking in strategical exploration of the environment. Then, to understand the mathematical principle of RLHF, we consider a standard mathematical formulation, the reverse-KL regularized contextual bandit for RLHF. Despite its widespread practical application, a rigorous theoretical analysis of this formulation remains open. We investigate its behavior in three distinct settings -- offline, online, and hybrid -- and propose efficient algorithms with finite-sample theoretical guarantees.\n  Moving towards practical applications, our framework, with a robust approximation of the information-theoretical policy improvement oracle, naturally gives rise to several novel RLHF algorithms. This includes an iterative version of the Direct Preference Optimization (DPO) algorithm for online settings, and a multi-step rejection sampling strategy for offline scenarios. Our empirical evaluations on real-world alignment experiment of large language model demonstrate that these proposed methods significantly surpass existing strong baselines, such as DPO and Rejection Sampling Optimization (RSO), showcasing the connections between solid theoretical foundations and their potent practical implementations.']"
103,23,103_classifier_outliers_detection_detecting,"['classifier', 'outliers', 'detection', 'detecting', 'outlier', 'detect', 'ood', 'detectors', 'datasets', 'benchmarks']","['Out of Distribution Detection via Neural Network\n Anchoring Our goal in this paper is to exploit heteroscedastic\n temperature scaling as a calibration strategy for\n out of distribution (OOD)\n detection. Heteroscedasticity here refers to the\n fact that the optimal temperature parameter for each\n sample can be different, as opposed to conventional\n approaches that use the same value for the entire\n distribution. To enable this, we propose a new\n training strategy called anchoring that can estimate\n appropriate temperature values for each sample,\n leading to state-of-the-art OOD detection\n performance across several benchmarks. Using NTK\n theory, we show that this temperature function\n estimate is closely linked to the epistemic\n uncertainty of the classifier, which explains its\n behavior. In contrast to some of the best-performing\n OOD detection approaches, our method does not\n require exposure to additional outlier datasets,\n custom calibration objectives, or model\n ensembling. Through empirical studies with different\n OOD detection settings – far OOD, near OOD, and\n semantically coherent OOD - we establish a highly\n effective OOD detection approach.', 'In or Out? Fixing ImageNet Out-of-Distribution Detection Evaluation Out-of-distribution (OOD) detection is the problem of identifying inputs which are unrelated to the in-distribution task. The OOD detection performance when the in-distribution (ID) is ImageNet-1K is commonly being tested on a small range of test OOD datasets. We find that most of the currently used test OOD datasets, including datasets from the open set recognition (OSR) literature, have severe issues: In some cases more than 50$%$ of the dataset contains objects belonging to one of the ID classes. These erroneous samples heavily distort the evaluation of OOD detectors. As a solution, we introduce with NINCO a novel test OOD dataset, each sample checked to be ID free, which with its fine-grained range of OOD classes allows for a detailed analysis of an OOD detector’s strengths and failure modes, particularly when paired with a number of synthetic “OOD unit-tests”. We provide detailed evaluations across a large set of architectures and OOD detection methods on NINCO and the unit-tests, revealing new insights about model weaknesses and the effects of pretraining on OOD detection performance. We provide code and data at https://github.com/j-cb/NINCO.', 'Why Out-of-Distribution detection experiments are not reliable - subtle experimental details muddle the OOD detector rankings Reliable detection of out-of-distribution (OOD) instances is becoming a critical requirement for machine learning systems deployed in safety-critical applications. Recently, many OOD detectors have been developed in the literature, and their performance has been evaluated using empirical studies based on well-established benchmark datasets. However, these studies do not provide a conclusive recommendation because the performance of OOD detection depends on the benchmark datasets. In this work, we want to question the reliability of the OOD detection performance numbers obtained from many of these empirical experiments. We report several experimental conditions that are not controlled and lead to significant changes in OOD detector performance and rankings of OOD methods. These include the technicalities related to how the DNN was trained (such as seed, train/test split, etc.), which do not change the accuracy of closed-set DNN models but may significantly change the performance of OOD detection methods that rely on representation from these DNNs. We performed extensive sensitivity studies in image and text domains to quantify the instability of OOD performance measures due to unintuitive experimental factors. These factors need to be more rigorously controlled and accounted for in many current OOD experiments. Experimental studies in OOD detection should improve methodological standards regarding experiment control and replication.']"
104,110,104_anomaly_outliers_outlier_inliers,"['anomaly', 'outliers', 'outlier', 'inliers', 'adversarial', 'anomalies', 'autoencoders', 'anomalous', 'autoencoder', 'novelty']","['Lipschitz Continuous Autoencoders in Application to Anomaly Detection Anomaly detection is the task of finding abnormal data that are distinct from normal behavior. Current deep learning-based anomaly detection methods train neural networks with normal data alone and calculate anomaly scores based on the trained model. In this work, we formalize current practices, build a theoretical framework of anomaly detection algorithms equipped with an objective function and a hypothesis space, and establish a desirable property of the anomaly detection algorithm, namely, admissibility. Admissibility implies that optimal autoencoders for normal data yield a larger reconstruction error for anomalous data than that for normal data on average. We then propose a class of admissible anomaly detection algorithms equipped with an integral probability metric-based objective function and a class of autoencoders, Lipschitz continuous autoencoders. The proposed algorithm for Wasserstein distance is implemented by minimizing an approximated Wasserstein distance with a penalty to enforce Lipschitz continuity with respect to Wasserstein distance. Through ablation studies, we demonstrate the efficacy of enforcing Lipschitz continuity of the proposed method. The proposed method is shown to be more effective in detecting anomalies than existing methods via applications to network traffic and image datasets.', 'Meta-learning for Robust Anomaly Detection We propose a meta-learning method to improve the anomaly detection performance on unseen target tasks that have only unlabeled data. Existing meta-learning methods for anomaly detection have shown remarkable performance but require labeled data in target tasks. Although they can treat unlabeled data as normal assuming anomalies in the unlabeled data are negligible, this assumption is often violated in practice. As a result, the methods have low performance. Our method meta-learns with related tasks that have labeled and unlabeled data such that the expected test anomaly detection performance is directly improved when the anomaly detector is adapted to given unlabeled data. Our method is based on autoencoders (AEs), which are widely used neural network-based anomaly detectors. We model anomalous attributes for each unlabeled instance in the reconstruction loss of the AE, which are used to prevent the anomalies from being reconstructed; they can remove the effect of the anomalies. We formulate adaptation to the unlabeled data as a learning problem of the last layer of the AE and the anomalous attributes. This formulation enables the optimum solution to be obtained with a closed-form alternate update formula, which is preferable to efficiently maximize the expected test anomaly detection performance. The effectiveness of our method is experimentally shown with four real-world datasets.', 'Deep One-Class Classification Despite the great advances made by deep learning in many machine learning problems, there is a relative dearth of deep learning approaches for anomaly detection. Those approaches which do exist involve networks trained to perform a task other than anomaly detection, namely generative models or compression, which are in turn adapted for use in anomaly detection; they are not trained on an anomaly detection based objective. In this paper we introduce a new anomaly detection method—Deep Support Vector Data Description—, which is trained on an anomaly detection based objective. The adaptation to the deep regime necessitates that our neural network and training procedure satisfy certain properties, which we demonstrate theoretically. We show the effectiveness of our method on MNIST and CIFAR-10 image benchmark datasets as well as on the detection of adversarial examples of GTSRB stop signs.']"
105,14,105_tracking_tracker_trackers_trackingnet,"['tracking', 'tracker', 'trackers', 'trackingnet', 'track', 'tracked', 'cnn', 'tracks', 'detectionfree', 'benchmarks']","['Probabilistic Adaptive Spatial-Temporal Regularized\n Correlation Filters for UAV Tracking Most existing trackers based on spatial-temporal\n regularized correlation filters exploit response map\n variation to adapt regularization terms to object\n appearance changes automatically. However, these\n trackers ignore the high uncertainty of the response\n map when the object is occluded or similar objects\n around, making them unable to learn reliable filters\n accurately. Furthermore, most correlation filters\n use linear interpolation directly to update the\n filter model at each frame, which may cause model\n degradation once the tracking result is inaccurate\n or missing. In this work, we propose a novel\n probabilistic adaptive spatial-temporal regularized\n correlation filters (PASTRCF) to solve the two\n issues mentioned above. A probabilistic model\n constructing the reliability of the response map is\n introduced to accurately utilize the information in\n the response map to learn regularization\n coefficients adaptively. The adaptive threshold\n mechanism provides an appropriate strategy to update\n the filter model to alleviate model\n degradation. Extensive experiments on UAV benchmarks\n have proven the favorable performance of our method\n compared to the state-of-art trackers, with robust\n tracking while ensuring real-time performance.', 'UAST: Uncertainty-Aware Siamese Tracking Visual object tracking is basically formulated as target classification and bounding box estimation. Recent anchor-free Siamese trackers rely on predicting the distances to four sides for efficient regression but fail to estimate accurate bounding box in complex scenes. We argue that these approaches lack a clear probabilistic explanation, so it is desirable to model the uncertainty and ambiguity representation of target estimation. To address this issue, this paper presents an Uncertainty-Aware Siamese Tracker (UAST) by developing a novel distribution-based regression formulation with localization uncertainty. We exploit regression vectors to directly represent the discretized probability distribution for four offsets of boxes, which is general, flexible and informative. Based on the resulting distributed representation, our method is able to provide a probabilistic value of uncertainty. Furthermore, considering the high correlation between the uncertainty and regression accuracy, we propose to learn a joint representation head of classification and localization quality for reliable tracking, which also avoids the inconsistency of classification and quality estimation between training and inference. Extensive experiments on several challenging tracking benchmarks demonstrate the effectiveness of UAST and its superiority over other Siamese trackers.', 'Learning Disentangled Representation in Pruning for\n Real-Time UAV Tracking Efficiency is a critical issue in UAV tracking\n because of the limitations of computing resources,\n battery capacity, and maximum load of unmanned\n aerial vehicle (UAV). However, deep learning\n (DL)-based trackers hardly achieve real-time\n tracking on a single CPU despite their high tracking\n precision. To the contrary, discriminative\n correlation filters (DCF)-based trackers have high\n efficiency but their precision is barely\n satisfactory. Despite the precision is inferior,\n DCF-based trackers instead of DL-based ones are\n widely applied in UAV tracking to trade precision\n for efficiency. This paper aims to improve the\n efficiency of the DL-based tracker SiamFC++, in\n particular, for UAV tracking using the model\n compression technique, i.e., rank-based filter\n pruning, which has not been well explored\n before. Meanwhile, to combat the potential loss of\n precision caused by pruning we exploit disentangled\n representation learning to disentangle the output\n feature of the backbone into two parts: the\n identity-related features and the identity-unrelated\n features. Only the identity-related features are\n used for subsequent classification and regression\n tasks to improve the effectiveness of the feature\n representation. With the proposed disentangled\n representation in pruning, we achieved higher\n precisions when compressing the original model\n SiamFC++ with a global pruning ratio of\n 0.5. Extensive experiments on four public UAV\n benchmarks, i.e., UAV123@10fps, UAVDT, DTB70, and\n Vistrone2018, show that the proposed tracker\n DP-SiamFC++ strikes a remarkable balance between\n efficiency and precision, and achieves\n state-of-the-art performance in UAV tracking.']"
106,22,106_stein_steins_kernelized_goodnessoffit,"['stein', 'steins', 'kernelized', 'goodnessoffit', 'kernels', 'distributions', 'statistical', 'tests', 'testing', 'statistic']","['Using Perturbation to Improve Goodness-of-Fit Tests based on Kernelized Stein Discrepancy Kernelized Stein discrepancy (KSD) is a score-based discrepancy widely used in goodness-of-fit tests. It can be applied even when the target distribution has an unknown normalising factor, such as in Bayesian analysis. We show theoretically and empirically that the KSD test can suffer from low power when the target and the alternative distributions have the same well-separated modes but differ in mixing proportions. We propose to perturb the observed sample via Markov transition kernels, with respect to which the target distribution is invariant. This allows us to then employ the KSD test on the perturbed sample. We provide numerical evidence that with suitably chosen transition kernels the proposed approach can lead to substantially higher power than the KSD test.', ' Standardisation-function Kernel Stein Discrepancy: A Unifying View on Kernel Stein Discrepancy Tests for Goodness-of-fit   Non-parametric goodness-of-fit testing procedures based on kernel Stein discrepancies (KSD) are promising approaches to validate general unnormalised distributions in various scenarios. Existing works focused on studying kernel choices to boost test performances. However, the choices of (non-unique) Stein operators also have considerable effect on the test performances. Inspired by the standardisation technique that was originally developed to better derive approximation properties for normal distributions, we present a unifying framework, called standardisation-function kernel Stein discrepancy (Sf-KSD), to study different Stein operators in KSD-based tests for goodness-of-fit. We derive explicitly how the proposed framework relates to existing KSD-based tests and show that Sf-KSD can be used as a guide to develop novel kernel-based non-parametric tests on complex data scenarios, e.g. truncated distributions or compositional data. Experimental results demonstrate that the proposed tests control type-I error well and achieve higher test power than existing approaches. ', 'Goodness-of-Fit Testing for Discrete Distributions via Stein Discrepancy Recent work has combined Stein’s method with reproducing kernel Hilbert space theory to develop nonparametric goodness-of-fit tests for un-normalized probability distributions. However, the currently available tests apply exclusively to distributions with smooth density functions. In this work, we introduce a kernelized Stein discrepancy measure for discrete spaces, and develop a nonparametric goodness-of-fit test for discrete distributions with intractable normalization constants. Furthermore, we propose a general characterization of Stein operators that encompasses both discrete and continuous distributions, providing a recipe for constructing new Stein operators. We apply the proposed goodness-of-fit test to three statistical models involving discrete distributions, and our experiments show that the proposed test typically outperforms a two-sample test based on the maximum mean discrepancy.']"
107,23,107_scenes_3d_generative_renderings,"['scenes', '3d', 'generative', 'renderings', 'rendering', 'scene', 'neural', 't3d', '3daware', 'sceneweaver']","['NeRF-VAE: A Geometry Aware 3D Scene Generative Model We propose NeRF-VAE, a 3D scene generative model that incorporates geometric structure via Neural Radiance Fields (NeRF) and differentiable volume rendering. In contrast to NeRF, our model takes into account shared structure across scenes, and is able to infer the structure of a novel scene—without the need to re-train—using amortized inference. NeRF-VAE’s explicit 3D rendering process further contrasts previous generative models with convolution-based rendering which lacks geometric structure. Our model is a VAE that learns a distribution over radiance fields by conditioning them on a latent scene representation. We show that, once trained, NeRF-VAE is able to infer and render geometrically-consistent scenes from previously unseen 3D environments of synthetic scenes using very few input images. We further demonstrate that NeRF-VAE generalizes well to out-of-distribution cameras, while convolutional models do not. Finally, we introduce and study an attention-based conditioning mechanism of NeRF-VAE’s decoder, which improves model performance.', 'Equivariant Neural Rendering We propose a framework for learning neural scene representations directly from images, without 3D supervision. Our key insight is that 3D structure can be imposed by ensuring that the learned representation transforms like a real 3D scene. Specifically, we introduce a loss which enforces equivariance of the scene representation with respect to 3D transformations. Our formulation allows us to infer and render scenes in real time while achieving comparable results to models requiring minutes for inference. In addition, we introduce two challenging new datasets for scene representation and neural rendering, including scenes with complex lighting and backgrounds. Through experiments, we show that our model achieves compelling results on these datasets as well as on standard ShapeNet benchmarks.', 'SceneWeaver: Text-Driven Scene Generation with Geometry-aware Gaussian Splatting With the widespread use of virtual reality applications, 3D scene generation has become a challenging new research frontier. 3D scenes have highly complex structures, so it is crucial to ensure that the output is dense, coherent, and includes all necessary structures. Many current 3D scene generation methods rely on pre-trained text-to-image diffusion models and monocular depth estimators, but they often lack rich geometric constraint information within the scene, leading to geometric distortion in the generated results. Therefore, we propose a two-stage geometry-aware progressive scene generation framework, SceneWeaver, which creates diverse, high-quality 3D scenes from text or image inputs. In the first stage, we introduce a multi-level depth refinement mechanism combined with image inpainting and point cloud updating strategies to construct a high-quality initial point cloud. In the second stage, 3D Gaussians are initialized based on the point cloud and continuously optimized. To address the challenge of insufficient geometric constraints in the Gaussian Splatting optimization process, we utilize the rich appearance and geometry information within the scene to perform a geometry-aware optimization, resulting in high-quality scene generation results. Comprehensive experiments across multiple scenes demonstrate the significant potential and advantages of our framework compared with several baselines.']"
108,30,108_pointcloud_pointclouds_pointnet_shapenet,"['pointcloud', 'pointclouds', 'pointnet', 'shapenet', 'surfaces', 'mesh', '3d', 'surface', 'meshes', 'shapes']","['A Statistical Manifold Framework for Point Cloud Data Many problems in machine learning involve data sets in which each data point is a point cloud in $\\mathbb{R}^D$. A growing number of applications require a means of measuring not only distances between point clouds, but also angles, volumes, derivatives, and other more advanced concepts. To formulate and quantify these concepts in a coordinate-invariant way, we develop a Riemannian geometric framework for point cloud data. By interpreting each point in a point cloud as a sample drawn from some given underlying probability density, the space of point cloud data can be given the structure of a statistical manifold – each point on this manifold represents a point cloud – with the Fisher information metric acting as a natural Riemannian metric. Two autoencoder applications of our framework are presented: (i) smoothly deforming one 3D object into another via interpolation between the two corresponding point clouds; (ii) learning an optimal set of latent space coordinates for point cloud data that best preserves angles and distances, and thus produces a more discriminative representation space. Experiments with large-scale standard benchmark point cloud data show greatly improved classification accuracy vis-á-vis existing methods. Code is available at https://github.com/seungyeon-k/SMF-public.', 'NeuralIndicator: Implicit Surface Reconstruction from Neural Indicator Priors The neural implicit surface reconstruction from unorganized points is still challenging, especially when the point clouds are incomplete and/or noisy with complex topology structure. Unlike previous approaches performing neural implicit surface learning relying on local shape priors, this paper proposes to utilize global shape priors to regularize the neural implicit function learning for more reliable surface reconstruction. To this end, we first introduce a differentiable module to generate a smooth indicator function, which globally encodes both the indicative prior and local SDFs of the entire input point cloud. Benefit from this, we propose a new framework, called NeuralIndicator, to jointly learn both the smooth indicator function and neural implicit function simultaneously, using the global shape prior encoded by smooth indicator function to effectively regularize the neural implicit function learning, towards reliable and high-fidelity surface reconstruction from unorganized points without any normal information. Extensive evaluations on synthetic and real-scan datasets show that our approach consistently outperforms previous approaches, especially when point clouds are incomplete and/or noisy with complex topology structure.', 'Hypernetwork approach to generating point clouds In this work, we propose a novel method for generating 3D point clouds that leverage properties of hyper networks. Contrary to the existing methods that learn only the representation of a 3D object, our approach simultaneously finds a representation of the object and its 3D surfaces. The main idea of our HyperCloud method is to build a hyper network that returns weights of a particular neural network (target network) trained to map points from a uniform unit ball distribution into a 3D shape. As a consequence, a particular 3D shape can be generated using point-by-point sampling from the assumed prior distribution and transforming sampled points with the target network. Since the hyper network is based on an auto-encoder architecture trained to reconstruct realistic 3D shapes, the target network weights can be considered a parametrisation of the surface of a 3D shape, and not a standard representation of point cloud usually returned by competitive approaches. The proposed architecture allows to find mesh-based representation of 3D objects in a generative manner, while providing point clouds en pair in quality with the state-of-the-art methods.']"
109,86,109_lidar_3d_depth_scenenet,"['lidar', '3d', 'depth', 'scenenet', 'multicamera', 'camera', 'odometry', 'cameras', 'pose', 'panoramic']","['Probabilistic and Geometric Depth: Detecting Objects in Perspective 3D object detection is an important capability needed in various practical applications such as driver assistance systems. Monocular 3D detection, a representative general setting among image-based approaches, provides a more economical solution than conventional settings relying on LiDARs but still yields unsatisfactory results. This paper first presents a systematic study on this problem. We observe that the current monocular 3D detection can be simplified as an instance depth estimation problem: The inaccurate instance depth blocks all the other 3D attribute predictions from improving the overall detection performance. Moreover, recent methods directly estimate the depth based on isolated instances or pixels while ignoring the geometric relations across different objects. To this end, we construct geometric relation graphs across predicted objects and use the graph to facilitate depth estimation. As the preliminary depth estimation of each instance is usually inaccurate in this ill-posed setting, we incorporate a probabilistic representation to capture the uncertainty. It provides an important indicator to identify confident predictions and further guide the depth propagation. Despite the simplicity of the basic idea, our method, PGD, obtains significant improvements on KITTI and nuScenes benchmarks, achieving 1st place out of all monocular vision-only methods while still maintaining real-time efficiency. Code and models will be released at https://github.com/open-mmlab/mmdetection3d.', 'Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR Self-supervised monocular depth prediction provides a cost-effective solution to obtain the 3D location of each pixel. However, the existing approaches usually lead to unsatisfactory accuracy, which is critical for autonomous robots. In this paper, we propose FusionDepth, a novel two-stage network to advance the self-supervised monocular dense depth learning by leveraging low-cost sparse (e.g. 4-beam) LiDAR. Unlike the existing methods that use sparse LiDAR mainly in a manner of time-consuming iterative post-processing, our model fuses monocular image features and sparse LiDAR features to predict initial depth maps. Then, an efficient feed-forward refine network is further designed to correct the errors in these initial depth maps in pseudo-3D space with real-time performance. Extensive experiments show that our proposed model significantly outperforms all the state-of-the-art self-supervised methods, as well as the sparse-LiDAR-based methods on both self-supervised monocular depth prediction and completion tasks. With the accurate dense depth prediction, our model outperforms the state-of-the-art sparse-LiDAR-based method (Pseudo-LiDAR++) by more than 68% for the downstream task monocular 3D object detection on the KITTI Leaderboard. Code is available at https://github.com/AutoAILab/FusionDepth', 'Revisiting Depth-guided Methods for Monocular 3D Object Detection by Hierarchical Balanced Depth Monocular 3D object detection has seen significant advancements with the incorporation of depth information. However, there remains a considerable performance gap compared to LiDAR-based methods, largely due to inaccurate depth estimation. We argue that this issue stems from the commonly used pixel-wise depth map loss, which inherently creates the imbalance of loss weighting between near and distant objects. To address these challenges, we propose MonoHBD (Monocular Hierarchical Balanced Depth), a comprehensive solution with the hierarchical mechanism. We introduce the Hierarchical Depth Map (HDM) structure that incorporates depth bins and depth offsets to enhance the localization accuracy for objects. Leveraging RoIAlign, our Balanced Depth Extractor (BDE) module captures both scene-level depth relationships and object-specific depth characteristics while considering the geometry properties through the inclusion of camera calibration parameters. Furthermore, we propose a novel depth map loss that regularizes object-level depth features to mitigate imbalanced loss propagation. Our model reaches state-of-the-art results on the KITTI 3D object detection benchmark while supporting real-time detection. Excessive ablation studies are also conducted to prove the efficacy of our proposed modules.']"
110,40,110_adversarial_malicious_robustness_safeguarding,"['adversarial', 'malicious', 'robustness', 'safeguarding', 'unsafe', 'threats', 'robust', 'security', 'vulnerable', 'defenses']","['Adversarial Attacks and Defenses in Large Language Models: Old and New Threats Over the past decade, there has been extensive research aimed at enhancing the robustness of neural networks, yet this problem remains vastly unsolved. Here, one major impediment has been the overestimation of the robustness of new defense approaches due to faulty defense evaluations. Flawed robustness evaluations necessitate rectifications in subsequent works, dangerously slowing down the research and providing a false sense of security. In this context, we will face substantial challenges associated with an impending adversarial arms race in natural language processing, specifically with closed-source Large Language Models (LLMs), such as ChatGPT, Google Bard, or Anthropic’s Claude. We provide a first set of prerequisites to improve the robustness assessment of new approaches and reduce the amount of faulty evaluations. Additionally, we identify embedding space attacks on LLMs as another viable threat model for the purposes of generating malicious content in open-sourced models. Finally, we demonstrate on a recently proposed defense that, without LLM-specific best practices in place, it is easy to overestimate the robustness of a new approach.', 'On Prompt-Driven Safeguarding for Large Language Models Prepending model inputs with safety prompts is a common practice for safeguarding large language models (LLMs) against queries with harmful intents. However, the underlying working mechanisms of safety prompts have not been unraveled yet, restricting the possibility of automatically optimizing them to improve LLM safety. In this work, we investigate how LLMs’ behavior (i.e., complying with or refusing user queries) is affected by safety prompts from the perspective of model representation. We find that in the representation space, the input queries are typically moved by safety prompts in a ""higher-refusal"" direction, in which models become more prone to refusing to provide assistance, even when the queries are harmless. On the other hand, LLMs are naturally capable of distinguishing harmful and harmless queries without safety prompts. Inspired by these findings, we propose a method for safety prompt optimization, namely DRO (Directed Representation Optimization). Treating a safety prompt as continuous, trainable embeddings, DRO learns to move the queries’ representations along or opposite the refusal direction, depending on their harmfulness. Experiments with eight LLMs on out-of-domain and jailbreak benchmarks demonstrate that DRO remarkably improves the safeguarding performance of human-crafted safety prompts, without compromising the models’ general performance.', 'RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content Recent advancements in Large Language Models (LLMs) have showcased remarkable capabilities across various tasks in different domains. However, the emergence of biases and the potential for generating harmful content in LLMs, particularly under malicious inputs, pose significant challenges. Current mitigation strategies, while effective, are not resilient under adversarial attacks. This paper introduces Resilient Guardrails for Large Language Models (RigorLLM), a novel framework designed to efficiently and effectively moderate harmful and unsafe inputs and outputs for LLMs. By employing a multi-faceted approach that includes energy-based training data augmentation through Langevin dynamics, optimizing a safe suffix for inputs via minimax optimization, and integrating a fusion-based model combining robust KNN with LLMs based on our data augmentation, RigorLLM offers a robust solution to harmful content moderation. Our experimental evaluations demonstrate that RigorLLM not only outperforms existing baselines like OpenAI API and Perspective API in detecting harmful content but also exhibits unparalleled resilience to jailbreaking attacks. The innovative use of constrained optimization and a fusion-based guardrail approach represents a significant step forward in developing more secure and reliable LLMs, setting a new standard for content moderation frameworks in the face of evolving digital threats.']"
111,51,111_quantization_quantizer_quantizationaware_quantizing,"['quantization', 'quantizer', 'quantizationaware', 'quantizing', 'quantize', 'quantized', 'imagenet', 'precision', 'bits', 'binarization']","['FLIQS: One-Shot Mixed-Precision Floating-Point and Integer Quantization Search arXiv:2308.03290v2 Announce Type: replace-cross \nAbstract: Quantization has become a mainstream compression technique for reducing model size, computational requirements, and energy consumption for modern deep neural networks (DNNs). With improved numerical support in recent hardware, including multiple variants of integer and floating point, mixed-precision quantization has become necessary to achieve high-quality results with low model cost. Prior mixed-precision methods have performed either a post-training quantization search, which compromises on accuracy, or a differentiable quantization search, which leads to high memory usage from branching. Therefore, we propose the first one-shot mixed-precision quantization search that eliminates the need for retraining in both integer and low-precision floating point models. We evaluate our search (FLIQS) on multiple convolutional and vision transformer networks to discover Pareto-optimal models. Our approach improves upon uniform precision, manual mixed-precision, and recent integer quantization search methods. With integer models, we increase the accuracy of ResNet-18 on ImageNet by 1.31% and ResNet-50 by 0.90% with equivalent model cost over previous methods. Additionally, for the first time, we explore a novel mixed-precision floating-point search and improve MobileNetV2 by up to 0.98% compared to prior state-of-the-art FP8 models. Finally, we extend FLIQS to simultaneously search a joint quantization and neural architecture space and improve the ImageNet accuracy by 2.69% with similar model cost on a MobileNetV2 search space.', 'FLIQS: One-Shot Mixed-Precision Floating-Point and Integer Quantization Search Quantization has become a mainstream compression technique for reducing model size, computational requirements, and energy consumption for modern deep neural networks (DNNs).  With improved numerical support in recent hardware, including multiple variants of integer and floating point, mixed-precision quantization has become necessary to achieve high-quality results with low model cost.  Prior mixed-precision methods have performed either a post-training quantization search, which compromises on accuracy, or a differentiable quantization search, which leads to high memory usage from branching. Therefore, we propose the first one-shot mixed-precision quantization search that eliminates the need for retraining in both integer and low-precision floating point models. We evaluate our search (FLIQS) on multiple convolutional and vision transformer networks to discover Pareto-optimal models. Our approach improves upon uniform precision, manual mixed-precision, and recent integer quantization search methods.  With integer models, we increase the accuracy of ResNet-18 on ImageNet by 1.3% points and ResNet-50 by 0.90% points with equivalent model cost over previous methods. Additionally, for the first time, we explore a novel mixed-precision floating-point search and improve MobileNetV2 by up to 0.98% points compared to prior state-of-the-art FP8 models. Finally, we extend FLIQS to simultaneously search a joint quantization and neural architecture space and improve the ImageNet accuracy by 2.69% points with similar model cost on a MobileNetV2 search space.', 'Greedy Search Algorithm for Mixed Precision in Post-Training Quantization of Convolutional Neural Network Inspired by Submodular Optimization For lower bit-widths such as less than 8-bit, many quantization strategies include re-training in order to recover accuracy degradation. However, the re-training works against rapid deployment for wide distribution of quantized models. Therefore, post-training quantization has been getting more attention in recent years. In one example, partial quantization according to the layer sensitivity based on the accuracy after each quantization has been proposed; however, the effects of one layer quantization on the other layers has not taken into account. To further reduce the accuracy degradation, we propose a quantization scheme that considers the effects by continuously updating the accuracy after each layer quantization. Additionally, for more data compression, we extend that scheme to mixed precision, which applies a layer-by-layer fitted bit-width. Since the search space for bit allocation per layer increases exponentially with the number of layers $N$, Existing methods require computationally intensive approach such as network training. Here, we derive practical solutions to the bit allocation problem in polynomial time $O(N^2)$ using a deterministic greedy search algorithm inspired by submodular optimization without any training. For example, the proposed algorithm completes a search on ResNet18 for ImageNet in 1 hour for a single GPU. Compared to the case without updating the layer sensitivity, our method improves the accuracy of the quantized model by more than 1% with multiple convolutional neural networks. For examples, 6-bit quantization of MobileNetV2 achieves 80.1% reduction of model size with -1.10% accuracy degradation. 4-bit quantization of ResNet50 achieves 82.9% size reduction with -0.194% accuracy degradation. Furthermore, results show that the proposed method reduces the accuracy degradation by more than about 0.7% compared to various latest post-training quantization strategies.']"
112,18,112_quantization_quantizing_quantizationaware_quantize,"['quantization', 'quantizing', 'quantizationaware', 'quantize', 'memory', 'quantized', 'compression', 'bits', 'memorycost', 'compressing']","['SqueezeLLM: Dense-and-Sparse Quantization Generative Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks. However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements. This has forced existing deployment frameworks to use multi-GPU inference pipelines, which are often complex and costly, or to use smaller and less performant models. In this work, we demonstrate that the main bottleneck for generative inference with LLMs is memory bandwidth, rather than compute, specifically for single batch inference. While quantization has emerged as a promising solution by representing weights with reduced precision, previous efforts have often resulted in notable performance degradation. To address this, we introduce SqueezeLLM, a post-training quantization framework that not only enables lossless compression to ultra-low precisions of up to 3-bit, but also achieves higher quantization performance under the same memory constraint. Our framework incorporates two novel ideas: (i) sensitivity-based non-uniform quantization, which searches for the optimal bit precision assignment based on second-order information; and (ii) the Dense-and-Sparse decomposition that stores outliers and sensitive weight values in an efficient sparse format. When applied to the LLaMA models, our 3-bit quantization significantly reduces the perplexity gap from the FP16 baseline by up to 2.1x as compared to the state-of-the-art methods with the same memory requirement. Furthermore, when deployed on an A6000 GPU, our quantized models achieve up to 2.3x speedup compared to the baseline. Our code is available at https://github.com/SqueezeAILab/SqueezeLLM.', 'Residual vector quantization for KV cache compression in large language model KV cache compression methods have mainly relied on scalar quantization techniques to reduce the memory requirements during decoding. In this work, we apply residual vector quantization, which has been widely used for high fidelity audio compression, to compress KV cache in large language models (LLM). We adapt the standard recipe with minimal changes to compress the output of any key or value projection matrix in a pretrained LLM: we scale the vector by its standard deviation, divide channels into groups and then quantize each group with the same residual vector quantizer. We learn the codebook using exponential moving average and there are no other learnable parameters including the input and output projections normally used in a vector quantization set up. We find that a residual depth of 8 recovers most of the performance of the unquantized model. We also find that grouping non-contiguous channels together works better than grouping contiguous channels for compressing key matrix and the method further benefits from a light weight finetuning of LLM together with the quantization. Overall, the proposed technique is competitive with existing quantization methods while being much simpler and results in \xa05.5x compression compared to half precision.', 'SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models Large language models (LLMs) show excellent performance but are compute- and memory-intensive. Quantization can reduce memory and accelerate inference. However, existing methods cannot maintain accuracy and hardware efficiency at the same time. We propose SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation. SmoothQuant enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, and LLaMA family. We demonstrate up to 1.56$\\times$ speedup and 2$\\times$ memory reduction for LLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM within a single node. Our work offers a turn-key solution that reduces hardware costs and democratizes LLMs.']"
113,41,113_robot_robotics_robots_humanrobot,"['robot', 'robotics', 'robots', 'humanrobot', 'corobots', 'interaction', 'interact', 'human', 'interactions', 'interactionsp']","['Learning Robot Objectives from Physical Human Interaction When humans and robots work in close proximity, physical interaction is inevitable.  Traditionally, robots treat physical interaction as a disturbance, and resume their original behavior after the interaction ends. In contrast, we argue that physical human interaction is informative: it is useful information about how the robot should be doing its task.  We formalize learning from such interactions as a dynamical system in which the task objective has parameters that are part of the hidden state, and physical human interactions are observations about these parameters. We derive an online approximation of the robot’s optimal policy in this system, and test it in a user study. The results suggest that learning from physical interaction leads to better robot task performance with less human effort.', 'Learning Robot Objectives from Physical Human Interaction <p>Humans physically interact with each other every day – from grabbing someone’s hand when they are about to spill their drink, to giving your friend a nudge to steer them in the right direction, physical interaction is an intuitive way to convey information about personal preferences and how to perform a task correctly.</p>\n\n<p>So why aren’t we physically interacting with current robots the way we do with each other? Seamless physical interaction between a human and a robot requires a lot: lightweight robot designs, reliable torque or force sensors, safe and reactive control schemes, the ability to predict the intentions of human collaborators, and more! Luckily, robotics has made many advances in the design of <a href=""http://www.roboticgizmos.com/wp-content/uploads/2016/10/20/jaco2.gif"">personal robots</a> specifically developed with humans in mind.</p>\n\n<p>However, consider the example from the beginning where you grab your friend’s hand as they are about to spill their drink. Instead of your friend who is spilling, imagine it was a robot. Because state-of-the-art robot planning and control algorithms typically assume human physical interventions are disturbances, once you let go of the robot, it will resume its erroneous trajectory and continue spilling the drink. The key to this gap comes from how robots reason about physical interaction: instead of thinking about <em>why</em> the human physically intervened and replanning in accordance with what the human wants, most robots simply resume their original behavior after the interaction ends.</p>\n\n<p>We argue that <strong>robots should treat physical human interaction as useful information about how they should be doing the task</strong>. We formalize reacting to physical interaction as an objective (or reward) learning problem and propose a solution that enables robots to change their behaviors <em>while they are performing a task</em> according to the information gained during these interactions.</p>\n\n<!--more-->\n\n<h2 id=""reasoning-about-physical-interaction-unknown-disturbance-versus-intentional-information"">Reasoning About Physical Interaction: Unknown Disturbance versus Intentional Information</h2>\n\n<p>The field of <em>physical human-robot interaction</em> (pHRI) studies the design, control, and planning problems that arise from close physical interaction between a human and a robot in a shared workspace. Prior research in pHRI has developed safe and responsive control methods to react to a physical interaction that happens while the robot is performing a task. Proposed by <a href=""http://summerschool.stiff-project.org/fileadmin/pdf/Hog1985.pdf"">Hogan et. al.</a>, impedance control is one of the most commonly used methods to move a robot along a desired trajectory when there are people in the workspace. With this control method, the <strong>robot acts like a spring</strong>: it allows the person to push it, but moves back to an original desired position after the human stops applying forces. While this strategy is very fast and enables the robot to safely adapt to the human’s forces, the robot does not leverage these interventions to update its understanding of the task. Left alone, the robot would continue to perform the task in the same way as it had planned before any human interactions.</p>\n\n<p><img style=""float:right; margin:20px"" src=""http://bair.berkeley.edu/static/blog/phri/impedance_control.gif"" alt=""impedance_control"" width=""50%"" /></p>\n\n<p>Why is this the case? It boils down to what assumptions the robot makes about its knowledge of the task and the meaning of the forces it senses. Typically, a robot is given a notion of its task in the form of an <em>objective function</em>. This objective function encodes rewards for different aspects of the task like  “reach a goal at location X”  or  “move close to the table while staying far away from people”. The robot uses its objective function to produce a motion that best satisfies all the aspects of the task: for example, the robot would move toward goal X while choosing a path that is far from a human and close to the table. If the robot’s original objective function was correct, then any physical interaction is simply a disturbance from its correct path. Thus, the robot should allow the physical interaction to perturb it for safety purposes, but it will return to the original path it planned since it stubbornly believes it is correct.</p>\n\n<p>In contrast, we argue that human interventions are often intentional and occur because the robot is doing something wrong. While the robot’s original behavior may have been optimal with respect to its pre-defined objective function, the fact that a human intervention was necessary implies that <strong>the original objective function was not quite right</strong>. Thus, physical human interactions are no longer disturbances but rather informative observations about what the robot’s true objective should be. With this in mind, we take inspiration from <a href=""http://ai.stanford.edu/~ang/papers/icml00-irl.pdf"">inverse reinforcement learning</a> (IRL), where the robot observes some behavior (e.g., being pushed away from the table) and tries to infer an unknown objective function (e.g., “stay farther away from the table”). Note that while many IRL methods focus on the robot doing better <em>the next time</em> it performs the task, we focus on the robot completing its <em>current</em> task correctly.</p>\n\n<h2 id=""formalizing-reacting-to-phri"">Formalizing Reacting to pHRI</h2>\n\n<p>With our insight on physical human-robot interactions, we can formalize pHRI as a dynamical system, where the robot is unsure about the correct objective function and the human’s interactions provide it with information. This formalism defines a broad class of pHRI algorithms, which includes existing methods such as impedance control, and enables us to derive a novel online learning method.</p>\n\n<p>We will focus on two parts of the formalism: (1) the structure of the objective function and (2) the observation model that lets the robot reason about the objective given a human physical interaction. Let <script type=""math/tex"">x</script> be the robot’s state (e.g., position and velocity) and <script type=""math/tex"">u_R</script> be the robot’s action (e.g., the torque it applies to its joints). The human can physically interact with the robot by applying an external torque, called <script type=""math/tex"">u_H</script>, and the robot moves to the next state via its dynamics, <script type=""math/tex"">\\dot{x} = f(x,u_R+u_H)</script>.</p>\n\n<h3 id=""the-robot-objective-doing-the-task-right-with-minimal-human-interaction"">The Robot Objective: Doing the Task Right with Minimal Human Interaction</h3>\n\n<p>In pHRI, we want the robot to learn from the human, but at the same time we do not want to overburden the human with constant physical intervention. Hence, we can write down an objective for the robot that optimizes both completing the task and minimizing the amount of interaction required, ultimately trading off between the two.</p>\n\n<script type=""math/tex; mode=display"">r(x,u_R,u_H;\\theta) = \\theta^{\\top} \\phi(x,u_R,u_H) - ||u_H||^2</script>\n\n<p>Here, <script type=""math/tex"">\\phi(x,u_R,u_H)</script> encodes the task-related features (e.g., “distance to table”, “distance to human”, “distance to goal”) and <script type=""math/tex"">\\theta</script> determines the relative weight of each of these features. In the function, <script type=""math/tex"">\\theta</script> encapsulates the true objective – if the robot knew exactly how to weight all the aspects of its task, then it could compute how to perform the task optimally. However, this parameter is not known by the robot! Robots will not always know the right way to perform a task, and certainly not the human-preferred way.</p>\n\n<h3 id=""the-observation-model-inferring-the-right-objective-from-human-interaction"">The Observation Model: Inferring the Right Objective from Human Interaction</h3>\n\n<p>As we have argued, the robot should observe the human’s actions to infer the unknown task objective. To link the direct human forces that the robot measures with the objective function, the robot uses an <em>observation model</em>. Building on prior work in  <a href=""https://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf"">maximum entropy IRL</a> as well as the Bolzmann distributions used in <a href=""http://web.mit.edu/clbaker/www/papers/cogsci2007.pdf"">cognitive science models</a> of human behavior, we model the human’s interventions as corrections which approximately maximize the robot’s expected reward at state <script type=""math/tex"">x</script> while taking action <script type=""math/tex"">u_R+u_H</script>. This expected reward emcompasses the immediate and future rewards and is captured by the <script type=""math/tex"">Q</script>-value:</p>\n\n<script type=""math/tex; mode=display"">P(u_H \\mid x, u_R; \\theta) \\propto e^{Q(x,u_R+u_H;\\theta)}</script>\n\n<p>Intuitively, this model says that a human is more likely to choose a physical correction that, when combined with the robot’s action, leads to a desirable (i.e., high-reward) behavior.</p>\n\n<h2 id=""learning-from-physical-human-robot-interactions-in-real-time"">Learning from Physical Human-Robot Interactions in Real-Time</h2>\n\n<p>Much like teaching another human, we expect that the robot will continuously learn while we interact with it. However, the learning framework that we have introduced requires that the robot solve a Partially Observable Markov Decision Process (POMDP); unfortunately, it is well known that solving POMDPs exactly is at best computationally expensive, and at worst intractable. Nonetheless, we can derive approximations from this formalism that can enable the robot to learn and act while humans are interacting.</p>\n\n<p>To achieve such in-task learning, we make three approximations summarized below:</p>\n\n<p><strong>1) Separate estimating the true objective from solving for the optimal control policy.</strong> This means at every timestep, the robot updates its belief over possible <script type=""math/tex"">\\theta</script> values, and then re-plans an optimal control policy with the new distribution.</p>\n\n<p><strong>2) Separate planning from control</strong>. Computing an optimal control policy means computing the optimal action to take at every state in a continuous state, action, and belief space. Although re-computing a full optimal <em>policy</em> after every interaction is not tractable in real-time, we can re-compute an optimal <em>trajectory</em> from the current state in real-time. This means that the robot first plans a trajectory that best satisfies the current estimate of the objective, and then uses an impedance controller to track this trajectory.  The use of impedance control here gives us the nice properties described earlier, where people can physically modify the robot’s state while still being safe during interaction.</p>\n\n<p>Looking back at our estimation step, we will make a similar shift to trajectory space and modify our observation model to reflect this:</p>\n\n<script type=""math/tex; mode=display"">P(u_H \\mid x, u_R; \\theta) \\propto e^{Q(x,u_R+u_H;\\theta)} \\rightarrow P(\\xi_H \\mid \\xi_R; \\theta) \\propto e^{R(\\xi_H, \\xi_R;\\theta)}</script>\n\n<p>Now, our observation model depends only on the cumulative reward <script type=""math/tex"">R</script> along a trajectory, which is easily computed by summing up the reward at each timestep. With this approximation, when reasoning about the true objective, the robot only has to consider the likelihood of a human’s preferred trajectory, <script type=""math/tex"">\\xi_H</script>, given the current trajectory it is executing, <script type=""math/tex"">\\xi_R</script>.</p>\n\n<p>But what is the human’s preferred trajectory, <script type=""math/tex"">\\xi_H</script>? The robot only gets to directly measure the human’s force $u_H$. One way to infer what is the human’s preferred trajectory is by propagating the human’s force throughout the robot’s current trajectory, <script type=""math/tex"">\\xi_R</script>. Figure 1. builds up the trajectory deformation based on prior work from <a href=""http://dylanlosey.com/wp-content/uploads/2016/07/TRO_2017.pdf"">Losey and O’Malley</a>, starting from the robot’s original trajectory, then the force application, and then the deformation to produce <script type=""math/tex"">\\xi_H</script>.</p>\n\n<p style=""text-align:center;"">\n<img width=""50%"" src=""http://bair.berkeley.edu/static/blog/phri/deformation_process.png"" alt=""deformation_process"" /><br />\n<i>\nFig 1. To infer the human’s prefered trajectory given the current planned trajectory, the robot first measures the human’s interaction force, $u_H$, and then smoothly deforms the waypoints near interaction point to get the human’s preferred trajectory, $\\xi_H$.\n</i>\n</p>\n\n<p><strong>3) Plan with maximum a posteriori (MAP) estimate of <script type=""math/tex"">\\theta</script></strong>. Finally, because <script type=""math/tex"">\\theta</script> is a continuous variable and potentially high-dimensional, and since our observation model is not Gaussian, rather than planning with the full belief over <script type=""math/tex"">\\theta</script>, we will plan only with the MAP estimate. We find that the MAP estimate under a 2nd order Taylor Series Expansion about the robot’s current trajectory with a Gaussian prior is equivalent to running online gradient descent:</p>\n\n<script type=""math/tex; mode=display"">\\theta^{t+1} = \\theta^{t} + \\alpha(\\Phi(\\xi^t_H) - \\Phi(\\xi^t_R))</script>\n\n<p>At every timestep, the robot updates its estimate of <script type=""math/tex"">\\theta</script> in the direction of the cumulative feature difference, <script type=""math/tex"">\\Phi(\\xi) = \\sum_{x^t \\in \\xi} \\phi(x^t)</script>, between its current optimal trajectory and the human’s preferred trajectory. In the Learning from Demonstration literature, this update rule is analogous to online <a href=""https://www.ri.cmu.edu/pub_files/pub4/ratliff_nathan_2006_1/ratliff_nathan_2006_1.pdf"">Max Margin Planning</a>; it is also analogous to <a href=""https://arxiv.org/pdf/1601.00741.pdf"">coactive learning</a>, where the user modifies waypoints for the current task to teach a reward function for future tasks.</p>\n\n<p>Ultimately, putting these three steps together leads us to an elegant approximate solution to the original POMDP. At every timestep, the robot plans a trajectory <script type=""math/tex"">\\xi_R</script> and begins to move. The human can physically interact, enabling the robot to sense their force $u_H$. The robot uses the human’s force to deform its original trajectory and produce the human’s desired trajectory, <script type=""math/tex"">\\xi_H</script>. Then the robot reasons about what aspects of the task are different between its original and the human’s preferred trajectory, and updates <script type=""math/tex"">\\theta</script> in the direction of that difference. Using the new feature weights, the robot replans a trajectory that better aligns with the human’s preferences.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/static/blog/phri/algorithm.gif"" alt=""algorithm"" />\n</p>\n\n<p>For a more thorough description of our formalism and approximations, please see <a href=""http://proceedings.mlr.press/v78/bajcsy17a/bajcsy17a.pdf"">our recent paper from the 2017 Conference on Robot Learning</a>.</p>\n\n<h2 id=""learning-from-humans-in-the-real-world"">Learning from Humans in the Real World</h2>\n\n<p>To evaluate the benefits of in-task learning on a real personal robot, we recruited 10 participants for a user study. Each participant interacted with the robot running our proposed online learning method as well as a baseline where the robot did not learn from physical interaction and simply ran impedance control.</p>\n\n<p>Fig 2. shows the three experimental household manipulation tasks, in each of which the robot started with an initially incorrect objective that participants had to correct. For example, the robot would move a cup from the shelf to the table, but without worrying about tilting the cup (perhaps not noticing that there is liquid inside).</p>\n\n<p style=""text-align:center;"">\n<img width=""30%"" src=""http://bair.berkeley.edu/static/blog/phri/task1.png"" title=""cup"" />\n<img width=""30%"" src=""http://bair.berkeley.edu/static/blog/phri/task2.png"" title=""table"" />\n<img width=""30%"" src=""http://bair.berkeley.edu/static/blog/phri/task3.png"" title=""laptop"" />\n<br />\n<i>\nFig 2. Trajectory generated with initial objective marked in black, and the desired trajectory from true objective in blue. Participants need to correct the robot to teach it to hold the cup upright (left), move closer to the table (center), and avoid going over the laptop (right).  </i>\n</p>\n\n<p>We measured the robot’s performance with respect to the true objective, the total effort the participant exerted, the total amount of interaction time, and the responses of a 7-point Likert scale survey.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/static/blog/phri/task1.gif"" alt=""cup gif"" /><br />\n<i>\nIn Task 1, participants have to physically intervene when they see the robot tilting the cup and teach the robot to keep the cup upright.  \n</i>\n</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/static/blog/phri/task2.gif"" alt=""table gif"" /><br />\n<i>\nTask 2 had participants teaching the robot to move closer to the table.\n</i>\n</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/static/blog/phri/task3.gif"" alt=""laptop gif"" /><br />\n<i>\nFor Task 3, the robot’s original trajectory goes over a laptop. Participants have to physically teach the robot to move around the laptop instead of over it.\n</i>\n</p>\n\n<p>The results of our user studies suggest that learning from physical interaction leads to better robot task performance with less human effort. Participants were able to <strong>get the robot to execute the correct behavior faster with less effort and interaction time</strong> when the robot was actively learning from their interactions during the task. Additionally, <strong>participants believed the robot understood their preferences more, took less effort to interact with, and was a more collaborative partner</strong>.</p>\n\n<p style=""text-align:center;"">\n<img width=""30%"" src=""http://bair.berkeley.edu/static/blog/phri/taskCost_cameraready.png"" title=""task cost"" />\n<img width=""30%"" src=""http://bair.berkeley.edu/static/blog/phri/taskEffort_cameraready.png"" title=""task effort"" />\n<img width=""30%"" src=""http://bair.berkeley.edu/static/blog/phri/taskTime_cameraready.png"" title=""task time"" />\n<br />\n<i>\nFig 3. Learning from interaction significantly outperformed not learning for each of our objective measures, including task cost, human effort, interaction time.\n</i>\n</p>\n\n<p>Ultimately, we propose that robots should not treat human interactions as disturbances, but rather as informative actions. We showed that robots imbued with this sort of reasoning are capable of updating their understanding of the task they are performing and completing it correctly, rather than relying on people to guide them until the task is done.</p>\n\n<p>This work is merely a step in exploring learning robot objectives from pHRI. Many open questions remain including developing solutions that can handle dynamical aspects (like preferences about the timing of the motion) and how and when to generalize learned objectives to new tasks. Additionally, robot reward functions will often have many task-related features and human interactions may only give information about a certain subset of relevant weights. Our recent work in HRI 2018 studied how a robot can disambiguate what the person is trying to correct by learning about only a single feature weight at a time. Overall, not only do we need algorithms that can learn from physical interaction with humans, but these methods must also reason about the inherent difficulties humans experience when trying to kinesthetically teach a complex – and possibly unfamiliar – robotic system.</p>\n\n<hr />\n\n<p>Thank you to Dylan Losey and Anca Dragan for their helpful feedback in writing this blog post.</p>\n\n<hr />\n\n<p>This post is based on the following papers:</p>\n\n<ul>\n  <li>\n    <p>A. Bajcsy* , D.P. Losey*, M.K. O’Malley, and A.D. Dragan. <strong>Learning Robot Objectives from Physical Human Robot Interaction</strong>. Conference on Robot Learning (CoRL), 2017.</p>\n  </li>\n  <li>\n    <p>A. Bajcsy , D.P. Losey, M.K. O’Malley, and A.D. Dragan. <strong>Learning from Physical Human Corrections, One Feature at a Time</strong>. International Conference on Human-Robot Interaction (HRI), 2018.</p>\n  </li>\n</ul>\n\n', 'Learning Robot Objectives from Physical Human Interaction <p>Humans physically interact with each other every day – from grabbing someone’s hand when they are about to spill their drink, to giving your friend a nudge to steer them in the right direction, physical interaction is an intuitive way to convey information about personal preferences and how to perform a task correctly.</p>\n\n<p>So why aren’t we physically interacting with current robots the way we do with each other? Seamless physical interaction between a human and a robot requires a lot: lightweight robot designs, reliable torque or force sensors, safe and reactive control schemes, the ability to predict the intentions of human collaborators, and more! Luckily, robotics has made many advances in the design of <a href=""http://www.roboticgizmos.com/wp-content/uploads/2016/10/20/jaco2.gif"">personal robots</a> specifically developed with humans in mind.</p>\n\n<p>However, consider the example from the beginning where you grab your friend’s hand as they are about to spill their drink. Instead of your friend who is spilling, imagine it was a robot. Because state-of-the-art robot planning and control algorithms typically assume human physical interventions are disturbances, once you let go of the robot, it will resume its erroneous trajectory and continue spilling the drink. The key to this gap comes from how robots reason about physical interaction: instead of thinking about <em>why</em> the human physically intervened and replanning in accordance with what the human wants, most robots simply resume their original behavior after the interaction ends.</p>\n\n<p>We argue that <strong>robots should treat physical human interaction as useful information about how they should be doing the task</strong>. We formalize reacting to physical interaction as an objective (or reward) learning problem and propose a solution that enables robots to change their behaviors <em>while they are performing a task</em> according to the information gained during these interactions.</p>\n\n<!--more-->\n\n<h2 id=""reasoning-about-physical-interaction-unknown-disturbance-versus-intentional-information"">Reasoning About Physical Interaction: Unknown Disturbance versus Intentional Information</h2>\n\n<p>The field of <em>physical human-robot interaction</em> (pHRI) studies the design, control, and planning problems that arise from close physical interaction between a human and a robot in a shared workspace. Prior research in pHRI has developed safe and responsive control methods to react to a physical interaction that happens while the robot is performing a task. Proposed by <a href=""http://summerschool.stiff-project.org/fileadmin/pdf/Hog1985.pdf"">Hogan et. al.</a>, impedance control is one of the most commonly used methods to move a robot along a desired trajectory when there are people in the workspace. With this control method, the <strong>robot acts like a spring</strong>: it allows the person to push it, but moves back to an original desired position after the human stops applying forces. While this strategy is very fast and enables the robot to safely adapt to the human’s forces, the robot does not leverage these interventions to update its understanding of the task. Left alone, the robot would continue to perform the task in the same way as it had planned before any human interactions.</p>\n\n<p><img style=""float:right; margin:20px"" src=""http://bair.berkeley.edu/static/blog/phri/impedance_control.gif"" alt=""impedance_control"" width=""50%"" /></p>\n\n<p>Why is this the case? It boils down to what assumptions the robot makes about its knowledge of the task and the meaning of the forces it senses. Typically, a robot is given a notion of its task in the form of an <em>objective function</em>. This objective function encodes rewards for different aspects of the task like  “reach a goal at location X”  or  “move close to the table while staying far away from people”. The robot uses its objective function to produce a motion that best satisfies all the aspects of the task: for example, the robot would move toward goal X while choosing a path that is far from a human and close to the table. If the robot’s original objective function was correct, then any physical interaction is simply a disturbance from its correct path. Thus, the robot should allow the physical interaction to perturb it for safety purposes, but it will return to the original path it planned since it stubbornly believes it is correct.</p>\n\n<p>In contrast, we argue that human interventions are often intentional and occur because the robot is doing something wrong. While the robot’s original behavior may have been optimal with respect to its pre-defined objective function, the fact that a human intervention was necessary implies that <strong>the original objective function was not quite right</strong>. Thus, physical human interactions are no longer disturbances but rather informative observations about what the robot’s true objective should be. With this in mind, we take inspiration from <a href=""http://ai.stanford.edu/~ang/papers/icml00-irl.pdf"">inverse reinforcement learning</a> (IRL), where the robot observes some behavior (e.g., being pushed away from the table) and tries to infer an unknown objective function (e.g., “stay farther away from the table”). Note that while many IRL methods focus on the robot doing better <em>the next time</em> it performs the task, we focus on the robot completing its <em>current</em> task correctly.</p>\n\n<h2 id=""formalizing-reacting-to-phri"">Formalizing Reacting to pHRI</h2>\n\n<p>With our insight on physical human-robot interactions, we can formalize pHRI as a dynamical system, where the robot is unsure about the correct objective function and the human’s interactions provide it with information. This formalism defines a broad class of pHRI algorithms, which includes existing methods such as impedance control, and enables us to derive a novel online learning method.</p>\n\n<p>We will focus on two parts of the formalism: (1) the structure of the objective function and (2) the observation model that lets the robot reason about the objective given a human physical interaction. Let <script type=""math/tex"">x</script> be the robot’s state (e.g., position and velocity) and <script type=""math/tex"">u_R</script> be the robot’s action (e.g., the torque it applies to its joints). The human can physically interact with the robot by applying an external torque, called <script type=""math/tex"">u_H</script>, and the robot moves to the next state via its dynamics, <script type=""math/tex"">\\dot{x} = f(x,u_R+u_H)</script>.</p>\n\n<h3 id=""the-robot-objective-doing-the-task-right-with-minimal-human-interaction"">The Robot Objective: Doing the Task Right with Minimal Human Interaction</h3>\n\n<p>In pHRI, we want the robot to learn from the human, but at the same time we do not want to overburden the human with constant physical intervention. Hence, we can write down an objective for the robot that optimizes both completing the task and minimizing the amount of interaction required, ultimately trading off between the two.</p>\n\n<script type=""math/tex; mode=display"">r(x,u_R,u_H;\\theta) = \\theta^{\\top} \\phi(x,u_R,u_H) - ||u_H||^2</script>\n\n<p>Here, <script type=""math/tex"">\\phi(x,u_R,u_H)</script> encodes the task-related features (e.g., “distance to table”, “distance to human”, “distance to goal”) and <script type=""math/tex"">\\theta</script> determines the relative weight of each of these features. In the function, <script type=""math/tex"">\\theta</script> encapsulates the true objective – if the robot knew exactly how to weight all the aspects of its task, then it could compute how to perform the task optimally. However, this parameter is not known by the robot! Robots will not always know the right way to perform a task, and certainly not the human-preferred way.</p>\n\n<h3 id=""the-observation-model-inferring-the-right-objective-from-human-interaction"">The Observation Model: Inferring the Right Objective from Human Interaction</h3>\n\n<p>As we have argued, the robot should observe the human’s actions to infer the unknown task objective. To link the direct human forces that the robot measures with the objective function, the robot uses an <em>observation model</em>. Building on prior work in  <a href=""https://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf"">maximum entropy IRL</a> as well as the Bolzmann distributions used in <a href=""http://web.mit.edu/clbaker/www/papers/cogsci2007.pdf"">cognitive science models</a> of human behavior, we model the human’s interventions as corrections which approximately maximize the robot’s expected reward at state <script type=""math/tex"">x</script> while taking action <script type=""math/tex"">u_R+u_H</script>. This expected reward emcompasses the immediate and future rewards and is captured by the <script type=""math/tex"">Q</script>-value:</p>\n\n<script type=""math/tex; mode=display"">P(u_H \\mid x, u_R; \\theta) \\propto e^{Q(x,u_R+u_H;\\theta)}</script>\n\n<p>Intuitively, this model says that a human is more likely to choose a physical correction that, when combined with the robot’s action, leads to a desirable (i.e., high-reward) behavior.</p>\n\n<h2 id=""learning-from-physical-human-robot-interactions-in-real-time"">Learning from Physical Human-Robot Interactions in Real-Time</h2>\n\n<p>Much like teaching another human, we expect that the robot will continuously learn while we interact with it. However, the learning framework that we have introduced requires that the robot solve a Partially Observable Markov Decision Process (POMDP); unfortunately, it is well known that solving POMDPs exactly is at best computationally expensive, and at worst intractable. Nonetheless, we can derive approximations from this formalism that can enable the robot to learn and act while humans are interacting.</p>\n\n<p>To achieve such in-task learning, we make three approximations summarized below:</p>\n\n<p><strong>1) Separate estimating the true objective from solving for the optimal control policy.</strong> This means at every timestep, the robot updates its belief over possible <script type=""math/tex"">\\theta</script> values, and then re-plans an optimal control policy with the new distribution.</p>\n\n<p><strong>2) Separate planning from control</strong>. Computing an optimal control policy means computing the optimal action to take at every state in a continuous state, action, and belief space. Although re-computing a full optimal <em>policy</em> after every interaction is not tractable in real-time, we can re-compute an optimal <em>trajectory</em> from the current state in real-time. This means that the robot first plans a trajectory that best satisfies the current estimate of the objective, and then uses an impedance controller to track this trajectory.  The use of impedance control here gives us the nice properties described earlier, where people can physically modify the robot’s state while still being safe during interaction.</p>\n\n<p>Looking back at our estimation step, we will make a similar shift to trajectory space and modify our observation model to reflect this:</p>\n\n<script type=""math/tex; mode=display"">P(u_H \\mid x, u_R; \\theta) \\propto e^{Q(x,u_R+u_H;\\theta)} \\rightarrow P(\\xi_H \\mid \\xi_R; \\theta) \\propto e^{R(\\xi_H, \\xi_R;\\theta)}</script>\n\n<p>Now, our observation model depends only on the cumulative reward <script type=""math/tex"">R</script> along a trajectory, which is easily computed by summing up the reward at each timestep. With this approximation, when reasoning about the true objective, the robot only has to consider the likelihood of a human’s preferred trajectory, <script type=""math/tex"">\\xi_H</script>, given the current trajectory it is executing, <script type=""math/tex"">\\xi_R</script>.</p>\n\n<p>But what is the human’s preferred trajectory, <script type=""math/tex"">\\xi_H</script>? The robot only gets to directly measure the human’s force $u_H$. One way to infer what is the human’s preferred trajectory is by propagating the human’s force throughout the robot’s current trajectory, <script type=""math/tex"">\\xi_R</script>. Figure 1. builds up the trajectory deformation based on prior work from <a href=""http://dylanlosey.com/wp-content/uploads/2016/07/TRO_2017.pdf"">Losey and O’Malley</a>, starting from the robot’s original trajectory, then the force application, and then the deformation to produce <script type=""math/tex"">\\xi_H</script>.</p>\n\n<p style=""text-align:center;"">\n<img width=""50%"" src=""http://bair.berkeley.edu/static/blog/phri/deformation_process.png"" alt=""deformation_process"" /><br />\n<i>\nFig 1. To infer the human’s prefered trajectory given the current planned trajectory, the robot first measures the human’s interaction force, $u_H$, and then smoothly deforms the waypoints near interaction point to get the human’s preferred trajectory, $\\xi_H$.\n</i>\n</p>\n\n<p><strong>3) Plan with maximum a posteriori (MAP) estimate of <script type=""math/tex"">\\theta</script></strong>. Finally, because <script type=""math/tex"">\\theta</script> is a continuous variable and potentially high-dimensional, and since our observation model is not Gaussian, rather than planning with the full belief over <script type=""math/tex"">\\theta</script>, we will plan only with the MAP estimate. We find that the MAP estimate under a 2nd order Taylor Series Expansion about the robot’s current trajectory with a Gaussian prior is equivalent to running online gradient descent:</p>\n\n<script type=""math/tex; mode=display"">\\theta^{t+1} = \\theta^{t} + \\alpha(\\Phi(\\xi^t_H) - \\Phi(\\xi^t_R))</script>\n\n<p>At every timestep, the robot updates its estimate of <script type=""math/tex"">\\theta</script> in the direction of the cumulative feature difference, <script type=""math/tex"">\\Phi(\\xi) = \\sum_{x^t \\in \\xi} \\phi(x^t)</script>, between its current optimal trajectory and the human’s preferred trajectory. In the Learning from Demonstration literature, this update rule is analogous to online <a href=""https://www.ri.cmu.edu/pub_files/pub4/ratliff_nathan_2006_1/ratliff_nathan_2006_1.pdf"">Max Margin Planning</a>; it is also analogous to <a href=""https://arxiv.org/pdf/1601.00741.pdf"">coactive learning</a>, where the user modifies waypoints for the current task to teach a reward function for future tasks.</p>\n\n<p>Ultimately, putting these three steps together leads us to an elegant approximate solution to the original POMDP. At every timestep, the robot plans a trajectory <script type=""math/tex"">\\xi_R</script> and begins to move. The human can physically interact, enabling the robot to sense their force $u_H$. The robot uses the human’s force to deform its original trajectory and produce the human’s desired trajectory, <script type=""math/tex"">\\xi_H</script>. Then the robot reasons about what aspects of the task are different between its original and the human’s preferred trajectory, and updates <script type=""math/tex"">\\theta</script> in the direction of that difference. Using the new feature weights, the robot replans a trajectory that better aligns with the human’s preferences.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/static/blog/phri/algorithm.gif"" alt=""algorithm"" />\n</p>\n\n<p>For a more thorough description of our formalism and approximations, please see <a href=""http://proceedings.mlr.press/v78/bajcsy17a/bajcsy17a.pdf"">our recent paper from the 2017 Conference on Robot Learning</a>.</p>\n\n<h2 id=""learning-from-humans-in-the-real-world"">Learning from Humans in the Real World</h2>\n\n<p>To evaluate the benefits of in-task learning on a real personal robot, we recruited 10 participants for a user study. Each participant interacted with the robot running our proposed online learning method as well as a baseline where the robot did not learn from physical interaction and simply ran impedance control.</p>\n\n<p>Fig 2. shows the three experimental household manipulation tasks, in each of which the robot started with an initially incorrect objective that participants had to correct. For example, the robot would move a cup from the shelf to the table, but without worrying about tilting the cup (perhaps not noticing that there is liquid inside).</p>\n\n<p style=""text-align:center;"">\n<img width=""30%"" src=""http://bair.berkeley.edu/static/blog/phri/task1.png"" title=""cup"" />\n<img width=""30%"" src=""http://bair.berkeley.edu/static/blog/phri/task2.png"" title=""table"" />\n<img width=""30%"" src=""http://bair.berkeley.edu/static/blog/phri/task3.png"" title=""laptop"" />\n<br />\n<i>\nFig 2. Trajectory generated with initial objective marked in black, and the desired trajectory from true objective in blue. Participants need to correct the robot to teach it to hold the cup upright (left), move closer to the table (center), and avoid going over the laptop (right).  </i>\n</p>\n\n<p>We measured the robot’s performance with respect to the true objective, the total effort the participant exerted, the total amount of interaction time, and the responses of a 7-point Likert scale survey.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/static/blog/phri/task1.gif"" alt=""cup gif"" /><br />\n<i>\nIn Task 1, participants have to physically intervene when they see the robot tilting the cup and teach the robot to keep the cup upright.  \n</i>\n</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/static/blog/phri/task2.gif"" alt=""table gif"" /><br />\n<i>\nTask 2 had participants teaching the robot to move closer to the table.\n</i>\n</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/static/blog/phri/task3.gif"" alt=""laptop gif"" /><br />\n<i>\nFor Task 3, the robot’s original trajectory goes over a laptop. Participants have to physically teach the robot to move around the laptop instead of over it.\n</i>\n</p>\n\n<p>The results of our user studies suggest that learning from physical interaction leads to better robot task performance with less human effort. Participants were able to <strong>get the robot to execute the correct behavior faster with less effort and interaction time</strong> when the robot was actively learning from their interactions during the task. Additionally, <strong>participants believed the robot understood their preferences more, took less effort to interact with, and was a more collaborative partner</strong>.</p>\n\n<p style=""text-align:center;"">\n<img width=""30%"" src=""http://bair.berkeley.edu/static/blog/phri/taskCost_cameraready.png"" title=""task cost"" />\n<img width=""30%"" src=""http://bair.berkeley.edu/static/blog/phri/taskEffort_cameraready.png"" title=""task effort"" />\n<img width=""30%"" src=""http://bair.berkeley.edu/static/blog/phri/taskTime_cameraready.png"" title=""task time"" />\n<br />\n<i>\nFig 3. Learning from interaction significantly outperformed not learning for each of our objective measures, including task cost, human effort, interaction time.\n</i>\n</p>\n\n<p>Ultimately, we propose that robots should not treat human interactions as disturbances, but rather as informative actions. We showed that robots imbued with this sort of reasoning are capable of updating their understanding of the task they are performing and completing it correctly, rather than relying on people to guide them until the task is done.</p>\n\n<p>This work is merely a step in exploring learning robot objectives from pHRI. Many open questions remain including developing solutions that can handle dynamical aspects (like preferences about the timing of the motion) and how and when to generalize learned objectives to new tasks. Additionally, robot reward functions will often have many task-related features and human interactions may only give information about a certain subset of relevant weights. Our recent work in HRI 2018 studied how a robot can disambiguate what the person is trying to correct by learning about only a single feature weight at a time. Overall, not only do we need algorithms that can learn from physical interaction with humans, but these methods must also reason about the inherent difficulties humans experience when trying to kinesthetically teach a complex – and possibly unfamiliar – robotic system.</p>\n\n<hr />\n\n<p>Thank you to Dylan Losey and Anca Dragan for their helpful feedback in writing this blog post.</p>\n\n<hr />\n\n<p>This post is based on the following papers:</p>\n\n<ul>\n  <li>\n    <p>A. Bajcsy* , D.P. Losey*, M.K. O’Malley, and A.D. Dragan. <strong>Learning Robot Objectives from Physical Human Robot Interaction</strong>. Conference on Robot Learning (CoRL), 2017.</p>\n  </li>\n  <li>\n    <p>A. Bajcsy , D.P. Losey, M.K. O’Malley, and A.D. Dragan. <strong>Learning from Physical Human Corrections, One Feature at a Time</strong>. International Conference on Human-Robot Interaction (HRI), 2018.</p>\n  </li>\n</ul>\n\n']"
114,19,114_handwriting_visualizations_interactive_visualizing,"['handwriting', 'visualizations', 'interactive', 'visualizing', 'visualization', 'neural', 'visualize', 'generative', 'neuro', 'networks']","['Experiments in Handwriting with a Neural Network Several interactive visualizations of a generative model of handwriting. Some are fun, some are serious.', 'Experiments in Handwriting with a Neural Network Several interactive visualizations of a generative model of handwriting. Some are fun, some are serious.', 'Don’t trust your eyes: on the (un)reliability of feature visualizations How do neural networks extract patterns from pixels? Feature visualizations attempt to answer this important question by visualizing highly activating patterns through optimization. Today, visualization methods form the foundation of our knowledge about the internal workings of neural networks, as a type of mechanistic interpretability. Here we ask: How reliable are feature visualizations? We start our investigation by developing network circuits that trick feature visualizations into showing arbitrary patterns that are completely disconnected from normal network behavior on natural input. We then provide evidence for a similar phenomenon occurring in standard, unmanipulated networks: feature visualizations are processed very differently from standard input, casting doubt on their ability to ""explain"" how neural networks process natural images. This can be used as a sanity check for feature visualizations. We underpin our empirical findings by theory proving that the set of functions that can be reliably understood by feature visualization is extremely small and does not include general black-box neural networks. Therefore, a promising way forward could be the development of networks that enforce certain structures in order to ensure more reliable feature visualizations.']"
115,60,115_locomotion_legged_legs_robot,"['locomotion', 'legged', 'legs', 'robot', 'humanoid', 'robots', 'motions', 'reinforcement', 'walk', 'gait']","['One Policy to Run Them All: an End-to-end Learning Approach to Multi-Embodiment Locomotion Deep Reinforcement Learning techniques are achieving state-of-the-art results in robust legged locomotion. While there exists a wide variety of legged platforms such as quadruped, humanoids, and hexapods, the field is still missing a single learning framework that can control all these different embodiments easily and effectively and possibly transfer, zero or few-shot, to unseen robot embodiments. To close this gap, we introduce URMA, the Unified Robot Morphology Architecture. Our framework brings the end-to-end Multi-Task Reinforcement Learning approach to the realm of legged robots, enabling the learned policy to control any type of robot morphology. The key idea of our method is to allow the network to learn an abstract locomotion controller that can be seamlessly shared between embodiments thanks to our morphology-agnostic encoders and decoders. This flexible architecture can be seen as a first step in building a foundation model for legged robot locomotion. Our experiments show that URMA can learn a locomotion policy on multiple embodiments that can be easily transferred to unseen robot platforms in simulation and the real world.', 'Minimizing Energy Consumption Leads to the Emergence of Gaits in Legged Robots Legged locomotion is commonly studied and expressed as a discrete set of gait patterns, like walk, trot, gallop, which are usually treated as given and pre-programmed in legged robots for efficient locomotion at different speeds. However, fixing a set of pre-programmed gaits limits the generality of locomotion. Recent animal motor studies show that these conventional gaits are only prevalent in ideal flat terrain conditions while real-world locomotion is unstructured and more like bouts of intermittent steps. What principles could lead to both structured and unstructured patterns across mammals and how to synthesize them in robots? In this work, we take an analysis-by-synthesis approach and learn to move by minimizing mechanical energy. We demonstrate that learning to minimize energy consumption plays a key role in the emergence of natural locomotion gaits at different speeds in real quadruped robots. The emergent gaits are structured in ideal terrains and look similar to that of horses and sheep. The same approach leads to unstructured gaits in rough terrains which is consistent with the findings in animal motor control. We validate our hypothesis in both simulation and real hardware across natural terrains. Videos at https://energy-locomotion.github.io', 'Data Efficient Reinforcement Learning for Legged Robots We present a model-based reinforcement learning framework for robot locomotion that achieves walking based on only 4.5 minutes of data collected on a quadruped robot. To accurately model the robot’s dynamics over a long horizon, we introduce a loss function that tracks the model’s prediction over multiple timesteps. We adapt model predictive control to account for planning latency, which allows the learned model to be used for real time control. Additionally, to ensure safe exploration during model learning, we embed prior knowledge of leg trajectories into the action space. The resulting system achieves fast and robust locomotion. Unlike model-free methods, which optimize for a particular task, our planner can use the same learned dynamics for various tasks, simply by changing the reward function.1 To the best of our knowledge, our approach is more than an order of magnitude more sample efficient than current model-free methods.']"
116,15,116_voice_voices_audiovisual_alexa,"['voice', 'voices', 'audiovisual', 'alexa', 'voicecontrolled', 'voiced', 'speechenabled', 'spokenlanguage', 'vocal', 'interactive']","['Los desafíos y oportunidades de las voces sintéticas Compartimos las lecciones extraídas de un avance a pequeña escala de Voice Engine, un modelo para crear voces personalizadas.', 'Amazon Polly introduces a new French female voice, Léa Amazon Polly now offers a choice of a second female French voice, L&eacute;a, in addition to the current female voice, Celine. Amazon Polly also has a French male voice, Mathieu. L&eacute;a is a warm and natural-sounding voice with Parisian accent. Listen to the spoken introduction from L&eacute;a. Listen&nbsp;now Voiced by Amazon Polly With the addition […]', 'New Developer Preview: Use Amazon Polly voices in Alexa skills <a href=""https://aws.amazon.com/polly"" target=""_blank"" rel=""noopener noreferrer"">Amazon Polly</a> is a service that turns text into lifelike speech. Using Amazon Polly you can create applications that talk and build entirely new categories of speech-enabled products. Starting today, you can \n<a href=""https://alexa.au1.qualtrics.com/jfe/form/SV_bruwDnML6hZCKmp"" target=""_blank"" rel=""noopener noreferrer"">apply</a> to participate in a developer preview that allows you to use eight English (U.S.) Amazon Polly voices to narrate your Alexa skills. If your skill uses only a single voice today, you can try changing the voice or adding different voices in the right places to provide an even more engaging experience. Developers in the preview can select a different voice for any utterance by constructing output speech using the Structured Speech Markup Language (SSML) and specifying an Amazon Polly voice using the \n<code class=""lang-code"">voice</code> tag for free in Alexa skills. Note that SSML tags that are only available through the Amazon Polly service and not through Alexa skills will not be available when you use this new capability.']"
117,115,117_audiovisual_voice_audio_voices,"['audiovisual', 'voice', 'audio', 'voices', 'autoencoder', 'acoustic', 'polyphonic', 'generative', 'speaker', 'multispeaker']","[""The Efficacy of Self-Supervised Speech Models for Audio Representations Self-supervised learning (SSL) speech models, which can serve as powerful upstream models to extract meaningful speech representations, have achieved unprecedented success in speech representation learning.  However, their effectiveness on non-speech datasets is relatively less explored. In this work, we propose an ensemble framework, with a combination of ensemble techniques, to fuse SSL speech models' embeddings. Extensive experiments on speech and non-speech audio datasets are conducted to investigate the representation abilities of our ensemble method and its single constituent model. Ablation studies are carried out to evaluate the performances of different ensemble techniques, such as feature averaging and concatenation. All experiments are conducted during NeurIPS 2021 HEAR Challenge as a standard evaluation pipeline provided by competition officials. Results demonstrate SSL speech models' strong abilities on various non-speech tasks, while we also note that they fail to deal with fine-grained music tasks, such as pitch classification and note onset detection. In addition, feature ensemble is shown to have great potential on producing more holistic representations, as our proposed framework generally surpasses state-of-the-art SSL speech/audio models and has superior performance on various datasets compared with other teams in HEAR Challenge. Our code is available at https://github.com/tony10101105/HEAR-2021-NeurIPS-Challenge—NTU-GURA."", 'A$^3$T: Alignment-Aware Acoustic and Text Pretraining for Speech Synthesis and Editing Recently, speech representation learning has improved many speech-related tasks such as speech recognition, speech classification, and speech-to-text translation. However, all the above tasks are in the direction of speech understanding, but for the inverse direction, speech synthesis, the potential of representation learning is yet to be realized, due to the challenging nature of generating high-quality speech. To address this problem, we propose our framework, Alignment-Aware Acoustic-Text Pretraining (A$^3$T), which reconstructs masked acoustic signals with text input and acoustic-text alignment during training. In this way, the pretrained model can generate high quality reconstructed spectrogram, which can be applied to the speech editing and unseen speaker TTS directly. Experiments show A$^3$T outperforms SOTA models on speech editing, and improves multi-speaker speech synthesis without the external speaker verification model.', 'Meta-StyleSpeech : Multi-Speaker Adaptive Text-to-Speech Generation With rapid progress in neural text-to-speech (TTS) models, personalized speech generation is now in high demand for many applications. For practical applicability, a TTS model should generate high-quality speech with only a few audio samples from the given speaker, that are also short in length. However, existing methods either require to fine-tune the model or achieve low adaptation quality without fine-tuning. In this work, we propose StyleSpeech, a new TTS model which not only synthesizes high-quality speech but also effectively adapts to new speakers. Specifically, we propose Style-Adaptive Layer Normalization (SALN) which aligns gain and bias of the text input according to the style extracted from a reference speech audio. With SALN, our model effectively synthesizes speech in the style of the target speaker even from a single speech audio. Furthermore, to enhance StyleSpeech’s adaptation to speech from new speakers, we extend it to Meta-StyleSpeech by introducing two discriminators trained with style prototypes, and performing episodic training. The experimental results show that our models generate high-quality speech which accurately follows the speaker’s voice with single short-duration (1-3 sec) speech audio, significantly outperforming baselines.']"
118,89,118_videos_scenes_audiovideo_spatiotemporal,"['videos', 'scenes', 'audiovideo', 'spatiotemporal', 'objectcentric', 'attention', 'visual', 'video', 'recognition', 'learning']","['Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization In light of recent advances in multimodal Large Language Models (LLMs), there is increasing attention to scaling them from image-text data to more informative real-world videos. Compared to static images, video poses unique challenges for effective large-scale pre-training due to the modeling of its spatiotemporal dynamics. In this paper, we address such limitations in video-language pre-training with an efficient video decomposition that represents each video as keyframes and temporal motions. These are then adapted to an LLM using well-designed tokenizers that discretize visual and temporal information as a few tokens, thus enabling unified generative pre-training of videos, images, and text. At inference, the generated tokens from the LLM are carefully recovered to the original continuous pixel space to create various video content. Our proposed framework is both capable of comprehending and generating image and video content, as demonstrated by its competitive performance across 13 multimodal benchmarks in image and video understanding and generation. Our code and models are available at https://video-lavit.github.io.', 'Time Is MattEr: Temporal Self-supervision for Video Transformers Understanding temporal dynamics of video is an essential aspect of learning better video representations. Recently, transformer-based architectural designs have been extensively explored for video tasks due to their capability to capture long-term dependency of input sequences. However, we found that these Video Transformers are still biased to learn spatial dynamics rather than temporal ones, and debiasing the spurious correlation is critical for their performance. Based on the observations, we design simple yet effective self-supervised tasks for video models to learn temporal dynamics better. Specifically, for debiasing the spatial bias, our method learns the temporal order of video frames as extra self-supervision and enforces the randomly shuffled frames to have low-confidence outputs. Also, our method learns the temporal flow direction of video tokens among consecutive frames for enhancing the correlation toward temporal dynamics. Under various video action recognition tasks, we demonstrate the effectiveness of our method and its compatibility with state-of-the-art Video Transformers.', 'Generative Video Transformer: Can Objects be the Words? Transformers have been successful for many natural language processing tasks. However, applying transformers to the video domain for tasks such as long-term video generation and scene understanding has remained elusive due to the high computational complexity and the lack of natural tokenization. In this paper, we propose the ObjectCentric Video Transformer (OCVT) which utilizes an object-centric approach for decomposing scenes into tokens suitable for use in a generative video transformer. By factoring the video into objects, our fully unsupervised model is able to learn complex spatio-temporal dynamics of multiple interacting objects in a scene and generate future frames of the video. Our model is also significantly more memory-efficient than pixel-based models and thus able to train on videos of length up to 70 frames with a single 48GB GPU. We compare our model with previous RNN-based approaches as well as other possible video transformer baselines. We demonstrate OCVT performs well when compared to baselines in generating future frames. OCVT also develops useful representations for video reasoning, achieving start-of-the-art performance on the CATER task.']"
119,27,119_sampling_probabilistic_algorithms_determinantal,"['sampling', 'probabilistic', 'algorithms', 'determinantal', 'markov', 'ndpps', 'point', 'ndpp', 'determinant', 'detmathbfbss']","['Markov Properties of Discrete Determinantal Point Processes Determinantal point processes (DPPs) are probabilistic models for repulsion. When used to represent the occurrence of random subsets of a finite base set, DPPs allow to model global negative associations in a mathematically elegant and direct way. Discrete DPPs have become popular and computationally tractable models for solving several machine learning tasks that require the selection of diverse objects, and have been successfully applied in numerous real-life problems. Despite their popularity, the statistical properties of such models have not been adequately explored. In this note, we derive the Markov properties of discrete DPPs and show how they can be expressed using graphical models.', 'A Greedy Approximation for k-Determinantal Point Processes Determinantal point processes (DPPs) are an important concept in random matrix theory and combinatorics, and increasingly in machine learning. Samples from these processes exhibit a form of self-avoidance, so they are also helpful in guiding algorithms that explore to reduce uncertainty, such as in active learning, Bayesian optimization, reinforcement learning, and marginalization in graphical models. The best-known algorithms for sampling from DPPs exactly require significant computational expense, which can be unwelcome in machine learning applications when the cost of sampling is relatively low and capturing the precise repulsive nature of the DPP may not be critical. We suggest an inexpensive approximate strategy for sampling a fixed number of points (as would typically be desired in a machine learning setting) from a so-called $k$-DPP based on iterative inverse transform sampling. We prove that our algorithm satisfies a $(1 - 1/\\epsilon)$ approximation guarantee relative to exact sampling from the $k$-DPP, and provide an efficient implementation for many common kernels used in machine learning, including the Gaussian and Matérn class. Finally, we compare the empirical runtime of our method to exact and Markov-Chain-Monte-Carlo (MCMC) samplers and investigate the approximation quality in a Bayesian Quadrature (BQ) setting.', 'Fixed-point algorithms for learning determinantal point processes Determinantal point processes (DPPs) offer an elegant tool for encoding probabilities over subsets of a ground set. Discrete DPPs are parametrized by a positive semidefinite matrix (called the DPP kernel), and estimating this kernel is key to learning DPPs from observed data. We consider the task of learning the DPP kernel, and develop for it a surprisingly simple yet effective new algorithm. Our algorithm offers the following benefits over previous approaches: (a) it is much simpler; (b) it yields equally good and sometimes even better local maxima; and (c) it runs an order of magnitude faster on large problems. We present experimental results on both real and simulated data to illustrate the numerical performance of our technique.']"
120,35,120_multitask_learning_tasks_taskdata,"['multitask', 'learning', 'tasks', 'taskdata', 'task', 'sparse', 'regularization', 'predictors', 'learned', 'generalization']","['Sparse coding for multitask and transfer learning We investigate the use of sparse coding and dictionary learning in the context of multitask and transfer learning. The central assumption of our learning method is that the tasks parameters are well approximated by sparse linear combinations of the atoms of a dictionary on a high or infinite dimensional space. This assumption, together with the large quantity of available data in the multitask and transfer learning settings, allows a principled choice of the dictionary. We provide bounds on the generalization error of this approach, for both settings. Numerical experiments on one synthetic and two real datasets show the advantage of our method over single task learning, a previous method based on orthogonal and dense representation of the tasks and a related method learning task grouping.', 'Efficient Multitask Feature and Relationship Learning We consider a multitask learning problem, in which several predictors are learned jointly. Prior research has shown that learning the relations between tasks, and between the input features, together with the predictor, can lead to better generalization and interpretability, which proved to be useful for applications in many domains. In this paper, we consider a formulation of multitask learning that learns the relationships both between tasks and between features, represented through a task covariance and a feature covariance matrix, respectively. First, we demonstrate that existing methods proposed for this problem present an issue that may lead to ill-posed optimization. We then propose an alternative formulation, as well as an efficient algorithm to optimize it. Using ideas from optimization and graph theory, we propose an efficient coordinate-wise minimization algorithm that has a closed form solution for each block subproblem. Our experiments show that the proposed optimization method is orders of magnitude faster than its competitors. We also provide a nonlinear extension that is able to achieve better generalization than existing methods.', 'Online Learning of Multiple Tasks and Their Relationships We propose an Online MultiTask Learning (OMTL) framework which simultaneously learns the task weight vectors as well as the task relatedness adaptively from the data. Our work is in contrast with prior work on online multitask learning which assumes fixed task relatedness, a priori. Furthermore, whereas prior work in such settings assume only positively correlated tasks, our framework can capture negative correlations as well. Our proposed framework learns the task relationship matrix by framing the objective function as a Bregman divergence minimization problem for positive definite matrices. Subsequently, we exploit this adaptively learned task-relationship matrix to select the most informative samples in an online multitask active learning setting. Experimental results on a number of real-world datasets and comparisons with numerous baselines establish the efficacy of our proposed approach.']"
121,15,121_convnets_convnet_cnn_cnns,"['convnets', 'convnet', 'cnn', 'cnns', 'imagenet', 'convolutional', 'convolution', 'attention', 'recognition', 'detectors']","['Are Large Kernels Better Teachers than Transformers for ConvNets? This paper reveals a new appeal of the recently emerged large-kernel Convolutional Neural Networks (ConvNets): as the teacher in Knowledge Distillation (KD) for small-kernel ConvNets. While Transformers have led state-of-the-art (SOTA) performance in various fields with ever-larger models and labeled data, small-kernel ConvNets are considered more suitable for resource-limited applications due to the efficient convolution operation and compact weight sharing. KD is widely used to boost the performance of small-kernel ConvNets. However, previous research shows that it is not quite effective to distill knowledge (e.g., global information) from Transformers to small-kernel ConvNets, presumably due to their disparate architectures. We hereby carry out a first-of-its-kind study unveiling that modern large-kernel ConvNets, a compelling competitor to Vision Transformers, are remarkably more effective teachers for small-kernel ConvNets, due to more similar architectures. Our findings are backed up by extensive experiments on both logit-level and feature-level KD ""out of the box"", with no dedicated architectural nor training recipe modifications. Notably, we obtain the <b>best-ever pure ConvNet</b> under 30M parameters with 83.1% top-1 accuracy on ImageNet, outperforming current SOTA methods including ConvNeXt V2 and Swin V2. We also find that beneficial characteristics of large-kernel ConvNets, e.g., larger effective receptive fields, can be seamlessly transferred to students through this large-to-small kernel distillation. Code is available at: https://github.com/VITA-Group/SLaK.', 'ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases Convolutional architectures have proven extremely successful for vision tasks. Their hard inductive biases enable sample-efficient learning, but come at the cost of a potentially lower performance ceiling. Vision Transformers (ViTs) rely on more flexible self-attention layers, and have recently outperformed CNNs for image classification. However, they require costly pre-training on large external datasets or distillation from pre-trained convolutional networks. In this paper, we ask the following question: is it possible to combine the strengths of these two architectures while avoiding their respective limitations? To this end, we introduce gated positional self-attention (GPSA), a form of positional self-attention which can be equipped with a “soft"" convolutional inductive bias. We initialise the GPSA layers to mimic the locality of convolutional layers, then give each attention head the freedom to escape locality by adjusting a gating parameter regulating the attention paid to position versus content information. The resulting convolutional-like ViT architecture, ConViT, outperforms the DeiT on ImageNet, while offering a much improved sample efficiency. We further investigate the role of locality in learning by first quantifying how it is encouraged in vanilla self-attention layers, then analysing how it is escaped in GPSA layers. We conclude by presenting various ablations to better understand the success of the ConViT. Our code and models are released publicly at https://github.com/facebookresearch/convit.', 'Training data-efficient image transformers & distillation through attention Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These high-performing vision transformers are pre-trained with hundreds of millions of images using a large infrastructure, thereby limiting their adoption. In this work, we produce competitive convolution-free transformers trained on ImageNet only using a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop) on ImageNet with no external data. We also introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention, typically from a convnet teacher. The learned transformers are competitive (85.2% top-1 acc.) with the state of the art on ImageNet, and similarly when transferred to other tasks. We will share our code and models.']"
122,19,122_markov_likelihoodfree_hidden_hmmas,"['markov', 'likelihoodfree', 'hidden', 'hmmas', 'models', 'learning', 'inference', 'bayesian', 'algorithms', 'statesplitting']","['Reduced-Rank Hidden Markov Models Hsu et al. (2009) recently proposed an efficient, accurate spectral learning algorithm for Hidden Markov Models (HMMs). In this paper we relax their assumptions and prove a tighter finite-sample error bound for the case of Reduced-Rank HMMs, i.e., HMMs with low-rank transition matrices. Since rank-$k$ RR-HMMs are a larger class of models than $k$-state HMMs while being equally efficient to work with, this relaxation greatly increases the learning algorithm’s scope. In addition, we generalize the algorithm and bounds to models where multiple observations are needed to disambiguate state, and to models that emit multivariate real-valued observations. Finally we prove consistency for learning Predictive State Representations, an even larger class of models. Experiments on synthetic data and a toy video, as well as on difficult robot vision data, yield accurate models that compare favorably with alternatives in simulation quality and prediction accuracy.', 'Sample-efficient neural likelihood-free Bayesian inference of implicit HMMs Likelihood-free inference methods based on neural conditional density estimation were shown to drastically reduce the simulation burden in comparison to classical methods such as ABC. When applied in the context of any latent variable model, such as a Hidden Markov model (HMM), these methods are designed to only estimate the parameters, rather than the joint distribution of the parameters and the hidden states. Naive application of these methods to a HMM, ignoring the inference of this joint posterior distribution, will thus produce an inaccurate estimate of the posterior predictive distribution, in turn hampering the assessment of goodness-of-fit. To rectify this problem, we propose a novel, sample-efficient likelihood-free method for estimating the high-dimensional hidden states of an implicit HMM. Our approach relies on learning directly the intractable posterior distribution of the hidden states, using an autoregressive-flow, by exploiting the Markov property. Upon evaluating our approach on some implicit HMMs, we found that the quality of the estimates retrieved using our method is comparable to what can be achieved using a much more computationally expensive SMC algorithm.', 'Learning Complex Uncertain States Changes via Asymmetric Hidden Markov Models: an Industrial Case In many problems involving multivariate time series, Hidden Markov Models (HMMs) are often employed to model complex behavior over time. HMMs can, however, require large number of states, that can lead to overfitting issues especially when limited data is available. In this work, we propose a family of models called Asymmetric Hidden Markov Models (HMM-As), that generalize the emission distributions to arbitrary Bayesian-network distributions. The new model allows for state-specific graphical structures defined over the space of observable features, what renders more compact state spaces and hence a better handling of the complexity-overfitting trade-off. We first define asymmetric HMMs, followed by the definition of a learning procedure inspired on the structural expectation-maximization framework allowing for decomposing learning per state. Then, we relate representation aspects of HMM-As to standard and independent HMMs. The last contribution of the paper is a set of experiments that elucidate the behavior of asymmetric HMMs on practical scenarios, including simulations and industry-based scenarios. The empirical results indicate that HMMs are limited when learning structured distributions, what is prevented by the more parsimonious representation of HMM-As. Furthermore, HMM-As showed to be promising in uncovering multiple graphical structures and providing better model fit in a case study from the domain of large-scale printers, thus providing additional problem insight.']"
123,14,123_preconditioning_preconditioner_preconditioners_preconditioned,"['preconditioning', 'preconditioner', 'preconditioners', 'preconditioned', 'optimizers', 'matrixfree', 'parallelization', 'carefullypreconditioned', 'regularization', 'iterative']","['On the Nyström Approximation for Preconditioning in Kernel Machines Kernel methods are a popular class of nonlinear predictive models in machine learning. Scalable algorithms for learning kernel models need to be iterative in nature, but convergence can be slow due to poor conditioning. Spectral preconditioning is an important tool to speed-up the convergence of such iterative algorithms for training kernel models. However computing and storing a spectral preconditioner can be expensive which can lead to large computational and storage overheads, precluding the application of kernel methods to problems with large datasets. A Nystrom approximation of the spectral preconditioner is often cheaper to compute and store, and has demonstrated success in practical applications. In this paper we analyze the trade-offs of using such an approximated preconditioner. Specifically, we show that a sample of logarithmic size (as a function of the size of the dataset) enables the Nyström-based approximated preconditioner to accelerate gradient descent nearly as well as the exact preconditioner, while also reducing the computational and storage overheads.', 'Matrix-Free Preconditioning in Online Learning We provide an online convex optimization algorithm with regret that interpolates between the regret of an algorithm using an optimal preconditioning matrix and one using a diagonal preconditioning matrix. Our regret bound is never worse than that obtained by diagonal preconditioning, and in certain setting even surpasses that of algorithms with full-matrix preconditioning. Importantly, our algorithm runs in the same time and space complexity as online gradient descent. Along the way we incorporate new techniques that mildly streamline and improve logarithmic factors in prior regret analyses. We conclude by benchmarking our algorithm on synthetic data and deep learning tasks.', 'Polynomial Preconditioning for Gradient Methods We study first-order methods with preconditioning for solving structured convex optimization problems. We propose a new family of preconditioners generated by the symmetric polynomials. They provide the first-order optimization methods with a provable improvement of the condition number, cutting the gaps between highest eigenvalues, without explicit knowledge of the actual spectrum. We give a stochastic interpretation of this preconditioning in terms of the coordinate volume sampling and compare it with other classical approaches, including the Chebyshev polynomials. We show how to incorporate a polynomial preconditioning into the Gradient and Fast Gradient Methods and establish their better global complexity bounds. Finally, we propose a simple adaptive search procedure that automatically ensures the best polynomial preconditioning for the Gradient Method, minimizing the objective along a low-dimensional Krylov subspace. Numerical experiments confirm the efficiency of our preconditioning strategies for solving various machine learning problems.']"
124,104,124_learning_metatraining_metanet_learningtolearn,"['learning', 'metatraining', 'metanet', 'learningtolearn', 'learned', 'training', 'metafeatures', 'classification', 'learn', 'benchmarks']","['Task similarity aware meta learning: theory-inspired improvement on MAML Few-shot learning ability is heavily desired for machine intelligence. By meta-learning a model initialization from training tasks with fast adaptation ability to new tasks, model-agnostic meta-learning (MAML) has achieved remarkable success in a number of few-shot learning applications. However, theoretical understandings on the learning ability of MAML remain absent yet, hindering developing new and more advanced meta learning methods in a principled way. In this work, we solve this problem by theoretically justifying the fast adaptation capability of MAML when applied to new tasks. Specifically, we prove that the learnt meta-initialization can benefit the fast adaptation to new tasks with only a few steps of gradient descent. This result explicitly reveals the benefits of the unique designs in MAML. Then we propose a theory-inspired task similarity aware MAML which clusters tasks into multiple groups according to the estimated optimal model parameters and learns group-specific initializations. The proposed method improves upon MAML by speeding up the adaptation and giving stronger few-shot learning ability. Experimental results on the few-shot classification tasks testify its advantages.', ' Is Bayesian Model-Agnostic Meta Learning Better than Model-Agnostic Meta Learning, Provably?   Meta learning aims at learning a model that can quickly adapt to unseen tasks. Widely used meta learning methods include model agnostic meta learning (MAML), implicit MAML, Bayesian MAML. Thanks to its ability of modeling uncertainty, Bayesian MAML often has advantageous empirical performance. However, the theoretical understanding of Bayesian MAML is still limited, especially on questions such as if and when Bayesian MAML has provably better performance than MAML. In this paper, we aim to provide theoretical justifications for Bayesian MAML’s advantageous performance by comparing the meta test risks of MAML and Bayesian MAML. In the meta linear regression, under both the distribution agnostic and linear centroid cases, we have established that Bayesian MAML indeed has provably lower meta test risks than MAML. We verify our theoretical results through experiments, the code of which is available at https://github.com/lishachen/Bayesian-MAML-vs-MAML. ', 'Metalearning with Very Few Samples Per Task     Metalearning and multitask learning are two frameworks for solving a group of related learning tasks more efficiently than we could hope to solve each of the individual tasks on their own.  In multitask learning, we are given a fixed set of related learning tasks and need to output one accurate model per task, whereas in metalearning we are given tasks that are drawn i.i.d. from a metadistribution and need to output some common information that can be easily specialized to new, previously unseen tasks from the metadistribution. In this work, we consider a binary classification setting where tasks are related by a shared representation, that is, every task $P$ of interest can be solved by a classifier of the form $f_{P} \\circ h$ where $h \\in \\mathcal{H}$ is a map from features to some representation space that is shared across tasks, and $f_{P} \\in \\mathcal{F}$ is a task-specific classifier from the representation space to labels.  The main question we ask in this work is how much data do we need to metalearn a good representation? Here, the amount of data is measured in terms of both the number of tasks $t$ that we need to see and the number of samples $n$ per task. We focus on the regime where the number of samples per task is extremely small. Our main result shows that, in a distribution-free setting where the feature vectors are in $\\mathbb{R}^d$, the representation is a linear map from $\\mathbb{R}^d \\to \\mathbb{R}^k$, and the task-specific classifiers are halfspaces in $\\mathbb{R}^k$, we can metalearn a representation with error $\\varepsilon$ using just $n = k+2$ samples per task, and $d \\cdot (1/\\varepsilon)^{O(k)}$ tasks.  Learning with so few samples per task is remarkable because metalearning would be impossible with $k+1$ samples per task, and because we cannot even hope to learn an accurate task-specific classifier with just $k+2$ samples per task.  To obtain this result, we develop a sample-and-task-complexity theory for distribution-free metalearning and multitask learning, which identifies what properties of $\\mathcal{F}$ and $\\mathcal{H}$ make metalearning possible with few samples per task.  Our theory also yields a simple characterization of distribution-free multitask learning.  Finally, we give sample-efficient reductions between metalearning and multitask learning, which, when combined with our characterization of multitask learning, give a characterization of metalearning in certain parameter regimes.']"
125,41,125_learning_zeroshot_embedding_classification,"['learning', 'zeroshot', 'embedding', 'classification', 'classifier', 'shot', 'datasets', 'fewshot', 'training', 'metriclearning']","['Contrastive prototype learning with augmented embeddings for few-shot learning Most recent few-shot learning (FSL) methods are based on meta-learning with episodic training. In each meta-training episode, a discriminative feature embedding and/or classifier are first constructed from a support set in an inner loop, and then evaluated in an outer loop using a query set for model updating. This query set sample centered learning objective is however intrinsically limited in addressing the lack of training data problem in the support set. In this paper, a novel contrastive prototype learning with augmented embeddings (CPLAE) model is proposed to overcome this limitation. First, data augmentations are introduced to both the support and query sets with each sample now being represented as an augmented embedding (AE) composed of concatenated embeddings of both the original and augmented versions. Second, a novel support set class prototype centered contrastive loss is proposed for contrastive prototype learning (CPL). With a class prototype as an anchor, CPL aims to pull the query samples of the same class closer and those of different classes further away. This support set sample centered loss is highly complementary to the existing query centered loss, fully exploiting the limited training data in each episode. Extensive experiments on several benchmarks demonstrate that our proposed CPLAE achieves new state-of-the-art.', 'Evolving Semantic Prototype Improves Generative Zero-Shot Learning In zero-shot learning (ZSL), generative methods synthesize class-related sample features based on predefined semantic prototypes. They advance the ZSL performance by synthesizing unseen class sample features for better training the classifier. We observe that each class’s predefined semantic prototype (also referred to as semantic embedding or condition) does not accurately match its real semantic prototype. So the synthesized visual sample features do not faithfully represent the real sample features, limiting the classifier training and existing ZSL performance. In this paper, we formulate this mismatch phenomenon as the visual-semantic domain shift problem. We propose a dynamic semantic prototype evolving (DSP) method to align the empirically predefined semantic prototypes and the real prototypes for class-related feature synthesis. The alignment is learned by refining sample features and semantic prototypes in a unified framework and making the synthesized visual sample features approach real sample features. After alignment, synthesized sample features from unseen classes are closer to the real sample features and benefit DSP to improve existing generative ZSL methods by 8.5%, 8.0%, and 9.7% on the standard CUB, SUN AWA2 datasets, the significant performance improvement indicates that evolving semantic prototype explores a virgin field in ZSL.', 'Learning Classifiers for Target Domain with Limited or No Labels In computer vision applications, such as domain adaptation (DA), few shot learning (FSL) and zero-shot learning (ZSL), we encounter new objects and environments, for which insufficient examples exist to allow for training “models from scratch,” and methods that adapt existing models, trained on the presented training environment, to the new scenario are required. We propose a novel visual attribute encoding method that encodes each image as a low-dimensional probability vector composed of prototypical part-type probabilities. The prototypes are learnt to be representative of all training data. At test-time we utilize this encoding as an input to a classifier. At test-time we freeze the encoder and only learn/adapt the classifier component to limited annotated labels in FSL; new semantic attributes in ZSL. We conduct extensive experiments on benchmark datasets. Our method outperforms state-of-art methods trained for the specific contexts (ZSL, FSL, DA).']"
126,17,126_detectors_detector_detection_detecting,"['detectors', 'detector', 'detection', 'detecting', 'detect', 'objects', 'features', 'discriminative', 'scenes', 'feature']","['What Makes for End-to-End Object Detection? Object detection has recently achieved a breakthrough for removing the last one non-differentiable component in the pipeline, Non-Maximum Suppression (NMS), and building up an end-to-end system. However, what makes for its one-to-one prediction has not been well understood. In this paper, we first point out that one-to-one positive sample assignment is the key factor, while, one-to-many assignment in previous detectors causes redundant predictions in inference. Second, we surprisingly find that even training with one-to-one assignment, previous detectors still produce redundant predictions. We identify that classification cost in matching cost is the main ingredient: (1) previous detectors only consider location cost, (2) by additionally introducing classification cost, previous detectors immediately produce one-to-one prediction during inference. We introduce the concept of score gap to explore the effect of matching cost. Classification cost enlarges the score gap by choosing positive samples as those of highest score in the training iteration and reducing noisy positive samples brought by only location cost. Finally, we demonstrate the advantages of end-to-end object detection on crowded scenes.', 'Efficient Sample Mining for Object Detection Object detectors based on the sliding window technique are usually trained in two successive steps: first, an initial classifier is trained on a population of positive samples (i.e. images of the object to detect) and negative samples randomly extracted from scenes which do not contain the object to detect. Then, the scenes are scanned with that initial classifier to enrich the initial set with negative samples incorrectly classified as positive. This bootstrapping process provides the learning algorithm with ""hard"" samples, which help to improve the decision boundary. Little work has been done on how to efficiently enrich the training set. While the standard bootstrapping approach densely visits the scenes, we propose to evaluate which regions of scenes can be discarded without any further computation to concentrate the search on promising areas. We apply our method to two standard object detection settings, pedestrian and face detection, and show that it provides a multi-fold speed up.', 'Active Detection via Adaptive Submodularity Efficient detection of multiple object instances is one of the fundamental challenges in computer vision. For certain object categories, even the best automatic systems are yet unable to produce high-quality detection results, and fully manual annotation would be an expensive process. How can detection algorithms interplay with human expert annotators? To make the best use of scarce (human) labeling resources, one needs to decide when to invoke the expert, such that the best possible performance can be achieved while requiring a minimum amount of supervision.   In this paper, we propose a principled approach to active object detection, and show that for a rich class of base detectors algorithms, one can derive a natural sequential decision problem for deciding when to invoke expert supervision. We further show that the objective function satisfies adaptive submodularity, which allows us to derive strong performance guarantees for our algorithm. We demonstrate the proposed algorithm on three real-world tasks, including a problem for biodiversity monitoring from micro UAVs in the Sumatra rain forest. Our results show that active detection not only outperforms its passive counterpart; for certain tasks, it also works significantly better than straightforward application of existing active learning techniques. To the best of our knowledge, our approach is the first to rigorously address the active detection problem from both empirical and theoretical perspectives.']"
127,20,127_facenet_faces_recognition_keras,"['facenet', 'faces', 'recognition', 'keras', 'recognitiona', 'face', 'facial', 'descriptors', 'vggface2', 'convolutional']","['A Gentle Introduction to Deep Learning for Face Recognition <p>Face recognition is the problem of identifying and verifying people in a photograph by their face. It is a task that is trivially performed by humans, even under varying light and when faces are changed by age or obstructed with accessories and facial hair. Nevertheless, it is remained a challenging computer vision problem for decades [&#8230;]</p>\n<p>The post <a rel=""nofollow"" href=""https://machinelearningmastery.com/introduction-to-deep-learning-for-face-recognition/"">A Gentle Introduction to Deep Learning for Face Recognition</a> appeared first on <a rel=""nofollow"" href=""https://machinelearningmastery.com"">Machine Learning Mastery</a>.</p>\n', 'How to Perform Face Recognition With VGGFace2 in Keras <p>Face recognition is a computer vision task of identifying and verifying a person based on a photograph of their face. Recently, deep learning convolutional neural networks have surpassed classical methods and are achieving state-of-the-art results on standard face recognition datasets. One example of a state-of-the-art model is the VGGFace and VGGFace2 model developed by researchers [&#8230;]</p>\n<p>The post <a rel=""nofollow"" href=""https://machinelearningmastery.com/how-to-perform-face-recognition-with-vggface2-convolutional-neural-network-in-keras/"">How to Perform Face Recognition With VGGFace2 in Keras</a> appeared first on <a rel=""nofollow"" href=""https://machinelearningmastery.com"">Machine Learning Mastery</a>.</p>\n', 'How to Develop a Face Recognition System Using FaceNet in Keras <p>Face recognition is a computer vision task of identifying and verifying a person based on a photograph of their face. FaceNet is a face recognition system developed in 2015 by researchers at Google that achieved then state-of-the-art results on a range of face recognition benchmark datasets. The FaceNet system can be used broadly thanks to [&#8230;]</p>\n<p>The post <a rel=""nofollow"" href=""https://machinelearningmastery.com/how-to-develop-a-face-recognition-system-using-facenet-in-keras-and-an-svm-classifier/"">How to Develop a Face Recognition System Using FaceNet in Keras</a> appeared first on <a rel=""nofollow"" href=""https://machinelearningmastery.com"">Machine Learning Mastery</a>.</p>\n']"
128,17,128_views_clustering_multiview_view,"['views', 'clustering', 'multiview', 'view', 'cluster', 'clusters', 'tensorized', 'viewspecific', 'singleview', 'clusteraware']","['COMIC: Multi-view Clustering Without Parameter Selection In this paper, we study two challenges in clustering analysis, namely, how to cluster multi-view data and how to perform clustering without parameter selection on cluster size. To this end, we propose a novel objective function to project raw data into one space in which the projection embraces the geometric consistency (GC) and the cluster assignment consistency (CAC). To be specific, the GC aims to learn a connection graph from a projection space wherein the data points are connected if and only if they belong to the same cluster. The CAC aims to minimize the discrepancy of pairwise connection graphs induced from different views based on the view-consensus assumption, <em>i.e.</em>, different views could produce the same cluster assignment structure as they are different portraits of the same object. Thanks to the view-consensus derived from the connection graph, our method could achieve promising performance in learning view-specific representation and eliminating the heterogeneous gaps across different views. Furthermore, with the proposed objective, it could learn almost all parameters including the cluster number from data without labor-intensive parameter selection. Extensive experimental results show the promising performance achieved by our method on five datasets comparing with nine state-of-the-art multi-view clustering approaches.', 'Integration of Single-view Graphs with Diffusion of Tensor Product Graphs for Multi-view Spectral Clustering Multi-view clustering takes diversity of multiple views (representations) into consideration. Multiple views may be obtained from various sources or different feature subsets and often provide complementary information to each other. In this paper, we propose a novel graph-based approach to integrate multiple representations to improve clustering performance. While original graphs have been widely used in many existing multi-view clustering approaches, the key idea of our approach is to integrate multiple views by exploring higher order information. In particular, given graphs constructed separately from single view data, we build cross-view tensor product graphs (TPGs), each of which is a Kronecker product of a pair of single-view graphs. Since each cross-view TPG captures higher order relationships of data under two different views, it is no surprise that we obtain more reliable similarities.  We linearly combine multiple cross-view TPGs to integrate higher order information. Efficient graph diffusion process on the fusion TPG helps to reveal the underlying cluster structure and boosts the clustering performance. Empirical study shows that the proposed approach outperforms state-of-the-art methods on benchmark datasets.', 'Deep Safe Incomplete Multi-view Clustering: Theorem and Algorithm Incomplete multi-view clustering is a significant but challenging task. Although jointly imputing incomplete samples and conducting clustering has been shown to achieve promising performance, learning from both complete and incomplete data may be worse than learning only from complete data, particularly when imputed views are semantic inconsistent with missing views. To address this issue, we propose a novel framework to reduce the clustering performance degradation risk from semantic inconsistent imputed views. Concretely, by the proposed bi-level optimization framework, missing views are dynamically imputed from the learned semantic neighbors, and imputed samples are automatically selected for training. In theory, the empirical risk of the model is no higher than learning only from complete data, and the model is never worse than learning only from complete data in terms of expected risk with high probability. Comprehensive experiments demonstrate that the proposed method achieves superior performance and efficient safe incomplete multi-view clustering.']"
129,15,129_clusteringfriendly_cluster_embedding_clustering,"['clusteringfriendly', 'cluster', 'embedding', 'clustering', 'clusters', 'supervised', 'clusteraware', 'autoencoder', 'regularization', 'tensorized']","['Deep Embedded Clustering with Data Augmentation Deep Embedded Clustering (DEC) surpasses traditional clustering algorithms by jointly performing feature learning and cluster assignment. Although a lot of variants have emerged, they all ignore a crucial ingredient, \\emph{data augmentation}, which has been widely employed in supervised deep learning models to improve the generalization. To fill this gap, in this paper, we propose the framework of Deep Embedded Clustering with Data Augmentation (DEC-DA). Specifically, we first train an autoencoder with the augmented data to construct the initial feature space. Then we constrain the embedded features with a clustering loss to further learn clustering-oriented features. The clustering loss is composed of the target (pseudo label) and the actual output of the feature learning model, where the target is computed by using clean (non-augmented) data, and the output by augmented data. This is analogous to supervised training with data augmentation and expected to facilitate unsupervised clustering too. Finally, we instantiate five DEC-DA based algorithms. Extensive experiments validate that incorporating data augmentation can improve the clustering performance by a large margin. Our DEC-DA algorithms become the new state of the art on various datasets.', 'Deep Spectral Clustering Learning Clustering is the task of grouping a set of examples so that similar examples are grouped into the same cluster while dissimilar examples are in different clusters. The quality of a clustering depends on two problem-dependent factors which are i) the chosen similarity metric and ii) the data representation. Supervised clustering approaches, which exploit labeled partitioned datasets have thus been proposed, for instance to learn a metric optimized to perform clustering. However, most of these approaches assume that the representation of the data is fixed and then learn an appropriate linear transformation. Some deep supervised clustering learning approaches have also been proposed. However, they rely on iterative methods to compute gradients resulting in high algorithmic complexity. In this paper, we propose a deep supervised clustering metric learning method that formulates a novel loss function. We derive a closed-form expression for the gradient that is efficient to compute: the complexity to compute the gradient is linear in the size of the training mini-batch and quadratic in the representation dimensionality. We further reveal how our approach can be seen as learning spectral clustering. Experiments on standard real-world datasets confirm state-of-the-art Recall@K performance.', 'Unsupervised Deep Embedding for Clustering Analysis Clustering is central to many data-driven application domains and has been studied extensively in terms of distance functions and grouping algorithms. Relatively little work has focused on learning representations for clustering. In this paper, we propose Deep Embedded Clustering (DEC), a method that simultaneously learns feature representations and cluster assignments using deep neural networks. DEC learns a mapping from the data space to a lower-dimensional feature space in which it iteratively optimizes a clustering objective. Our experimental evaluations on image and text corpora show significant improvement over state-of-the-art methods.']"
130,17,130_multimodal_multimodality_crossmodal_unimodal,"['multimodal', 'multimodality', 'crossmodal', 'unimodal', 'learning', 'modalityalternating', 'modality', 'audioset', 'fusion', 'networks']","['MMPareto: Boosting Multimodal Learning with Innocent Unimodal Assistance Multimodal learning methods with targeted unimodal learning objectives have exhibited their superior efficacy in alleviating the imbalanced multimodal learning problem. However, in this paper, we identify the previously ignored gradient conflict between multimodal and unimodal learning objectives, potentially misleading the unimodal encoder optimization. To well diminish these conflicts, we observe the discrepancy between multimodal loss and unimodal loss, where both gradient magnitude and covariance of the easier-to-learn multimodal loss are smaller than the unimodal one. With this property, we analyze Pareto integration under our multimodal scenario and propose MMPareto algorithm, which could ensure a final gradient with direction that is common to all learning objectives and enhanced magnitude to improve generalization, providing innocent unimodal assistance. Finally, experiments across multiple types of modalities and frameworks with dense cross-modal interaction indicate our superior and extendable method performance. Our method is also expected to facilitate multi-task cases with a clear discrepancy in task difficulty, demonstrating its ideal scalability. The source code and dataset are available at https://github.com/GeWu-Lab/MMPareto_ICML2024.', 'Understanding Unimodal Bias in Multimodal Deep Linear Networks Using multiple input streams simultaneously to train multimodal neural networks is intuitively advantageous but practically challenging. A key challenge is unimodal bias, where a network overly relies on one modality and ignores others during joint training. We develop a theory of unimodal bias with multimodal deep linear networks to understand how architecture and data statistics influence this bias. This is the first work to calculate the duration of the unimodal phase in learning as a function of the depth at which modalities are fused within the network, dataset statistics, and initialization. We show that the deeper the layer at which fusion occurs, the longer the unimodal phase. A long unimodal phase can lead to a generalization deficit and permanent unimodal bias in the overparametrized regime. Our results, derived for multimodal linear networks, extend to nonlinear networks in certain settings. Taken together, this work illuminates pathologies of multimodal learning under joint training, showing that late and intermediate fusion architectures can give rise to long unimodal phases and permanent unimodal bias. Our code is available at: https://yedizhang.github.io/unimodal-bias.html.', 'On the Computational Benefit of Multimodal Learning Human perception inherently operates in a multimodal manner. Similarly, as machines interpret the empirical world, their learning processes ought to be multimodal. The recent, remarkable successes in empirical multimodal learning underscore the significance of understanding this paradigm. Yet, a solid theoretical foundation for multimodal learning has eluded the field for some time. While a recent study by \\cite{zhoul} has shown the superior sample complexity of multimodal learning compared to its unimodal counterpart, another basic question remains: does multimodal learning also offer computational advantages over unimodal learning? This work initiates a study on the computational benefit of multimodal learning. We demonstrate that, under certain conditions, multimodal learning can outpace unimodal learning exponentially in terms of computation. Specifically, we present a learning task that is NP-hard for unimodal learning but is solvable in polynomial time by a multimodal algorithm. Our construction is based on a novel modification to the intersection of two half-spaces problem.']"
131,121,131_captioning_captions_caption_languageimage,"['captioning', 'captions', 'caption', 'languageimage', 'multimodal', 'imagetext', 'attention', 'descriptions', 'visual', 'embeddings']","['Captioning Novel Objects in Images <p>Given an image, humans can easily infer the salient entities in it, and describe the scene effectively, such as, where objects are located (in a forest or in a kitchen?), what attributes an object has (brown or white?), and, importantly, how objects interact with other objects in a scene (running in a field, or being held by a person etc.). The task of visual description aims to develop visual systems that generate contextual descriptions about objects in images. Visual description is challenging because it requires recognizing not only objects (bear), but other visual elements, such as actions (standing) and attributes (brown), and constructing a fluent sentence describing how objects, actions, and attributes are related in an image (such as the brown bear is standing on a rock in the forest).</p>\n\n<h2 id=""current-state-of-visual-description"">Current State of Visual Description</h2>\n\n<table class=""col-2"">\n  <tr>\n    <td style=""text-align:center;"">\n\t\t\t<img src=""http://bair.berkeley.edu/blog/assets/novel_image_captioning/bear.png"" alt=""Brown bear in forest"" />\n\t\t</td>\n    <td style=""text-align:center;"">\n\t\t\t<img src=""http://bair.berkeley.edu/blog/assets/novel_image_captioning/anteater.png"" alt=""Anteater in forest"" />\n\t\t</td>\n  </tr>\n  <tr>\n    <td><p>\n\t\t\t<a href=""http://jeffdonahue.com/lrcn/"">LRCN</a> [Donahue et al. ‘15]: A brown bear standing on top of a lush green field. <br />\n\t\t\t<a href=""http://captionbot.ai"">MS CaptionBot</a> [Tran et al. ‘16]: A large brown bear walking through a forest.\n\t\t</p></td>\n    <td><p>\n\t\t\t<a href=""http://jeffdonahue.com/lrcn/"">LRCN</a> [Donahue et al. ‘15]: A black <span style=""color:red;"">bear</span> is standing in the grass. <br />\n\t\t\t<a href=""http://captionbot.ai"">MS CaptionBot</a> [Tran et al. ‘16]: A <span style=""color:red;"">bear</span> that is eating some grass.\n\t\t</p></td>\n  </tr>\n</table>\n<p style=""text-align:center;""><i>\n\tDescriptions generated by existing captioners on two images. On the left is an image of an object (bear) that is present in training data. On the right is an object (anteater) that the model hasn\'t seen in training.\n</i></p>\n\n<p>Current visual description or image captioning models work quite well, but they can only describe objects seen in existing image captioning training datasets, and they require a large number of training examples to generate good captions. To learn how to describe an object like “jackal” or “anteater” in context, most description models require many examples of jackal or anteater images with corresponding descriptions.  However, current visual description datasets, like <a href=""mscoco.org"">MSCOCO</a>, do not include descriptions about all objects. In contrast, recent works in object recognition through Convolutional Neural Networks (CNNs) can recognize hundreds of categories of objects. While object recognition models can recognize jackals and anteaters, description models cannot compose sentences to describe these animals correctly in context.  In our work, we overcome this problem by building visual description systems which can describe new objects without pairs of images and sentences about these objects.</p>\n\n<h2 id=""the-task-describing-novel-objects"">The Task: Describing Novel Objects</h2>\n\n<p>Here we define our task more formally.  Given a dataset consisting of pairs of images and descriptions (paired image-sentence data, e.g. <a href=""mscoco.org"">MSCOCO</a>) as well as images with object labels but no descriptions (unpaired image data, such as <a href=""http://www.image-net.org/"">ImageNet</a>) we wish to learn how to describe objects unseen in paired image-sentence data. To do this we must build a model which can recognize different visual constituents (e.g., jackal, brown, standing, and field) and compose these in novel ways to form a coherent description.  Below we describe the core components of our description model.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/blog/assets/novel_image_captioning/image_0.png"" alt=""The novel visual description task"" />\n</p>\n\n<p>We aim to describe diverse objects which do not have training images with captions.</p>\n\n<!--more-->\n\n<h3 id=""using-external-sources-of-data"">Using External Sources of Data</h3>\n\n<p>In order to generate captions about diverse categories of objects outside the image-caption training data, we take advantage of external data sources. Specifically, we use ImageNet images with object labels as the unpaired image data source and sentences from unannotated text corpora such as Wikipedia as our text data source. These are used to train our visual recognition CNN and language model respectively.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/blog/assets/novel_image_captioning/image_1.png"" alt=""Train effectively on external resources"" /><br />\n<i>\nTrain effectively on external resources\n</i>\n</p>\n\n<h3 id=""capture-semantic-similarity"">Capture semantic similarity</h3>\n\n<p>We want to be able to describe unseen objects (e.g. from ImageNet) that are similar to objects that have been seen in the paired image-sentence training data. We use dense word embeddings to achieve this. Word embeddings are dense high dimensional representations of words where words with similar meaning are closer in the embedding space.</p>\n\n<p>In our previous work, called “Deep Compositional Captioning (DCC)” [1] we first train a caption model on MSCOCO paired  image caption dataset. Then to describe novel objects, for each novel object (such as an okapi) we use word embeddings to identify an object that’s most similar amongst the objects in the MSCOCO dataset (in this case zebra). We then transfer (copy) the parameters learned by the model from the seen object to the unseen object (i.e. copy weights in the network corresponding to zebra to those corresponding to okapi).</p>\n\n<h3 id=""novel-object-captioning"">Novel Object Captioning</h3>\n\n<p>While the DCC model is able to describe several unseen object categories, copying parameters from one object to another can create sentences with grammatical artifacts. E.g. for the object ‘racket’ the model copies weights from ‘tennis’, which results in sentences such as “A man playing racket on court”. In our more recent work [2], we incorporate the embeddings directly within our language model. Specifically, we use <a href=""https://nlp.stanford.edu/projects/glove/"">GloVe embeddings</a> in the input and output of our language model. This implicitly enables the model to capture semantic similarity when describing unseen objects. This enables our model to generate sentences such as “A tennis player swinging a racket at a ball”. Additionally, incorporating the embeddings directly within the network makes our model end-to-end trainable.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/blog/assets/novel_image_captioning/image_2.png"" alt=""Dense word embeddings to capture image similarity"" /><br />\n<i>\nIncorporate dense word embeddings in the language model to capture semantic similarity.\n</i>\n</p>\n\n<h3 id=""caption-model-and-forgetting-in-neural-networks"">Caption model and forgetting in neural networks.</h3>\n\n<p>We combine the outputs of the visual network and language model to a caption model. This model is similar to existing caption models which are also pre-trained on ImageNet. However, we observed that although the model is pre-trained on ImageNet, when the model is trained / tuned on the COCO image-caption dataset it tends to forget what it has seen before. The problem of forgetting in neural networks has also been observed by <a href=""https://arxiv.org/abs/1312.6211"">researchers at Montreal</a> as well as <a href=""https://arxiv.org/abs/1612.00796"">Google DeepMind</a> amongst others. In our work, we resolve this problem of forgetting using a joint training strategy.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/blog/assets/novel_image_captioning/image_3.png"" alt=""Joint training to overcome forgetting"" /><br />\n<i>\nShare parameters and train jointly on different data/tasks to overcome ""forgetting""\n</i>\n</p>\n\n<p>Specifically, our network has three components: a visual recognition network, a caption model, and a language model. All three components share parameters and are jointly trained. During training, each batch of inputs contains some images with labels, a different set of images and captions, and some plain sentences. These three inputs train the different components of the network. Since the parameters are shared between the three components, the network is jointly trained to recognize objects in images, caption images and generate sentences. This joint training helps the network overcome the problem of forgetting, and enables the model to generate descriptions for many novel object categories.</p>\n\n<h2 id=""whats-next"">What’s Next?</h2>\n\n<p>One of the most common errors in our model comes from not recognizing objects, and one way to mitigate this is to use better visual features. Another common error comes from generating sentences which are not fluent (A cat and a cat on a bed) or may not appeal to “common sense”  (e.g. ‘A woman is playing gymnastics’ is not particularly correct since one doesn’t “play” gymnastics). It would be interesting to develop solutions that can overcome these issues.</p>\n\n<p>While in this work, we proposes joint training as a strategy to overcome the problem of forgetting, it might not always be possible to train on lots of different tasks and datasets. A different way to approach the problem would be to build a model that can learn to compose descriptions based on visual information and object labels. Such a model should also be able to integrate objects on the fly i.e. currently we pre-train our model on a select set of objects, we should also think about how we can incrementally train our model on new data about some new concepts. Solving some of these problems can help develop better and more robust visual description models.</p>\n\n<p>[<a href=""https://vsubhashini.github.io/noc_examples.html"">Links to more examples</a>]</p>\n\n<p>[<a href=""http://vsubhashini.github.io/noc.html#code"">Links to trained models and code</a>]</p>\n\n<h3 id=""examples"">Examples</h3>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/blog/assets/novel_image_captioning/image_4.png"" alt=""Examples"" />\n</p>\n\n<hr />\n\n<p><strong>This blog post is based on the following research papers:</strong></p>\n\n<p>[1] L. A. Hendricks, S. Venugopalan, M. Rohrbach, R. Mooney, K. Saenko, and T. Darrell. Deep compositional captioning: Describing novel object categories without paired training data. In CVPR, 2016.</p>\n\n<p>[2] S. Venugopalan, L. A. Hendricks, M. Rohrbach, R. Mooney, K. Saenko, and T. Darrell. Captioning images with diverse objects. In CVPR, 2017.</p>\n', 'Captioning Novel Objects in Images <p>Given an image, humans can easily infer the salient entities in it, and describe the scene effectively, such as, where objects are located (in a forest or in a kitchen?), what attributes an object has (brown or white?), and, importantly, how objects interact with other objects in a scene (running in a field, or being held by a person etc.). The task of visual description aims to develop visual systems that generate contextual descriptions about objects in images. Visual description is challenging because it requires recognizing not only objects (bear), but other visual elements, such as actions (standing) and attributes (brown), and constructing a fluent sentence describing how objects, actions, and attributes are related in an image (such as the brown bear is standing on a rock in the forest).</p>\n\n<h2 id=""current-state-of-visual-description"">Current State of Visual Description</h2>\n\n<table class=""col-2"">\n  <tr>\n    <td style=""text-align:center;"">\n\t\t\t<img src=""http://bair.berkeley.edu/blog/assets/novel_image_captioning/bear.png"" alt=""Brown bear in forest"" />\n\t\t</td>\n    <td style=""text-align:center;"">\n\t\t\t<img src=""http://bair.berkeley.edu/blog/assets/novel_image_captioning/anteater.png"" alt=""Anteater in forest"" />\n\t\t</td>\n  </tr>\n  <tr>\n    <td><p>\n\t\t\t<a href=""http://jeffdonahue.com/lrcn/"">LRCN</a> [Donahue et al. ‘15]: A brown bear standing on top of a lush green field. <br />\n\t\t\t<a href=""http://captionbot.ai"">MS CaptionBot</a> [Tran et al. ‘16]: A large brown bear walking through a forest.\n\t\t</p></td>\n    <td><p>\n\t\t\t<a href=""http://jeffdonahue.com/lrcn/"">LRCN</a> [Donahue et al. ‘15]: A black <span style=""color:red;"">bear</span> is standing in the grass. <br />\n\t\t\t<a href=""http://captionbot.ai"">MS CaptionBot</a> [Tran et al. ‘16]: A <span style=""color:red;"">bear</span> that is eating some grass.\n\t\t</p></td>\n  </tr>\n</table>\n<p style=""text-align:center;""><i>\n\tDescriptions generated by existing captioners on two images. On the left is an image of an object (bear) that is present in training data. On the right is an object (anteater) that the model hasn\'t seen in training.\n</i></p>\n\n<p>Current visual description or image captioning models work quite well, but they can only describe objects seen in existing image captioning training datasets, and they require a large number of training examples to generate good captions. To learn how to describe an object like “jackal” or “anteater” in context, most description models require many examples of jackal or anteater images with corresponding descriptions.  However, current visual description datasets, like <a href=""mscoco.org"">MSCOCO</a>, do not include descriptions about all objects. In contrast, recent works in object recognition through Convolutional Neural Networks (CNNs) can recognize hundreds of categories of objects. While object recognition models can recognize jackals and anteaters, description models cannot compose sentences to describe these animals correctly in context.  In our work, we overcome this problem by building visual description systems which can describe new objects without pairs of images and sentences about these objects.</p>\n\n<h2 id=""the-task-describing-novel-objects"">The Task: Describing Novel Objects</h2>\n\n<p>Here we define our task more formally.  Given a dataset consisting of pairs of images and descriptions (paired image-sentence data, e.g. <a href=""mscoco.org"">MSCOCO</a>) as well as images with object labels but no descriptions (unpaired image data, such as <a href=""http://www.image-net.org/"">ImageNet</a>) we wish to learn how to describe objects unseen in paired image-sentence data. To do this we must build a model which can recognize different visual constituents (e.g., jackal, brown, standing, and field) and compose these in novel ways to form a coherent description.  Below we describe the core components of our description model.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/blog/assets/novel_image_captioning/image_0.png"" alt=""The novel visual description task"" />\n</p>\n\n<p>We aim to describe diverse objects which do not have training images with captions.</p>\n\n<!--more-->\n\n<h3 id=""using-external-sources-of-data"">Using External Sources of Data</h3>\n\n<p>In order to generate captions about diverse categories of objects outside the image-caption training data, we take advantage of external data sources. Specifically, we use ImageNet images with object labels as the unpaired image data source and sentences from unannotated text corpora such as Wikipedia as our text data source. These are used to train our visual recognition CNN and language model respectively.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/blog/assets/novel_image_captioning/image_1.png"" alt=""Train effectively on external resources"" /><br />\n<i>\nTrain effectively on external resources\n</i>\n</p>\n\n<h3 id=""capture-semantic-similarity"">Capture semantic similarity</h3>\n\n<p>We want to be able to describe unseen objects (e.g. from ImageNet) that are similar to objects that have been seen in the paired image-sentence training data. We use dense word embeddings to achieve this. Word embeddings are dense high dimensional representations of words where words with similar meaning are closer in the embedding space.</p>\n\n<p>In our previous work, called “Deep Compositional Captioning (DCC)” [1] we first train a caption model on MSCOCO paired  image caption dataset. Then to describe novel objects, for each novel object (such as an okapi) we use word embeddings to identify an object that’s most similar amongst the objects in the MSCOCO dataset (in this case zebra). We then transfer (copy) the parameters learned by the model from the seen object to the unseen object (i.e. copy weights in the network corresponding to zebra to those corresponding to okapi).</p>\n\n<h3 id=""novel-object-captioning"">Novel Object Captioning</h3>\n\n<p>While the DCC model is able to describe several unseen object categories, copying parameters from one object to another can create sentences with grammatical artifacts. E.g. for the object ‘racket’ the model copies weights from ‘tennis’, which results in sentences such as “A man playing racket on court”. In our more recent work [2], we incorporate the embeddings directly within our language model. Specifically, we use <a href=""https://nlp.stanford.edu/projects/glove/"">GloVe embeddings</a> in the input and output of our language model. This implicitly enables the model to capture semantic similarity when describing unseen objects. This enables our model to generate sentences such as “A tennis player swinging a racket at a ball”. Additionally, incorporating the embeddings directly within the network makes our model end-to-end trainable.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/blog/assets/novel_image_captioning/image_2.png"" alt=""Dense word embeddings to capture image similarity"" /><br />\n<i>\nIncorporate dense word embeddings in the language model to capture semantic similarity.\n</i>\n</p>\n\n<h3 id=""caption-model-and-forgetting-in-neural-networks"">Caption model and forgetting in neural networks.</h3>\n\n<p>We combine the outputs of the visual network and language model to a caption model. This model is similar to existing caption models which are also pre-trained on ImageNet. However, we observed that although the model is pre-trained on ImageNet, when the model is trained / tuned on the COCO image-caption dataset it tends to forget what it has seen before. The problem of forgetting in neural networks has also been observed by <a href=""https://arxiv.org/abs/1312.6211"">researchers at Montreal</a> as well as <a href=""https://arxiv.org/abs/1612.00796"">Google DeepMind</a> amongst others. In our work, we resolve this problem of forgetting using a joint training strategy.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/blog/assets/novel_image_captioning/image_3.png"" alt=""Joint training to overcome forgetting"" /><br />\n<i>\nShare parameters and train jointly on different data/tasks to overcome ""forgetting""\n</i>\n</p>\n\n<p>Specifically, our network has three components: a visual recognition network, a caption model, and a language model. All three components share parameters and are jointly trained. During training, each batch of inputs contains some images with labels, a different set of images and captions, and some plain sentences. These three inputs train the different components of the network. Since the parameters are shared between the three components, the network is jointly trained to recognize objects in images, caption images and generate sentences. This joint training helps the network overcome the problem of forgetting, and enables the model to generate descriptions for many novel object categories.</p>\n\n<h2 id=""whats-next"">What’s Next?</h2>\n\n<p>One of the most common errors in our model comes from not recognizing objects, and one way to mitigate this is to use better visual features. Another common error comes from generating sentences which are not fluent (A cat and a cat on a bed) or may not appeal to “common sense”  (e.g. ‘A woman is playing gymnastics’ is not particularly correct since one doesn’t “play” gymnastics). It would be interesting to develop solutions that can overcome these issues.</p>\n\n<p>While in this work, we proposes joint training as a strategy to overcome the problem of forgetting, it might not always be possible to train on lots of different tasks and datasets. A different way to approach the problem would be to build a model that can learn to compose descriptions based on visual information and object labels. Such a model should also be able to integrate objects on the fly i.e. currently we pre-train our model on a select set of objects, we should also think about how we can incrementally train our model on new data about some new concepts. Solving some of these problems can help develop better and more robust visual description models.</p>\n\n<p>[<a href=""https://vsubhashini.github.io/noc_examples.html"">Links to more examples</a>]</p>\n\n<p>[<a href=""http://vsubhashini.github.io/noc.html#code"">Links to trained models and code</a>]</p>\n\n<h3 id=""examples"">Examples</h3>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/blog/assets/novel_image_captioning/image_4.png"" alt=""Examples"" />\n</p>\n\n<hr />\n\n<p><strong>This blog post is based on the following research papers:</strong></p>\n\n<p>[1] L. A. Hendricks, S. Venugopalan, M. Rohrbach, R. Mooney, K. Saenko, and T. Darrell. Deep compositional captioning: Describing novel object categories without paired training data. In CVPR, 2016.</p>\n\n<p>[2] S. Venugopalan, L. A. Hendricks, M. Rohrbach, R. Mooney, K. Saenko, and T. Darrell. Captioning images with diverse objects. In CVPR, 2017.</p>\n', 'Improving Context Understanding in Multimodal Large Language Models via Multimodal Composition Learning Previous efforts using frozen Large Language Models (LLMs) for visual understanding, via image captioning or image-text retrieval tasks, face challenges when dealing with complex multimodal scenarios. In order to enhance the capabilities of Multimodal Large Language Models (MLLM) in comprehending the context of vision and language, we introduce Multimodal Composition Learning (MCL) for the purpose of mapping or aligning the vision and language input. In particular, we introduce two tasks: Multimodal-Context Captioning (MC-Cap) and Multimodal-Context Retrieval (MC-Ret) to guide a frozen LLM in comprehending the vision and language context. These specialized tasks are crafted to improve the LLM’s capacity for efficient processing and utilization of multimodal inputs, thereby enhancing its proficiency in generating more accurate text or visual representations. Extensive experiments on both retrieval tasks (i.e., zero-shot composed image retrieval, visual storytelling image retrieval and visual dialog image retrieval) and text generation tasks (i.e., visual question answering) demonstrate the effectiveness of the proposed method. The code is available at: https://github.com/dhg-wei/MCL.']"
132,20,132_manifolds_manifold_dimensionality_embeddings,"['manifolds', 'manifold', 'dimensionality', 'embeddings', 'embedding', 'lowdimensional', 'multimanifold', 'dimensional', 'highdimensional', 'submanifold']","[' Product Manifold Learning   We consider dimensionality reduction for data sets with two or more independent degrees of freedom. For example, measurements of deformable shapes with several parts that move independently fall under this characterization. Mathematically, if the space of each continuous independent motion is a manifold, then their combination forms a product manifold. In this paper, we present an algorithm for manifold factorization given a sample of points from the product manifold. Our algorithm is based on spectral graph methods for manifold learning and the separability of the Laplacian operator on product spaces. Recovering the factors of a manifold yields meaningful lower-dimensional representations, allowing one to focus on particular aspects of the data space while ignoring others. We demonstrate the potential use of our method for an important and challenging problem in structural biology: mapping the motions of proteins and other large molecules using cryo-electron microscopy data sets. ', 'Asymptotic Properties of Nonparametric Estimation on Manifold In many applications, the real high-dimensional data occupy only a very small part in the high dimensional ‘observation space’\n whose intrinsic dimension is small.\n The most popular model of such data is Manifold model which assumes that the data lie on or near an unknown manifold (Data Manifold, DM) of lower dimensionality\n embedded in an ambient high-dimensional input space (Manifold Assumption about high-dimensional data).\n Manifold Learning is a Dimensionality Reduction problem under the Manifold assumption about the processed data,\n and its goal is to construct a low-dimensional parameterization of the DM (global low-dimensional coordinates on the DM)\n from a finite dataset sampled from the DM.\n Manifold Assumption means that local neighborhood of each manifold point is equivalent to an area of low-dimensional Euclidean space.\n Because of this, most of Manifold Learning algorithms include two parts:\n ‘local part’ in which certain characteristics reflecting low-dimensional local structure of neighborhoods of all sample points\n are constructed via nonparametric estimation,\n and ‘global part’ in which global low-dimensional coordinates on the DM are constructed\n by solving the certain convex optimization problem for specific cost function depending on the local characteristics.\n Both statistical properties of ‘local part’ and its average over manifold are considered in the paper.\n The article is an extension of the paper (Yanovich, 2016) for the case of nonparametric estimation.', 'Linear Regression on Manifold Structured Data: the Impact of Extrinsic Geometry on Solutions In this paper, we study linear regression applied to data structured on a manifold. We assume that the data manifold is smooth and is embedded in a Euclidean space, and our objective is to reveal the impact of the data manifold’s extrinsic geometry on the regression. Specifically, we analyze the impact of the manifold’s curvatures (or higher order nonlinearity in the parameterization when the curvatures are locally zero) on the uniqueness of the regression solution. Our findings suggest that the corresponding linear regression does not have a unique solution when the manifold is flat. Otherwise, the manifold’s curvature (or higher order nonlinearity in the embedding) may contribute significantly, particularly in the solution associated with the normal directions of the manifold. Our findings thus reveal the role of data manifold geometry in ensuring the stability of regression models for out-of-distribution inferences.']"
133,31,133_hyperbolic_embeddings_embedding_hierarchical,"['hyperbolic', 'embeddings', 'embedding', 'hierarchical', 'dimensionality', 'lowdimensional', 'highdimensional', 'visualizing', 'spacemap', 'representations']","['Hyperbolic Representation Learning: Revisiting and Advancing The non-Euclidean geometry of hyperbolic spaces has recently garnered considerable attention in the realm of representation learning. Current endeavors in hyperbolic representation largely presuppose that the underlying hierarchies can be automatically inferred and preserved through the adaptive optimization process. This assumption, however, is questionable and requires further validation. In this work, we first introduce a position-tracking mechanism to scrutinize existing prevalent hyperbolic models, revealing that the learned representations are sub-optimal and unsatisfactory. To address this, we propose a simple yet effective method, hyperbolic informed embedding (HIE), by incorporating cost-free hierarchical information deduced from the hyperbolic distance of the node to the origin (i.e., induced hyperbolic norm) to advance existing hyperbolic models. The proposed method HIE is both task-agnostic and model-agnostic, enabling its seamless integration with a broad spectrum of models and tasks. Extensive experiments across various models and different tasks demonstrate the versatility and adaptability of the proposed method. Remarkably, our method achieves a remarkable improvement of up to 21.4% compared to the competing baselines.', 'Representation Tradeoffs for Hyperbolic Embeddings Hyperbolic embeddings offer excellent quality with few dimensions when embedding hierarchical data structures. We give a combinatorial construction that embeds trees into hyperbolic space with arbitrarily low distortion without optimization. On WordNet, this algorithm obtains a mean-average-precision of 0.989 with only two dimensions, outperforming existing work by 0.11 points. We provide bounds characterizing the precision-dimensionality tradeoff inherent in any hyperbolic embedding. To embed general metric spaces, we propose a hyperbolic generalization of multidimensional scaling (h-MDS). We show how to perform exact recovery of hyperbolic points from distances, provide a perturbation analysis, and give a recovery result that enables us to reduce dimensionality. Finally, we extract lessons from the algorithms and theory above to design a scalable PyTorch-based implementation that can handle incomplete information.', 'Large-Margin Classification in Hyperbolic Space Representing data in hyperbolic space can effectively capture latent hierarchical relationships. To enable accurate classification of points in hyperbolic space while respecting their hyperbolic geometry, we introduce hyperbolic SVM, a hyperbolic formulation of support vector machine classifiers, and describe its theoretical connection to the Euclidean counterpart. We also generalize Euclidean kernel SVM to hyperbolic space, allowing nonlinear hyperbolic decision boundaries and providing a geometric interpretation for a certain class of indefinite kernels. Hyperbolic SVM improves classification accuracy in simulation and in real-world problems involving complex networks and word embeddings. Our work enables end-to-end analyses based on the inherent hyperbolic geometry of the data without resorting to ill-fitting tools developed for Euclidean space.']"
134,104,134_reinforcement_learning_reward_critic,"['reinforcement', 'learning', 'reward', 'critic', 'rewards', 'replay', 'atari', 'exploration', 'dqn', 'deep']","['Experience Replay with Likelihood-free Importance Weights The use of past experiences to accelerate temporal difference (TD) learning of value functions, or experience replay, is a key component in deep reinforcement learning methods such as actor-critic.In this work, we propose to re-weight experiences based on their likelihood under the stationary distribution of the current policy, and justify this with a contraction argument over the Bellman evaluation operator. The resulting TD objective encourages small approximation errors on the value function over frequently encountered states.  To balance bias (from off-policy experiences) and variance (from on-policy experiences), we use a likelihood-free density ratio estimator between on-policy and off-policy experiences, and use the learned ratios as the prioritization weights. We apply the proposed approach empirically on Soft Actor Critic (SAC), Double DQN and Data-regularized Q(DrQ), over 12 Atari environments and 6 tasks from the DeepMind control suite. We achieve superior sample complexity on 9 out of 12 Atari environments and 16 out of 24 method-task combinations for DCS compared to the best baselines.', 'Continuous Deep Q-Learning with Model-based Acceleration Model-free reinforcement learning has been successfully applied to a range of challenging problems, and has recently been extended to handle large neural network policies and value functions. However, the sample complexity of model-free algorithms, particularly when using high-dimensional function approximators, tends to limit their applicability to physical systems. In this paper, we explore algorithms and representations to reduce the sample complexity of deep reinforcement learning for continuous control tasks. We propose two complementary techniques for improving the efficiency of such algorithms. First, we derive a continuous variant of the Q-learning algorithm, which we call normalized advantage functions (NAF), as an alternative to the more commonly used policy gradient and actor-critic methods. NAF representation allows us to apply Q-learning with experience replay to continuous tasks, and substantially improves performance on a set of simulated robotic control tasks. To further improve the efficiency of our approach, we explore the use of learned models for accelerating model-free reinforcement learning. We show that iteratively refitted local linear models are especially effective for this, and demonstrate substantially faster learning on domains where such models are applicable.', 'Improving Stochastic Policy Gradients in Continuous Control with Deep Reinforcement Learning using the Beta Distribution Recently, reinforcement learning with deep neural networks has achieved great success in challenging continuous control problems such as 3D locomotion and robotic manipulation. However, in real-world control problems, the actions one can take are bounded by physical constraints, which introduces a bias when the standard Gaussian distribution is used as the stochastic policy. In this work, we propose to use the Beta distribution as an alternative and analyze the bias and variance of the policy gradients of both policies. We show that the Beta policy is bias-free and provides significantly faster convergence and higher scores over the Gaussian policy when both are used with trust region policy optimization (TRPO) and actor critic with experience replay (ACER), the state-of-the-art on- and off-policy stochastic methods respectively, on OpenAI Gym’s and MuJoCo’s continuous control environments.']"
135,54,135_neural_architectures_architecture_searching,"['neural', 'architectures', 'architecture', 'searching', 'supernetworks', 'search', 'imagenet', 'benchmark', 'nas', 'networks']","['Random Search and Reproducibility for Neural Architecture Search Neural architecture search (NAS) is a promising research direction that has the potential to replace expert-designed networks with learned, task-specific architectures. In order to help ground the empirical results in this field, we propose new NAS baselines that build off the following observations: (i) NAS is a specialized hyperparameter optimization problem; and (ii) random search is a competitive baseline for hyperparameter optimization. Leveraging these observations, we evaluate both random search with early-stopping and a novel random search with weight-sharing algorithm on two standard NAS benchmarks—PTB and CIFAR-10. Our results show that random search with early-stopping is a competitive NAS baseline, e.g., it performs at least as well as ENAS, a leading NAS method, on both benchmarks. Additionally, random search with weight-sharing outperforms random search with early-stopping, achieving a state-of-the-art NAS result on PTB and a highly competitive result on CIFAR-10. Finally, we explore the existing reproducibility issues of published NAS results.', 'Neural ensemble search via Bayesian sampling Recently, neural architecture search (NAS) has been applied to automate the design of neural networks in real-world applications. A large number of algorithms have been developed to improve the search cost or the performance of the final selected architectures in NAS. Unfortunately, these NAS algorithms aim to select only one single well-performing architecture from their search spaces and thus have overlooked the capability of neural network ensemble (i.e., an ensemble of neural networks with diverse architectures) in achieving improved performance over a single final selected architecture. To this end, we introduce a novel neural ensemble search algorithm, called neural ensemble search via Bayesian sampling (NESBS), to effectively and efficiently select well-performing neural network ensembles from a NAS search space. In our extensive experiments, NESBS algorithm is shown to be able to achieve improved performance over state-of-the-art NAS algorithms while incurring a comparable search cost, thus indicating the superior performance of our NESBS algorithm over these NAS algorithms in practice.', 'EENAS: An Eﬀicient Evolutionary Algorithm for Neural\n Architecture Search Neural Architecture Search (NAS) has been widely\n applied to automatic neural architecture\n design. Traditional NAS methods often evaluate a\n large number of architectures, leading to expensive\n computation overhead.  To speed-up architecture\n search, recent NAS methods try to employ network\n estimation strategies for guidance of promising\n architecture selection.  In this paper, we have\n proposed an efficient evolutionary algorithm for\n NAS, which adapts the most advanced proxy of\n synthetic signal bases for architecture\n estimation. Extensive experiments show that our\n method outperforms state-of-the-art NAS methods, on\n NAS-Bench-101 search space and NAS-Bench-201 search\n space (CIFAR-10, CIFAR-100 and\n ImageNet16-120). Compared with existing works, our\n method could identify better architectures with\n greatly reduced search time.']"
136,11,136_metric_recognition_classification_similarityscore,"['metric', 'recognition', 'classification', 'similarityscore', 'similarity', 'segmentation', 'datasets', 'distances', 'manifold', 'riemannian']","['Two-Stage Metric Learning In this paper, we present a novel two-stage metric learning algorithm. We first map each learning instance to a probability distribution by computing its similarities to a set of fixed anchor points. Then, we define the distance in the input data space as the Fisher information distance on the associated statistical manifold. This induces in the input data space a new family of distance metric which presents unique properties. Unlike kernelized metric learning, we do not require the similarity measure to be positive semi-definite. Moreover, it can also be interpreted as a local metric learning algorithm with well defined distance approximation. We evaluate its performance on a number of datasets. It outperforms significantly other metric learning methods and SVM.', 'Log-Euclidean Metric Learning on Symmetric Positive Definite Manifold with Application to Image Set Classification The manifold of Symmetric Positive Definite (SPD) matrices has been successfully used for data representation in image set classification. By endowing the SPD manifold with Log-Euclidean Metric, existing methods typically work on vector-forms of SPD matrix logarithms. This however not only inevitably distorts the geometrical structure of the space of SPD matrix logarithms but also brings low efficiency especially when the dimensionality of SPD matrix is high. To overcome this limitation, we propose a novel metric learning approach to work directly on logarithms of SPD matrices. Specifically, our method aims to learn a tangent map that can directly transform the matrix logarithms from the original tangent space to a new tangent space of more discriminability. Under the tangent map framework, the novel metric learning can then be formulated as an optimization problem of seeking a Mahalanobis-like matrix, which can take the advantage of traditional metric learning techniques. Extensive evaluations on several image set classification tasks demonstrate the effectiveness of our proposed metric learning method.', 'An Information Geometry Approach for Distance Metric Learning Metric learning is an important problem in machine learning and  pattern recognition. In this paper, we propose a framework for  metric learning based on information geometry. The key idea is to  construct two kernel matrices for the given training data: one is  based on the distance metric and the other is based on the assigned  class labels. Inspired by the idea of information geometry, we  relate these two kernel matrices to two Gaussian distributions, and  the difference between the two kernel matrices is then computed by  the Kullback-Leibler (KL) divergence between the two Gaussian  distributions. The optimal distance metric is then found by  minimizing the divergence between the two distributions. Based on  this idea, we present two metric learning algorithms, one for linear  distance metric and the other for nonlinear distance with the  introduction of a kernel function. Unlike many existing algorithms  for metric learning that require solving a non-trivial optimization  problem and therefore are computationally expensive when the data  dimension is high, the proposed algorithms have a closed-form  solution and are computationally more efficient. Extensive  experiments with data classification and face recognition show that  the proposed algorithms are comparable to or better than the  state-of-the-art algorithms for metric learning.']"
137,85,137_offline_reinforcement_offlinetoonline_learning,"['offline', 'reinforcement', 'offlinetoonline', 'learning', 'offpolicy', 'rl', 'optimal', 'learned', 'policies', 'critic']","['Offline Reinforcement Learning: Fundamental Barriers for Value Function Approximation We consider the offline reinforcement learning problem, where the aim is to learn a decision making policy from logged data. Offline RL—particularly when coupled with (value) function approximation to allow for generalization in large or continuous state spaces—is becoming increasingly relevant in practice, because it avoids costly and time-consuming online data collection and is well suited to safety-critical domains. Existing sample complexity guarantees for offline value function approximation methods typically require both (1) distributional assumptions (i.e., good coverage) and (2) representational assumptions (i.e., ability to represent some or all $Q$-value functions) stronger than what is required for supervised learning. However, the necessity of these conditions and the fundamental limits of offline RL are not well understood in spite of decades of research. This led Chen and Jiang (2019) to conjecture that concentrability (the most standard notion of coverage) and realizability (the weakest representation condition)  alone are not sufficient for sample-efficient offline RL. We resolve this conjecture in the positive by proving that in general, even if both concentrability and realizability are satisfied, any algorithm requires sample complexity either polynomial in the size of the state space or exponential in other parameters to learn a non-trivial policy. Our results show that sample-efficient offline reinforcement learning requires either restrictive coverage conditions or representation conditions that go beyond supervised learning, and highlight a phenomenon called over-coverage which serves as a fundamental barrier for offline value function approximation methods. A consequence of our results for reinforcement learning with linear function approximation is that the separation between online and offline RL can be arbitrarily large, even in constant dimension.', 'Representation Matters: Offline Pretraining for Sequential Decision Making The recent success of supervised learning methods on ever larger offline datasets has spurred interest in the reinforcement learning (RL) field to investigate whether the same paradigms can be translated to RL algorithms. This research area, known as offline RL, has largely focused on offline policy optimization, aiming to find a return-maximizing policy exclusively from offline data. In this paper, we consider a slightly different approach to incorporating offline data into sequential decision-making. We aim to answer the question, what unsupervised objectives applied to offline datasets are able to learn state representations which elevate performance on downstream tasks, whether those downstream tasks be online RL, imitation learning from expert demonstrations, or even offline policy optimization based on the same offline dataset? Through a variety of experiments utilizing standard offline RL datasets, we find that the use of pretraining with unsupervised learning objectives can dramatically improve the performance of policy learning algorithms that otherwise yield mediocre performance on their own. Extensive ablations further provide insights into what components of these unsupervised objectives {–} e.g., reward prediction, continuous or discrete representations, pretraining or finetuning {–} are most important and in which settings.', 'Leveraging Offline Data in Online Reinforcement Learning Two central paradigms have emerged in the reinforcement learning (RL) community: online RL and offline RL. In the online RL setting, the agent has no prior knowledge of the environment, and must interact with it in order to find an $\\epsilon$-optimal policy. In the offline RL setting, the learner instead has access to a fixed dataset to learn from, but is unable to otherwise interact with the environment, and must obtain the best policy it can from this offline data. Practical scenarios often motivate an intermediate setting: if we have some set of offline data and may also interact with the environment, how can we best use the offline data to minimize the number of online interactions necessary to learn an $\\epsilon$-optimal policy. In this work, we consider this setting, which we call the FineTuneRL setting, for MDPs with linear structure. We characterize the necessary number of online samples needed in this setting given access to some offline dataset, and develop an algorithm, FTPedel, which is provably optimal, up to $H$ factors. We show through an explicit example that combining offline data with online interactions can lead to a provable improvement over either purely offline or purely online RL. Finally, our results illustrate the distinction between verifiable learning, the typical setting considered in online RL, and unverifiable learning, the setting often considered in offline RL, and show that there is a formal separation between these regimes.']"
138,14,138_timeseries_graphs_graph_forecasting,"['timeseries', 'graphs', 'graph', 'forecasting', 'networks', 'predicting', 'epidemic', 'temporal', 'epidemics', 'modeling']","['Temporal Graph Neural Networks for Irregular Data This paper proposes a temporal graph neural network model for forecasting of graph-structured irregularly observed time series. Our TGNN4I model is designed to handle both irregular time steps and partial observations of the graph. This is achieved by introducing a time-continuous latent state in each node, following a linear Ordinary Differential Equation (ODE) defined by the output of a Gated Recurrent Unit (GRU). The ODE has an explicit solution as a combination of exponential decay and periodic dynamics. Observations in the graph neighborhood are taken into account by integrating graph neural network layers in both the GRU state update and predictive model. The time-continuous dynamics additionally enable the model to make predictions at arbitrary time steps. We propose a loss function that leverages this and allows for training the model for forecasting over different time horizons. Experiments on simulated data and real-world data from traffic and climate modeling validate the usefulness of both the graph structure and time-continuous dynamics in settings with irregular observations.', 'Temporal Multiresolution Graph Neural Networks For Epidemic Prediction In this paper, we introduce Temporal Multiresolution Graph Neural Networks (TMGNN), the first architecture that both learns to construct the multiscale and multiresolution graph structures and incorporates the time-series signals to capture the temporal changes of the dynamic graphs. We have applied our proposed model to the task of predicting future spreading of epidemic and pandemic based on the historical time-series data collected from the actual COVID-19 pandemic and chickenpox epidemic in several European countries, and have obtained competitive results in comparison to other previous state-of-the-art temporal architectures and graph learning algorithms. We have shown that capturing the multiscale and multiresolution structures of graphs is important to extract either local or global information that play a critical role in understanding the dynamic of a global pandemic such as COVID-19 which started from a local city and spread to the whole world. Our work brings a promising research direction in forecasting and mitigating future epidemics and pandemics. Our source code is available at https://github.com/bachnguyenTE/temporal-mgn.', 'Temporal Multiresolution Graph Neural Networks For Epidemic Prediction In this paper, we introduce Temporal Multiresolution Graph Neural Networks (TMGNN), the first architecture that both learns to construct the multiscale and multiresolution graph structures and incorporates the time-series signals to capture the temporal changes of the dynamic graphs. We have applied our proposed model to the task of predicting future spreading of epidemic and pandemic based on the historical time-series data collected from the actual COVID-19 pandemic and chickenpox epidemic in several European countries, and have obtained competitive results in comparison to other previous state-of-the-art temporal architectures and graph learning algorithms. We have shown that capturing the multiscale and multiresolution structures of graphs is important to extract either local or global information that play a critical role in understanding the dynamic of a global pandemic such as COVID-19 which started from a local city and spread to the whole world. Our work brings a promising research direction in forecasting and mitigating future epidemics and pandemics. Our source code is available at https://github.com/bachnguyenTE/temporal-mgn.']"
139,21,139_copulas_copula_bayesian_multivariate,"['copulas', 'copula', 'bayesian', 'multivariate', 'bivariate', 'marginal', 'gaussian', 'posterior', 'models', 'inference']","['Hybrid Copula Bayesian Networks This paper introduces the hybrid copula Bayesian network (HCBN) model, a generalization of the copula Bayesian network (CBN) model developed by Elidan (2010) for continuous random variables to multivariate mixed probability distributions of discrete and continuous random variables. To this end, we extend the theorems proved by Nešlehovà (2007) from bivariate to multivariate copulas with discrete and continuous marginal distributions. Using the multivariate copula with discrete and continuous marginal distributions as a theoretical basis, we construct an HCBN that can model all possible permutations of discrete and continuous random variables for parent and child nodes, unlike the popular conditional linear Gaussian network model. Finally, we demonstrate on a numerous synthetic datasets and a real life dataset that our HCBN compares favorably, from a modeling and flexibility viewpoint, to other hybrid models including the conditional linear Gaussian and the mixture of truncated exponentials models.', 'Variational Gaussian Copula Inference We utilize copulas to constitute a unified framework for constructing and optimizing variational proposals in hierarchical Bayesian models. For models with continuous and non-Gaussian hidden variables, we propose a semiparametric and automated variational  Gaussian copula approach, in which the parametric Gaussian copula family is able to preserve multivariate posterior dependence, and the nonparametric transformations based on Bernstein polynomials provide ample flexibility in characterizing the univariate marginal posteriors.', 'Gaussian Process Vine Copulas for Multivariate Dependence Copulas allow to learn marginal distributions separately from the multivariate dependence structure (copula) that links them together into a density function.  Vine factorizations ease the learning of high-dimensional copulas by constructing a hierarchy of conditional bivariate copulas. However, to simplify inference, it is common to assume that each of these conditional bivariate copulas is independent from its conditioning variables.  In this paper, we relax this assumption by discovering the latent functions that specify the shape of a conditional copula given its conditioning variables.  We learn these functions by following a Bayesian approach based on sparse Gaussian processes with expectation propagation for scalable, approximate inference. Experiments on real-world datasets show that, when modeling all conditional dependencies, we obtain better estimates of the underlying copula of the data.']"
140,20,140_distillation_distilling_models_knowledge,"['distillation', 'distilling', 'models', 'knowledge', 'distill', 'students', 'teachers', 'teacher', 'model', 'student']","['Bayesian Knowledge Distillation: A Bayesian Perspective of Distillation with Uncertainty Quantification Knowledge distillation (KD) has been widely used for model compression and deployment acceleration. Nonetheless, the statistical insight of the remarkable performance of KD remains elusive, and methods for evaluating the uncertainty of the distilled model/student model are lacking. To address these issues, we establish a close connection between KD and a Bayesian model. In particular, we develop an innovative method named Bayesian Knowledge Distillation (BKD) to provide a transparent interpretation of the working mechanism of KD, and a suite of Bayesian inference tools for the uncertainty quantification of the student model. In BKD, the regularization imposed by the teacher model in KD is formulated as a teacher-informed prior for the student model’s parameters. Consequently, we establish the equivalence between minimizing the KD loss and estimating the posterior mode in BKD. Efficient Bayesian inference algorithms are developed based on the stochastic gradient Langevin Monte Carlo and examined with extensive experiments on uncertainty ranking and credible intervals construction for predicted class probabilities.', 'What Mechanisms Does Knowledge Distillation Distill? Knowledge distillation is a commonly-used compression method in ML due to the popularity of increasingly large-scale models, but it is unclear if all the information a teacher model contains is distilled into the smaller student model. We aim to formalize the concept of ‘knowledge’ to investigate how knowledge is transferred during distillation, focusing on shared invariant outputs to counterfactual changes of dataset latent variables (we call these latents mechanisms). We define a student model to be a good stand-in model for a teacher if it shares the teacher’s learned mechanisms, and find that Jacobian matching and contrastive representation learning are viable methods by which to train such models. While these methods do not result in perfect transfer of mechanisms, we show they often improve student fidelity or mitigate simplicity bias (as measured by the teacher-to-student KL divergence and accuracy on various out-of-distribution test datasets), especially on datasets with spurious statistical correlations.', 'Knowledge Distillation with Auxiliary Variable Knowledge distillation (KD) provides an efficient framework for transferring knowledge from a teacher model to a student model by aligning their predictive distributions. The existing KD methods adopt the same strategy as the teacher to formulate the student’s predictive distribution. However, employing the same distribution-modeling strategy typically causes sub-optimal knowledge transfer due to the discrepancy in model capacity between teacher and student models. Designing student-friendly teachers contributes to alleviating the capacity discrepancy, while it requires either complicated or student-specific training schemes. To cast off this dilemma, we propose to introduce an auxiliary variable to promote the ability of the student to model predictive distribution. The auxiliary variable is defined to be related to target variables, which will boost the model prediction. Specifically, we reformulate the predictive distribution with the auxiliary variable, deriving a novel objective function of KD. Theoretically, we provide insights to explain why the proposed objective function can outperform the existing KD methods. Experimentally, we demonstrate that the proposed objective function can considerably and consistently outperform existing KD methods.']"
141,30,141_augmentation_augmentations_learning_cnns,"['augmentation', 'augmentations', 'learning', 'cnns', 'imagenet', 'augmix', 'neural', 'deep', 'feature', 'training']","['Greedy Policy Search: A Simple Baseline for Learnable Test-Time Augmentation Test-time data augmentation—averaging the predictions of a machine learning model across multiple augmented samples of data—is a widely used technique that improves the predictive performance. While many advanced learnable data augmentation techniques have emerged in recent years, they are focused on the training phase. Such techniques are not necessarily optimal for test-time augmentation and can be outperformed by a policy consisting of simple crops and flips. The primary goal of this paper is to demonstrate that test-time augmentation policies can be successfully learned too. We introduce greedy policy search (GPS), a simple but high-performing method for learning a policy of test-time augmentation. We demonstrate that augmentation policies learned with GPS achieve superior predictive performance on image classification problems, provide better in-domain uncertainty estimation, and improve the robustness to domain shift.', 'A Kernel Theory of Modern Data Augmentation Data augmentation, a technique in which a training set is expanded with class-preserving transformations, is ubiquitous in modern machine learning pipelines. In this paper, we seek to establish a theoretical framework for understanding data augmentation. We approach this from two directions: First, we provide a general model of augmentation as a Markov process, and show that kernels appear naturally with respect to this model, even when we do not employ kernel classification. Next, we analyze more directly the effect of augmentation on kernel classifiers, showing that data augmentation can be approximated by first-order feature averaging and second-order variance regularization components. These frameworks both serve to illustrate the ways in which data augmentation affects the downstream learning model, and the resulting analyses provide novel connections between prior work in invariant kernels, tangent propagation, and robust optimization. Finally, we provide several proof-of-concept applications showing that our theory can be useful for accelerating machine learning workflows, such as reducing the amount of computation needed to train using augmented data, and predicting the utility of a transformation prior to training.', 'Automatic Data Augmentation via Invariance-Constrained Learning Underlying data structures, such as symmetries or invariance to transformations, are often exploited to improve the solution of learning tasks. However, embedding these properties in models or learning algorithms can be challenging and computationally intensive. Data augmentation, on the other hand, induces these symmetries during training by applying multiple transformations to the input data. Despite its ubiquity, its effectiveness depends on the choices of which transformations to apply, when to do so, and how often. In fact, there is both empirical and theoretical evidence that the indiscriminate use of data augmentation can introduce biases that outweigh its benefits. This work tackles these issues by automatically adapting the data augmentation while solving the learning task. To do so, it formulates data augmentation as an invariance constrained learning problem and leverages Monte Carlo Markov Chain (MCMC) sampling to solve it. The result is an algorithm that not only does away with a priori searches for augmentation distributions, but also dynamically controls if and when data augmentation is applied. We validate empirically our theoretical developments in automatic data augmentation benchmarks for CIFAR and ImageNet-100 datasets. Furthermore, our experiments show how this approach can be used to gather insights on the actual symmetries underlying a learning task.']"
142,18,142_shifts_adaptation_adapting_learning,"['shifts', 'adaptation', 'adapting', 'learning', 'shift', 'supervised', 'prediction', 'covariateshift', 'training', 'regression']","['Sequential Covariate Shift Detection Using Classifier Two-Sample Tests A standard assumption in supervised learning is that the training data and test data are from the same distribution. However, this assumption often fails to hold in practice, which can cause the learned model to perform poorly. We consider the problem of detecting covariate shift, where the covariate distribution shifts but the conditional distribution of labels given covariates remains the same. This problem can naturally be solved using a two-sample test{—}i.e., test whether the current test distribution of covariates equals the training distribution of covariates. Our algorithm builds on classifier tests, which train a discriminator to distinguish train and test covariates, and then use the accuracy of this discriminator as a test statistic. A key challenge is that classifier tests assume given a fixed set of test covariates. In practice, test covariates often arrive sequentially over time{—}e.g., a self-driving car observes a stream of images while driving. Furthermore, covariate shift can occur multiple times{—}i.e., shift and then shift back later or gradually shift over time. To address these challenges, our algorithm trains the discriminator online. Additionally, it evaluates test accuracy using each new covariate before taking a gradient step; this strategy avoids constructing a held-out test set, which can improve sample efficiency. We prove that this optimization preserves the correctness{—}i.e., our algorithm achieves a desired bound on the false positive rate. In our experiments, we show that our algorithm efficiently detects covariate shifts on multiple datasets{—}ImageNet, IWildCam, and Py150.', 'TransFusion: Covariate-Shift Robust Transfer Learning for High-Dimensional Regression The main challenge that sets transfer learning apart from traditional supervised learning is the distribution shift, reflected as the shift between the source and target models and that between the marginal covariate distributions. In this work, we tackle model shifts in the presence of covariate shifts in the high-dimensional regression setting. Specifically, we propose a two-step method with a novel fused regularizer that effectively leverages samples from source tasks to improve the learning performance on a target task with limited samples. Nonasymptotic bound is provided for the estimation error of the target model, showing the robustness of the proposed method to covariate shifts. We further establish conditions under which the estimator is minimax-optimal. Additionally, we extend the method to a distributed setting, allowing for a pretraining-finetuning strategy, requiring just one round of communication while retaining the estimation rate of the centralized version. Numerical tests validate our theory, highlighting the method’s robustness to covariate shifts.', 'Low-Dimensional Density Ratio Estimation for Covariate Shift Correction Covariate shift is a prevalent setting for supervised learning in the wild when the training and test data are drawn from different time periods, different but related domains, or via different sampling strategies.  This paper addresses a transfer learning setting, with covariate shift between source and target domains. Most existing methods for correcting covariate shift exploit density ratios of the features to reweight the source-domain data, and when the features are high-dimensional, the estimated density ratios may suffer large estimation variances, leading to poor performance of prediction under covariate shift. In this work, we investigate the dependence of covariate shift correction performance on the dimensionality of the features, and  propose a correction method that finds a low-dimensional representation of the features, which takes into account feature relevant to the target $Y$, and exploits the density ratio of this representation for importance reweighting. We discuss the factors that affect the performance of our method, and demonstrate its capabilities on both pseudo-real data and real-world applications.']"
143,108,143_adaptation_adversarial_learning_deep,"['adaptation', 'adversarial', 'learning', 'deep', 'domaininvariant', 'learn', 'trained', 'domains', 'training', 'domain']","['Unsupervised Domain Adaptation by Backpropagation Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of ""deep"" features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation. Overall, the approach can be implemented with little effort using any of the deep-learning packages. The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-of-the-art on Office datasets.', 'On Learning Invariant Representations for Domain Adaptation Due to the ability of deep neural nets to learn rich representations, recent advances in unsupervised domain adaptation have focused on learning domain-invariant features that achieve a small error on the source domain. The hope is that the learnt representation, together with the hypothesis learnt from the source domain, can generalize to the target domain. In this paper, we first construct a simple counterexample showing that, contrary to common belief, the above conditions are not sufficient to guarantee successful domain adaptation. In particular, the counterexample exhibits <em>conditional shift</em>: the class-conditional distributions of input features change between source and target domains. To give a sufficient condition for domain adaptation, we propose a natural and interpretable generalization upper bound that explicitly takes into account the aforementioned shift. Moreover, we shed new light on the problem by proving an information-theoretic lower bound on the joint error of <em>any</em> domain adaptation method that attempts to learn invariant representations. Our result characterizes a fundamental tradeoff between learning invariant representations and achieving small joint error on both domains when the marginal label distributions differ from source to target. Finally, we conduct experiments on real-world datasets that corroborate our theoretical findings. We believe these insights are helpful in guiding the future design of domain adaptation and representation learning algorithms.', 'Label-Noise Robust Domain Adaptation Domain adaptation aims to correct the classifiers when faced with distribution shift between source (training) and target (test) domains. State-of-the-art domain adaptation methods make use of deep networks to extract domain-invariant representations. However, existing methods assume that all the instances in the source domain are correctly labeled; while in reality, it is unsurprising that we may obtain a source domain with noisy labels. In this paper, we are the first to comprehensively investigate how label noise could adversely affect existing domain adaptation methods in various scenarios. Further, we theoretically prove that there exists a method that can essentially reduce the side-effect of noisy source labels in domain adaptation. Specifically, focusing on the generalized target shift scenario, where both label distribution $P_Y$ and the class-conditional distribution $P_{X|Y}$ can change, we discover that the denoising Conditional Invariant Component (DCIC) framework can provably ensures (1) extracting invariant representations given examples with noisy labels in the source domain and unlabeled examples in the target domain and (2) estimating the label distribution in the target domain with no bias. Experimental results on both synthetic and real-world data verify the effectiveness of the proposed method.']"
144,21,144_adaptivegradient_reinforcement_gradient_learning,"['adaptivegradient', 'reinforcement', 'gradient', 'learning', 'gradients', 'bias', 'optimization', 'policydistribution', 'stochastic', 'exploration']","['The Mirage of Action-Dependent Baselines in Reinforcement Learning Policy gradient methods are a widely used class of model-free reinforcement learning algorithms where a state-dependent baseline is used to reduce gradient estimator variance. Several recent papers extend the baseline to depend on both the state and action and suggest that this significantly reduces variance and improves sample efficiency without introducing bias into the gradient estimates. To better understand this development, we decompose the variance of the policy gradient estimator and numerically show that learned state-action-dependent baselines do not in fact reduce variance over a state-dependent baseline in commonly tested benchmark domains. We confirm this unexpected result by reviewing the open-source code accompanying these prior papers, and show that subtle implementation decisions cause deviations from the methods presented in the papers and explain the source of the previously observed empirical gains. Furthermore, the variance decomposition highlights areas for improvement, which we demonstrate by illustrating a simple change to the typical value function parameterization that can significantly improve performance.', 'A Baseline for Any Order Gradient Estimation in Stochastic Computation Graphs By enabling correct differentiation in Stochastic Computation Graphs (SCGs), the infinitely differentiable Monte-Carlo estimator (DiCE) can generate correct estimates for the higher order gradients that arise in, e.g., multi-agent reinforcement learning and meta-learning. However, the baseline term in DiCE that serves as a control variate for reducing variance applies only to first order gradient estimation, limiting the utility of higher-order gradient estimates. To improve the sample efficiency of DiCE, we propose a new baseline term for higher order gradient estimation. This term may be easily included in the objective, and produces unbiased variance-reduced estimators under (automatic) differentiation, without affecting the estimate of the objective itself or of the first order gradient estimate. It reuses the same baseline function (e.g., the state-value function in reinforcement learning) already used for the first order baseline. We provide theoretical analysis and numerical evaluations of this new baseline, which demonstrate that it can dramatically reduce the variance of DiCE’s second order gradient estimators and also show empirically that it reduces the variance of third and fourth order gradients. This computational tool can be easily used to estimate higher order gradients with unprecedented efficiency and simplicity wherever automatic differentiation is utilised, and it has the potential to unlock applications of higher order gradients in reinforcement learning and meta-learning.', 'Regularized Policy Gradients: Direct Variance Reduction in Policy Gradient Estimation Policy gradient algorithms are widely used in reinforcement learning problems with continuous action spaces, which update the policy parameters along the steepest direction of the expected return. However, large variance of policy gradient estimation often causes instability of policy update. In this paper, we propose to suppress the variance of gradient estimation by directly employing the variance of policy gradients as a regularizer. Through experiments, we demonstrate that the proposed variance-regularization technique combined with parameter-based exploration and baseline subtraction provides more reliable policy updates than non-regularized counterparts. ']"
145,94,145_reinforcement_learning_stochastic_markovian,"['reinforcement', 'learning', 'stochastic', 'markovian', 'critic', 'td', 'approximation', 'markov', 'tdc', 'policy']","['A Finite Time Analysis of Temporal Difference Learning With Linear Function Approximation Temporal difference learning (TD) is a simple iterative algorithm used to estimate the value function corresponding to a given policy in a Markov decision process. Although TD is one of the most widely used algorithms in reinforcement learning, its theoretical analysis has proved challenging and few guarantees on its statistical efficiency are available. In this work, we provide a \\emph{simple and explicit finite time analysis} of temporal difference learning with linear function approximation. Except for a few key insights, our analysis mirrors standard techniques for  analyzing stochastic gradient descent algorithms, and therefore inherits the simplicity and elegance of that literature. A final section of the paper shows that all of our main results extend to the study of a variant of Q-learning applied to optimal stopping problems.', 'Nonlinear Distributional Gradient Temporal-Difference Learning We devise a distributional variant of gradient temporal-difference (TD) learning. Distributional reinforcement learning has been demonstrated to outperform the regular one in the recent study \\citep{bellemare2017distributional}. In the policy evaluation setting, we design two new algorithms called distributional GTD2 and distributional TDC using the Cram{é}r distance on the distributional version of the Bellman error objective function, which inherits advantages of both the nonlinear gradient TD algorithms and the distributional RL approach. In the control setting, we propose the distributional Greedy-GQ using similar derivation. We prove the asymptotic almost-sure convergence of distributional GTD2 and TDC to a local optimal solution for general smooth function approximators, which includes neural networks that have been widely used in recent study to solve the real-life RL problems. In each step, the computational complexity of above three algorithms is linear w.r.t. the number of the parameters of the function approximator, thus can be implemented efficiently for neural networks.', 'Policy Evaluation for Variance in Average Reward Reinforcement Learning We consider an average reward reinforcement learning (RL) problem and work with asymptotic variance as a risk measure to model safety-critical applications. We design a temporal-difference (TD) type algorithm tailored for policy evaluation in this context. Our algorithm is based on linear stochastic approximation of an equivalent formulation of the asymptotic variance in terms of the solution of the Poisson equation. We consider both the tabular and linear function approximation settings, and establish $\\tilde {O}(1/k)$ finite time convergence rate, where $k$ is the number of steps of the algorithm. Our work paves the way for developing actor-critic style algorithms for variance-constrained RL. To the best of our knowledge, our result provides the first sequential estimator for asymptotic variance of a Markov chain with provable finite sample guarantees, which is of independent interest.']"
146,74,146_mixture_mixtures_dirichlet_nonparametric,"['mixture', 'mixtures', 'dirichlet', 'nonparametric', 'clustering', 'bayesian', 'cluster', 'posterior', 'clusters', 'buffet']","['Distributed Inference for Dirichlet Process Mixture Models Bayesian nonparametric mixture models based on the Dirichlet process (DP) have been widely used for solving problems like clustering, density estimation and topic modelling. These models make weak assumptions about the underlying process that generated the observed data. Thus, when more data are collected, the complexity of these models can change accordingly. These theoretical properties often lead to superior predictive performance when compared to traditional finite mixture models. However, despite the increasing amount of data available, the application of Bayesian nonparametric mixture models is so far limited to relatively small data sets. In this paper, we propose an efficient distributed inference algorithm for the DP and the HDP mixture model. The proposed method is based on a variant of the slice sampler for DPs. Since this sampler does not involve a pre-determined truncation, the stationary distribution of the sampling algorithm is unbiased. We provide both local thread-level and distributed machine-level parallel implementations and study the performance of this sampler through an extensive set of experiments on image and text data. When compared to existing inference algorithms, the proposed method exhibits state-of-the-art accuracy and strong scalability with up to 512 cores.', 'Parallel Markov Chain Monte Carlo for Nonparametric Mixture Models Nonparametric mixture models based on the Dirichlet process are an elegant alternative to finite models when the number of underlying components is unknown, but inference in such models can be slow. Existing attempts to parallelize inference in such models have relied on introducing approximations, which can lead to inaccuracies in the posterior estimate. In this paper, we describe auxiliary variable representations for the Dirichlet process and the hierarchical Dirichlet process that allow us to perform MCMC using the correct equilibrium distribution, in a distributed manner. We show that our approach allows scalable inference without the deterioration in estimate quality that accompanies existing methods.', 'Online Learning of a Dirichlet Process Mixture of Generalized Dirichlet Distributions for Simultaneous Clustering and Localized Feature Selection Online algorithms allow data instances to be processed in a sequential way, which is important for large-scale and real-time applications. In this paper, we propose a novel online clustering approach based on a Dirichlet process mixture of generalized Dirichlet (GD) distributions, which can be considered as an extension of the finite GD mixture model to the infinite case. Our approach is built on nonparametric Bayesian analysis where the determination of the number of clusters is sidestepped by assuming an infinite number of mixture components. Moreover, an unsupervised localized feature selection scheme is integrated with the proposed nonparametric framework to improve the clustering performance. By learning the proposed model in an online manner using a variational approach, all the involved parameters and features saliencies are estimated simultaneously and effectively in closed forms. The proposed online infinite mixture model is validated through both synthetic data sets and two challenging real-world applications namely text document clustering and online human face detection.']"
147,89,147_topics_topic_embeddings_topical,"['topics', 'topic', 'embeddings', 'topical', 'corpus', 'semantic', 'neural', 'dirichlet', 'generative', 'supervised']","['Inter and Intra Topic Structure Learning with Word Embeddings One important task of topic modeling for text analysis is interpretability. By discovering structured topics one is able to yield improved interpretability as well as modeling accuracy. In this paper, we propose a novel topic model with a deep structure that explores both inter-topic and intra-topic structures informed by word embeddings. Specifically, our model discovers inter topic structures in the form of topic hierarchies and discovers intra topic structures in the form of sub-topics, each of which is informed by word embeddings and captures a fine-grained thematic aspect of a normal topic. Extensive experiments demonstrate that our model achieves the state-of-the-art performance in terms of perplexity, document classification, and topic quality. Moreover, with topic hierarchies and sub-topics, the topics discovered in our model are more interpretable, providing an illuminating means to understand text data.', 'Effective Neural Topic Modeling with Embedding Clustering Regularization Topic models have been prevalent for decades with various applications. However, existing topic models commonly suffer from the notorious topic collapsing: discovered topics semantically collapse towards each other, leading to highly repetitive topics, insufficient topic discovery, and damaged model interpretability. In this paper, we propose a new neural topic model, Embedding Clustering Regularization Topic Model (ECRTM). Besides the existing reconstruction error, we propose a novel Embedding Clustering Regularization (ECR), which forces each topic embedding to be the center of a separately aggregated word embedding cluster in the semantic space. This enables each produced topic to contain distinct word semantics, which alleviates topic collapsing. Regularized by ECR, our ECRTM generates diverse and coherent topics together with high-quality topic distributions of documents. Extensive experiments on benchmark datasets demonstrate that ECRTM effectively addresses the topic collapsing issue and consistently surpasses state-of-the-art baselines in terms of topic quality, topic distributions of documents, and downstream classification tasks.', 'Neural Topic Model with Attention for Supervised Learning Topic modeling utilizing neural variational inference has shown promising results recently. Unlike traditional Bayesian topic models, neural topic models use deep neural network to approximate the intractable marginal distribution and thus gain strong generalisation ability. However, neural topic models are unsupervised model. Directly using the document-specific topic proportions in downstream prediction tasks could lead to sub-optimal performance. This paper presents Topic Attention Model (TAM), a supervised neural topic model that integrates an attention recurrent neural network (RNN) model. We design a novel way to utilize document-specific topic proportions and global topic vectors learned from neural topic model in the attention mechanism. We also develop backpropagation inference method that allows for joint model optimisation.  Experimental results on three public datasets show that TAM  not only significantly improves supervised learning tasks, including classification and regression, but also achieves lower perplexity for the document modeling.']"
148,36,148_lasso_sparse_gaussian_models,"['lasso', 'sparse', 'gaussian', 'models', 'matrixvariate', 'graphical', 'penalized', 'robust', 'optimization', 'estimating']","['Robust Gaussian Graphical Model Estimation with Arbitrary Corruption We study the problem of estimating the high-dimensional Gaussian graphical model where the data are arbitrarily corrupted. We propose a robust estimator for the sparse precision matrix in the high-dimensional regime. At the core of our method is a robust covariance matrix estimator, which is based on truncated inner product. We establish the statistical guarantee of our estimator on both estimation error and model selection consistency. In particular, we show that provided that the number of corrupted samples $n_2$ for each variable satisfies $n_2 \\lesssim \\sqrt{n}/\\sqrt{\\log d}$, where $n$ is the sample size and $d$ is the number of variables, the proposed robust precision matrix estimator attains the same statistical rate as the standard estimator for Gaussian graphical models. In addition, we propose a hypothesis testing procedure to assess the uncertainty of our robust estimator. We demonstrate the effectiveness of our method through extensive experiments on both synthetic data and real-world genomic data.', 'Large-Scale Optimization Algorithms for Sparse Conditional Gaussian Graphical Models This paper addresses the problem of scalable optimization for L1-regularized conditional Gaussian graphical models.  Conditional Gaussian graphical models generalize the well-known Gaussian graphical models to conditional distributions to model the output network influenced by conditioning input variables. While highly scalable optimization methods exist for sparse Gaussian graphical model estimation, state-of-the-art methods for conditional Gaussian graphical models are not efficient enough and more importantly, fail due to memory constraints for very large problems. In this paper, we propose a new optimization procedure based on a Newton method that efficiently iterates over two sub-problems, leading to drastic improvement in computation time compared to the previous methods. We then extend our method to scale to large problems under memory constraints, using block coordinate descent to limit memory usage while achieving fast convergence. Using synthetic and genomic data, we show that our methods can solve problems with millions of variables and tens of billions of parameters to high accuracy on a single machine.', 'Graphical Nonconvex Optimization via an Adaptive Convex Relaxation We consider the problem of learning high-dimensional Gaussian graphical models. The graphical lasso is one of the most popular methods for estimating Gaussian graphical models. However, it does not achieve the oracle rate of convergence. In this paper, we propose the graphical nonconvex optimization for optimal estimation in Gaussian graphical models, which is then approximated by a sequence of convex programs. Our proposal is computationally tractable and produces an estimator that achieves the oracle rate of convergence. The statistical error introduced by the sequential approximation using a sequence of convex programs is clearly demonstrated via a contraction property. The proposed methodology is then extended to modeling semiparametric graphical models. We show via numerical studies that the proposed estimator outperforms other popular methods for estimating Gaussian graphical models.']"
149,423,149_neural_neuron_regularization_networks,"['neural', 'neuron', 'regularization', 'networks', 'neurons', 'cnns', 'gradients', 'layers', 'deep', 'learning']","['A Deep Conditioning Treatment of Neural Networks We study the role of depth in training randomly initialized overparameterized neural networks. We give a general result showing that depth improves trainability of neural networks by improving the conditioning of certain kernel matrices of the input data. This result holds for arbitrary non-linear activation functions under a certain normalization. We provide versions of the result that hold for training just the top layer of the neural network, as well as for training all layers, via the neural tangent kernel. As applications of these general results, we provide a generalization of the results of Das et al. (2019) showing that learnability of deep random neural networks with a large class of non-linear activations degrades exponentially with depth. We also show how benign overfitting can occur in deep neural networks via the results of Bartlett et al. (2019b). We also give experimental evidence that normalized versions of ReLU are a viable alternative to more complex operations like Batch Normalization in training deep neural networks.', 'On the Proof of Global Convergence of Gradient Descent for Deep ReLU Networks with Linear Widths We give a simple proof for the global convergence of gradient descent in training deep ReLU networks with the standard square loss, and show some of its improvements over the state-of-the-art. In particular, while prior works require all the hidden layers to be wide with width at least $\\Omega(N^8)$ ($N$ being the number of training samples), we require a single wide layer of linear, quadratic or cubic width depending on the type of initialization. Unlike many recent proofs based on the Neural Tangent Kernel (NTK), our proof need not track the evolution of the entire NTK matrix, or more generally, any quantities related to the changes of activation patterns during training. Instead, we only need to track the evolution of the output at the last hidden layer, which can be done much more easily thanks to the Lipschitz property of ReLU. Some highlights of our setting: (i) all the layers are trained with standard gradient descent, (ii) the network has standard parameterization as opposed to the NTK one, and (iii) the network has a single wide layer as opposed to having all wide hidden layers as in most of NTK-related results.', 'A Convergence Theory for Deep Learning via Over-Parameterization Deep neural networks (DNNs) have demonstrated dominating performance in many fields; since AlexNet, networks used in practice are going wider and deeper. On the theoretical side, a long line of works have been focusing on why we can train neural networks when there is only one hidden layer. The theory of multi-layer networks remains unsettled. In this work, we prove simple algorithms such as stochastic gradient descent (SGD) can find Global Minima on the training objective of DNNs in Polynomial Time. We only make two assumptions: the inputs do not degenerate and the network is over-parameterized. The latter means the number of hidden neurons is sufficiently large: polynomial in L, the number of DNN layers and in n, the number of training samples. As concrete examples, starting from randomly initialized weights, we show that SGD attains 100% training accuracy in classification tasks, or minimizes regression loss in linear convergence speed eps \xa0 e^{-T}, with running time polynomial in n and L. Our theory applies to the widely-used but non-smooth ReLU activation, and to any smooth and possibly non-convex loss functions. In terms of network architectures, our theory at least applies to fully-connected neural networks, convolutional neural networks (CNN), and residual neural networks (ResNet).']"
150,27,150_graphbased_graphrnn_graphs_graph,"['graphbased', 'graphrnn', 'graphs', 'graph', 'subgraph', 'graphite', 'adjacency', 'nodes', 'generative', 'graphaf']","['Improving Graph Generation by Restricting Graph Bandwidth Deep graph generative modeling has proven capable of learning the distribution of complex, multi-scale structures characterizing real-world graphs. However, one of the main limitations of existing methods is their large output space, which limits generation scalability and hinders accurate modeling of the underlying distribution. To overcome these limitations, we propose a novel approach that significantly reduces the output space of existing graph generative models. Specifically, starting from the observation that many real-world graphs have low graph bandwidth, we restrict graph bandwidth during training and generation. Our strategy improves both generation scalability and quality without increasing architectural complexity or reducing expressiveness. Our approach is compatible with existing graph generative methods, and we describe its application to both autoregressive and one-shot models. We extensively validate our strategy on synthetic and real datasets, including molecular graphs. Our experiments show that, in addition to improving generation efficiency, our approach consistently improves generation quality and reconstruction accuracy. The implementation is made available.', 'Score-based Generative Modeling of Graphs via the System of Stochastic Differential Equations Generating graph-structured data requires learning the underlying distribution of graphs. Yet, this is a challenging problem, and the previous graph generative methods either fail to capture the permutation-invariance property of graphs or cannot sufficiently model the complex dependency between nodes and edges, which is crucial for generating real-world graphs such as molecules. To overcome such limitations, we propose a novel score-based generative model for graphs with a continuous-time framework. Specifically, we propose a new graph diffusion process that models the joint distribution of the nodes and edges through a system of stochastic differential equations (SDEs). Then, we derive novel score matching objectives tailored for the proposed diffusion process to estimate the gradient of the joint log-density with respect to each component, and introduce a new solver for the system of SDEs to efficiently sample from the reverse diffusion process. We validate our graph generation method on diverse datasets, on which it either achieves significantly superior or competitive performance to the baselines. Further analysis shows that our method is able to generate molecules that lie close to the training distribution yet do not violate the chemical valency rule, demonstrating the effectiveness of the system of SDEs in modeling the node-edge relationships.', 'Efficient and Degree-Guided Graph Generation via Discrete Diffusion Modeling Diffusion-based generative graph models have been proven effective in generating high-quality small graphs. However, they need to be more scalable for generating large graphs containing thousands of nodes desiring graph statistics. In this work, we propose EDGE, a new diffusion-based generative graph model that addresses generative tasks with large graphs. To improve computation efficiency, we encourage graph sparsity by using a discrete diffusion process that randomly removes edges at each time step and finally obtains an empty graph. EDGE only focuses on a portion of nodes in the graph at each denoising step. It makes much fewer edge predictions than previous diffusion-based models. Moreover, EDGE admits explicitly modeling the node degrees of the graphs, further improving the model performance. The empirical study shows that EDGE is much more efficient than competing methods and can generate large graphs with thousands of nodes. It also outperforms baseline models in generation quality: graphs generated by our approach have more similar graph statistics to those of the training graphs.']"
151,33,151_probabilistic_compilers_bayesian_programming,"['probabilistic', 'compilers', 'bayesian', 'programming', 'markov', 'program', 'programs', 'inference', 'mcmc', 'metropolishastings']","['Nonparametric Hamiltonian Monte Carlo Probabilistic programming uses programs to express generative models whose posterior probability is then computed by built-in inference engines. A challenging goal is to develop general purpose inference algorithms that work out-of-the-box for arbitrary programs in a universal probabilistic programming language (PPL). The densities defined by such programs, which may use stochastic branching and recursion, are (in general) nonparametric, in the sense that they correspond to models on an infinite-dimensional parameter space. However standard inference algorithms, such as the Hamiltonian Monte Carlo (HMC) algorithm, target distributions with a fixed number of parameters. This paper introduces the Nonparametric Hamiltonian Monte Carlo (NP-HMC) algorithm which generalises HMC to nonparametric models. Inputs to NP-HMC are a new class of measurable functions called “tree representable”, which serve as a language-independent representation of the density functions of probabilistic programs in a universal PPL. We provide a correctness proof of NP-HMC, and empirically demonstrate significant performance improvements over existing approaches on several nonparametric examples.', 'Inference Compilation and Universal Probabilistic Programming We introduce a method for using  deep neural networks to amortize the cost of inference in models from the family induced by universal probabilistic programming languages, establishing a framework that combines the strengths of probabilistic programming and deep learning methods.  We call what we do “compilation of inference” because our method transforms a denotational specification of an inference problem in the form of a probabilistic program written in a universal programming language into a trained neural network denoted in a neural network specification language.  When at test time this neural network is fed observational data and executed, it performs approximate inference in the original model specified by the probabilistic program.  Our training objective and learning procedure are designed to allow the trained neural network to be used as a proposal distribution in a sequential importance sampling inference engine.  We illustrate our method on mixture models and Captcha solving and show significant speedups in the efficiency of inference.', 'A Compilation Target for Probabilistic Programming Languages Forward inference techniques such as sequential Monte Carlo and particle Markov chain Monte Carlo for probabilistic programming can be implemented in any programming language by creative use of standardized operating system functionality including processes, forking, mutexes, and shared memory.   Exploiting this we have defined, developed, and tested a probabilistic programming language intermediate representation language we call probabilistic C, which itself can be compiled to machine code by standard compilers and linked to operating system libraries yielding an efficient, scalable, portable probabilistic programming compilation target.  This opens up a new hardware and systems research path for optimizing probabilistic programming systems.']"
152,42,152_relational_embeddings_embeddingbased_entities,"['relational', 'embeddings', 'embeddingbased', 'entities', 'relations', 'embedding', 'entity', 'knowledge', 'relation', 'subgraph']","['Knowledge Sheaves: A Sheaf-Theoretic Framework for Knowledge Graph Embedding Knowledge graph embedding involves learning representations of entities—the vertices of the graph—and relations—the edges of the graph—such that the resulting representations encode the known factual information represented by the knowledge graph and can be used in the inference of new relations. We show that knowledge graph embedding is naturally expressed in the topological and categorical language of cellular sheaves: a knowledge graph embedding can be described as an approximate global section of an appropriate knowledge sheaf over the graph, with consistency constraints induced by the knowledge graph’s schema. This approach provides a generalized framework for reasoning about knowledge graph embedding models and allows for the expression of a wide range of prior constraints on embeddings. Further, the resulting embeddings can be easily adapted for reasoning over composite relations without special training. We implement these ideas to highlight the benefits of the extensions inspired by this new perspective.', 'Topic-Based Embeddings for Learning from Large Knowledge Graphs We present a scalable probabilistic framework for learning from multi-relational data given in form of entity-relation-entity triplets, with a potentially massive number of entities and relations (e.g., in multi-relational networks, knowledge bases, etc.). We define each triplet via a relation-specific bilinear function of the embeddings  of entities associated with it (these embeddings correspond to “topics”). To handle massive number of relations and the data sparsity problem (very few observations per relation), we also extend this model to allow sharing of parameters across relations, which leads to a substantial reduction in the number of parameters to be learned. In addition to yielding excellent predictive performance (e.g., for knowledge base completion tasks), the interpretability of our topic-based embedding framework enables easy qualitative analyses. Computational cost of our models scales in the number of positive triplets, which makes it easy to scale to massive real-world multi-relational data sets, which are usually extremely sparse. We develop simple-to-implement batch as well as online Gibbs sampling algorithms and demonstrate the effectiveness of our models on tasks such as multi-relational link-prediction, and learning from large knowledge bases.', 'InGram: Inductive Knowledge Graph Embedding via Relation Graphs Inductive knowledge graph completion has been considered as the task of predicting missing triplets between new entities that are not observed during training. While most inductive knowledge graph completion methods assume that all entities can be new, they do not allow new relations to appear at inference time. This restriction prohibits the existing methods from appropriately handling real-world knowledge graphs where new entities accompany new relations. In this paper, we propose an INductive knowledge GRAph eMbedding method, InGram, that can generate embeddings of new relations as well as new entities at inference time. Given a knowledge graph, we define a relation graph as a weighted graph consisting of relations and the affinity weights between them. Based on the relation graph and the original knowledge graph, InGram learns how to aggregate neighboring embeddings to generate relation and entity embeddings using an attention mechanism. Experimental results show that InGram outperforms 14 different state-of-the-art methods on varied inductive learning scenarios.']"
153,12,153_convnets_cnns_cnn_imagenet,"['convnets', 'cnns', 'cnn', 'imagenet', 'inception', 'convolutional', 'recognition', 'neural', 'layers', 'networks']","['Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units Recently, convolutional neural networks (CNNs) have been used as a powerful tool to solve many problems of machine learning and computer vision. In this paper, we aim to provide insight on the property of convolutional neural networks, as well as a generic method to improve the performance of many CNN architectures. Specifically, we first examine existing CNN models and observe an intriguing property that the filters in the lower layers form pairs (i.e., filters with opposite phase). Inspired by our observation, we propose a novel, simple yet effective activation scheme called concatenated ReLU (CReLU) and theoretically analyze its reconstruction property in CNNs. We integrate CReLU into several state-of-the-art CNN architectures and demonstrate improvement in their recognition performance on CIFAR-10/100 and ImageNet datasets with fewer trainable parameters. Our results suggest that better understanding of the properties of CNNs can lead to significant performance improvement with a simple modification.', 'LegoNet: Efficient Convolutional Neural Networks with Lego Filters This paper aims to build efficient convolutional neural networks using a set of Lego filters. Many successful building blocks, e.g., inception and residual modules, have been designed to refresh state-of-the-art records of CNNs on visual recognition tasks. Beyond these high-level modules, we suggest that an ordinary filter in the neural network can be upgraded to a sophisticated module as well. Filter modules are established by assembling a shared set of Lego filters that are often of much lower dimensions. Weights in Lego filters and binary masks to stack Lego filters for these filter modules can be simultaneously optimized in an end-to-end manner as usual. Inspired by network engineering, we develop a split-transform-merge strategy for an efficient convolution by exploiting intermediate Lego feature maps. The compression and acceleration achieved by Lego Networks using the proposed Lego filters have been theoretically discussed. Experimental results on benchmark datasets and deep models demonstrate the advantages of the proposed Lego filters and their potential real-world applications on mobile devices.', 'Bank of Weight Filters for Deep CNNs Convolutional neural networks (CNNs) are seen to be extremely effective in many large object recognition tasks. One of the reasons for this is that they learn appropriate features also from the training data. The convolutional layers of a CNN have these feature generating filters whose weights are learnt. However, this entails learning millions of weights (across different layers) and hence learning times are very large even on the best available hardware. In some studies in transfer learning it has been observed that the network learnt on one task can be reused on another task (by some finetuning). In this context, this paper presents a systematic study of the exchangeability of weight filters of CNNs across different object recognition tasks. The paper proposes the concept of bank of weight-filters (BWF) which consists of all the weight vectors of filters learnt by different CNNs on different tasks. The BWF can be viewed at multiple levels of granularity such as network-level, layer-level and filter-level. Through extensive empirical investigations we show that one can efficiently learn CNNs for new tasks by randomly selecting from the bank of filters for initializing the convolutional layers of the new CNN. Our study is done at all the multiple levels of granularity mentioned above. Our results show that the concept of BWF proposed here would offer a very good strategy for initializing the filters while learning CNNs. We also show that the dependency among the filters and the layers of the CNN is not strict. One can choose any pre-trained filter instead of a fixed pre-trained net, as a whole, for initialization. This paper is a first step in the direction of creating and characterizing a Universal BWF for efficient learning of CNNs.']"
154,126,154_pruning_sparse_prune_pruned,"['pruning', 'sparse', 'prune', 'pruned', 'imagenet', 'cnn', 'cnns', 'networks', 'neural', 'subnetworks']","['Automatic Attention Pruning: Improving and Automating Model Pruning using Attentions Pruning is a promising approach to compress deep learning models in order to deploy them on resource-constrained edge devices. However, many existing pruning solutions are based on unstructured pruning, which yields models that cannot efficiently run on commodity hardware; and they often require users to manually explore and tune the pruning process, which is time-consuming and often leads to sub-optimal results. To address these limitations, this paper presents Automatic Attention Pruning (AAP), an adaptive, attention-based, structured pruning approach to automatically generate small, accurate, and hardware-efficient models that meet user objectives. First, it proposes iterative structured pruning using activation-based attention maps to effectively identify and prune unimportant filters. Then, it proposes adaptive pruning policies for automatically meeting the pruning objectives of accuracy-critical, memory-constrained, and latency-sensitive tasks. A comprehensive evaluation shows that AAP substantially outperforms the state-of-the-art structured pruning works for a variety of model architectures. Our code is at: https://github.com/kaiqi123/Automatic-Attention-Pruning.git.', 'Why Random Pruning Is All We Need to Start Sparse Random masks define surprisingly effective sparse neural network models, as has been shown empirically. The resulting sparse networks can often compete with dense architectures and state-of-the-art lottery ticket pruning algorithms, even though they do not rely on computationally expensive prune-train iterations and can be drawn initially without significant computational overhead. We offer a theoretical explanation of how random masks can approximate arbitrary target networks if they are wider by a logarithmic factor in the inverse sparsity $1 / \\log(1/\\text{sparsity})$. This overparameterization factor is necessary at least for 3-layer random networks, which elucidates the observed degrading performance of random networks at higher sparsity. At moderate to high sparsity levels, however, our results imply that sparser networks are contained within random source networks so that any dense-to-sparse training scheme can be turned into a computationally more efficient sparse-to-sparse one by constraining the search to a fixed random mask. We demonstrate the feasibility of this approach in experiments for different pruning methods and propose particularly effective choices of initial layer-wise sparsity ratios of the random source network. As a special case, we show theoretically and experimentally that random source networks also contain strong lottery tickets.', 'i-SpaSP: Structured Neural Pruning via Sparse Signal Recovery We propose a novel, structured pruning algorithm for neural networks—the iterative, Sparse Structured Pruning algorithm, dubbed as i-SpaSP. Inspired by ideas from sparse signal recovery, i-SpaSP operates by iteratively identifying a larger set of important parameter groups (e.g., filters or neurons) within a network that contribute most to the residual between pruned and dense network output, then thresholding these groups based on a smaller, pre-defined pruning ratio. For both two-layer and multi-layer network architectures with ReLU activations, we show the error induced by pruning with i-SpaSP decays polynomially, where the degree of this polynomial becomes arbitrarily large based on the sparsity of the dense network’s hidden representations. In our experiments, i-SpaSP is evaluated across a variety of datasets (i.e., MNIST, ImageNet, and XNLI) and architectures (i.e., feed forward networks, ResNet34, MobileNetV2, and BERT), where it is shown to discover high-performing sub-networks and improve upon the pruning efficiency of provable baseline methodologies by several orders of magnitude. Put simply, i-SpaSP is easy to implement with automatic differentiation, achieves strong empirical results, comes with theoretical convergence guarantees, and is efficient, thus distinguishing itself as one of the few computationally efficient, practical, and provable pruning algorithms.']"
155,10,155_graphs_networks_nodes_graph,"['graphs', 'networks', 'nodes', 'graph', 'embeddings', 'embedding', 'node', 'vertexes', 'edges', 'edge']","[' Bayesian Link Prediction with Deep Graph Convolutional Gaussian Processes   Link prediction aims to reveal missing edges in a graph. We introduce a deep graph convolutional Gaussian process model for this task, which addresses recent challenges in graph machine learning with oversmoothing and overfitting. Using simplified graph convolutions, we transform a Gaussian process to leverage the topological information of the graph domain. To scale the Gaussian process model to larger graphs, we introduce a variational inducing point method that places pseudo-inputs on a graph-structured domain. Multiple Gaussian processes are assembled into a hierarchy whose structure allows skipping convolutions and thus counteracting oversmoothing. The proposed model represents the first Gaussian process for link prediction that makes use of both node features and topological information. We evaluate our model on multiple graph data sets with up to thousands of nodes and report consistent improvements over competitive link prediction approaches. ', 'Learning Dynamic Context Graph Embedding Graph embeddings represent nodes as low-dimensional vectors to preserve the proximity between nodes and communities of graphs for network analysis. The temporal edges (e.g., relationships, contacts, and emails) in dynamic graphs are important for graph evolution analysis, but few existing methods in graph embeddings can capture the dynamic information from temporal edges. In this study, we propose a dynamic graph embedding method to analyze the evolution patterns of dynamic graphs effectively. Our method uses diffuse context sampling to preserve the proximity between nodes, and applies dynamic context graph embeddings to train discrete-time graph embeddings in the same vector space without alignments to preserve the temporal continuity of stable nodes. We compare our method with several state-of-the-art methods for link prediction, and the experiments demonstrate that our method generally performs better at the task. Our method is further verified using a real-world dynamic graph by visualizing the evolution of its community structure at different timesteps.', 'K-Truss Based Temporal Graph Convolutional Network for Dynamic Graphs Learning latent representations of nodes in graphs is important for many real-world applications, such as recommender systems, traffic prediction and fraud detection. Most of the existing research on graph representation learning has focused on static graphs. However, many real-world graphs are dynamic and their structures change over time, which makes learning dynamic node representations challenging. We propose a novel k-truss based temporal graph convolutional network named TTGCN to learn potential node representations on dynamic graphs. Specifically, TTGCN utilizes a novel truss-based graph convolutional layer named TrussGCN to capture the topology and hierarchical structure information of graphs, and combines it with a temporal evolution module to capture complex temporal dependencies. We conduct link prediction experiments on five different dynamic graph datasets. Experimental results demonstrate the superiority of TTGCN for dynamic graph embedding, as it consistently outperforms several state-of-the-art baselines in the link prediction task. In addition, our ablation experiments demonstrate the effectiveness of adopting TrussGCN in a dynamic graph embedding method.']"
156,16,156_clustering_subspaces_sparse_subspace,"['clustering', 'subspaces', 'sparse', 'subspace', 'cluster', 'dimensionalityreduced', 'clusters', 'spectralclusteringbased', 'unionofsubspaces', 'lowdimensional']","['Sparse Subspace Clustering with Missing Entries We consider the problem of clustering incomplete data drawn from a union of subspaces. Classical subspace clustering methods are not applicable to this problem because the data are incomplete, while classical low-rank matrix completion methods may not be applicable because data in multiple subspaces may not be low rank. This paper proposes and evaluates two new approaches for subspace clustering and completion. The first one generalizes the sparse subspace clustering algorithm so that it can obtain a sparse representation of the data using only the observed entries. The second one estimates a suitable kernel matrix by assuming a random model for the missing entries and obtains the sparse representation from this kernel. Experiments on synthetic and real data show the advantages and disadvantages of the proposed methods, which all outperform the natural approach (low-rank matrix completion followed by sparse subspace clustering) when the data matrix is high-rank or the percentage of missing entries is large.', 'Noisy Sparse Subspace Clustering This paper considers the problem of subspace clustering under noise. Specifically, we study the behavior of Sparse Subspace Clustering (SSC) when either adversarial or random noise is added to the unlabelled input data points, which are assumed to lie in a union of low-dimensional subspaces.  We show that a modified version of SSC is \\emphprovably effective in correctly identifying the underlying subspaces, even with noisy data. This extends theoretical guarantee of this algorithm to the practical setting and provides justification to the success of SSC in a class of real applications.', 'A Deterministic Analysis of Noisy Sparse Subspace Clustering for Dimensionality-reduced Data Subspace clustering groups data into several lowrank subspaces. In this paper, we propose a theoretical framework to analyze a popular optimization-based algorithm, Sparse Subspace Clustering (SSC), when the data dimension is compressed via some random projection algorithms. We show SSC provably succeeds if the random projection is a subspace embedding, which includes random Gaussian projection, uniform row sampling, FJLT, sketching, etc. Our analysis applies to the most general deterministic setting and is able to handle both adversarial and stochastic noise. It also results in the first algorithm for privacy-preserved subspace clustering.']"
157,25,157_planning_reinforcement_deepmdp_exploration,"['planning', 'reinforcement', 'deepmdp', 'exploration', 'plans', 'learning', 'learned', 'samplingbased', 'learn', 'reward']","['Planning from Images with Deep Latent Gaussian Process Dynamics Planning is a powerful approach to control problems with known environment dynamics. In unknown environments the agent needs to learn a model of the system dynamics to make planning applicable. This is particularly challenging when the underlying states are only indirectly observable through high-dimensional observations such as images. We propose to learn a deep latent Gaussian process dynamics (DLGPD) model that learns low-dimensional system dynamics from environment interactions with visual observations. The method infers latent state representations from observations using neural networks and models the system dynamics in the learned latent space with Gaussian processes. All parts of the model can be trained jointly by optimizing a lower bound on the likelihood of transitions in image space. We evaluate the proposed approach on the pendulum swing-up task while using the learned dynamics model for planning in latent space in order to solve the control problem. We also demonstrate that our method can quickly adapt a trained agent to changes in the system dynamics from just a few rollouts. We compare our approach to a state-of-the-art purely deep learning based method and demonstrate the advantages of combining Gaussian processes with deep learning for data efficiency and transfer learning.', 'Expansive Latent Planning for Sparse Reward Offline Reinforcement Learning Sampling-based motion planning algorithms excel at searching global solution paths in geometrically complex settings. However, classical approaches, such as RRT, are difficult to scale beyond low-dimensional search spaces and rely on privileged knowledge e.g. about collision detection and underlying state distances. In this work, we take a step towards the integration of sampling-based planning into the reinforcement learning framework to solve sparse-reward control tasks from high-dimensional inputs. Our method, called VELAP, determines sequences of waypoints through sampling-based exploration in a learned state embedding. Unlike other sampling-based techniques, we iteratively expand a tree-based memory of visited latent areas, which is leveraged to explore a larger portion of the latent space for a given number of search iterations. We demonstrate state-of-the-art results in learning control from offline data in the context of vision-based manipulation under sparse reward feedback. Our method extends the set of available planning tools in model-based reinforcement learning by adding a latent planner that searches globally for feasible paths instead of being bound to a fixed prediction horizon.', 'Expansive Latent Planning for Sparse Reward Offline Reinforcement Learning Sampling-based motion planning algorithms excel at searching global solution paths in geometrically complex settings. However, classical approaches, such as RRT, are difficult to scale beyond low-dimensional search spaces and rely on privileged knowledge e.g. about collision detection and underlying state distances. In this work, we take a step towards the integration of sampling-based planning into the reinforcement learning framework to solve sparse-reward control tasks from high-dimensional inputs. Our method, called VELAP, determines sequences of waypoints through sampling-based exploration in a learned state embedding. Unlike other sampling-based techniques, we iteratively expand a tree-based memory of visited latent areas, which is leveraged to explore a larger portion of the latent space for a given number of search iterations. We demonstrate state-of-the-art results in learning control from offline data in the context of vision-based manipulation under sparse reward feedback. Our method extends the set of available planning tools in model-based reinforcement learning by adding a latent planner that searches globally for feasible paths instead of being bound to a fixed prediction horizon.']"
158,93,158_rnns_rnn_lstm_lstms,"['rnns', 'rnn', 'lstm', 'lstms', 'recurrent', 'neural', 'backpropagation', 'memory', 'sequential', 'longterm']","['Resurrecting Recurrent Neural Networks for Long Sequences Recurrent Neural Networks (RNNs) offer fast inference on long sequences but are hard to optimize and slow to train. Deep state-space models (SSMs) have recently been shown to perform remarkably well on long sequence modeling tasks, and have the added benefits of fast parallelizable training and RNN-like fast inference. However, while SSMs are superficially similar to RNNs, there are important differences that make it unclear where their performance boost over RNNs comes from. We show that careful design of deep RNNs using standard signal propagation arguments can recover the impressive performance of deep SSMs on long-range reasoning tasks, while matching their training speed. To achieve this, we analyze and ablate a series of changes to standard RNNs including linearizing and diagonalizing the recurrence, using better parameterizations and initializations, and ensuring careful normalization of the forward pass. Our results provide new insights on the origins of the impressive performance of deep SSMs, and introduce an RNN block called the Linear Recurrent Unit (or LRU) that matches both their performance on the Long Range Arena benchmark and their computational efficiency.', 'UnICORNN: A recurrent model for learning very long time dependencies The design of recurrent neural networks (RNNs) to accurately process sequential inputs with long-time dependencies is very challenging on account of the exploding and vanishing gradient problem. To overcome this, we propose a novel RNN architecture which is based on a structure preserving discretization of a Hamiltonian system of second-order ordinary differential equations that models networks of oscillators. The resulting RNN is fast, invertible (in time), memory efficient and we derive rigorous bounds on the hidden state gradients to prove the mitigation of the exploding and vanishing gradient problem. A suite of experiments are presented to demonstrate that the proposed RNN provides state of the art performance on a variety of learning tasks with (very) long-time dependencies.', 'Dynamical Isometry and a Mean Field Theory of RNNs: Gating Enables Signal Propagation in Recurrent Neural Networks Recurrent neural networks have gained widespread use in modeling sequence data across various domains. While many successful recurrent architectures employ a notion of gating, the exact mechanism that enables such remarkable performance is not well understood. We develop a theory for signal propagation in recurrent networks after random initialization using a combination of mean field theory and random matrix theory. To simplify our discussion, we introduce a new RNN cell with a simple gating mechanism that we call the minimalRNN and compare it with vanilla RNNs. Our theory allows us to define a maximum timescale over which RNNs can remember an input. We show that this theory predicts trainability for both recurrent architectures. We show that gated recurrent networks feature a much broader, more robust, trainable region than vanilla RNNs, which corroborates recent experimental findings. Finally, we develop a closed-form critical initialization scheme that achieves dynamical isometry in both vanilla RNNs and minimalRNNs. We show that this results in significantly improved training dynamics. Finally, we demonstrate that the minimalRNN achieves comparable performance to its more complex counterparts, such as LSTMs or GRUs, on a language modeling task.']"
159,10,159_visual_trained_vision_visuomotor,"['visual', 'trained', 'vision', 'visuomotor', 'visionbased', 'robotic', 'representations', 'convolutions', 'videos', 'robot']","['Task-Oriented Hierarchical Object Decomposition for Visuomotor Control Good pre-trained visual representations could enable robots to learn visuomotor policy efficiently. Still, existing representations take a one-size-fits-all-tasks approach that comes with two important drawbacks: (1) Being completely task-agnostic, these representations cannot effectively ignore any task-irrelevant information in the scene, and (2) They often lack the representational capacity to handle unconstrained/complex real-world scenes. Instead, we propose to train a large combinatorial family of representations organized by scene entities: objects and object parts. This hierarchical object decomposition for task-oriented representations (HODOR) permits selectively assembling different representations specific to each task while scaling in representational capacity with the complexity of the scene and the task. In our experiments, we find that HODOR outperforms prior pre-trained representations, both scene vector representations and object-centric representations, for sample-efficient imitation learning across 5 simulated and 5 real-world manipulation tasks. We further find that the invariances captured in HODOR are inherited into downstream policies, which can robustly generalize to out-of-distribution test conditions, permitting zero-shot skill chaining. Appendix and videos: https://sites.google.com/view/ hodor-corl24', 'Adapting Pretrained ViTs with Convolution Injector for Visuo-Motor Control Vision Transformers (ViT), when paired with large-scale pretraining, have shown remarkable performance across various computer vision tasks, primarily due to their weak inductive bias. However, while such weak inductive bias aids in pretraining scalability, this may hinder the effective adaptation of ViTs for visuo-motor control tasks as a result of the absence of control-centric inductive biases. Such absent inductive biases include spatial locality and translation equivariance bias which convolutions naturally offer. To this end, we introduce Convolution Injector (CoIn), an add-on module that injects convolutions which are rich in locality and equivariance biases into a pretrained ViT for effective adaptation in visuo-motor control. We evaluate CoIn with three distinct types of pretrained ViTs (CLIP, MVP, VC-1) across 12 varied control tasks within three separate domains (Adroit, MetaWorld, DMC), and demonstrate that CoIn consistently enhances control task performance across all experimented environments and models, validating the effectiveness of providing pretrained ViTs with control-centric biases.', 'What Makes Pre-Trained Visual Representations Successful for Robust Manipulation? Inspired by the success of transfer learning in computer vision, roboticists have investigated visual pre-training as a means to improve the learning efficiency and generalization ability of policies learned from pixels. To that end, past work has favored large object interaction datasets, such as first-person videos of humans completing diverse tasks, in pursuit of manipulation-relevant features. Although this approach improves the efficiency of policy learning, it remains unclear how reliable these representations are in the presence of distribution shifts that arise commonly in robotic applications. Surprisingly, we find that visual representations designed for control tasks do not necessarily generalize under subtle changes in lighting and scene texture or the introduction of distractor objects. To understand what properties _do_ lead to robust representations, we compare the performance of 15 pre-trained vision models under different visual appearances. We find that emergent segmentation ability is a strong predictor of out-of-distribution generalization among ViT models. The rank order induced by this metric is more predictive than metrics that have previously guided generalization research within computer vision and machine learning, such as downstream ImageNet accuracy, in-domain accuracy, or shape-bias as evaluated by cue-conflict performance. We test this finding extensively on a suite of distribution shifts in ten tasks across two simulated manipulation environments. On the ALOHA setup, segmentation score predicts real-world performance after offline training with 50 demonstrations.']"
160,13,160_reinforcement_exploration_reward_rewardfree,"['reinforcement', 'exploration', 'reward', 'rewardfree', 'abstraction', 'learning', 'explorationem', 'explore', 'reachability', 'objectivespolicy']","['Task-Optimal Exploration in Linear Dynamical Systems Exploration in unknown environments is a fundamental problem in reinforcement learning and control. In this work, we study task-guided exploration and determine what precisely an agent must learn about their environment in order to complete a particular task. Formally, we study a broad class of decision-making problems in the setting of linear dynamical systems, a class that includes the linear quadratic regulator problem. We provide instance- and task-dependent lower bounds which explicitly quantify the difficulty of completing a task of interest. Motivated by our lower bound, we propose a computationally efficient experiment-design based exploration algorithm. We show that it optimally explores the environment, collecting precisely the information needed to complete the task, and provide finite-time bounds guaranteeing that it achieves the instance- and task-optimal sample complexity, up to constant factors. Through several examples of the linear quadratic regulator problem, we show that performing task-guided exploration provably improves on exploration schemes which do not take into account the task of interest. Along the way, we establish that certainty equivalence decision making is instance- and task-optimal, and obtain the first algorithm for the linear quadratic regulator problem which is instance-optimal. We conclude with several experiments illustrating the effectiveness of our approach in practice.', 'On Reward-Free RL with Kernel and Neural Function Approximations: Single-Agent MDP and Markov Game To achieve sample efficiency in reinforcement learning (RL), it necessitates to efficiently explore the underlying environment. Under the offline setting, addressing the exploration challenge lies in collecting an offline dataset with sufficient coverage. Motivated by such a challenge, we study the reward-free RL problem, where an agent aims to thoroughly explore the environment without any pre-specified reward function. Then, given any extrinsic reward, the agent computes the optimal policy via offline RL with data collected in the exploration stage. Moreover, we tackle this problem under the context of function approximation, leveraging powerful function approximators. Specifically, we propose to explore via an optimistic variant of the value-iteration algorithm incorporating kernel and neural function approximations, where we adopt the associated exploration bonus as the exploration reward. Moreover, we design exploration and planning algorithms for both single-agent MDPs and zero-sum Markov games and prove that our methods can achieve $\\widetilde{\\mathcal{O}}(1 /\\varepsilon^2)$ sample complexity for generating a $\\varepsilon$-suboptimal policy or $\\varepsilon$-approximate Nash equilibrium when given an arbitrary extrinsic reward. To the best of our knowledge, we establish the first provably efficient reward-free RL algorithm with kernel and neural function approximators.', 'Geometric Active Exploration in Markov Decision Processes: the Benefit of Abstraction How can a scientist use a Reinforcement Learning (RL) algorithm to design experiments over a dynamical system’s state space? In the case of finite and Markovian systems, an area called <em>Active Exploration</em> (AE) relaxes the optimization problem of experiments design into Convex RL, a generalization of RL admitting a wider notion of reward. Unfortunately, this framework is currently not scalable and the potential of AE is hindered by the vastness of experiments spaces typical of scientific discovery applications. However, these spaces are often endowed with natural geometries, e.g., permutation invariance in molecular design, that an agent could leverage to improve the statistical and computational efficiency of AE. To achieve this, we bridge AE and MDP homomorphisms, which offer a way to exploit known geometric structures via abstraction. Towards this goal, we make two fundamental contributions: we extend MDP homomorphisms formalism to Convex RL, and we present, to the best of our knowledge, the first analysis that formally captures the benefit of abstraction via homomorphisms on sample efficiency. Ultimately, we propose the Geometric Active Exploration (GAE) algorithm, which we analyse theoretically and experimentally in environments motivated by problems in scientific discovery.']"
161,128,161_reinforcement_mdps_mdp_rewardfree,"['reinforcement', 'mdps', 'mdp', 'rewardfree', 'reward', 'planning', 'optimal', 'regret', 'markov', 'rewards']","['Reward-Free RL is No Harder Than Reward-Aware RL in Linear Markov Decision Processes Reward-free reinforcement learning (RL) considers the setting where the agent does not have access to a reward function during exploration, but must propose a near-optimal policy for an arbitrary reward function revealed only after exploring. In the the tabular setting, it is well known that this is a more difficult problem than reward-aware (PAC) RL—where the agent has access to the reward function during exploration—with optimal sample complexities in the two settings differing by a factor of $|\\mathcal{S}|$, the size of the state space. We show that this separation does not exist in the setting of linear MDPs. We first develop a computationally efficient algorithm for reward-free RL in a $d$-dimensional linear MDP with sample complexity scaling as $\\widetilde{\\mathcal{O}}(d^2 H^5/\\epsilon^2)$. We then show a lower bound with matching dimension-dependence of $\\Omega(d^2 H^2/\\epsilon^2)$, which holds for the reward-aware RL setting. To our knowledge, our approach is the first computationally efficient algorithm to achieve optimal $d$ dependence in linear MDPs, even in the single-reward PAC setting. Our algorithm relies on a novel procedure which efficiently traverses a linear MDP, collecting samples in any given “feature direction”, and enjoys a sample complexity scaling optimally in the (linear MDP equivalent of the) maximal state visitation probability. We show that this exploration procedure can also be applied to solve the problem of obtaining “well-conditioned” covariates in linear MDPs.', ' Reinforcement Learning in Parametric MDPs with Exponential Families   Extending model-based regret minimization strategies for Markov decision processes (MDPs) beyond discrete state-action spaces requires structural assumptions on the reward and transition models. Existing parametric approaches establish regret guarantees by making strong assumptions about either the state transition distribution or the value function as a function of state-action features, and often do not satisfactorily capture classical problems like linear dynamical systems or factored MDPs. This paper introduces a new MDP transition model defined by a collection of linearly parameterized exponential families with $d$ unknown parameters. For finite-horizon episodic RL with horizon $H$ in this MDP model, we propose a model-based upper confidence RL algorithm (Exp-UCRL) that solves a penalized maximum likelihood estimation problem to learn the $d$-dimensional representation of the transition distribution, balancing the exploitation-exploration tradeoff using confidence sets in the exponential family space. We demonstrate the efficiency of our algorithm by proving a frequentist (worst-case) regret bound that is of order $\\tilde O(d\\sqrt{H^3 N})$, sub-linear in total time $N$, linear in dimension $d$, and polynomial in the planning horizon $H$. This is achieved by deriving a novel concentration inequality for conditional exponential families that might be of independent interest. The exponential family MDP model also admits an efficient posterior sampling-style algorithm for which a similar guarantee on the Bayesian regret is shown. ', 'Variational Regret Bounds for Reinforcement Learning We consider undiscounted reinforcement learning in Markov decision processes (MDPs) where \\textit{both} the reward functions and the state-transition probabilities may vary (gradually or abruptly) over time.  For this problem setting, we propose an algorithm and provide performance guarantees for the regret evaluated against the optimal non-stationary policy. The upper bound on the regret is given in terms of the total variation in the MDP. This is the first variational regret bound for the general reinforcement learning setting. ']"
162,23,162_simulationbased_posterior_likelihoodfree_likelihoods,"['simulationbased', 'posterior', 'likelihoodfree', 'likelihoods', 'models', 'bayesian', 'likelihood', 'modeling', 'inference', 'neural']","['Sequential Neural Score Estimation: Likelihood-Free Inference with Conditional Score Based Diffusion Models In Simulation-based inference (SBI) conditional density estimators are used to approximate the posterior distribution from simulated data. Score-based diffusion models have shown remarkable success in generative modeling, and they can also be framed as conditional density estimators. This paper introduces score-based diffusion models for SBI. On a set of SBI benchmarking tasks, they perform similarly or better than existing methods.', 'Jana: Jointly amortized neural approximation of complex Bayesian models This work proposes “jointly amortized neural approximation” (JANA) of intractable likelihood functions and posterior densities arising in Bayesian surrogate modeling and simulation-based inference. We train three complementary networks in an end-to-end fashion: 1) a summary network to compress individual data points, sets, or time series into informative embedding vectors; 2) a posterior network to learn an amortized approximate posterior; and 3) a likelihood network to learn an amortized approximate likelihood. Their interaction opens a new route to amortized marginal likelihood and posterior predictive estimation – two important ingredients of Bayesian workflows that are often too expensive for standard methods. We benchmark the fidelity of JANA on a variety of simulation models against state of-the-art Bayesian methods and propose a powerful and interpretable diagnostic for joint calibration. In addition, we investigate the ability of recurrent likelihood networks to emulate complex time series models without resorting to hand-crafted summary statistics.', 'All-in-one simulation-based inference Amortized Bayesian inference trains neural networks to solve stochastic inference problems using model simulations, thereby making it possible to rapidly perform Bayesian inference for any newly observed data. However, current simulation-based amortized inference methods are simulation-hungry and inflexible: They require the specification of a fixed parametric prior, simulator, and inference tasks ahead of time. Here, we present a new amortized inference method—the Simformer—which overcomes these limitations. By training a probabilistic diffusion model with transformer architectures, the Simformer outperforms current state-of-the-art amortized inference approaches on benchmark tasks and is substantially more flexible: It can be applied to models with function-valued parameters, it can handle inference scenarios with missing or unstructured data, and it can sample arbitrary conditionals of the joint distribution of parameters and data, including both posterior and likelihood. We showcase the performance and flexibility of the Simformer on simulators from ecology, epidemiology, and neuroscience, and demonstrate that it opens up new possibilities and application domains for amortized Bayesian inference on simulation-based models.']"
163,70,163_diffusionbased_diffusion_denoising_stochastic,"['diffusionbased', 'diffusion', 'denoising', 'stochastic', 'generative', 'sampling', 'models', 'sde', 'modeling', 'probabilistic']","['A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models Diffusion models have shown remarkable performance in generation problems over various domains including images, videos, text, and audio. A practical bottleneck of diffusion models is their sampling speed, due to the repeated evaluation of score estimation networks during the inference. In this work, we propose a novel framework capable of adaptively allocating compute required for the score estimation, thereby reducing the overall sampling time of diffusion models. We observe that the amount of computation required for the score estimation may vary along the time step for which the score is estimated. Based on this observation, we propose an early-exiting scheme, where we skip the subset of parameters in the score estimation network during the inference, based on a time-dependent exit schedule. Using the diffusion models for image synthesis, we show that our method could significantly improve the sampling throughput of the diffusion models without compromising image quality. Furthermore, we also demonstrate that our method seamlessly integrates with various types of solvers for faster sampling, capitalizing on their compatibility to enhance overall efficiency.', 'Particle Denoising Diffusion Sampler Denoising diffusion models have become ubiquitous for generative modeling. The core idea is to transport the data distribution to a Gaussian by using a diffusion. Approximate samples from the data distribution are then obtained by estimating the time-reversal of this diffusion using score matching ideas. We follow here a similar strategy to sample from unnormalized probability densities and compute their normalizing constants. However, the time-reversed diffusion is here simulated by using an original iterative particle scheme relying on a novel score matching loss. Contrary to standard denoising diffusion models, the resulting Particle Denoising Diffusion Sampler (PDDS) provides asymptotically consistent estimates under mild assumptions. We demonstrate PDDS on multimodal and high dimensional sampling tasks.', 'To smooth a cloud or to pin it down: Expressiveness guarantees and insights on score matching in denoising diffusion models Denoising diffusion models are a class of generative models that have recently achieved state-of-the-art results across many domains. Gradual noise is added to the data using a diffusion process, which transforms the data distribution into a Gaussian. Samples from the generative model are then obtained by simulating an approximation of the time reversal of this diffusion initialized by Gaussian samples. Recent research has explored the sampling error achieved by diffusion models under the assumption of an absolute error $\\epsilon$ achieved via a neural approximation of the score. To the best of our knowledge, no work formally quantifies the error of such neural approximation to the score. In this paper, we close the gap and present quantitative error bounds for approximating the score of denoising diffusion models using neural networks leveraging ideas from stochastic control. Finally, through simulation, we explore some of the insights that arise from our results confirming that diffusion models based on the Ornstein-Uhlenbeck (OU) process require fewer parameters to better approximate the score than those based on the Fölmer drift / Pinned Brownian Motion.']"
164,40,164_denoising_denoisers_denoiser_deblurring,"['denoising', 'denoisers', 'denoiser', 'deblurring', 'inverse', 'imaging', 'restoration', 'inpainting', 'diffusion', 'meanreverting']","['Proximal Denoiser for Convergent Plug-and-Play Optimization with Nonconvex Regularization Plug-and-Play (PnP) methods solve ill-posed inverse problems through iterative proximal algorithms by replacing a proximal operator by a denoising operation. When applied with deep neural network denoisers, these methods have shown state-of-the-art visual performance for image restoration problems. However, their theoretical convergence analysis is still incomplete. Most of the existing convergence results consider nonexpansive denoisers, which is non-realistic, or limit their analysis to strongly convex data-fidelity terms in the inverse problem to solve. Recently, it was proposed to train the denoiser as a gradient descent step on a functional parameterized by a deep neural network. Using such a denoiser guarantees the convergence of the PnP version of the Half-Quadratic-Splitting (PnP-HQS) iterative algorithm. In this paper, we show that this gradient denoiser can actually correspond to the proximal operator of another scalar function. Given this new result, we exploit the convergence theory of proximal algorithms in the nonconvex setting to obtain convergence results for PnP-PGD (Proximal Gradient Descent) and PnP-ADMM (Alternating Direction Method of Multipliers). When built on top of a smooth gradient denoiser, we show that PnP-PGD and PnP-ADMM are convergent and target stationary points of an explicit functional. These convergence results are confirmed with numerical experiments on deblurring, super-resolution and inpainting.', 'Solving General Noisy Inverse Problem via Posterior Sampling: A Policy Gradient Viewpoint Solving image inverse problems (e.g., super-resolution and inpainting) requires generating a high fidelity image that matches the given input (the low-resolution image or the masked image). By using the input image as guidance, we can leverage a pretrained diffusion generative model to solve a wide range of image inverse tasks without task specific model fine-tuning. To precisely estimate the guidance score function of the input image, we propose Diffusion Policy Gradient (DPG), a tractable computation method by viewing the intermediate noisy images as policies and the target image as the states selected by the policy. Experiments show that our method is robust to both Gaussian and Poisson noise degradation on multiple linear and non-linear inverse tasks, resulting into a higher image restoration quality on FFHQ, ImageNet and LSUN datasets.', 'Plug-and-Play image restoration with Stochastic deNOising REgularization Plug-and-Play (PnP) algorithms are a class of iterative algorithms that address image inverse problems by combining a physical model and a deep neural network for regularization. Even if they produce impressive image restoration results, these algorithms rely on a non-standard use of a denoiser on images that are less and less noisy along the iterations, which contrasts with recent algorithms based on Diffusion Models (DM), where the denoiser is applied only on re-noised images. We propose a new PnP framework, called Stochastic deNOising REgularization (SNORE), which applies the denoiser only on images with noise of the adequate level. It is based on an explicit stochastic regularization, which leads to a stochastic gradient descent algorithm to solve ill-posed inverse problems. A convergence analysis of this algorithm and its annealing extension is provided. Experimentally, we prove that SNORE is competitive with respect to state-of-the-art methods on deblurring and inpainting tasks, both quantitatively and qualitatively.']"
165,32,165_reinforcement_skills_learned_learning,"['reinforcement', 'skills', 'learned', 'learning', 'skill', 'rewards', 'reward', 'exploration', 'learn', 'tasks']","['Variational Curriculum Reinforcement Learning for Unsupervised Discovery of Skills Mutual information-based reinforcement learning (RL) has been proposed as a promising framework for retrieving complex skills autonomously without a task-oriented reward function through mutual information (MI) maximization or variational empowerment. However, learning complex skills is still challenging, due to the fact that the order of training skills can largely affect sample efficiency. Inspired by this, we recast variational empowerment as curriculum learning in goal-conditioned RL with an intrinsic reward function, which we name Variational Curriculum RL (VCRL). From this perspective, we propose a novel approach to unsupervised skill discovery based on information theory, called Value Uncertainty Variational Curriculum (VUVC). We prove that, under regularity conditions, VUVC accelerates the increase of entropy in the visited states compared to the uniform curriculum. We validate the effectiveness of our approach on complex navigation and robotic manipulation tasks in terms of sample efficiency and state coverage speed. We also demonstrate that the skills discovered by our method successfully complete a real-world robot navigation task in a zero-shot setup and that incorporating these skills with a global planner further increases the performance.', 'Constrained Ensemble Exploration for Unsupervised Skill Discovery Unsupervised Reinforcement Learning (RL) provides a promising paradigm for learning useful behaviors via reward-free per-training. Existing methods for unsupervised RL mainly conduct empowerment-driven skill discovery or entropy-based exploration. However, empowerment often leads to static skills, and pure exploration only maximizes the state coverage rather than learning useful behaviors. In this paper, we propose a novel unsupervised RL framework via an ensemble of skills, where each skill performs partition exploration based on the state prototypes. Thus, each skill can explore the clustered area locally, and the ensemble skills maximize the overall state coverage. We adopt state-distribution constraints for the skill occupancy and the desired cluster for learning distinguishable skills. Theoretical analysis is provided for the state entropy and the resulting skill distributions. Based on extensive experiments on several challenging tasks, we find our method learns well-explored ensemble skills and achieves superior performance in various downstream tasks compared to previous methods.', 'Accelerating Reinforcement Learning with Learned Skill Priors Intelligent agents rely heavily on prior experience when learning a new task, yet most modern reinforcement learning (RL) approaches learn every task from scratch. One approach for leveraging prior knowledge is to transfer skills learned on prior tasks to the new task. However, as the amount of prior experience increases, the number of transferable skills grows too, making it challenging to explore the full set of available skills during downstream learning. Yet, intuitively, not all skills should be explored with equal probability; for example information about the current state can hint which skills are promising to explore. In this work, we propose to implement this intuition by learning a prior over skills. We propose a deep latent variable model that jointly learns an embedding space of skills and the skill prior from offline agent experience. We then extend common maximum-entropy RL approaches to use skill priors to guide downstream learning. We validate our approach, SPiRL (Skill-Prior RL), on complex navigation and robotic manipulation tasks and show that learned skill priors are essential for effective skill transfer from rich datasets. Videos and code are available at https://clvrai.com/spirl.']"
166,35,166_networks_cascades_cascade_diffusion,"['networks', 'cascades', 'cascade', 'diffusion', 'nodes', 'graphs', 'propagation', 'maximization', 'estimating', 'inference']","['Estimating Diffusion Network Structures: Recovery Conditions, Sample Complexity & Soft-thresholding Algorithm Information spreads across social and technological networks, but often the network structures are hidden from us and we only observe the traces left by the diffusion processes, called cascades. Can we recover the hidden network structures from these observed cascades? What kind of cascades and how many cascades do we need? Are there some network structures which are more difficult than others to recover? Can we design efficient inference algorithms with provable guarantees?    Despite the increasing availability of cascade data and methods for inferring networks from these data, a thorough theoretical understanding of the above questions remains largely unexplored in the literature. In this paper, we investigate the network structure inference problem for a general family of continuous-time diffusion models using an l1-regularized likelihood maximization framework. We show that, as long as the cascade sampling process satisfies a natural incoherence condition, our framework can recover the correct network structure with high probability if we observe O(d^3 log N) cascades, where d is the maximum number of parents of a node and N is the total number of nodes. Moreover, we develop a simple and efficient soft-thresholding inference algorithm, which we use to illustrate the consequences of our theoretical results, and show that our framework outperforms other alternatives in practice.', 'Influence Function Learning in Information Diffusion Networks Can we learn the influence of a set of people in a social network from cascades of information diffusion? This question is often addressed by a two-stage approach: first learn a diffusion model, and then calculate the influence based on the learned model. Thus, the success of this approach relies heavily on the correctness of the diffusion model which is hard to verify for real world data. In this paper, we exploit the insight that the influence functions in many diffusion models are coverage functions, and propose a novel parameterization of such functions using a convex combination of random basis functions. Moreover, we propose an efficient maximum likelihood based algorithm to learn such functions directly from cascade data, and hence bypass the need to specify a particular diffusion model in advance. We provide both theoretical and empirical analysis for our approach, showing that the proposed approach can provably learn the influence function with low sample complexity, be robust to the unknown diffusion models, and significantly outperform existing approaches in both synthetic and real world data.', 'Network Inference and Influence Maximization from Samples Influence maximization is the task of selecting a small number of seed nodes in a social network to maximize the spread of the influence from these seeds, and it has been widely investigated in the past two decades. In the canonical setting, the whole social network as well as its diffusion parameters is given as input. In this paper, we consider the more realistic sampling setting where the network is unknown and we only have a set of passively observed cascades that record the set of activated nodes at each diffusion step. We study the task of influence maximization from these cascade samples (IMS), and present constant approximation algorithms for this task under mild conditions on the seed set distribution. To achieve the optimization goal, we also provide a novel solution to the network inference problem, that is, learning diffusion parameters and the network structure from the cascade data. Comparing with prior solutions, our network inference algorithm requires weaker assumptions and does not rely on maximum-likelihood estimation and convex programming. Our IMS algorithms enhance the learning-and-then-optimization approach by allowing a constant approximation ratio even when the diffusion parameters are hard to learn, and we do not need any assumption related to the network structure or diffusion parameters.']"
167,36,167_nlp_semantic_word2vec_embeddings,"['nlp', 'semantic', 'word2vec', 'embeddings', 'corpus', 'embedding', 'embeddingbased', 'tagging', 'words', 'word']","['A Word Sense Disambiguation Method Based on Multiple Sense Graph Word Sense Disambiguation is a process to determine the best meaning of an ambiguous word according to its contextual semantic information. Many methods ofWord Sense Disambiguation cannot deal with polysemous words well because they only consider the meaning of the adjacent words before and after ambiguous words, and cannot consider the meaning of all words in the sentence globally. In order to solve the above problems, this paper proposes a word sense disambiguation method based on Multiple Sense Graph. This method applies the BERT model to generate word sense vectors, and globally considers the feature relationship between the ambiguous word and all words in the context. In addition, this method applies the PageRank algorithm to score the importance of each sense vector of the word, and the scoring results are sorted to obtain the best sense of the ambiguous word. The experimental results indicate that the proposed BERT-PageRank method improves the evaluation index compared with the other two semantic disambiguation methods. In summary, the proposed method improves the accuracy of word sense disambiguation to obtain the best word sense.', 'A Unified Framework for Jointly Learning Distributed Representations of Word and Attributes Distributed word representations have achieved great success in natural language processing (NLP) area. However, most distributed models focus on local context properties and learn task-specific representations individually, therefore lack the ability to fuse multi-attributes and learn jointly. In this paper, we propose a unified framework which jointly learns distributed representations of word and attributes: characteristics of word. In our models, we consider three types of attributes: topic, lemma and document. Besides learning distributed attribute representations, we find that using additional attributes is beneficial to improve word representations. Several experiments are conducted to evaluate the performance of the learned topic representations, document representations, and improved word representations, respectively. The experimental results show that our models achieve significant and competitive results. ', 'Learning Word Representations with Hierarchical Sparse Coding We propose a new method for learning word representations using hierarchical regularization in sparse coding inspired by the linguistic study of word meanings. We show an efficient learning algorithm based on stochastic proximal methods that is significantly faster than previous approaches, making it possible to perform hierarchical sparse coding on a corpus of billions of word tokens. Experiments on various benchmark tasks—word similarity ranking, syntactic and semantic analogies, sentence completion, and sentiment analysis—demonstrate that the method outperforms or is competitive with state-of-the-art methods.']"
168,11,168_summarization_bfsummarization_textrank_summaries,"['summarization', 'bfsummarization', 'textrank', 'summaries', 'sentences', 'summaryreview', 'nlp', 'corpora', 'documentsummary', 'text']","['PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets.', 'MeanSum: A Neural Model for Unsupervised Multi-Document Abstractive Summarization Abstractive summarization has been studied using neural sequence transduction methods with datasets of large, paired document-summary examples. However, such datasets are rare and the models trained from them do not generalize to other domains. Recently, some progress has been made in learning sequence-to-sequence mappings with only unpaired examples. In our work, we consider the setting where there are only documents (product or business reviews) with no summaries provided, and propose an end-to-end, neural model architecture to perform unsupervised abstractive summarization. Our proposed model consists of an auto-encoder where the mean of the representations of the input reviews decodes to a reasonable summary-review. We consider variants of the proposed architecture and perform an ablation study to show the importance of specific components. We show through metrics and human evaluation that the generated summaries are highly abstractive, fluent, relevant, and representative of the average sentiment of the input reviews. Finally, we collect a ground-truth evaluation dataset and show that our model outperforms a strong extractive baseline.', 'Hybrid Summarization with Semantic Weighting Reward and Latent Structure Detector Text summarization has been a significant challenge in the Nature Process Language (NLP) field. The approach of dealing with text summarization can be roughly divided into two main paradigms: extractive and abstractive manner. The former allows capturing the most representative snippets in a document while the latter generates a summary by understanding the latent meaning in a material with a language generation model. Recently, studies found that jointly employing the extractive and abstractive summarization models can take advantage of their complementary advantages, creating both concise and informative summaries. However, the reinforced summarization models mainly depend on the ROUGE-based reward, which only has the ability to quantify the extent of word-matching rather than semantic-matching between document and summary. Meanwhile, documents are usually collected with redundant or noisy information due to the existence of repeated or irrelevant information in real-world applications. Therefore, only depending on ROUGE-based reward to optimize the reinforced summarization models may lead to biased summary generation. In this paper, we propose a novel deep \\bf{Hy}brid \\bf{S}ummarization with semantic weighting \\bf{R}eward and latent structure \\bf{D}etector (HySRD). Specifically, HySRD introduces a new reward mechanism that simultaneously takes advantage of semantic and syntactic information among documents and summaries. To effectively model the accuracy semantics, a latent structure detector is designed to incorporate the high-level latent structures in the sentence representation for information selection. Extensive experiments have been conducted on two well-known benchmark datasets \\emph{CNN/Daily Mail} (short input document) and \\emph{BigPatent} (long input document). The automatic evaluation shows that our approach significantly outperforms the state-of-the-art of hybrid summarization models.']"
169,108,169_transformers_attention_transformer_softmax,"['transformers', 'attention', 'transformer', 'softmax', 'transformerbased', 'learning', 'memory', 'tasks', 'learn', 'efficient']","['Do Efficient Transformers Really Save Computation? As transformer-based language models are trained on increasingly large datasets and with vast numbers of parameters, finding more efficient alternatives to the standard Transformer has become very valuable. While many efficient Transformers and Transformer alternatives have been proposed, none provide theoretical guarantees that they are a suitable replacement for the standard Transformer. This makes it challenging to identify when to use a specific model and what directions to prioritize for further investigation. In this paper, we aim to understand the capabilities and limitations of efficient Transformers, specifically the Sparse Transformer and the Linear Transformer. We focus on their reasoning capability as exhibited by Chain-of-Thought (CoT) prompts and follow previous works to model them as Dynamic Programming (DP) problems. Our results show that while these models are expressive enough to solve general DP tasks, contrary to expectations, they require a model size that scales with the problem size. Nonetheless, we identify a class of DP problems for which these models can be more efficient than the standard Transformer. We confirm our theoretical results through experiments on representative DP tasks, adding to the understanding of efficient Transformers’ practical strengths and weaknesses.', 'How Transformers Learn Causal Structure with Gradient Descent The incredible success of transformers on sequence modeling tasks can be largely attributed to the self-attention mechanism, which allows information to be transferred between different parts of a sequence. Self-attention allows transformers to encode causal structure which makes them particularly suitable for sequence modeling. However, the process by which transformers learn such causal structure via gradient-based training algorithms remains poorly understood. To better understand this process, we introduce an in-context learning task that requires learning latent causal structure. We prove that gradient descent on a simplified two-layer transformer learns to solve this task by encoding the latent causal graph in the first attention layer. The key insight of our proof is that the gradient of the attention matrix encodes the mutual information between tokens. As a consequence of the data processing inequality, the largest entries of this gradient correspond to edges in the latent causal graph. As a special case, when the sequences are generated from in-context Markov chains, we prove that transformers learn an induction head (Olsson et al., 2022). We confirm our theoretical findings by showing that transformers trained on our in-context learning task are able to recover a wide variety of causal structures.', 'Improving Transformers with Probabilistic Attention Keys Multi-head attention is a driving force behind state-of-the-art transformers, which achieve remarkable performance across a variety of natural language processing (NLP) and computer vision tasks. It has been observed that for many applications, those attention heads learn redundant embedding, and most of them can be removed without degrading the performance of the model. Inspired by this observation, we propose Transformer with a Mixture of Gaussian Keys (Transformer-MGK), a novel transformer architecture that replaces redundant heads in transformers with a mixture of keys at each head. These mixtures of keys follow a Gaussian mixture model and allow each attention head to focus on different parts of the input sequence efficiently. Compared to its conventional transformer counterpart, Transformer-MGK accelerates training and inference, has fewer parameters, and requires fewer FLOPs to compute while achieving comparable or better accuracy across tasks. Transformer-MGK can also be easily extended to use with linear attention. We empirically demonstrate the advantage of Transformer-MGK in a range of practical applications, including language modeling and tasks that involve very long sequences. On the Wikitext-103 and Long Range Arena benchmark, Transformer-MGKs with 4 heads attain comparable or better performance to the baseline transformers with 8 heads.']"
170,12,170_gflownet_gflownets_gflownetsem_flow,"['gflownet', 'gflownets', 'gflownetsem', 'flow', 'generative', 'flows', 'networks', 'nongflownet', 'epgflownet', 'epgflownets']","['Generative Flow Networks as Entropy-Regularized RL The recently proposed generative flow networks (GFlowNets) are a method of training a policy to sample compositional discrete objects with probabilities proportional to a given reward via a sequence of actions. GFlowNets exploit the sequential nature of the problem, drawing parallels with reinforcement learning (RL). Our work extends the connection between RL and GFlowNets to a general case. We demonstrate how the task of learning a generative flow network can be efficiently redefined as an entropy-regularized RL problem with a specific reward and regularizer structure. Furthermore, we illustrate the practical efficiency of this reformulation by applying standard soft RL algorithms to GFlowNet training across several probabilistic modeling tasks. Contrary to previously reported results, we show that entropic RL approaches can be competitive against established GFlowNet training methods. This perspective opens a direct path for integrating RL principles into the realm of generative flow networks.', 'Stochastic Generative Flow Networks Generative Flow Networks (or GFlowNets for short) are a family of probabilistic agents that learn to sample complex combinatorial structures through the lens of “inference as control”. They have shown great potential in generating high-quality and diverse candidates from a given energy landscape. However, existing GFlowNets can be applied only to deterministic environments, and fail in more general tasks with stochastic dynamics, which can limit their applicability. To overcome this challenge, this paper introduces Stochastic GFlowNets, a new algorithm that extends GFlowNets to stochastic environments. By decomposing state transitions into two steps, Stochastic GFlowNets isolate environmental stochasticity and learn a dynamics model to capture it. Extensive experimental results demonstrate that Stochastic GFlowNets offer significant advantages over standard GFlowNets as well as MCMC- and RL-based approaches, on a variety of standard benchmarks with stochastic dynamics.', 'A theory of continuous generative flow networks Generative flow networks (GFlowNets) are amortized variational inference algorithms that are trained to sample from unnormalized target distributions over compositional objects. A key limitation of GFlowNets until this time has been that they are restricted to discrete spaces. We present a theory for generalized GFlowNets, which encompasses both existing discrete GFlowNets and ones with continuous or hybrid state spaces, and perform experiments with two goals in mind. First, we illustrate critical points of the theory and the importance of various assumptions. Second, we empirically demonstrate how observations about discrete GFlowNets transfer to the continuous case and show strong results compared to non-GFlowNet baselines on several previously studied tasks. This work greatly widens the perspectives for the application of GFlowNets in probabilistic inference and various modeling settings.']"
171,11,171_adversarial_graphstructured_attacks_vulnerabilities,"['adversarial', 'graphstructured', 'attacks', 'vulnerabilities', 'graphs', 'attack', 'graphene', 'security', 'graph', 'nodes']","['Graph Contrastive Backdoor Attacks Graph Contrastive Learning (GCL) has attracted considerable interest due to its impressive node representation learning capability. Despite the wide application of GCL techniques, little attention has been paid to the security of GCL. In this paper, we systematically study the vulnerability of GCL in the presence of malicious backdoor adversaries. In particular, we propose <em>GCBA</em>, the first backdoor attack for graph contrastive learning. GCBA incorporates three attacks: poisoning, crafting, and natural backdoor, each targeting one stage of the GCL pipeline. We formulate our attacks as optimization problems and solve them with a novel discrete optimization technique to overcome the discrete nature of graph-structured data. By extensively evaluating GCBA on multiple datasets and GCL methods, we show that our attack can achieve high attack success rates while preserving stealthiness. We further consider potential countermeasures to our attack and conclude that existing defenses are insufficient to mitigate GCBA. We show that as a complex paradigm involving data and model republishing, GCL is vulnerable to backdoor attacks, and specifically designed defenses are needed to mitigate the backdoor attacks on GCL.', 'Adversarial Attack on Graph Structured Data Deep learning on graph structures has shown exciting results in various applications. However, few attentions have been paid to the robustness of such models, in contrast to numerous research work for image or text adversarial attack and defense. In this paper, we focus on the adversarial attacks that fool deep learning models by modifying the combinatorial structure of data. We first propose a reinforcement learning based attack method that learns the generalizable attack policy, while only requiring prediction labels from the target classifier. We further propose attack methods based on genetic algorithms and gradient descent in the scenario where additional prediction confidence or gradients are available. We use both synthetic and real-world data to show that, a family of Graph Neural Network models are vulnerable to these attacks, in both graph-level and node-level classification tasks. We also show such attacks can be used to diagnose the learned classifiers.', ' Detection and Defense of Topological Adversarial Attacks on Graphs   Graph neural network (GNN) models achieve superior performance when classifying nodes in graph-structured data. Given that state-of-the-art GNNs share many similarities with their CNN cousins and that CNNs suffer adversarial vulnerabilities, there has also been interest in exploring analogous vulnerabilities in GNNs. Indeed, recent work has demonstrated that node classification performance of several graph models, including the popular graph convolution network (GCN) model, can be severely degraded through adversarial perturbations to the graph structure and the node features. In this work, we take a first step towards detecting adversarial attacks against graph models. We first propose a straightforward single node threshold test for detecting nodes subject to targeted attacks. Subsequently, we describe a kernel-based two-sample test for detecting whether a given subset of nodes within a graph has been maliciously corrupted. The efficacy of our algorithms is established via thorough experiments using commonly used node classification benchmark datasets. We also illustrate the potential practical benefit of our detection method by demonstrating its application to a real-world Bitcoin transaction network. ']"
172,11,172_bayesian_probabilistic_odes_solvers,"['bayesian', 'probabilistic', 'odes', 'solvers', 'likelihood', 'solver', 'numerical', 'differential', 'gaussian', 'estimation']","[' Probabilistic Numerical Method of Lines for Time-Dependent Partial Differential Equations   This work develops a class of probabilistic algorithms for the numerical solution of nonlinear, time-dependent partial differential equations (PDEs). Current state-of-the-art PDE solvers treat the space- and time-dimensions separately, serially, and with black-box algorithms, which obscures the interactions between spatial and temporal approximation errors and misguides the quantification of the overall error. To fix this issue, we introduce a probabilistic version of a technique called method of lines. The proposed algorithm begins with a Gaussian process interpretation of finite difference methods, which then interacts naturally with filtering-based probabilistic ordinary differential equation (ODE) solvers because they share a common language: Bayesian inference. Joint quantification of space- and time-uncertainty becomes possible without losing the performance benefits of well-tuned ODE solvers. Thereby, we extend the toolbox of probabilistic programs for differential equation simulation to PDEs. ', 'Probabilistic ODE Solutions in Millions of Dimensions Probabilistic solvers for ordinary differential equations (ODEs) have emerged as an efficient framework for uncertainty quantification and inference on dynamical systems. In this work, we explain the mathematical assumptions and detailed implementation schemes behind solving high-dimensional ODEs with a probabilistic numerical algorithm. This has not been possible before due to matrix-matrix operations in each solver step, but is crucial for scientifically relevant problems—most importantly, the solution of discretised partial differential equations. In a nutshell, efficient high-dimensional probabilistic ODE solutions build either on independence assumptions or on Kronecker structure in the prior model. We evaluate the resulting efficiency on a range of problems, including the probabilistic numerical simulation of a differential equation with millions of dimensions.', 'Data-Adaptive Probabilistic Likelihood Approximation for Ordinary Differential Equations Estimating the parameters of ordinary differential equations (ODEs) is of fundamental importance in many scientific applications. While ODEs are typically approximated with deterministic algorithms, new research on probabilistic solvers indicates that they produce more reliable parameter estimates by better accounting for numerical errors. However, many ODE systems are highly sensitive to their parameter values. This produces deep local maxima in the likelihood function – a problem which existing probabilistic solvers have yet to resolve. Here we present a novel probabilistic ODE likelihood approximation, DALTON, which can dramatically reduce parameter sensitivity by learning from noisy ODE measurements in a data-adaptive manner. Our approximation scales linearly in both ODE variables and time discretization points, and is applicable to ODEs with both partially-unobserved components and non-Gaussian measurement models. Several examples demonstrate that DALTON produces more accurate parameter estimates via numerical optimization than existing probabilistic ODE solvers, and even in some cases than the exact ODE likelihood itself.']"
173,172,173_gaussian_gps_sparse_models,"['gaussian', 'gps', 'sparse', 'models', 'nonparametric', 'gplvm', 'kernels', 'gp', 'gpr', 'variational']","['Nonparametric Gaussian Process Covariances via Multidimensional Convolutions A key challenge in the practical application of Gaussian processes (GPs) is selecting a proper covariance function. The process convolutions construction of GPs allows some additional flexibility, but still requires choosing a proper smoothing kernel, which is non-trivial. Previous approaches have built covariance functions by using GP priors over the smoothing kernel, and by extension the covariance, as a way to bypass the need to specify it in advance. However, these models have been limited in several ways: they are restricted to single dimensional inputs, e.g. time; they only allow modelling of single outputs and they do not scale to large datasets since inference is not straightforward. In this paper, we introduce a nonparametric process convolution formulation for GPs that alleviates these weaknesses. We achieve this using a functional sampling approach based on Matheron’s rule to perform fast sampling using interdomain inducing variables. We test the performance of our model on benchmarks for single output, multi-output and large-scale GP regression, and find that our approach can provide improvements over standard GP models, particularly for larger datasets.', 'Scalable Gaussian Processes with Billions of Inducing Inputs via Tensor Train Decomposition We propose a method (TT-GP) for approximate inference in Gaussian Process (GP) models. We build on previous scalable GP research including stochastic variational inference based on inducing inputs, kernel interpolation, and structure exploiting algebra. The key idea of our method is to use Tensor Train decomposition for variational parameters, which allows us to train GPs with billions of inducing inputs and achieve state-of-the-art results on several benchmarks. Further, our approach allows for training kernels based on deep neural networks without any modifications to the underlying GP model. A neural network learns a multidimensional embedding for the data, which is used by the GP to make the final prediction. We train GP and neural network parameters end-to-end without pretraining, through maximization of GP marginal likelihood. We show the efficiency of the proposed approach on several regression and classification benchmark datasets including MNIST, CIFAR-10, and Airline. ', 'Sparse Orthogonal Variational Inference for Gaussian Processes We introduce a new interpretation of sparse variational approximations for Gaussian processes using inducing points, which can lead to more scalable algorithms than previous methods. It is based on decomposing a Gaussian process as a sum of two independent processes: one spanned by a finite basis of inducing points and the other capturing the remaining variation. We show that this formulation recovers existing approximations and at the same time allows to obtain tighter lower bounds on the marginal likelihood and new stochastic variational inference algorithms. We demonstrate the efficiency of these algorithms in several Gaussian process models ranging from standard regression to multi-class classification using (deep) convolutional Gaussian processes and report state-of-the-art results on CIFAR-10 among purely GP-based models.']"
174,38,174_crowdsourcing_crowdsourced_crowdworkers_crowds,"['crowdsourcing', 'crowdsourced', 'crowdworkers', 'crowds', 'labeling', 'labeled', 'labelers', 'annotators', 'crowd', 'labels']","['Analysis of Minimax Error Rate for Crowdsourcing and Its Application to Worker Clustering Model While crowdsourcing has become an important means to label data, there is great interest in estimating the ground truth from unreliable labels produced by crowdworkers. The Dawid and Skene (DS) model is one of the most well-known models in the study of crowdsourcing. Despite its practical popularity, theoretical error analysis for the DS model has been conducted only under restrictive assumptions on class priors, confusion matrices, or the number of labels each worker provides. In this paper, we derive a minimax error rate under more practical setting for a broader class of crowdsourcing models including the DS model as a special case. We further propose the worker clustering model, which is more practical than the DS model under real crowdsourcing settings. The wide applicability of our theoretical analysis allows us to immediately investigate the behavior of this proposed model, which can not be analyzed by existing studies. Experimental results showed that there is a strong similarity between the lower bound of the minimax error rate derived by our theoretical analysis and the empirical error of the estimated value.', 'Label Consistency-based Worker Filtering for Crowdsourcing In crowdsourcing scenarios, we can obtain multiple noisy labels from different crowd workers on the Internet for each instance and then infer its unknown true label via a label integration method. However, noisy labels often have a serious negative impact on label integration. In this case, most existing works always focus on designing more complex label integration methods to infer unknown true labels more accurately from multiple noisy labels, but little attention has been paid to another perspective, i.e., purifying noisy labels before label integration. In this paper, we aim to purify noisy labels for existing label integration methods and propose a label consistency-based worker filtering (LCWF) algorithm. In LCWF, we consider that if all low-quality workers are filtered out and only high-quality workers remain, the label consistency should be high. Therefore, we utilize label consistency to filter out low-quality workers. Firstly, we directly transform the worker filtering problem into a discrete optimization problem and utilize label consistency to define the fitness function for this problem. Then, we search for the optimal solution to this problem by a genetic algorithm. Finally, we filter out all labels from low-quality workers according to the optimal solution we obtained. Experimental results on simulated and real-world datasets demonstrate that LCWF can effectively purify noisy labels and improve the integration accuracy of existing label integration methods.', 'No Oops, You Won’t Do It Again: Mechanisms for Self-correction in Crowdsourcing Crowdsourcing is a very popular means of obtaining the large amounts of labeled data that modern machine learning methods require. Although cheap and fast to obtain, crowdsourced labels suffer from significant amounts of error, thereby degrading the performance of downstream machine learning tasks. With the goal of improving the quality of the labeled data, we seek to mitigate the many errors that occur due to silly mistakes or inadvertent errors by crowdsourcing workers. We propose a two-stage setting for crowdsourcing where the worker first answers the questions, and is then allowed to change her answers after looking at a (noisy) reference answer. We mathematically formulate this process and develop mechanisms to incentivize workers to act appropriately. Our mathematical guarantees show that our mechanism incentivizes the workers to answer honestly in both stages, and refrain from answering randomly in the first stage or simply copying in the second. Numerical experiments reveal a significant boost in performance that such ""self-correction"" can provide when using crowdsourcing to train machine learning algorithms.']"
175,107,175_uncertainty_neural_accuracy_prediction,"['uncertainty', 'neural', 'accuracy', 'prediction', 'deep', 'networks', 'bayesian', 'learning', 'models', 'predictions']","['Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs – extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout’s uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout’s uncertainty in deep reinforcement learning.', 'Unlabelled Data Improves Bayesian Uncertainty Calibration under Covariate Shift Modern neural networks have proven to be powerful function approximators, providing state-of-the-art performance in a multitude of applications. They however fall short in their ability to quantify confidence in their predictions — this is crucial in high-stakes applications that involve critical decision-making. Bayesian neural networks (BNNs) aim at solving this problem by placing a prior distribution over the network’s parameters, thereby inducing a posterior distribution that encapsulates predictive uncertainty. While existing variants of BNNs based on Monte Carlo dropout produce reliable (albeit approximate) uncertainty estimates over in-distribution data, they tend to exhibit over-confidence in predictions made on target data whose feature distribution differs from the training data, i.e., the covariate shift setup. In this paper, we develop an approximate Bayesian inference scheme based on posterior regularisation, wherein unlabelled target data are used as “pseudo-labels” of model confidence that are used to regularise the model’s loss on labelled source data. We show that this approach significantly improves the accuracy of uncertainty quantification on covariate-shifted data sets, with minimal modification to the underlying model architecture. We demonstrate the utility of our method in the context of transferring prognostic models of prostate cancer across globally diverse populations.', 'Density Uncertainty Layers for Reliable Uncertainty Estimation Assessing the predictive uncertainty of deep neural networks is crucial for safety-related applications of deep learning. Although Bayesian deep learning offers a principled framework for estimating model uncertainty, the common approaches that approximate the parameter posterior often fail to deliver reliable estimates of predictive uncertainty. In this paper, we propose a novel criterion for reliable predictive uncertainty: a model’s predictive variance should be grounded in the empirical density of the input. That is, the model should produce higher uncertainty for inputs that are improbable in the training data and lower uncertainty for inputs that are more probable. To operationalize this criterion, we develop the density uncertainty layer, a stochastic neural network architecture that satisfies the density uncertain criterion by design. We study density uncertainty layers on the UCI and CIFAR-10/100 uncertainty benchmarks. Compared to existing approaches, density uncertainty layers provide more reliable uncertainty estimates and robust out-of-distribution detection performance.']"
176,31,176_optimization_bilevel_minimization_gradientbased,"['optimization', 'bilevel', 'minimization', 'gradientbased', 'optimal', 'hypergradient', 'hessian', 'gradient', 'hypergradients', 'hessianvector']","['A Lower Bound and a Near-Optimal Algorithm for Bilevel Empirical Risk Minimization Bilevel optimization problems, which are problems where two optimization problems are nested, have more and more applications in machine learning. In many practical cases, the upper and the lower objectives correspond to empirical risk minimization problems and therefore have a sum structure. In this context, we propose a bilevel extension of the celebrated SARAH algorithm. We demonstrate that the algorithm requires $O((n+m)^{1/2}\\epsilon^{-1})$ oracle calls to achieve $\\epsilon$-stationarity with $n+m$ the total number of samples, which improves over all previous bilevel algorithms. Moreover, we provide a lower bound on the number of oracle calls required to get an approximate stationary point of the objective function of the bilevel problem. This lower bound is attained by our algorithm, making it optimal in terms of sample complexity.', 'Penalty Method for Inversion-Free Deep Bilevel Optimization Solving a bilevel optimization problem is at the core of several machine learning problems such as hyperparameter tuning, data denoising, meta- and few-shot learning, and trainingdata poisoning. Different from simultaneous or multi-objective optimization, the steepest descent direction for minimizing the upper-level cost in a bilevel problem requires the inverse of the Hessian of the lower-level cost. In this work, we propose a novel algorithm for solving bilevel optimization problems based on the classical penalty function approach. Our method avoids computing the Hessian inverse and can handle constrained bilevel problems easily. We prove the convergence of the method under mild conditions and show that the exact hypergradient is obtained asymptotically. Our method’s simplicity and small space and time complexities enable us to effectively solve large-scale bilevel problems involving deep neural networks. We present results on data denoising, few-shot learning, and training-data poisoning problems in a large-scale setting. Our results show that our approach outperforms or is comparable to previously proposed methods based on automatic differentiation and approximate inversion in terms of accuracy, run-time, and convergence speed', 'On Penalty-based Bilevel Gradient Descent Method Bilevel optimization enjoys a wide range of applications in hyper-parameter optimization, meta-learning and reinforcement learning. However, bilevel problems are difficult to solve and recent progress on scalable bilevel algorithms mainly focuses on bilevel optimization problems where the lower-level objective is either strongly convex or unconstrained. In this work, we tackle the bilevel problem through the lens of the penalty method. We show that under certain conditions, the penalty reformulation recovers the solutions of the original bilevel problem. Further, we propose the penalty-based bilevel gradient descent algorithm and establish its finite-time convergence for the constrained bilevel problem without lower-level strong convexity. The experimental results showcase the efficiency of the proposed algorithm.']"
177,197,177_clusterings_cluster_clustering_clusters,"['clusterings', 'cluster', 'clustering', 'clusters', 'algorithms', 'algorithm', 'fairness', 'dbscan', 'outliers', 'scalable']","['Fair Clustering Using Antidote Data Clustering algorithms are widely utilized for many modern data science applications. This motivates the need to make outputs of clustering algorithms fair. Traditionally, new fair algorithmic variants to clustering algorithms are developed for specific notions of fairness. However, depending on the application context, different definitions of fairness might need to be employed. As a result, new algorithms and analysis need to be proposed for each combination of clustering algorithm and fairness definition. Additionally, each new algorithm would need to be reimplemented for deployment in a real-world system. Hence, we propose an alternate approach to group-level fairness in center-based clustering inspired by research on data poisoning attacks. We seek to augment the original dataset with a small number of data points, called antidote data. When clustering is undertaken on this new dataset, the output is fair, for the chosen clustering algorithm and fairness definition. We formulate this as a general bi-level optimization problem which can accommodate any center-based clustering algorithms and fairness notions. We then categorize approaches for solving this bi-level optimization for two different problem settings. Extensive experiments on different clustering algorithms and fairness notions show that our algorithms can achieve desired levels of fairness on many real-world datasets with a very small percentage of antidote data added. We also find that our algorithms achieve lower fairness costs and competitive clustering performance compared to other state-of-the-art fair clustering algorithms.', 'Improved Approximation for Fair Correlation Clustering Correlation clustering is a ubiquitous paradigm in unsupervised machine learning where addressing unfairness is a major challenge. Motivated by this, we study fair correlation clustering where the data points may belong to different protected groups and the goal is to ensure fair representation of all groups across clusters. Our paper significantly generalizes and improves on the quality guarantees of previous work of Ahmadian et al. as follows. * We allow the user to specify an arbitrary upper bound on the representation of each group in a cluster. * Our algorithm allows individuals to have multiple protected features and ensure fairness simultaneously across them all. * We prove guarantees for clustering quality and fairness in this general setting. Furthermore, this improves on the results for the special cases studied in previous work. Our experiments on real-world data demonstrate that our clustering quality compared to the optimal solution is much better than what our theoretical result suggests.', 'Clustering: Science or Art? We examine whether the quality of different clustering algorithms\ncan be compared by a general, scientifically sound procedure which\nis independent of particular clustering algorithms. We argue that\nthe major obstacle is the difficulty in evaluating a clustering\nalgorithm without taking into account the context: why does the user\ncluster his data in the first place, and what does he want to do\nwith the clustering afterwards? We argue that clustering should not\nbe treated as an application-independent mathematical problem, but\nshould always be studied in the context of its end-use.  Different\ntechniques to evaluate clustering algorithms have to be developed\nfor different uses of clustering. To simplify this procedure we\nargue that it will be useful to build a “taxonomy of clustering\nproblems” to identify clustering applications which can be treated\nin a unified way and that such an effort will be more fruitful than\nattempting the impossible – developing “optimal” domain-independent\nclustering algorithms or even classifying clustering algorithms in\nterms of how they work.\n']"
178,26,178_accuracypreserving_calibrate_calibration_calibrationmapbased,"['accuracypreserving', 'calibrate', 'calibration', 'calibrationmapbased', 'accuracy', 'calibrated', 'calibratable', 'classifiers', 'calibratability', 'calibrator']","['Two Sides of Miscalibration: Identifying Over and Under-Confidence Prediction for Network Calibration Proper confidence calibration of deep neural networks is essential for reliable predictions in safety-critical tasks. Miscalibration can lead to model over-confidence and/or under-confidence; i.e., the model’s confidence in its prediction can be greater or less than the model’s accuracy. Recent studies have highlighted the over-confidence issue by introducing calibration techniques and demonstrated success on various tasks. However, miscalibration through under-confidence has not yet to receive much attention. In this paper, we address the necessity of paying attention to the under-confidence issue. We first introduce a novel metric, a miscalibration score, to identify the overall and class-wise calibration status, including being over or under-confident. Our proposed metric reveals the pitfalls of existing calibration techniques, where they often overly calibrate the model and worsen under-confident predictions. Then we utilize the class-wise miscalibration score as a proxy to design a calibration technique that can tackle both over and under-confidence. We report extensive experiments that show our proposed methods substantially outperforming existing calibration techniques. We also validate our proposed calibration technique on an automatic failure detection task with a risk-coverage curve, reporting that our methods improve failure detection as well as trustworthiness of the model. The code are available at \\url{https://github.com/AoShuang92/miscalibration_TS}.', 'Trainable Calibration Measures for Neural Networks from Kernel Mean Embeddings Modern neural networks have recently been found to be poorly calibrated, primarily in the direction of over-confidence. Methods like entropy penalty and temperature smoothing improve calibration by clamping confidence, but in doing so compromise the many legitimately confident predictions. We propose a more principled fix that minimizes an explicit calibration error during training. We present MMCE, a RKHS kernel based measure of calibration that is efficiently trainable alongside the negative likelihood loss without careful hyper-parameter tuning. Theoretically too, MMCE is a sound measure of calibration that is minimized at perfect calibration, and whose finite sample estimates are consistent and enjoy fast convergence rates. Extensive experiments on several network architectures demonstrate that MMCE is a fast, stable, and accurate method to minimize calibration error while maximally preserving the number of high confidence predictions.', 'Calibration techniques for node classification using graph neural networks on medical image data Miscalibration of deep neural networks (DNNs) can lead to unreliable predictions and hinder their use in clinical decision-making. This miscalibration is often caused by overconfident probability estimates. Calibration techniques such as model ensembles, regularization terms, and post-hoc scaling of the predictions can be employed to improve the calibration performance of DNNs. In contrast to DNNs, graph neural networks (GNNs) tend to exhibit underconfidence. In this study, we investigate the efficacy of calibration techniques developed for DNNs when applied to GNNs trained on medical image data, and compare the calibration performance of binary and multiclass node classification on a benchmark dataset and a medical image dataset. We find that post-hoc methods using Platt scaling or Temperature scaling, or methods that add a regularization term to the loss function during training are most effective to improve calibration. Our results further indicate that these calibration techniques are more effective for multiclass classification tasks compared to binary classification tasks.']"
179,24,179_uncertainty_probabilistic_quantifying_classification,"['uncertainty', 'probabilistic', 'quantifying', 'classification', 'epistemic', 'quantification', 'reliability', 'prediction', 'confidence', 'measures']","['Quantifying aleatoric and epistemic uncertainty in machine learning: Are conditional entropy and mutual information appropriate measures? The quantification of aleatoric and epistemic uncertainty in terms of conditional entropy and mutual information, respectively, has recently become quite common in machine learning. While the properties of these measures, which are rooted in information theory, seem appealing at first glance, we identify various incoherencies that call their appropriateness into question. In addition to the measures themselves, we critically discuss the idea of an additive decomposition of total uncertainty into its aleatoric and epistemic constituents. Experiments across different computer vision tasks support our theoretical findings and raise concerns about current practice in uncertainty quantification.', 'Quantification of Credal Uncertainty in Machine Learning: A Critical Analysis and Empirical Comparison The representation and quantification of uncertainty has received increasing attention in machine learning in the recent past. The formalism of credal sets provides an interesting alternative in this regard, especially as it combines the representation of epistemic (lack of knowledge) and aleatoric (statistical) uncertainty in a rather natural way. In this paper, we elaborate on uncertainty measures for credal sets from the perspective of machine learning. More specifically, we provide an overview of proposals, discuss existing measures in a critical way, and also propose a new measure that is more tailored to the machine learning setting. Based on an experimental study, we conclude that theoretically well-justified measures also lead to better performance in practice. Besides, we corroborate the difficulty of the disaggregation problem, that is, of decomposing the amount of total uncertainty into aleatoric and epistemic uncertainty in a sound manner, thereby complementing theoretical findings with empirical evidence.', 'Collision Probability Matching Loss for Disentangling Epistemic Uncertainty from Aleatoric Uncertainty Two important aspects of machine learning, uncertainty and calibration, have previously been studied separately. The first aspect involves knowing whether inaccuracy is due to the epistemic uncertainty of the model, which is theoretically reducible, or to the aleatoric uncertainty in the data per se, which thus becomes the upper bound of model performance. As for the second aspect, numerous calibration methods have been proposed to correct predictive probabilities to better reflect the true probabilities of being correct. In this paper, we aim to obtain the squared error of predictive distribution from the true distribution as epistemic uncertainty. Our formulation, based on second-order Rényi entropy, integrates the two problems into a unified framework and obtains the epistemic (un)certainty as the difference between the aleatoric and predictive (un)certainties. As an auxiliary loss to ordinary losses, such as cross-entropy loss, the proposed collision probability matching (CPM) loss matches the cross-collision probability between the true and predictive distributions to the collision probability of the predictive distribution, where these probabilities correspond to accuracy and confidence, respectively. Unlike previous Shannon-entropy-based uncertainty methods, the proposed method makes the aleatoric uncertainty directly measurable as test-retest reliability, which is a summary statistic of the true distribution frequently used in scientific research on humans. We provide mathematical proof and strong experimental evidence for our formulation using both a real dataset consisting of real human ratings toward emotional faces and simulation.']"
180,16,180_learns_interactive_language_agent,"['learns', 'interactive', 'language', 'agent', 'planning', 'agents', 'tasks', 'exploration', 'languagefree', 'environments']","['Interactive Grounded Language Understanding in a Collaborative Environment: Retrospective on Iglu 2022 Competition Human intelligence possesses the extraordinary ability to adapt rapidly to new tasks and multi-modal environments. This capacity emerges at an early age, as humans acquire new skills and learn to solve problems by imitating others or following natural language instructions. To facilitate research in this area, we recently hosted the second \\emph{IGLU: Interactive Grounded Language Understanding in a Collaborative Environment} competition. The primary objective of the competition is to address the challenge of creating interactive agents that can learn to solve complex tasks by receiving grounded natural language instructions in a collaborative environment. Given the complexity of this challenge, we divided it into two sub-tasks: first, deciding whether the provided grounded instruction requires clarification, and second, following a clear grounded instruction to complete the task description.', 'Learning to Model the World With Language To interact with humans and act in the world, agents need to understand the range of language that people use and relate it to the visual world. While current agents can learn to execute simple language instructions, we aim to build agents that leverage diverse language—language like ""this button turns on the TV"" or ""I put the bowls away""—that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that <em>agents should interpret such diverse language as a signal that helps them predict the future</em>: what they will observe, how the world will behave, and which situations will be rewarded. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We instantiate this in Dynalang, an agent that learns a multimodal world model to predict future text and image representations, and learns to act from imagined model rollouts. While current methods that learn language-conditioned policies degrade in performance with more diverse types of language, we show that Dynalang learns to leverage environment descriptions, game rules, and instructions to excel on tasks ranging from game-playing to navigating photorealistic home scans. Finally, we show that our method enables additional capabilities due to learning a generative model: Dynalang can be pretrained on text-only data, enabling learning from offline datasets, and generate language grounded in an environment.', 'Interactive Grounded Language Understanding  in a Collaborative Environment: IGLU 2021 Human intelligence has the remarkable ability to quickly adapt to new tasks and environments. Starting from a very young age, humans acquire new skills and learn how to solve new tasks either by imitating the behavior of others or by following provided natural language instructions. To facilitate research in this direction, we propose IGLU: Interactive Grounded Language Understanding in a Collaborative Environment. The primary goal of the competition is to approach the problem of how to build interactive agents that learn to solve a task while provided with grounded natural language instructions in a collaborative environment. Understanding the complexity of the challenge, we split it into sub-tasks to make it feasible for participants.']"
181,12,181_generative_projectedgan_recall_models,"['generative', 'projectedgan', 'recall', 'models', 'metrics', 'fidelitydiversity', 'diversity', 'generated', 'metric', 'divergence']","['How Faithful is your Synthetic Data? Sample-level Metrics for Evaluating and Auditing Generative Models Devising domain- and model-agnostic evaluation metrics for generative models is an important and as yet unresolved problem. Most existing metrics, which were tailored solely to the image synthesis setup, exhibit a limited capacity for diagnosing the different modes of failure of generative models across broader application domains. In this paper, we introduce a 3-dimensional evaluation metric, ($\\alpha$-Precision, $\\beta$-Recall, Authenticity), that characterizes the fidelity, diversity and generalization performance of any generative model in a domain-agnostic fashion. Our metric unifies statistical divergence measures with precision-recall analysis, enabling sample- and distribution-level diagnoses of model fidelity and diversity. We introduce generalization as an additional, independent dimension (to the fidelity-diversity trade-off) that quantifies the extent to which a model copies training data{—}a crucial performance indicator when modeling sensitive data with requirements on privacy. The three metric components correspond to (interpretable) probabilistic quantities, and are estimated via sample-level binary classification. The sample-level nature of our metric inspires a novel use case which we call model auditing, wherein we judge the quality of individual samples generated by a (black-box) model, discarding low-quality samples and hence improving the overall model performance in a post-hoc manner.', 'Reliable Fidelity and Diversity Metrics for Generative Models Devising indicative evaluation metrics for the image generation task remains an open problem. The most widely used metric for measuring the similarity between real and generated images has been the Frechet Inception Distance (FID) score. Since it does not differentiate the fidelity and diversity aspects of the generated images, recent papers have introduced variants of precision and recall metrics to diagnose those properties separately. In this paper, we show that even the latest version of the precision and recall metrics are not reliable yet. For example, they fail to detect the match between two identical distributions, they are not robust against outliers, and the evaluation hyperparameters are selected arbitrarily. We propose density and coverage metrics that solve the above issues. We analytically and experimentally show that density and coverage provide more interpretable and reliable signals for practitioners than the existing metrics.', 'Emergent Asymmetry of Precision and Recall for Measuring Fidelity and Diversity of Generative Models in High Dimensions Precision and Recall are two prominent metrics of generative performance, which were proposed to separately measure the fidelity and diversity of generative models. Given their central role in comparing and improving generative models, understanding their limitations are crucially important. To that end, in this work, we identify a critical flaw in the common approximation of these metrics using k-nearest-neighbors, namely, that the very interpretations of fidelity and diversity that are assigned to Precision and Recall can fail in high dimensions, resulting in very misleading conclusions. Specifically, we empirically and theoretically show that as the number of dimensions grows, two model distributions with supports at equal point-wise distance from the support of the real distribution, can have vastly different Precision and Recall regardless of their respective distributions, hence an emergent asymmetry in high dimensions. Based on our theoretical insights, we then provide simple yet effective modifications to these metrics to construct symmetric metrics regardless of the number of dimensions. Finally, we provide experiments on real-world datasets to illustrate that the identified flaw is not merely a pathological case, and that our proposed metrics are effective in alleviating its impact.']"
182,10,182_ising_complexity_nodes_models,"['ising', 'complexity', 'nodes', 'models', 'nodeperturbed', 'treestructured', 'latenttree', 'algorithms', 'boundedtreewidth', 'sampling']","['Learning and Testing Latent-Tree Ising Models Efficiently We provide time- and sample-efficient algorithms for learning and testing latent-tree Ising models, i.e.\xa0Ising models that may only be observed at their leaf nodes. On the learning side, we obtain efficient algorithms for learning a tree-structured Ising model whose leaf node distribution is close in Total Variation Distance, improving on the results of \\cite{cryan2001evolutionary}. On the testing side, we provide an efficient algorithm with fewer samples for testing whether two latent-tree Ising models have leaf-node distributions that are close or far in Total Variation distance. We obtain our algorithms by showing novel localization results for the total variation distance between the leaf-node distributions of tree-structured Ising models, in terms of their marginals on pairs of leaves.', 'Inference and Sampling of $K_33$-free Ising Models We call an Ising model tractable when it is possible to compute its partition function value (statistical inference) in polynomial time. The tractability also implies an ability to sample configurations of this model in polynomial time. The notion of tractability extends the basic case of planar zero-field Ising models. Our starting point is to describe algorithms for the basic case, computing partition function and sampling efficiently. Then, we extend our tractable inference and sampling algorithms to models whose triconnected components are either planar or graphs of $O(1)$ size. In particular, it results in a polynomial-time inference and sampling algorithms for $K_{33}$ (minor)-free topologies of zero-field Ising models—a generalization of planar graphs with a potentially unbounded genus.', 'The Mean-Field Approximation: Information Inequalities, Algorithms, and Complexity The mean field approximation to the Ising model is a canonical variational tool that is used for analysis and inference in Ising models. We provide a simple and optimal bound for the KL error of the mean field approximation for Ising models on general graphs, and extend it to higher order Markov random fields. Our bound improves on previous bounds obtained in work in the graph limit literature by Borgs, Chayes, Lov{á}sz, S{ó}s, and Vesztergombi and recent works by Basak and Mukherjee, and Eldan. Our bound is tight up to lower order terms.  Building on the methods used to prove the bound, along with techniques from combinatorics and optimization,  we study the algorithmic problem of estimating the (variational) free energy for Ising models and general Markov random fields. For a graph $G$ on $n$ vertices and interaction matrix $J$ with Frobenius norm $\\|{J} \\|_F$, we provide algorithms that approximate the free energy within an additive error of $\\epsilon n \\|J\\|_F$ in time $\\exp(poly(1/\\epsilon))$. We also show that approximation within $(n \\|J\\|_F)^{1-\\delta}$ is NP-hard for every $\\delta > 0$. Finally, we provide more efficient approximation algorithms, which find the optimal mean field approximation, for ferromagnetic Ising models and for Ising models satisfying Dobrushin’s condition.   ']"
183,37,183_translations_multilingual_translation_bilingual,"['translations', 'multilingual', 'translation', 'bilingual', 'monolingual', 'languages', 'language', 'crosslingual', 'decoding', 'linguistic']","['Non-autoregressive Machine Translation with Disentangled Context Transformer State-of-the-art neural machine translation models generate a translation from left to right and every step is conditioned on the previously generated tokens. The sequential nature of this generation process causes fundamental latency in inference since we cannot generate multiple tokens in each sentence in parallel. We propose an attention-masking based model, called Disentangled Context (DisCo) transformer, that simultaneously generates all tokens given different contexts. The DisCo transformer is trained to predict every output token given an arbitrary subset of the other reference tokens. We also develop the parallel easy-first inference algorithm, which iteratively refines every token in parallel and reduces the number of required iterations. Our extensive experiments on 7 translation directions with varying data sizes demonstrate that our model achieves competitive, if not better, performance compared to the state of the art in non-autoregressive machine translation while significantly reducing decoding time on average.', 'Implementing Multilingual Translation with T5 and Transformers This post is divided into three parts; they are: • Setting up the translation pipeline • Translation with alternatives • Quality estimation Text translation is a fundamental task in natural language processing, and it inspired the invention of the original transformer model.', 'On Learning Language-Invariant Representations for Universal Machine Translation The goal of universal machine translation is to learn to translate between any pair of languages. Despite impressive empirical results and an increasing interest in massively multilingual models, theoretical analysis on translation errors made by such universal machine translation models is only nascent. In this paper, we formally prove certain impossibilities of this endeavour in general, as well as prove positive results in the presence of additional (but natural) structure of data. For the former, we derive a lower bound on the translation error in the many-to-many translation setting, which shows that any algorithm aiming to learn shared sentence representations among multiple language pairs has to make a large translation error on at least one of the translation tasks, if no assumption on the structure of the languages is made. For the latter, we show that if the paired documents in the corpus follow a natural \\emph{encoder-decoder} generative process, we can expect a natural notion of “generalization”: a linear number of language pairs, rather than quadratic, suffices to learn a good representation. Our theory also explains what kinds of connection graphs between pairs of languages are better suited: ones with longer paths result in worse sample complexity. We believe our theoretical insights and implications contribute to the future algorithmic design of universal machine translation.']"
184,41,184_memory_decoding_attention_tokenizer,"['memory', 'decoding', 'attention', 'tokenizer', 'efficient', 'faster', 'speedup', 'speculative', 'compression', 'throughput']","['Clover: Regressive Lightweight Speculative Decoding with Sequential Knowledge arXiv:2405.00263v1 Announce Type: cross \nAbstract: Large language models (LLMs) suffer from low efficiency as the mismatch between the requirement of auto-regressive decoding and the design of most contemporary GPUs. Specifically, billions to trillions of parameters must be loaded to the GPU cache through its limited memory bandwidth for computation, but only a small batch of tokens is actually computed. Consequently, the GPU spends most of its time on memory transfer instead of computation. Recently, parallel decoding, a type of speculative decoding algorithms, is becoming more popular and has demonstrated impressive efficiency improvement in generation. It introduces extra decoding heads to large models, enabling them to predict multiple subsequent tokens simultaneously and verify these candidate continuations in a single decoding step. However, this approach deviates from the training objective of next token prediction used during pre-training, resulting in a low hit rate for candidate tokens. In this paper, we propose a new speculative decoding algorithm, Clover, which integrates sequential knowledge into the parallel decoding process. This enhancement improves the hit rate of speculators and thus boosts the overall efficiency. Clover transmits the sequential knowledge from pre-speculated tokens via the Regressive Connection, then employs an Attention Decoder to integrate these speculated tokens. Additionally, Clover incorporates an Augmenting Block that modifies the hidden states to better align with the purpose of speculative generation rather than next token prediction. The experiment results demonstrate that Clover outperforms the baseline by up to 91% on Baichuan-Small and 146% on Baichuan-Large, respectively, and exceeds the performance of the previously top-performing method, Medusa, by up to 37% on Baichuan-Small and 57% on Baichuan-Large, respectively.', 'Speculative Streaming: Fast LLM Inference without Auxiliary Models Speculative decoding is a prominent technique to accelerate large language model inference by leveraging predictions from an auxiliary draft model. While effective, in application-specific settings, it often involves fine-tuning both draft and target models to achieve high acceptance rates. As the number of downstream tasks grows, draft models add significant complexity to inference systems. Recently several single model architectures viz. Medusa have been proposed to speculate tokens in non-autoregressive manner, however, their effectiveness is limited due to lack of dependency between speculated tokens. We introduce a novel speculative decoding method that integrates drafting within the target model by using Multi-stream attention and incorporates future token planning into supervised fine-tuning objective. To the best of our knowledge, it is the first parameter-efficient approach that scales well with number of downstream tasks while improving downstream metrics. Speculative Streaming speeds up decoding by 1.9 - 3X in a diverse set of tasks, such as Summarization, Structured Queries, and Meaning Representation, while improving generation quality and using 10000X fewer extra parameters than alternative architectures, making it ideal for resource-constrained devices. Our approach can also be effectively deployed in lossless settings for generic chatbot applications that do not necessitate fine-tuning. In such setups, we achieve 2.9 - 3.2X speedup while maintaining the integrity of the base model’s output.', 'When Linear Attention Meets Autoregressive Decoding: Towards More Effective and Efficient Linearized Large Language Models Autoregressive Large Language Models (LLMs) have achieved impressive performance in language tasks but face two significant bottlenecks: (1) quadratic complexity in the attention module as the number of tokens increases, and (2) limited efficiency due to the sequential processing nature of autoregressive LLMs during generation. While linear attention and speculative decoding offer potential solutions, their applicability and synergistic potential for enhancing autoregressive LLMs remain uncertain. We conduct the first comprehensive study on the efficacy of existing linear attention methods for autoregressive LLMs, integrating them with speculative decoding. We introduce an augmentation technique for linear attention that ensures compatibility with speculative decoding, enabling more efficient training and serving of LLMs. Extensive experiments and ablation studies involving seven existing linear attention models and five encoder/decoder-based LLMs consistently validate the effectiveness of our augmented linearized LLMs. Notably, our approach achieves up to a 6.67 reduction in perplexity on the LLaMA model and up to a 2$\\times$ speedup during generation compared to prior linear attention methods. Codes and models are available at https://github.com/GATECH-EIC/Linearized-LLM.']"
185,39,185_pruning_parameterefficient_sparse_benchmark,"['pruning', 'parameterefficient', 'sparse', 'benchmark', 'scaling', 'language', 'pruned', 'lowrank', 'models', 'generalization']","['Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity Large Language Models (LLMs), renowned for their remarkable performance across diverse domains, present a challenge due to their colossal model size when it comes to practical deployment. In response to this challenge, efforts have been directed toward the application of traditional network pruning techniques to LLMs, uncovering a massive number of parameters can be pruned in one-shot without hurting performance. Building upon insights gained from pre-LLM models, particularly BERT-level language models, prevailing LLM pruning strategies have consistently adhered to the practice of uniformly pruning all layers at equivalent sparsity levels, resulting in robust performance. However, this observation stands in contrast to the prevailing trends observed in the field of vision models, where non-uniform layerwise sparsity typically yields substantially improved results. To elucidate the underlying reasons for this disparity, we conduct a comprehensive analysis of the distribution of token features within LLMs. In doing so, we discover a strong correlation with the emergence of outliers, defined as features exhibiting significantly greater magnitudes compared to their counterparts in feature dimensions. Inspired by this finding, we introduce a novel LLM pruning methodology that incorporates a tailored set of <b>non-uniform layerwise sparsity ratios</b> specifically designed for LLM pruning, termed as <b>O</b>utlier <b>W</b>eighed <b>L</b>ayerwise sparsity (<b>OWL</b>). The sparsity ratio of OWL is directly proportional to the outlier ratio observed within each layer, facilitating a more effective alignment between layerwise weight sparsity and outlier ratios. Our empirical evaluation, conducted across the LLaMA-V1/V2, Vicuna, OPT, and Mistral, spanning various benchmarks, demonstrates the distinct advantages offered by OWL over previous methods. For instance, OWL exhibits a remarkable performance gain, surpassing the state-of-the-art Wanda and SparseGPT by <b>61.22</b> and <b>6.80</b> perplexity at a high sparsity level of 70%, respectively, while delivering <b>2.6$\\times$</b> end-to-end inference speed-up in the DeepSparse inference engine. Code is available at https://github.com/luuyin/OWL.git.', 'How to Prune Your Language Model: Recovering Accuracy on the “Sparsity May Cry” Benchmark Pruning large language models (LLMs) from the BERT family has emerged as a standard compression benchmark, and several pruning methods have been proposed for this task.  The recent “Sparsity May Cry” (SMC) benchmark  put into question the validity of all existing methods, exhibiting a more complex setup where many known pruning methods appear to fail.  We revisit the question of accurate BERT-pruning during fine-tuning on downstream datasets, and propose a set of general guidelines for successful pruning, even on the challenging SMC benchmark. First, we perform a cost-vs-benefits analysis of pruning model components, such as the embeddings and the classification head; second, we provide a simple-yet-general way of scaling training,  sparsification and learning rate schedules relative to the desired target sparsity; finally, we investigate the importance of proper parametrization for Knowledge Distillation in the context of LLMs. Our simple insights lead to state-of-the-art results, both on classic BERT-pruning benchmarks, as well as on the SMC benchmark, showing that even classic gradual magnitude pruning (GMP) can yield competitive results, with the right approach.', 'Pruner-Zero: Evolving Symbolic Pruning Metric From Scratch for Large Language Models Despite the remarkable capabilities, Large Language Models (LLMs) face deployment challenges due to their extensive size. Pruning methods drop a subset of weights to accelerate, but many of them require retraining, which is prohibitively expensive and computationally demanding. Recently, post-training pruning approaches introduced novel metrics, enabling the pruning of LLMs without retraining. However, these metrics require the involvement of human experts and tedious trial and error. To efficiently identify superior pruning metrics, we develop an automatic framework for searching symbolic pruning metrics using genetic programming. In particular, we devise an elaborate search space encompassing the existing pruning metrics to discover the potential symbolic pruning metric. We propose an opposing operation simplification strategy to increase the diversity of the population. In this way, Pruner-Zero allows auto-generation of symbolic pruning metrics. Based on the searched results, we explore the correlation between pruning metrics and performance after pruning and summarize some principles. Extensive experiments on LLaMA and LLaMA-2 on language modeling and zero-shot tasks demonstrate that our Pruner-Zero obtains superior performance than SOTA post-training pruning methods. Code at: https://github.com/pprp/Pruner-Zero.']"
186,34,186_probabilistic_graphs_inference_models,"['probabilistic', 'graphs', 'inference', 'models', 'approximations', 'propagation', 'bethe', 'graph', 'algorithm', 'belief']","['A Lower Bound on the Partition Function of Attractive Graphical Models in the Continuous Case Computing the partition function of an arbitrary graphical model is generally intractable. As a result, approximate inference techniques such as loopy belief propagation and expectation propagation are used to compute an approximation to the true partition function.  However, due to general issues of intractability in the continuous case, our understanding of these approximations is relatively limited.  In particular, a number of theoretical results known for these approximations in the discrete case are missing in the continuous case.  In this work, we use graph covers to extend several such results from the discrete case to the continuous case.  Specifically, we provide a graph cover based upper bound for continuous graphical models, and we use this characterization (along with a continuous analog of a discrete correlation-type inequality) to show that the Bethe partition function also provides a lower bound on the true partition function of attractive graphical models in the continuous case.', 'Fixing the Bethe approximation: How structural modifications in a graph improve belief propagation Belief propagation is an iterative method for inference in probabilistic graphical models. Its well-known relationship to a classical concept from statistical physics, the Bethe free energy, puts it on a solid theoretical foundation. If belief propagation fails to approximate the marginals, then this is often due to a failure of the Bethe approximation. In this work, we show how modifications in a graphical model can be a great remedy for fixing the Bethe approximation. Specifically, we analyze how the removal of edges influences and improves belief propagation, and demonstrate that this positive effect is particularly distinct for dense graphs.', 'Message Passing for Collective Graphical Models Collective graphical models (CGMs) are a formalism for inference and learning about a population of independent and identically distributed individuals when only noisy aggregate data are available. We highlight a close connection between approximate MAP inference in CGMs and marginal inference in standard graphical models. The connection leads us to derive a novel Belief Propagation (BP) style algorithm for collective graphical models. Mathematically, the algorithm is a strict generalization of BP—it can be viewed as an extension to minimize the Bethe free energy plus additional energy terms that are non-linear functions of the marginals. For CGMs, the algorithm is much more efficient than previous approaches to inference. We demonstrate its performance on two synthetic experiments concerning bird migration and collective human mobility.']"
187,37,187_sparsemap_algorithms_lpsparsemap_map,"['sparsemap', 'algorithms', 'lpsparsemap', 'map', 'relaxations', 'relaxation', 'algorithm', 'maxproduct', 'inference', 'optimum']","['Partial Optimality of  Dual Decomposition for MAP Inference in Pairwise MRFs Markov random fields (MRFs) are a powerful tool for modelling statistical dependencies for a set of random variables using a graphical representation. An important computational problem related to MRFs, called maximum a posteriori (MAP) inference, is finding a joint variable assignment with the maximal probability. It is well known that the two popular optimisation techniques for this task, linear programming (LP) relaxation and dual decomposition (DD), have a strong connection both providing an optimal solution to the MAP problem  when a corresponding LP relaxation is tight. However, less is known about their relationship in the opposite and more realistic case. In this paper, we explain how the fully integral assignments obtained via DD partially agree with the optimal fractional assignments via LP relaxation when the latter is not tight. In particular, for binary pairwise MRFs the corresponding result suggests that both methods share the partial optimality property of their solutions.', 'Linear Approximation to ADMM for MAP inference Maximum a posteriori (MAP) inference is one of the fundamental inference tasks in graphical models.  MAP inference is in general NP-hard, making approximate methods of interest for many problems. One successful class of approximate inference algorithms is based on linear programming (LP) relaxations. The augmented Lagrangian method can be used to overcome a lack of strict convexity in LP relaxations, and the Alternating Direction Method of Multipliers (ADMM) provides an elegant algorithm for finding the saddle point of the augmented Lagrangian. Here we present an ADMM-based algorithm to solve the primal form of the MAP-LP whose closed form updates are based on a linear approximation technique. Our technique gives efficient, closed form updates that converge to the global optimum of the LP relaxation. We compare our algorithm to two existing ADMM-based MAP-LP methods, showing that our technique is faster on general, non-binary or non-pairwise models.', 'Accelerated Message Passing for Entropy-Regularized MAP Inference Maximum a posteriori (MAP) inference in discrete-valued Markov random fields is a fundamental problem in machine learning that involves identifying the most likely configuration of random variables given a distribution. Due to the difficulty of this combinatorial problem, linear programming (LP) relaxations are commonly used to derive specialized message passing algorithms that are often interpreted as coordinate descent on the dual LP. To achieve more desirable computational properties, a number of methods regularize the LP with an entropy term, leading to a class of smooth message passing algorithms with convergence guarantees. In this paper, we present randomized methods for accelerating these algorithms by leveraging techniques that underlie classical accelerated gradient methods. The proposed algorithms incorporate the familiar steps of standard smooth message passing algorithms, which can be viewed as coordinate minimization steps. We show that these accelerated variants achieve faster rates for finding $\\epsilon$-optimal points of the unregularized problem, and, when the LP is tight, we prove that the proposed algorithms recover the true MAP solution in fewer iterations than standard message passing algorithms.']"
188,47,188_clustering_cluster_spectral_clusters,"['clustering', 'cluster', 'spectral', 'clusters', 'hypergraphs', 'graphs', 'nodes', 'algorithms', 'eigenvectors', 'digraphs']","['Fast Approximate Spectral Clustering for Dynamic Networks Spectral clustering is a widely studied problem, yet its complexity is prohibitive for dynamic graphs of even modest size. We claim that it is possible to reuse information of past cluster assignments to expedite computation. Our approach builds on a recent idea of sidestepping the main bottleneck of spectral clustering, i.e., computing the graph eigenvectors, by a polynomial-based randomized sketching technique. We show that the proposed algorithm achieves clustering assignments with quality approximating that of spectral clustering and that it can yield significant complexity benefits when the graph dynamics are appropriately bounded. In our experiments, our method clusters 30k node graphs 3.9$\\times$ faster in average and deviates from the correct assignment by less than 0.1%.', 'Theory of Spectral Method for Union of Subspaces-Based Random Geometry Graph Spectral method is a commonly used scheme to cluster data points lying close to Union of Subspaces, a task known as Subspace Clustering. The typical usage is to construct a Random Geometry Graph first and then apply spectral method to the graph to obtain clustering result. The latter step has been coined the name Spectral Clustering. As far as we know, in spite of the significance of both steps in spectral-method-based Subspace Clustering, all existing theoretical results focus on the first step of constructing the graph, but ignore the final step to correct false connections through spectral clustering. This paper establishes a theory to show the power of this method for the first time, in which we demonstrate the mechanism of spectral clustering by analyzing a simplified algorithm under the widely used semi-random model. Based on this theory, we prove the efficiency of Subspace Clustering in fairly broad conditions. The insights and analysis techniques developed in this paper might also have implications for other random graph problems.', 'A Tighter Analysis of Spectral Clustering, and Beyond This work studies the classical spectral clustering algorithm which embeds the vertices of some graph G=(V_G, E_G) into R^k using k eigenvectors of some matrix of G, and applies k-means to partition V_G into k clusters. Our first result is a tighter analysis on the performance of spectral clustering, and explains why it works under some much weaker condition than the ones studied in the literature. For the second result, we show that, by applying fewer than k eigenvectors to construct the embedding, spectral clustering is able to produce better output for many practical instances; this result is the first of its kind in spectral clustering. Besides its conceptual and theoretical significance, the practical impact of our work is demonstrated by the empirical analysis on both synthetic and real-world data sets, in which spectral clustering produces comparable or better results with fewer than k eigenvectors.']"
189,78,189_robot_robots_robotic_visionlanguageaction,"['robot', 'robots', 'robotic', 'visionlanguageaction', 'robotics', 'visionlanguage', 'multimodal', 'embodied', 'language', 'planning']","['Tell Me Where to Go: A Composable Framework for Context-Aware Embodied Robot Navigation Humans have the remarkable ability to navigate through unfamiliar environments by solely relying on our prior knowledge and descriptions of the environment. For robots to perform the same type of navigation, they need to be able to associate natural language descriptions with their associated physical environment with a limited amount of prior knowledge. Recently, Large Language Models (LLMs) have been able to reason over billions of parameters and utilize them in multi-modal chat-based natural language responses. However, LLMs lack real-world awareness and their outputs are not always predictable. In this work, we develop a low-bandwidth framework that solves this lack of real-world generalization by creating an intermediate layer between an LLM and a robot navigation framework in the form of Python code. Our intermediate shoehorns the vast prior knowledge inherent in an LLM model into a series of input and output API instructions that a mobile robot can understand. We evaluate our method across four different environments and command classes on a mobile robot and highlight our framework’s ability to interpret contextual commands.', 'Do As I Can, Not As I Say: Grounding Language in Robotic Affordances Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment. For example, asking a language model to describe how to clean a spill might result in a reasonable narrative, but it may not be applicable to a particular agent, such as a robot, that needs to perform this task in a particular environment. We propose to provide real-world grounding by means of pretrained skills, which are used to constrain the model to propose natural language actions that are both feasible and contextually appropriate. The robot can act as the language model’s “hands and eyes,” while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so  that  the  language model  provides  high-level  knowledge about the procedures for performing complex and temporally extended instructions,  while  value  functions  associated  with  these  skills  provide  the  grounding necessary to connect this knowledge to a particular physical environment. We evaluate our method on a number of real-world robotic tasks, where we show the need for real-world grounding and that this approach is capable of completing long-horizon, abstract, natural language instructions on a mobile manipulator. The project’s website, video, and open source can be found at say-can.github.io.', 'Inner Monologue: Embodied Reasoning through Planning with Language Models Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent’s own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.']"
190,59,190_programming_coding_languagetocode_compiler,"['programming', 'coding', 'languagetocode', 'compiler', 'program', 'interpreter', 'programmer', 'programs', 'code', 'generate']","['Latent Programmer: Discrete Latent Codes for Program Synthesis A key problem in program synthesis is searching over the large space of possible programs. Human programmers might decide the high-level structure of the desired program before thinking about the details; motivated by this intuition, we consider two-level search for program synthesis, in which the synthesizer first generates a plan, a sequence of symbols that describes the desired program at a high level, before generating the program. We propose to learn representations of programs that can act as plans to organize such a two-level search. Discrete latent codes are appealing for this purpose, and can be learned by applying recent work on discrete autoencoders. Based on these insights, we introduce the Latent Programmer (LP), a program synthesis method that first predicts a discrete latent code from input/output examples, and then generates the program in the target language. We evaluate the LP on two domains, demonstrating that it yields an improvement in accuracy, especially on longer programs for which search is most difficult.', 'Exploiting Code Symmetries for Learning Program Semantics This paper tackles the challenge of teaching code semantics to Large Language Models (LLMs) for program analysis by incorporating code symmetries into the model architecture. We introduce a group-theoretic framework that defines code symmetries as semantics-preserving transformations, where forming a code symmetry group enables precise and efficient reasoning of code semantics. Our solution, SymC, develops a novel variant of self-attention that is provably equivariant to code symmetries from the permutation group defined over the program dependence graph. SymC obtains superior performance on five program analysis tasks, outperforming state-of-the-art code models, including GPT-4, without any pre-training. Our results suggest that code LLMs that encode the code structural prior via the code symmetry group generalize better and faster.', 'LEVER: Learning to Verify Language-to-Code Generation with Execution The advent of large language models trained on code (code LLMs) has led to significant progress in language-to-code generation. State-of-the-art approaches in this area combine LLM decoding with sample pruning and reranking using test cases or heuristics based on the execution results. However, it is challenging to obtain test cases for many real-world language-to-code applications, and heuristics cannot well capture the semantic features of the execution results, such as data type and value range, which often indicates the correctness of the program. In this work, we propose LEVER, a simple approach to improve language-to-code generation by learning to verify the generated programs with their execution results. Specifically, we train verifiers to determine whether a program sampled from the LLMs is correct or not based on the natural language input, the program itself and its execution results. The sampled programs are reranked by combining the verification score with the LLM generation probability, and marginalizing over programs with the same execution results. On four datasets across the domains of table QA, math QA and basic Python programming, LEVER consistently improves over the base code LLMs (4.6% to 10.9% with code-davinci-002) and achieves new state-of-the-art results on all of them.']"
191,100,191_answering_tutoring_prompts_comprehension,"['answering', 'tutoring', 'prompts', 'comprehension', 'prompting', 'explanations', 'language', 'automated', 'tutors', 'reasoning']","['Characterizing Truthfulness in Large Language Model Generations with Local Intrinsic Dimension We study how to characterize and predict the truthfulness of texts generated from large language models (LLMs), which serves as a crucial step in building trust between humans and LLMs. Although several approaches based on entropy or verbalized uncertainty have been proposed to calibrate model predictions, these methods are often intractable, sensitive to hyperparameters, and less reliable when applied in generative tasks with LLMs. In this paper, we suggest investigating internal activations and quantifying LLM’s truthfulness using the local intrinsic dimension (LID) of model activations. Through experiments on four question answering (QA) datasets, we demonstrate the effectiveness of our proposed method. Additionally, we study intrinsic dimensions in LLMs and their relations with model layers, autoregressive language modeling, and the training of LLMs, revealing that intrinsic dimensions can be a powerful approach to understanding LLMs.', 'Improving Factuality and Reasoning in Language Models through Multiagent Debate Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such ""society of minds"" approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding.', 'Automated Assessment of Students’ Code Comprehension using LLM Assessing students’ answers and in particular natural language answers is a crucial challenge in the field of education. Advances in transformer-based models such as Large Language Models(LLMs), have led to significant progress in various natural language tasks. Nevertheless, amidst the growing trend of evaluating LLMs across diverse tasks, evaluating LLMs in the realm of automated answer assessment has not received much attention. To address this gap, we explore the potential of using LLMs for automated assessment of student’s short and open-ended answers in program comprehension tasks. Particularly, we use LLMs to compare students’ explanations with expert explanations in the context of line-by-line explanations of computer programs. For comparison purposes, we assess both decoder-only Large Language Models (LLMs) and encoder-based Semantic Textual Similarity (STS) models in the context of assessing the correctness of students’ explanation of computer code. Our findings indicate that decoder-only LLMs, when prompted in few-shot and chain-of-thought setting perform comparable to fine-tuned encoder-based models in evaluating students’ short answers in the programming domain.']"
192,22,192_domainliftability_domainliftable_probabilistic_lifted,"['domainliftability', 'domainliftable', 'probabilistic', 'lifted', 'lifting', 'liftable', 'markov', 'inference', 'relational', 'algorithms']","['Generating and Sampling Orbits for Lifted Probabilistic Inference A key goal in the design of probabilistic inference algorithms is identifying and exploit- ing properties of the distribution that make inference tractable. Lifted inference algorithms identify symmetry as a property that enables efficient inference and seek to scale with the degree of symmetry of a probability model. A limitation of existing exact lifted inference techniques is that they do not apply to non- relational representations like factor graphs. In this work we provide the first example of an exact lifted inference algorithm for arbitrary discrete factor graphs. In addition we describe a lifted Markov-Chain Monte-Carlo algorithm that provably mixes rapidly in the degree of symmetry of the distribution.', 'Lifted MAP Inference for Markov Logic Networks In this paper, we present a new approach for lifted MAP inference in Markov Logic Networks (MLNs). Our approach is based on the following key result that we prove in the paper: if an MLN has no shared terms then MAP inference over it can be reduced to MAP inference over a Markov network having the following properties: (i) the number of random variables in the Markov network is equal to the number of first-order atoms in the MLN; and (ii) the domain size of each variable in the Markov network is equal to the number of groundings of the corresponding first-order atom. We show that inference over this Markov network is exponentially more efficient than ground inference, namely inference over the Markov network obtained by grounding all first-order atoms in the MLN. We improve this result further by showing that if non-shared MLNs contain no self joins, namely every atom appears at most once in each of its formulas, then all variables in the corresponding Markov network need only be bi-valued.  Our approach is quite general and can be easily applied to an arbitrary MLN by simply grounding all of its shared terms. The key feature of our approach is that because we reduce lifted inference to propositional inference, we can use any propositional MAP inference algorithm for performing lifted MAP inference. Within our approach, we experimented with two propositional MAP inference algorithms: Gurobi and MaxWalkSAT. Our experiments on several benchmark MLNs clearly demonstrate our approach is superior to ground MAP inference in terms of scalability and solution quality.', 'Lifted Weight Learning of Markov Logic Networks Revisited We study lifted weight learning of Markov logic networks. We show that there is an algorithm for maximum-likelihood learning of 2-variable Markov logic networks which runs in time polynomial in the domain size. Our results are based on existing lifted-inference algorithms and recent algorithmic results on computing maximum entropy distributions.']"
193,26,193_matchings_matching_multigraph_subgraph,"['matchings', 'matching', 'multigraph', 'subgraph', 'correspondences', 'correspondence', 'graphs', 'algorithms', 'match', 'bipartite']","['Random Graph Matching with Improved Noise Robustness Graph matching, also known as network alignment, refers to finding a bijection between the vertex sets of two given graphs so as to maximally align their edges. This fundamental computational problem arises frequently in multiple fields such as computer vision and biology. Recently, there has been a plethora of work studying efficient algorithms for graph matching under probabilistic models. In this work, we propose a new algorithm for graph matching: Our algorithm associates each vertex with a signature vector using a multistage procedure and then matches a pair of vertices from the two graphs if their signature vectors are close to each other. We show that, for two Erdős–Rényi graphs with edge correlation $1-\\alpha$, our algorithm recovers the underlying matching exactly with high probability when $\\alpha \\le 1 / (\\log \\log n)^C$, where $n$ is the number of vertices in each graph and $C$ denotes a positive universal constant. This improves the condition $\\alpha \\le 1 / (\\log n)^C$ achieved in previous work.', 'Efficient Algorithms for Exact Graph Matching on Correlated Stochastic Block Models with Constant Correlation We consider the problem of graph matching, or learning vertex correspondence, between two correlated stochastic block models (SBMs). The graph matching problem arises in various fields, including computer vision, natural language processing and bioinformatics, and in particular, matching graphs with inherent community structure has significance related to de-anonymization of correlated social networks. Compared to the correlated Erdos-Renyi (ER) model, where various efficient algorithms have been developed, among which a few algorithms have been proven to achieve the exact matching with constant edge correlation, no low-order polynomial algorithm has been known to achieve exact matching for the correlated SBMs with constant correlation. In this work, we propose an efficient algorithm for matching graphs with community structure, based on the comparison between partition trees rooted from each vertex, by extending the idea of Mao et al. (2021) to graphs with communities. The partition tree divides the large neighborhoods of each vertex into disjoint subsets using their edge statistics to different communities. Our algorithm is the first low-order polynomial-time algorithm achieving exact matching between two correlated SBMs with high probability in dense graphs.', 'Stochastic Iterative Graph Matching Recent works apply Graph Neural Networks (GNNs) to graph matching tasks and show promising results. Considering that model outputs are complex matchings, we devise several techniques to improve the learning of GNNs and obtain a new model, Stochastic Iterative Graph MAtching (SIGMA). Our model predicts a distribution of matchings, instead of a single matching, for a graph pair so the model can explore several probable matchings. We further introduce a novel multi-step matching procedure, which learns how to refine a graph pair’s matching results incrementally. The model also includes dummy nodes so that the model does not have to find matchings for nodes without correspondence. We fit this model to data via scalable stochastic optimization. We conduct extensive experiments across synthetic graph datasets as well as biochemistry and computer vision applications. Across all tasks, our results show that SIGMA can produce significantly improved graph matching results compared to state-of-the-art models. Ablation studies verify that each of our components (stochastic training, iterative matching, and dummy nodes) offers noticeable improvement.']"
194,90,194_communities_graphs_subgraphs_subgraph,"['communities', 'graphs', 'subgraphs', 'subgraph', 'nodes', 'hypergraph', 'graphon', 'community', 'graph', 'edges']","['Computational Lower Bounds for Community Detection on Random Graphs This paper studies the problem of detecting the presence of a small dense community planted in a large Erdős-Rényi random graph \\calG(N,q), where the edge probability within the community exceeds q by a constant factor. Assuming the hardness of the planted clique detection problem, we show that the  computational  complexity of detecting the community exhibits the following phase transition phenomenon: As the graph size N grows and the graph becomes sparser according to q=N^-α, there exists a critical value of α= \\frac23, below which there exists a computationally intensive procedure that can detect far smaller communities than any computationally efficient procedure, and above which a linear-time procedure is statistically optimal. The results also lead to the average-case hardness results for recovering the dense community and approximating the densest K-subgraph. ', 'Community Recovery in the Degree-Heterogeneous Stochastic Block Model We consider the problem of recovering communities in a random directed graph with planted communities. To model real-world directed graphs such as the Twitter or Instagram graphs that exhibit very heterogeneous degree sequences, we introduce the Degree-Heterogeneous Stochastic Block Model (DHSBM), a generalization of the classic Stochastic Block Model (SBM), where the vertex set is partitioned into communities and each vertex $u$ has two (unknown) associated probabilities, $p_u$ and $q_u$, $p_u  > q_u$. An arc from $u$ to $v$ is generated with probability $p_u$ if $u$ and $v$ are in the same community and with probability $q_u$ otherwise.  Given a graph generated from this model, the goal is to retrieve the communities. The DHSBM allows to generate graphs with planted communities while allowing heterogeneous degree distributions, a quite important feature of real-world networks.  In the case where there are two communities, we present an iterative greedy linear-time algorithm that recovers them whenever $\\min_u \\frac{p_u - q_u}{\\sqrt{p_u}} = \\Omega(\\sqrt{\\log (n)/n})$. We also show that, up to a constant, this condition is necessary. Our results also extend to the standard (undirected) SBM, where $p_u = p$ and $q_u= q$ for all nodes $u$. Our algorithm presents the first linear-time algorithm that recovers exactly the communities at the asymptotic information-theoretic threshold, improving over previous near-linear time spectral approaches.', 'Exact Community Recovery in Correlated Stochastic Block Models We consider the problem of learning latent community structure from multiple correlated networks. We study edge-correlated stochastic block models with two balanced communities, focusing on the regime where the average degree is logarithmic in the number of vertices. Our main result derives the precise information-theoretic threshold for exact community recovery using multiple correlated graphs. This threshold captures the interplay between the community recovery and graph matching tasks. In particular, we uncover and characterize a region of the parameter space where exact community recovery is possible using multiple correlated graphs, even though (1) this is information-theoretically impossible using a single graph and (2) exact graph matching is also information-theoretically impossible. In this regime, we develop a novel algorithm that carefully synthesizes algorithms from the community recovery and graph matching literatures.']"
195,116,195_bayesian_bayes_networks_probabilistic,"['bayesian', 'bayes', 'networks', 'probabilistic', 'models', 'nodes', 'inference', 'learning', 'classifiers', 'likelihood']","['A Score-and-Search Approach to Learning Bayesian Networks with Noisy-OR Relations A Bayesian network is a probabilistic graphical model that consists of a directed acyclic graph (DAG), where each node is a random variable and attached to each node is a conditional probability distribution (CPD). A Bayesian network can be learned from data using the well-known score-and-search approach, and within this approach a key consideration is how to simultaneously learn the global structure in the form of the underlying DAG and the local structure in the CPDs. Several useful forms of local structure have been identified in the literature but thus far the score-and-search approach has only been extended to handle local structure in form of context-specific independence. In this paper, we show how to extend the score-and-search approach to the important and widely useful case of noisy-OR relations. We provide an effective gradient descent algorithm to score a candidate noisy-OR using the widely used BIC score and we provide pruning rules that allow the search to successfully scale to medium sized networks. Our empirical results provide evidence for the success of our approach to learning Bayesian networks that incorporate noisy-OR relations.', 'Using Mixed-Effects Models to Learn Bayesian Networks from Related Data Sets We commonly assume that data are a homogeneous set of observations when learning the structure of Bayesian networks. However, they often comprise different data sets that are related but not homogeneous because they have been collected in different ways or from different populations. In a previous work, we proposed a closed-form Bayesian Hierarchical Dirichlet score for discrete data that pools information across related data sets to learn a single encompassing network structure, while taking into account the differences in their probabilistic structures. In this paper, we provide an analogous solution for learning a Bayesian network from continuous data using mixed-effects models to pool information across the related data sets. We study its structural, parametric, predictive and classification accuracy and we show that it outperforms both conditional Gaussian Bayesian networks (that do not perform any pooling) and classical Gaussian Bayesian networks (that disregard the heterogeneous nature of the data). The improvement is marked for low sample sizes and for unbalanced data sets. ', 'Finding Optimal Bayesian Networks with Local Structure The idea of using decision trees as local models in Bayesian networks is revisited. A class of dyadic decision trees—proposed previously only for continuous conditioning variables—is augmented by incorporating categorical variables with arbitrary context-specific recursive splitting of their state spaces. It is shown that the resulting model class admits computationally feasible maximization of a Bayes score in a range of moderate-size problem instances. In particular, it enables global optimization of the Bayesian network structure, including the local structure, using state-of-the-art exact algorithms.  The paper also introduces a related model class that extends ordinary conditional probability tables to continuous variables by employing an adaptive discretization approach.  The two model classes are compared empirically by learning Bayesian networks from benchmark real-world and synthetic data sets. The relative strengths of the model classes are discussed.']"
196,37,196_networks_sumproductquotient_sumproductmax_sumproduct,"['networks', 'sumproductquotient', 'sumproductmax', 'sumproduct', 'learnspn', 'probabilistic', 'inference', 'spn', 'generative', 'models']","['Collapsed Variational Inference for Sum-Product Networks Sum-Product Networks (SPNs) are probabilistic inference machines that admit exact inference in linear time in the size of the network. Existing parameter learning approaches for SPNs are largely based on the maximum likelihood principle and are subject to overfitting compared to more Bayesian approaches. Exact Bayesian posterior inference for SPNs is computationally intractable. Even approximation techniques such as standard variational inference and posterior sampling for SPNs are computationally infeasible even for networks of moderate size due to the large number of local latent variables per instance. In this work, we propose a novel deterministic collapsed variational inference algorithm for SPNs that is computationally efficient, easy to implement and at the same time allows us to incorporate prior information into the optimization formulation. Extensive experiments show a significant improvement in accuracy compared with a maximum likelihood based approach.', 'Learning Sum-Product Networks with Direct and Indirect Variable Interactions Sum-product networks (SPNs) are a deep probabilistic representation that allows for efficient, exact inference.  SPNs generalize many other tractable models, including thin junction trees, latent tree models, and many types of mixtures.  Previous work on learning SPN structure has mainly focused on using top-down or bottom-up clustering to find mixtures, which capture variable interactions indirectly through implicit latent variables.  In contrast, most work on learning graphical models, thin junction trees, and arithmetic circuits has focused on finding direct interactions among variables.  In this paper, we present ID-SPN, a new algorithm for learning SPN structure that unifies the two approaches. In experiments on 20 benchmark datasets, we find that the combination of direct and indirect interactions leads to significantly better accuracy than several state-of-the-art algorithms for learning SPNs and other tractable models.', 'Learning the Structure of Sum-Product Networks Sum-product networks (SPNs) are a new class of deep probabilistic models. SPNs can have unbounded treewidth but inference in them is always tractable. An SPN is either a univariate distribution, a product of SPNs over disjoint variables, or a weighted sum of SPNs over the same variables. We propose the first algorithm for learning the structure of SPNs that takes full advantage of their expressiveness. At each step, the algorithm attempts to divide the current variables into approximately independent subsets. If successful, it returns the product of recursive calls on the subsets; otherwise it returns the sum of recursive calls on subsets of similar instances from the current training set. A comprehensive empirical study shows that the learned SPNs are typically comparable to graphical models in likelihood but superior in inference speed and accuracy.']"
197,13,197_estimating_estimators_estimation_information,"['estimating', 'estimators', 'estimation', 'information', 'nonparametric', 'mutual', 'likelihood', 'estimator', 'nonparanormal', 'marginalinvariant']","['Fast Variational Estimation of Mutual Information for Implicit and Explicit Likelihood Models Computing mutual information (MI) of random variables lacks a closed-form in nontrivial models. Variational MI approximations are widely used as flexible estimators for this purpose, but computing them typically requires solving a costly nonconvex optimization. We prove that a widely used class of variational MI estimators can be solved via moment matching operations in place of the numerical optimization methods that are typically required. We show that the same moment matching solution yields variational estimates for so-called “implicit” models that lack a closed form likelihood function. Furthermore, we demonstrate that this moment matching solution has multiple orders of magnitude computational speed up compared to the standard optimization based solutions. We show that theoretical results are supported by numerical evaluation in fully parameterized Gaussian mixture models and a generalized linear model with implicit likelihood due to nuisance variables. We also demonstrate on the implicit simulation-based likelihood SIR epidemiology model, where we avoid costly likelihood free inference and observe many orders of magnitude speedup.', 'Nonparanormal Information Estimation We study the problem of using i.i.d. samples from an unknown multivariate probability distribution p to estimate the mutual information of p. This problem has recently received attention in two settings: (1) where p is assumed to be Gaussian and (2) where p is assumed only to lie in a large nonparametric smoothness class. Estimators proposed for the Gaussian case converge in high dimensions when the Gaussian assumption holds, but are brittle, failing dramatically when p is not Gaussian, while estimators proposed for the nonparametric case fail to converge with realistic sample sizes except in very low dimension. Hence, there is a lack of robust mutual information estimators for many realistic data. To address this, we propose estimators for mutual information when p is assumed to be a nonparanormal (or Gaussian copula) model, a semiparametric compromise between Gaussian and nonparametric extremes. Using theoretical bounds and experiments, we show these estimators strike a practical balance between robustness and scalability.', 'Estimating Total Correlation with Mutual Information Estimators Total correlation (TC) is a fundamental concept in information theory that measures statistical dependency among multiple random variables. Recently, TC has shown noticeable effectiveness as a regularizer in many learning tasks, where the correlation among multiple latent embeddings requires to be jointly minimized or maximized. However, calculating precise TC values is challenging, especially when the closed-form distributions of embedding variables are unknown. In this paper, we introduce a unified framework to estimate total correlation values with sample-based mutual information (MI) estimators. More specifically, we discover a relation between TC and MI and propose two types of calculation paths (tree-like and line-like) to decompose TC into MI terms. With each MI term being bounded, the TC values can be successfully estimated. Further, we provide theoretical analyses concerning the statistical consistency of the proposed TC estimators. Experiments are presented on both synthetic and real-world scenarios, where our estimators demonstrate effectiveness in all TC estimation, minimization, and maximization tasks.']"
198,164,198_gans_gan_generative_netgan,"['gans', 'gan', 'generative', 'netgan', 'adversarial', 'inception', 'generating', 'generators', 'networks', 'deep']","['A Classification-Based Study of Covariate Shift in GAN Distributions A basic, and still largely unanswered, question in the context of Generative Adversarial Networks (GANs) is whether they are truly able to capture all the fundamental characteristics of the distributions they are trained on. In particular, evaluating the diversity of GAN distributions is challenging and existing methods provide only a partial understanding of this issue. In this paper, we develop quantitative and scalable tools for assessing the diversity of GAN distributions. Specifically, we take a classification-based perspective and view loss of diversity as a form of covariate shift introduced by GANs. We examine two specific forms of such shift: mode collapse and boundary distortion. In contrast to prior work, our methods need only minimal human supervision and can be readily applied to state-of-the-art GANs on large, canonical datasets. Examining popular GANs using our tools indicates that these GANs have significant problems in reproducing the more distributional properties of their training dataset.', 'Do GANs always have Nash equilibria? Generative adversarial networks (GANs) represent a zero-sum game between two machine players, a generator and a discriminator, designed to learn the distribution of data. While GANs have achieved state-of-the-art performance in several benchmark learning tasks, GAN minimax optimization still poses great theoretical and empirical challenges. GANs trained using first-order optimization methods commonly fail to converge to a stable solution where the players cannot improve their objective, i.e., the Nash equilibrium of the underlying game. Such issues raise the question of the existence of Nash equilibria in GAN zero-sum games. In this work, we show through theoretical and numerical results that indeed GAN zero-sum games may have no Nash equilibria. To characterize an equilibrium notion applicable to GANs, we consider the equilibrium of a new zero-sum game with an objective function given by a proximal operator applied to the original objective, a solution we call the proximal equilibrium. Unlike the Nash equilibrium, the proximal equilibrium captures the sequential nature of GANs, in which the generator moves first followed by the discriminator. We prove that the optimal generative model in Wasserstein GAN problems provides a proximal equilibrium. Inspired by these results, we propose a new approach, which we call proximal training, for solving GAN problems. We perform several numerical experiments indicating the existence of proximal equilibria in GANs.', 'Bridging the Gap Between f-GANs and Wasserstein GANs Generative adversarial networks (GANs) variants approximately minimize divergences between the model and the data distribution using a discriminator. Wasserstein GANs (WGANs) enjoy superior empirical performance, however, unlike in f-GANs, the discriminator does not provide an estimate for the ratio between model and data densities, which is useful in applications such as inverse reinforcement learning. To overcome this limitation, we propose an new training objective where we additionally optimize over a set of importance weights over the generated samples. By suitably constraining the feasible set of importance weights, we obtain a family of objectives which includes and generalizes the original f-GAN and WGAN objectives. We show that a natural extension outperforms WGANs while providing density ratios as in f-GAN, and demonstrate empirical success on distribution modeling, density ratio estimation and image generation.']"
199,45,199_features_feature_supervised_classification,"['features', 'feature', 'supervised', 'classification', 'selection', 'svm', 'selecting', 'dimensionality', 'selected', 'kernel']","['Contextual Feature Selection with Conditional Stochastic Gates Feature selection is a crucial tool in machine learning and is widely applied across various scientific disciplines. Traditional supervised methods generally identify a universal set of informative features for the entire population. However, feature relevance often varies with context, while the context itself may not directly affect the outcome variable. Here, we propose a novel architecture for contextual feature selection where the subset of selected features is conditioned on the value of <em>context variables</em>. Our new approach, Conditional Stochastic Gates (c-STG), models the importance of features using conditional Bernoulli variables whose parameters are predicted based on contextual variables. We introduce a hypernetwork that maps context variables to feature selection parameters to learn the context-dependent gates along with a prediction model. We further present a theoretical analysis of our model, indicating that it can improve performance and flexibility over population-level methods in complex feature selection settings. Finally, we conduct an extensive benchmark using simulated and real-world datasets across multiple domains demonstrating that c-STG can lead to improved feature selection capabilities while enhancing prediction accuracy and interpretability.', 'Kernel Feature Selection via Conditional Covariance Minimization <p>Feature selection is a common method for dimensionality reduction that\nencourages model interpretability. With large data sets becoming ever more\nprevalent, feature selection has seen widespread usage across a variety of\nreal-world tasks in recent years, including text classification, gene selection\nfrom microarray data, and face recognition. We study the problem of supervised\nfeature selection, which entails finding a subset of the input features that\nexplains the output well. This practice can reduce the computational expense of\ndownstream learning by removing features that are redundant or noisy, while\nsimultaneously providing insight into the data through the features that remain.</p>\n\n<p>Feature selection algorithms can generally be divided into three main\ncategories: filter methods, wrapper methods, and embedded methods. Filter\nmethods select features based on intrinsic properties of the data, independent\nof the learning algorithm to be used. For example, we may compute the\ncorrelation between each feature and the response variable, and select the\nvariables with the highest correlation. Wrapper methods are more specialized in\ncontrast, aiming to find features that optimize the performance of a specific\npredictor. For example, we may train multiple SVMs, each with a different subset\nof features, and choose the subset of features with the lowest loss on the\ntraining data. Because there are exponentially many subsets of features, wrapper\nmethods often employ greedy algorithms. Finally, embedded methods are\nmultipurpose techniques that incorporate feature selection and prediction into a\nsingle problem, often by optimizing an objective combining a goodness-of-fit\nterm with a penalty on the number of parameters. One example is the LASSO method\nfor constructing a linear model, which penalizes the coefficients with an\n$\\ell_1$ penalty.</p>\n\n<p>In this post, we propose conditional covariance minimization (CCM), a feature\nselection method that aims to unify the first two perspectives. We first\ndescribe our approach in the sections that follow. We then demonstrate through\nseveral synthetic experiments that our method is capable of capturing joint\nnonlinear relationships between collections of features. Finally, we show that\nour algorithm has performance comparable to or better than several other popular\nfeature selection algorithms on a variety of real-world tasks.</p>\n\n<!--more-->\n\n<h1 id=""formulating-feature-selection"">Formulating feature selection</h1>\n\n<p>One way to view the problem of feature selection is from the lens of dependence.\nIdeally, we would like to identify a subset of features\n$\\mathcal{T}$ of a pre-selected size $m$ such that the remaining features are\nconditionally independent of the responses given $\\mathcal{T}$. However, this\nmay not be achievable when $m$ is small. We therefore quantify the extent of the\nremaining conditional dependence using some metric, and aim to minimize it over\nall subsets $\\mathcal{T}$ of the appropriate size.</p>\n\n<p>Alternatively, we might like to find the subset of features $\\mathcal{T}$ that\ncan most effectively predict the output $Y$ within the context of a specific learning\nproblem. The prediction error in our framework is defined as the mean square\nerror between the labels and the predictions made by the best classifier\nselected from a class of functions.</p>\n\n<h1 id=""our-method"">Our method</h1>\n\n<p>We propose a criterion that can simultaneously characterize dependence and\nprediction error in regression. Roughly, we first introduce two function spaces\non the domain of a subset of features $X_\\mathcal{T}$ and the domain of the\nresponse variable $Y$ respectively. Each function space is a complete inner\nproduct space (Hilbert space) equipped with a kernel function which spans the\nwhole space and has the ‘‘reproducing property’’. Such a function space is\ncalled a <em>Reproducing Kernel Hilbert Space</em> (RKHS). Then we define an operator\nfor the RKHS over the domain of the response variable that characterizes the\nconditional dependence of the response variable on the input data given the\nselected features. Such an operator is called the <em>conditional covariance\noperator</em>. We use the trace of the operator computed with respect to the\nempirical distribution as our optimization criterion, which is also the\nestimated regression error of the best predictor within the given RKHS over the\ndomain of the input data.  Directly minimizing this criterion over subsets of\nfeatures is computationally intractable. Instead, we formulate a relaxed problem\nby weighting each feature with a real-valued scalar between 0 and 1, and add an\n$\\ell_1$-penalty over the weights. The objective of the relaxed problem can be\nrepresented in terms of kernel matrices, and is readily optimized using\ngradient-based approaches.</p>\n\n<h1 id=""results"">Results</h1>\n\n<p>We evaluate our approach on both synthetic and real-world data sets. We compare\nwith several strong existing algorithms, including recursive feature elimination\n(RFE), Minimum Redundancy Maximum Relevance (mRMR), BAHSIC, and filter methods\nusing mutual information (MI) and Pearson’s correlation (PC). RFE is a very\npopular wrapper method that greedily selects features based on the scores they\nreceive from a classifier. mRMR selects features that capture different\ninformation from one another but each correlate well with the response variable.\nBAHSIC is a kernel method that greedily optimizes the dependence between\nselected features and the response variable. Lastly, filter methods employing MI\nor PC greedily optimize the respective metrics between selected subsets of\nfeatures and the response.</p>\n\n<h2 id=""synthetic-data"">Synthetic data</h2>\n\n<p>We use the following synthetic data sets:</p>\n\n<ul>\n  <li>\n    <p>Orange Skin. Given $Y=-1$, ten features $(X_1,\\dots,X_{10})$ are independent\nstandard normal random variables. Given $Y=1$, the first four features are\nstandard normal random variables conditioned on $9 \\leq \\sum_{j=1}^4 X_j^2\n\\leq 16$, and the remaining six features $(X_5,\\dots,X_{10})$ are independent\nstandard normal random variables.</p>\n  </li>\n  <li>\n    <p>3-dimensional XOR as 4-way classification. Consider the 8 corners of the\n3-dimensional hypercube $(v_1, v_2, v_3) \\in \\{-1,1\\}^3$, and group them by\nthe tuples $(v_1 v_3, v_2 v_3)$, leaving 4 sets of vectors paired with their\nnegations $\\{v^{(i)}, -v^{(i)}\\}$. Given a class $i$, a sample is generated\nby selecting $v^{(i)}$ or $-v^{(i)}$ with equal probability and adding some\nnoise.  Each sample additionally has 7 standard normal noise features for a\ntotal of 10 dimensions.</p>\n  </li>\n  <li>\n    <p>Additive nonlinear regression. Consider the following additive model:</p>\n\n    <script type=""math/tex; mode=display"">Y=-2\\sin(2X_1)+\\max(X_2,0)+X_3+\\exp(-X_4)+\\varepsilon.</script>\n\n    <p>Each sample additionally has 6 noise features for a total of 10 dimensions.\nAll features and the noise $\\varepsilon$ are generated from standard normal\ndistributions.</p>\n  </li>\n</ul>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/static/blog/kernels/synthetic_data.png"" alt=""orange_skin"" /><br />\n<i>\nLeft: Orange Skin in 2d. Right: XOR in 2d.\n</i>\n</p>\n\n<p>The first data set represents a standard nonlinear binary classification task.\nThe second data set is a multi-class classification task where each feature is\nindependent of $Y$ by itself but a combination of three features has a joint\neffect on $Y$. The third data set arises from an additive model for nonlinear\nregression.</p>\n\n<p>Each data set has $d=10$ dimensions in total, but only $m=3$ or $4$ true\nfeatures. Since the identity of these features is known, we can evaluate the\nperformance of a given feature selection algorithm by computing the median rank\nit assigns to the real features, with lower median ranks indicating better\nperformance.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/static/blog/kernels/synthetic_results.png"" alt=""synthetic_results"" /><br />\n<i>\nThe above plots show the median rank (y-axis) of the true features as\na function of sample size (x-axis) for the simulated data sets. Lower median\nranks are better. The dotted line indicates the optimal median rank.\n</i>\n</p>\n\n<p>On the binary and 4-way classification tasks, our method outperforms all other\nalgorithms, succeeding in identifying the true features using fewer than 50\nsamples where others require close to 100 or even fail to converge. On the\nadditive nonlinear model, several algorithms perform well, and our method is on\npar with the best of them across all sample sizes.</p>\n\n<h2 id=""real-world-data"">Real-world data</h2>\n\n<p>We now turn our attention to a collection of real-word tasks, studying the\nperformance of our method and other nonlinear approaches (mRMR, BAHSIC, MI) when\nused in conjunction with a kernel SVM for downstream classification.</p>\n\n<p>We carry out experiments on 12 standard benchmark tasks from the ASU feature\nselection website and the UCI repository. A summary of our data sets is provided\nin the following table.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/static/blog/kernels/table.png"" alt=""table"" width=""600"" /><br />\n</p>\n\n<p>The data sets are drawn from several domains including gene data, image data,\nand voice data, and span both the low-dimensional and high-dimensional regimes.</p>\n\n<p>For every task, we run each algorithm being evaluated to obtain ranks for all\nfeatures. Performance is then measured by training a kernel SVM on the top $m$\nfeatures and computing the resulting accuracy. Our results are shown in the\nfollowing figures.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/static/blog/kernels/real_results.png"" alt=""real_results"" /><br />\n<i>\nThe above plots show classification accuracy (y-axis) versus number of\nselected features (x-axis) for our real-world benchmark data sets. Higher\naccuracies are better.\n</i>\n</p>\n\n<p>Compared with three other popular methods for nonlinear feature selection, we\nfind that our method is the strongest performer in the large majority of cases,\nsometimes by a substantial margin as in the case of TOX-171. While our method is\noccasionally outperformed in the beginning when the number of selected features\nis small, it either ties or overtakes the leading method by the end in all but\none instance.</p>\n\n<h1 id=""conclusion"">Conclusion</h1>\n\n<p>In this post, we propose conditional covariance minimization (CCM), an approach\nto feature selection based on minimizing the trace of the conditional covariance\noperator. The idea is to select the features that maximally account for the\ndependence of the response on the covariates. We accomplish this by relaxing an\nintractable discrete formulation of the problem to obtain a continuous\napproximation suitable for gradient-based optimization. We demonstrate the\neffectiveness of our approach on multiple synthetic and real-world experiments,\nfinding that it often outperforms other state-of-the-art approaches, including\nanother competitive kernel feature selection method based on the Hilbert-Schmidt\nindependence criterion.</p>\n\n<h1 id=""more-information"">More Information</h1>\n\n<p>For more information about our algorithm, please take a look at the following links:</p>\n\n<ul>\n  <li><a href=""https://papers.nips.cc/paper/7270-kernel-feature-selection-via-conditional-covariance-minimization"">Our paper</a></li>\n  <li><a href=""https://github.com/Jianbo-Lab/CCM"">Our code</a></li>\n</ul>\n\n<p>Please let us know if you have any questions or suggestions.</p>\n\n', 'Kernel Feature Selection via Conditional Covariance Minimization <p>Feature selection is a common method for dimensionality reduction that\nencourages model interpretability. With large data sets becoming ever more\nprevalent, feature selection has seen widespread usage across a variety of\nreal-world tasks in recent years, including text classification, gene selection\nfrom microarray data, and face recognition. We study the problem of supervised\nfeature selection, which entails finding a subset of the input features that\nexplains the output well. This practice can reduce the computational expense of\ndownstream learning by removing features that are redundant or noisy, while\nsimultaneously providing insight into the data through the features that remain.</p>\n\n<p>Feature selection algorithms can generally be divided into three main\ncategories: filter methods, wrapper methods, and embedded methods. Filter\nmethods select features based on intrinsic properties of the data, independent\nof the learning algorithm to be used. For example, we may compute the\ncorrelation between each feature and the response variable, and select the\nvariables with the highest correlation. Wrapper methods are more specialized in\ncontrast, aiming to find features that optimize the performance of a specific\npredictor. For example, we may train multiple SVMs, each with a different subset\nof features, and choose the subset of features with the lowest loss on the\ntraining data. Because there are exponentially many subsets of features, wrapper\nmethods often employ greedy algorithms. Finally, embedded methods are\nmultipurpose techniques that incorporate feature selection and prediction into a\nsingle problem, often by optimizing an objective combining a goodness-of-fit\nterm with a penalty on the number of parameters. One example is the LASSO method\nfor constructing a linear model, which penalizes the coefficients with an\n$\\ell_1$ penalty.</p>\n\n<p>In this post, we propose conditional covariance minimization (CCM), a feature\nselection method that aims to unify the first two perspectives. We first\ndescribe our approach in the sections that follow. We then demonstrate through\nseveral synthetic experiments that our method is capable of capturing joint\nnonlinear relationships between collections of features. Finally, we show that\nour algorithm has performance comparable to or better than several other popular\nfeature selection algorithms on a variety of real-world tasks.</p>\n\n<!--more-->\n\n<h1 id=""formulating-feature-selection"">Formulating feature selection</h1>\n\n<p>One way to view the problem of feature selection is from the lens of dependence.\nIdeally, we would like to identify a subset of features\n$\\mathcal{T}$ of a pre-selected size $m$ such that the remaining features are\nconditionally independent of the responses given $\\mathcal{T}$. However, this\nmay not be achievable when $m$ is small. We therefore quantify the extent of the\nremaining conditional dependence using some metric, and aim to minimize it over\nall subsets $\\mathcal{T}$ of the appropriate size.</p>\n\n<p>Alternatively, we might like to find the subset of features $\\mathcal{T}$ that\ncan most effectively predict the output $Y$ within the context of a specific learning\nproblem. The prediction error in our framework is defined as the mean square\nerror between the labels and the predictions made by the best classifier\nselected from a class of functions.</p>\n\n<h1 id=""our-method"">Our method</h1>\n\n<p>We propose a criterion that can simultaneously characterize dependence and\nprediction error in regression. Roughly, we first introduce two function spaces\non the domain of a subset of features $X_\\mathcal{T}$ and the domain of the\nresponse variable $Y$ respectively. Each function space is a complete inner\nproduct space (Hilbert space) equipped with a kernel function which spans the\nwhole space and has the ‘‘reproducing property’’. Such a function space is\ncalled a <em>Reproducing Kernel Hilbert Space</em> (RKHS). Then we define an operator\nfor the RKHS over the domain of the response variable that characterizes the\nconditional dependence of the response variable on the input data given the\nselected features. Such an operator is called the <em>conditional covariance\noperator</em>. We use the trace of the operator computed with respect to the\nempirical distribution as our optimization criterion, which is also the\nestimated regression error of the best predictor within the given RKHS over the\ndomain of the input data.  Directly minimizing this criterion over subsets of\nfeatures is computationally intractable. Instead, we formulate a relaxed problem\nby weighting each feature with a real-valued scalar between 0 and 1, and add an\n$\\ell_1$-penalty over the weights. The objective of the relaxed problem can be\nrepresented in terms of kernel matrices, and is readily optimized using\ngradient-based approaches.</p>\n\n<h1 id=""results"">Results</h1>\n\n<p>We evaluate our approach on both synthetic and real-world data sets. We compare\nwith several strong existing algorithms, including recursive feature elimination\n(RFE), Minimum Redundancy Maximum Relevance (mRMR), BAHSIC, and filter methods\nusing mutual information (MI) and Pearson’s correlation (PC). RFE is a very\npopular wrapper method that greedily selects features based on the scores they\nreceive from a classifier. mRMR selects features that capture different\ninformation from one another but each correlate well with the response variable.\nBAHSIC is a kernel method that greedily optimizes the dependence between\nselected features and the response variable. Lastly, filter methods employing MI\nor PC greedily optimize the respective metrics between selected subsets of\nfeatures and the response.</p>\n\n<h2 id=""synthetic-data"">Synthetic data</h2>\n\n<p>We use the following synthetic data sets:</p>\n\n<ul>\n  <li>\n    <p>Orange Skin. Given $Y=-1$, ten features $(X_1,\\dots,X_{10})$ are independent\nstandard normal random variables. Given $Y=1$, the first four features are\nstandard normal random variables conditioned on $9 \\leq \\sum_{j=1}^4 X_j^2\n\\leq 16$, and the remaining six features $(X_5,\\dots,X_{10})$ are independent\nstandard normal random variables.</p>\n  </li>\n  <li>\n    <p>3-dimensional XOR as 4-way classification. Consider the 8 corners of the\n3-dimensional hypercube $(v_1, v_2, v_3) \\in \\{-1,1\\}^3$, and group them by\nthe tuples $(v_1 v_3, v_2 v_3)$, leaving 4 sets of vectors paired with their\nnegations $\\{v^{(i)}, -v^{(i)}\\}$. Given a class $i$, a sample is generated\nby selecting $v^{(i)}$ or $-v^{(i)}$ with equal probability and adding some\nnoise.  Each sample additionally has 7 standard normal noise features for a\ntotal of 10 dimensions.</p>\n  </li>\n  <li>\n    <p>Additive nonlinear regression. Consider the following additive model:</p>\n\n    <script type=""math/tex; mode=display"">Y=-2\\sin(2X_1)+\\max(X_2,0)+X_3+\\exp(-X_4)+\\varepsilon.</script>\n\n    <p>Each sample additionally has 6 noise features for a total of 10 dimensions.\nAll features and the noise $\\varepsilon$ are generated from standard normal\ndistributions.</p>\n  </li>\n</ul>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/static/blog/kernels/synthetic_data.png"" alt=""orange_skin"" /><br />\n<i>\nLeft: Orange Skin in 2d. Right: XOR in 2d.\n</i>\n</p>\n\n<p>The first data set represents a standard nonlinear binary classification task.\nThe second data set is a multi-class classification task where each feature is\nindependent of $Y$ by itself but a combination of three features has a joint\neffect on $Y$. The third data set arises from an additive model for nonlinear\nregression.</p>\n\n<p>Each data set has $d=10$ dimensions in total, but only $m=3$ or $4$ true\nfeatures. Since the identity of these features is known, we can evaluate the\nperformance of a given feature selection algorithm by computing the median rank\nit assigns to the real features, with lower median ranks indicating better\nperformance.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/static/blog/kernels/synthetic_results.png"" alt=""synthetic_results"" /><br />\n<i>\nThe above plots show the median rank (y-axis) of the true features as\na function of sample size (x-axis) for the simulated data sets. Lower median\nranks are better. The dotted line indicates the optimal median rank.\n</i>\n</p>\n\n<p>On the binary and 4-way classification tasks, our method outperforms all other\nalgorithms, succeeding in identifying the true features using fewer than 50\nsamples where others require close to 100 or even fail to converge. On the\nadditive nonlinear model, several algorithms perform well, and our method is on\npar with the best of them across all sample sizes.</p>\n\n<h2 id=""real-world-data"">Real-world data</h2>\n\n<p>We now turn our attention to a collection of real-word tasks, studying the\nperformance of our method and other nonlinear approaches (mRMR, BAHSIC, MI) when\nused in conjunction with a kernel SVM for downstream classification.</p>\n\n<p>We carry out experiments on 12 standard benchmark tasks from the ASU feature\nselection website and the UCI repository. A summary of our data sets is provided\nin the following table.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/static/blog/kernels/table.png"" alt=""table"" width=""600"" /><br />\n</p>\n\n<p>The data sets are drawn from several domains including gene data, image data,\nand voice data, and span both the low-dimensional and high-dimensional regimes.</p>\n\n<p>For every task, we run each algorithm being evaluated to obtain ranks for all\nfeatures. Performance is then measured by training a kernel SVM on the top $m$\nfeatures and computing the resulting accuracy. Our results are shown in the\nfollowing figures.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/static/blog/kernels/real_results.png"" alt=""real_results"" /><br />\n<i>\nThe above plots show classification accuracy (y-axis) versus number of\nselected features (x-axis) for our real-world benchmark data sets. Higher\naccuracies are better.\n</i>\n</p>\n\n<p>Compared with three other popular methods for nonlinear feature selection, we\nfind that our method is the strongest performer in the large majority of cases,\nsometimes by a substantial margin as in the case of TOX-171. While our method is\noccasionally outperformed in the beginning when the number of selected features\nis small, it either ties or overtakes the leading method by the end in all but\none instance.</p>\n\n<h1 id=""conclusion"">Conclusion</h1>\n\n<p>In this post, we propose conditional covariance minimization (CCM), an approach\nto feature selection based on minimizing the trace of the conditional covariance\noperator. The idea is to select the features that maximally account for the\ndependence of the response on the covariates. We accomplish this by relaxing an\nintractable discrete formulation of the problem to obtain a continuous\napproximation suitable for gradient-based optimization. We demonstrate the\neffectiveness of our approach on multiple synthetic and real-world experiments,\nfinding that it often outperforms other state-of-the-art approaches, including\nanother competitive kernel feature selection method based on the Hilbert-Schmidt\nindependence criterion.</p>\n\n<h1 id=""more-information"">More Information</h1>\n\n<p>For more information about our algorithm, please take a look at the following links:</p>\n\n<ul>\n  <li><a href=""https://papers.nips.cc/paper/7270-kernel-feature-selection-via-conditional-covariance-minimization"">Our paper</a></li>\n  <li><a href=""https://github.com/Jianbo-Lab/CCM"">Our code</a></li>\n</ul>\n\n<p>Please let us know if you have any questions or suggestions.</p>\n\n']"
200,39,200_flow_flows_generative_normalizing,"['flow', 'flows', 'generative', 'normalizing', 'normalising', 'expressive', 'models', 'modeling', 'densities', 'likelihoods']","['VQ-Flows: Vector quantized local normalizing flows Normalizing flows provide an elegant approach to  generative modeling that allows for efficient sampling and exact density  evaluation of unknown data distributions. However, current techniques have  significant limitations in their expressivity when the data distribution  is supported on a low-dimensional manifold or has a non-trivial topology.  We introduce a novel statistical framework for learning a mixture of  local normalizing flows as “chart maps” over the data manifold.  Our framework augments the expressivity of recent approaches while  preserving the signature property of normalizing flows, that they admit  exact density evaluation. We learn a suitable atlas of charts for the data  manifold via a vector quantized auto-encoder (VQ-AE) and the distributions  over them using a conditional flow. We validate experimentally that our  probabilistic framework enables existing approaches to better model data  distributions over complex manifolds.', 'Training Normalizing Flows from Dependent Data Normalizing flows are powerful non-parametric statistical models that function as a hybrid between density estimators and generative models. Current learning algorithms for normalizing flows assume that data points are sampled independently, an assumption that is frequently violated in practice, which may lead to erroneous density estimation and data generation. We propose a likelihood objective of normalizing flows incorporating dependencies between the data points, for which we derive a flexible and efficient learning algorithm suitable for different dependency structures. We show that respecting dependencies between observations can improve empirical results on both synthetic and real-world data, and leads to higher statistical power in a downstream application to genome-wide association studies.', 'The Expressive Power of a Class of Normalizing Flow Models Normalizing flows have received a great deal of recent attention as they allow flexible generative modeling as well as easy likelihood computation. While a wide variety of flow models have been proposed, there is little formal understanding of the representation power of these models. In this work, we study some basic normalizing flows and rigorously establish bounds on their expressive power. Our results indicate that while these flows are highly expressive in one dimension, in higher dimensions their representation power may be limited, especially when the flows have moderate depth. ']"
201,205,201_graphs_graphstructured_networks_graph,"['graphs', 'graphstructured', 'networks', 'graph', 'subgraph', 'nodes', 'neural', 'mpnns', 'gnn', 'mpnn']","['A New Perspective On the Expressive Equivalence Between Graph Convolution and Attention Models Graph neural networks (GNNs) have demonstrated impressive achievements in diverse graph tasks, and research on their expressive power has experienced significant growth in recent years. The well-known Weisfeiler and Lehman (WL) isomorphism test has been widely used to assess GNNs’ ability to distinguish graph structures. However, despite being considered less expressive than other GNNs in graph-level tasks based on the WL test, two prominent GNN models, namely graph convolution networks (GCN) and attention-based graph networks (GAT), still exhibit strong performance in node-level classification tasks. In this paper, we present a comprehensive analysis of their expressive power using a novel evaluation metric: the number of linear regions. We demonstrate that by enhancing GCN with refined graph Ricci curvature, our proposed high-rank graph convolution network (HRGCN) can match or even surpass the prediction advantage of attention models. Thus, the two models exhibit equivalent node-level expressive powers. This fresh perspective highlights the evaluation of GNNs’ expressive power in node-level classifications rather than solely at the graph level. Experimental results showcase that the proposed HRGCN model outperforms the state-of-the-art in various classification and prediction tasks.', 'Graph Neural Networks Use Graphs When They Shouldn’t Predictions over graphs play a crucial role in various domains, including social networks and medicine. Graph Neural Networks (GNNs) have emerged as the dominant approach for learning on graph data. Although a graph-structure is provided as input to the GNN, in some cases the best solution can be obtained by ignoring it. While GNNs have the ability to ignore the graph-structure in such cases, it is not clear that they will. In this work, we show that GNNs actually tend to overfit the given graph-structure in the sense that they use it even when a better solution can be obtained by ignoring it. We analyze the implicit bias of gradient-descent learning of GNNs and prove that when the ground truth function does not use the graphs, GNNs are not guaranteed to learn a solution that ignores the graph, even with infinite data. We examine this phenomenon with respect to different graph distributions and find that regular graphs are more robust to this overfitting. We also prove that within the family of regular graphs, GNNs are guaranteed to extrapolate when learning with gradient descent. Finally, based on our empirical and theoretical findings, we demonstrate on real-data how regular graphs can be leveraged to reduce graph overfitting and enhance performance.', 'Graph Neural Networks can Recover the Hidden Features Solely from the Graph Structure Graph Neural Networks (GNNs) are popular models for graph learning problems. GNNs show strong empirical performance in many practical tasks. However, the theoretical properties have not been completely elucidated. In this paper, we investigate whether GNNs can exploit the graph structure from the perspective of the expressive power of GNNs. In our analysis, we consider graph generation processes that are controlled by hidden (or latent) node features, which contain all information about the graph structure. A typical example of this framework is kNN graphs constructed from the hidden features. In our main results, we show that GNNs can recover the hidden node features from the input graph alone, even when all node features, including the hidden features themselves and any indirect hints, are unavailable. GNNs can further use the recovered node features for downstream tasks. These results show that GNNs can fully exploit the graph structure by themselves, and in effect, GNNs can use both the hidden and explicit node features for downstream tasks. In the experiments, we confirm the validity of our results by showing that GNNs can accurately recover the hidden features using a GNN architecture built based on our theoretical analysis.']"
202,20,202_subgraphx_subgraph_subgraphlevel_networks,"['subgraphx', 'subgraph', 'subgraphlevel', 'networks', 'subgraphs', 'graphs', 'graph', 'nodes', 'neural', 'explanations']","['Generating In-Distribution Proxy Graphs for Explaining Graph Neural Networks Graph Neural Networks (GNNs) have become a building block in graph data processing, with wide applications in critical domains. The growing needs to deploy GNNs in high-stakes applications necessitate explainability for users in the decision-making processes. A popular paradigm for the explainability of GNNs is to identify explainable subgraphs by comparing their labels with the ones of original graphs. This task is challenging due to the substantial distributional shift from the original graphs in the training set to the set of explainable subgraphs, which prevents accurate prediction of labels with the subgraphs. To address it, in this paper, we propose a novel method that generates proxy graphs for explainable subgraphs that are in the distribution of training data. We introduce a parametric method that employs graph generators to produce proxy graphs. A new training objective based on information theory is designed to ensure that proxy graphs not only adhere to the distribution of training data but also preserve explanatory factors. Such generated proxy graphs can be reliably used to approximate the predictions of the labels of explainable subgraphs. Empirical evaluations across various datasets demonstrate our method achieves more accurate explanations for GNNs.', 'EiG-Search: Generating Edge-Induced Subgraphs for GNN Explanation in Linear Time Understanding and explaining the predictions of Graph Neural Networks (GNNs), is crucial for enhancing their safety and trustworthiness. Subgraph-level explanations are gaining attention for their intuitive appeal. However, most existing subgraph-level explainers face efficiency challenges in explaining GNNs due to complex search processes. The key challenge is to find a balance between intuitiveness and efficiency while ensuring transparency. Additionally, these explainers usually induce subgraphs by nodes, which may introduce less-intuitive disconnected nodes in the subgraph-level explanations or omit many important subgraph structures. In this paper, we reveal that inducing subgraph explanations by edges is more comprehensive than other subgraph inducing techniques. We also emphasize the need of determining the subgraph explanation size for each data instance, as different data instances may involve different important substructures. Building upon these considerations, we introduce a training-free approach, named EiG-Search. We employ an efficient linear-time search algorithm over the edge-induced subgraphs, where the edges are ranked by an enhanced gradient-based importance. We conduct extensive experiments on a total of seven datasets, demonstrating its superior performance and efficiency both quantitatively and qualitatively over the leading baselines.', 'On Explainability of Graph Neural Networks via Subgraph Explorations We consider the problem of explaining the predictions of graph neural networks (GNNs), which otherwise are considered as black boxes. Existing methods invariably focus on explaining the importance of graph nodes or edges but ignore the substructures of graphs, which are more intuitive and human-intelligible. In this work, we propose a novel method, known as SubgraphX, to explain GNNs by identifying important subgraphs. Given a trained GNN model and an input graph, our SubgraphX explains its predictions by efficiently exploring different subgraphs with Monte Carlo tree search. To make the tree search more effective, we propose to use Shapley values as a measure of subgraph importance, which can also capture the interactions among different subgraphs. To expedite computations, we propose efficient approximation schemes to compute Shapley values for graph data. Our work represents the first attempt to explain GNNs via identifying subgraphs explicitly and directly. Experimental results show that our SubgraphX achieves significantly improved explanations, while keeping computations at a reasonable level.']"
203,18,203_boltzmann_autoencoder_recognition_learning,"['boltzmann', 'autoencoder', 'recognition', 'learning', 'rbms', 'rbm', 'generative', 'handwritten', 'deep', 'modeling']","['Inductive Principles for Restricted Boltzmann Machine Learning Recent research has seen the proposal of several new inductive principles designed specifically to avoid the problems associated with maximum likelihood learning in models with intractable partition functions. In this paper, we study learning methods for binary restricted Boltzmann machines (RBMs) based on ratio matching and generalized score matching. We compare these new RBM learning methods to a range of existing learning methods including stochastic maximum likelihood, contrastive divergence, and pseudo-likelihood. We perform an extensive empirical evaluation across multiple tasks and data sets.', 'Deep Boltzmann Machines We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and data-independent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer “pre-training” phase that allows variational inference to be initialized by a single bottom-up pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.', 'Factored 3-Way Restricted Boltzmann Machines For Modeling Natural Images Deep belief nets have been successful in modeling handwritten characters, but it has proved more difficult to apply them to real images. The problem lies in the restricted Boltzmann machine (RBM) which is used as a module for learning deep belief nets one layer at a time. The Gaussian-Binary RBMs that have been used to model real-valued data are not a good way to model the covariance structure of natural images. We propose a factored 3-way RBM that uses the states of its hidden units to represent abnormalities in the local covariance structure of an image. This provides a probabilistic framework for the widely used simple/complex cell architecture. Our model learns binary features that work very well for object recognition on the “tiny images” data set. Even better features are obtained by then using standard binary RBM’s to learn a deeper model.']"
204,21,204_reinforcement_learning_reward_inverse,"['reinforcement', 'learning', 'reward', 'inverse', 'rewards', 'apprenticeship', 'learned', 'markov', 'optimal', 'rl']","['A Bayesian Approach to Robust Inverse Reinforcement Learning We consider a Bayesian approach to offline model-based inverse reinforcement learning (IRL). The proposed framework differs from existing offline model-based IRL approaches by performing simultaneous estimation of the expert’s reward function and subjective model of environment dynamics. We make use of a class of prior distributions which parameterizes how accurate the expert’s model of the environment is to develop efficient algorithms to estimate the expert’s reward and subjective dynamics in high-dimensional settings. Our analysis reveals a novel insight that the estimated policy exhibits robust performance when the expert is believed (a priori) to have a highly accurate model of the environment. We verify this observation in the MuJoCo environments and show that our algorithms outperform state-of-the-art offline IRL algorithms.', 'Relative Entropy Inverse Reinforcement Learning We consider the problem of imitation learning where the examples, demonstrated by an expert, cover only a small part of a large state space. Inverse Reinforcement Learning (IRL) provides an efficient tool for generalizing the demonstration, based on the assumption that the expert is optimally acting in a Markov Decision Process (MDP). Most of the past work on IRL requires that a (near)-optimal policy can be computed for different reward functions. However, this requirement can hardly be satisfied in systems with a large, or continuous, state space. In this paper, we propose a model-free IRL algorithm, where the relative entropy between the empirical distribution of the state-action trajectories under a baseline policy and their distribution under the learned policy is minimized by stochastic gradient descent. We compare this new approach to well-known IRL algorithms using learned MDP models. Empirical results on simulated car racing, gridworld and ball-in-a-cup problems show that our approach is able to learn good policies from a small number of demonstrations.', 'Inverse Reinforcement Learning with Simultaneous Estimation of Rewards and Dynamics Inverse Reinforcement Learning (IRL) describes the problem of learning an unknown reward function of a Markov Decision Process (MDP) from observed behavior of an agent. Since the agent’s behavior originates in its policy and MDP policies depend on both the stochastic system dynamics as well as the reward function, the solution of the inverse problem is significantly influenced by both. Current IRL approaches assume that if the transition model is unknown, additional samples from the system’s dynamics are accessible, or the observed behavior provides enough samples of the system’s dynamics to solve the inverse problem accurately. These assumptions are often not satisfied. To overcome this, we present a gradient-based IRL approach that simultaneously estimates the system’s dynamics. By solving the combined optimization problem, our approach takes into account the bias of the demonstrations, which stems from the generating policy. The evaluation on a synthetic MDP and a transfer learning task shows improvements regarding the sample efficiency as well as the accuracy of the estimated reward functions and transition models.']"
205,32,205_supervised_classification_classifiers_learning,"['supervised', 'classification', 'classifiers', 'learning', 'unlabeled', 'semisupervised', 'pseudolabels', 'pseudolabel', 'labeled', 'selflearning']","['InterLUDE: Interactions between Labeled and Unlabeled Data to Enhance Semi-Supervised Learning Semi-supervised learning (SSL) seeks to enhance task performance by training on both labeled and unlabeled data. Mainstream SSL image classification methods mostly optimize a loss that additively combines a supervised classification objective with a regularization term derived <em>solely</em> from unlabeled data. This formulation often neglects the potential for interaction between labeled and unlabeled images. In this paper, we introduce InterLUDE, a new approach to enhance SSL made of two parts that each benefit from labeled-unlabeled interaction. The first part, embedding fusion, interpolates between labeled and unlabeled embeddings to improve representation learning. The second part is a new loss, grounded in the principle of consistency regularization, that aims to minimize discrepancies in the model’s predictions between labeled versus unlabeled inputs. Experiments on standard closed-set SSL benchmarks and a medical SSL task with an uncurated unlabeled set show clear benefits to our approach. On the STL-10 dataset with only 40 labels, InterLUDE achieves <b>3.2%</b> error rate, while the best previous method reports 6.3%.', ' Unlabeled Data Help: Minimax Analysis and Adversarial Robustness   The recent proposed self-supervised learning (SSL) approaches successfully demonstrate the great potential of supplementing learning algorithms with additional unlabeled data. However, it is still unclear whether the existing SSL algorithms can fully utilize the information of both labelled and unlabeled data. This paper gives an affirmative answer for the reconstruction-based SSL algorithm (Lee et al., 2020) under several statistical models. While existing literature only focuses on establishing the upper bound of the convergence rate, we provide a rigorous minimax analysis, and successfully justify the rate-optimality of the reconstruction-based SSL algorithm under different data generation models. Furthermore, we incorporate the reconstruction-based SSL into the exist- ing adversarial training algorithms and show that learning from unlabeled data helps improve the robustness. ', 'Generative Semi-supervised Learning with Meta-Optimized Synthetic Samples Semi-supervised learning (SSL) is a promising approach for training deep classification models using labeled and unlabeled datasets. However, existing SSL methods rely on a large unlabeled dataset, which may not always be available in many real-world applications due to legal constraints (e.g., GDPR). In this paper, we investigate the research question: \\textit{Can we train SSL models without real unlabeled datasets?} Instead of using real unlabeled datasets, we propose an SSL method using synthetic datasets generated from generative foundation models trained on datasets containing millions of samples in diverse domains (e.g., ImageNet). Our main concepts are to identify synthetic samples that emulate unlabeled samples from generative foundation models and to train classifiers using these synthetic samples. To achieve this, our method is formulated as an alternating optimization problem: (i) meta-learning of generative foundation models and (ii) SSL of classifiers using real labeled and synthetic unlabeled samples. For (i), we propose a meta-learning objective that optimizes latent variables to generate samples that resemble real labeled samples and minimize the validation loss. For (ii), we propose a simple unsupervised loss function that regularizes the feature extractors of classifiers to maximize the performance improvement obtained from synthetic samples. We confirm that our method outperforms baselines using generative foundation models on SSL. We also demonstrate that our methods outperform SSL using real unlabeled datasets in scenarios with extremely small amounts of labeled datasets. This suggests that synthetic samples have the potential to provide improvement gains more efficiently than real unlabeled data.']"
206,127,206_imitation_reinforcement_demonstrations_imitate,"['imitation', 'reinforcement', 'demonstrations', 'imitate', 'robotics', 'learning', 'control', 'actions', 'robot', 'exploration']","['DART: Noise Injection for Robust Imitation Learning One approach to Imitation Learning is  Behavior Cloning, in which a robot observes a supervisor and infers a control policy. A known problem with this “off-policy"" approach is that the robot’s errors compound when drifting away from the supervisor’s demonstrations.  On-policy, techniques alleviate this by iteratively collecting corrective actions for the current robot policy. However, these techniques can be difficult for human supervisors, add significant computation burden, and require the robot to visit potentially dangerous states during training.  We propose an off-policy approach that \\emphinjects noise into the supervisor’s policy while demonstrating. This forces the supervisor and robot to explore and recover from errors without letting them compound. We propose a new algorithm, DART, that collects demonstrations with injected noise, and optimizes the noise level to approximate the error of the robot’s trained policy during data collection.  We  provide a theoretical analysis to illustrate that DART reduces covariate shift more than Behavior Cloning for a robot with non-zero error. We evaluate DART in two domains: in simulation with an algorithmic supervisor on the MuJoCo locomotive tasks and in physical experiments with human supervisors training a Toyota HSR robot to perform grasping in clutter.  For challenging tasks like Humanoid, DART can be up to $280%$ faster in computation time and only decreases the supervisor’s cumulative reward by $5%$ during training, whereas DAgger executes policies that have $80%$ less cumulative reward than the supervisor.  On the grasping in clutter task, DART obtains on average $62%$ performance increase over Behavior Cloning.', 'DART: Noise Injection for Robust Imitation Learning <p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/blog/assets/dart/bed_making_gif.gif"" alt=""Bed-Making GIF"" width=""600"" /><br />\n<i>\nToyota HSR Trained with DART to Make a Bed.\n</i>\n</p>\n\n<p>In Imitation Learning (IL), also known as Learning from Demonstration (LfD), a\nrobot learns a control policy from analyzing demonstrations of the policy\nperformed by an algorithmic or human supervisor. For example, to teach a robot\nmake a bed, a human would tele-operate a robot to perform the task to provide\nexamples.  The robot then learns a control policy, mapping from images/states to\nactions which we hope will generalize to states that were not encountered during\ntraining.</p>\n\n<p>There are two variants of IL: Off-Policy, or Behavior Cloning, where the\ndemonstrations are given independent of the robot’s policy.  However, when the\nrobot encounters novel risky states it may not have learned corrective actions.\nThis occurs because of “covariate shift”  a known challenge, where the states\nencountered during training differ from the states encountered during testing,\nreducing robustness. Common approaches to reduce covariate shift are On-Policy\nmethods, such as DAgger, where the evolving robot’s policy is executed and the\nsupervisor provides corrective feedback. However, On-Policy methods can be\ndifficult for human supervisors, potentially dangerous, and computationally\nexpensive.</p>\n\n<p>This post presents a robust Off-Policy algorithm called DART and summarizes how\ninjecting noise into the supervisor’s actions can improve robustness. The\ninjected noise allows the supervisor to provide corrective examples for the type\nof errors the trained robot is likely to make. However, because the optimized\nnoise is small, it alleviates the difficulties of On-Policy methods. Details on\nDART are in a paper that will be presented at <a href=""http://www.robot-learning.org/"">the 1st Conference on Robot Learning in\nNovember</a>.</p>\n\n<p>We evaluate DART in  simulation with an algorithmic supervisor on MuJoCo tasks\n(Walker, Humanoid, Hopper, Half-Cheetah) and physical experiments with human\nsupervisors training a Toyota HSR robot to perform grasping in clutter, where a\nrobot must search through clutter for a goal object.  Finally, we show how\nDART can be applied in a complex system that leverages both classical robotics\nand learning techniques to teach the first robot to make a bed. For\nresearchers who want to study and use robust Off-Policy approaches, <strong>we\nadditionally announce the release of \n<a href=""https://berkeleyautomation.github.io/DART/"">our codebase</a>\non GitHub</strong>.</p>\n\n<!--more-->\n\n<h1 id=""imitation-learnings-compounding-errors"">Imitation Learning’s Compounding Errors</h1>\n\n<p>In the late 80s, Behavior Cloning was applied to teach cars how to drive, with a\nproject known as ALVINN (Autonomous Land Vehicle in a Neural Network). In\nALVINN, a neural network was trained on driving demonstrations and learned a\npolicy that mapped  images of the road to the supervisor’s steering angle.\nUnfortunately, after learning, the policy was unstable, as indicated in the\nfollowing video:</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/blog/assets/dart/alvinn.gif"" alt=""ALVINN."" /><br />\n<i>\nALVINN Suffering from Covariate Shift.\n</i>\n</p>\n\n<p>The car would start drifting to side of the road and not know how to recover.\nThe reason for the car’s instability was that no data was collected on the\nside of the road.  During the data collection the supervisor always drove along\nthe center of the road; however, if the robot began to drift from the\ndemonstrations, it would not know how to recover because it saw no examples.</p>\n\n<p>This example, along with many others that researchers have tried, shows that\nImitation Learning cannot be entirely solved with Behavior Cloning. In\ntraditional Supervised Learning, the training distribution is de-coupled from\nthe learned model, whereas in Imitation Learning, <em>the robot’s policy affects\nwhat state is queried next</em>. Thus the training and testing distributions are no\nlonger equivalent, and this mismatch is known as \n<strong><a href=""http://sifaka.cs.uiuc.edu/jiang4/domain_adaptation/survey/node8.html"">covariate shift</a></strong>.</p>\n\n<p>To reduce covariate shift, the objective of Imitation Learning had to be\nmodified. The robot should now be expected to match the supervisor on the states\nit is likely to visit. Thus, if ALVINN is likely to drift to the side of the\nroad, we expect that it will know what to do in those states.</p>\n\n<p>A robot’s policy and a supervisor’s policy can be denoted as $\\pi_{\\theta}$ and\n<script type=""math/tex"">\\pi_{\\theta^*}</script>, where $\\pi$ is a function mapping state to action and\n$\\theta$ is a parametrization, like weights in a neural network.   We can\nmeasure how close two policies are by what actions they apply at a given state,\nwhich we refer to as the surrogate loss, $l$.  A common surrogate loss is the\nsquared Euclidean distance:</p>\n\n<script type=""math/tex; mode=display"">l(\\pi_{\\theta}(x), \\pi_{\\theta^*}(x)) = \\|\\pi_{\\theta^*}(x)\n-\\pi_{\\theta}(x)\\|^2_2.</script>\n\n<p>Finally, we need a distribution over trajectories  $p(\\xi|\\theta)$, which\nindicate the trajectories, $\\xi$, that are likely under the current policy\n$\\pi_{\\theta}$.  Our objective can then be written as follows:</p>\n\n<script type=""math/tex; mode=display"">\\underset{\\theta}{\\mbox{min}}\\; E_{p(\\xi|\\theta)} \\underbrace{\\sum^T_{t=1}\nl(\\pi_{\\theta}(x_t), \\pi_{\\theta^*}(x_t)) }_{J(\\theta,\\theta^*|\\xi)}.</script>\n\n<p>Hence we want to minimize the expected surrogate loss on the distribution of\nstates induced by the robot’s policy. This objective is challenging to solve\nbecause we don’t know what the robot’s policy is until after data has been\ncollected, which creates a <em>chicken and egg</em> situation. We will now discuss an\niterative On-Policy approach to overcome this problem.</p>\n\n<h1 id=""reducing-shift-with-on-policy-methods"">Reducing Shift with On-Policy Methods</h1>\n\n<p>A large body of work from Ross and Bagnell [6,7], has examined the theoretical\nconsequences of covariate shift. In particular, they proposed the DAgger\nalgorithm to help correct for it. DAgger can be thought of as an On-Policy\nalgorithm — which rolls out the current robot policy during learning.</p>\n\n<p>The key idea of DAgger is to collect data from the current robot policy and\nupdate the model on the aggregate dataset. Implementation of DAgger requires\niteratively rolling out the current robot policy, querying a supervisor for\nfeedback on the states visited by the robot, and then updating the robot on the\naggregate dataset across all iterations.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/blog/assets/dart/DAgger.png"" alt=""DAGGER."" width=""600"" /><br />\n<i>\nThe DAgger Algorithm.\n</i>\n</p>\n\n<p>Two years ago, we used DAgger to teach a robot to perform grasping in clutter\n(shown below), which requires a robot to search through objects via pushing to\nreach a desired goal object. Imitation Learning was advantageous in this task\nbecause we didn’t need to explicitly model the collision of multiple non-convex\nobjects.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/blog/assets/dart/izzy.gif"" alt=""Mechnical Search 1."" width=""250"" height=""250"" /><br />\n<i>\nPlanar Grasping in Clutter.\n</i>\n</p>\n\n<p>Our planar robot had a neural network policy that mapped images of the workspace\nto a control signal. We trained it with DAgger on 160 expert demonstrations.\nWhile we were able to teach the robot how to perform the task with a 90% success\nrate, we encountered several major hurdles that made it challenging to increase\nthe complexity of the task.</p>\n\n<h1 id=""challenges-with-on-policy-methods"">Challenges with On-Policy Methods</h1>\n\n<p>After applying DAgger to teach our robot, we wanted to study and better\nunderstand 3 key limitations related to On-Policy methods in order to scale up\nto more challenging tasks.</p>\n\n<h2 id=""limitation-1-providing-feedback"">Limitation 1: Providing Feedback</h2>\n\n<p>In order to apply feedback to our robot, we had to do so retroactively with a\nlabeling interface, shown below.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/blog/assets/dart/feedback.gif"" alt=""Mechnical Search 2."" width=""250"" height=""250"" /><br />\n<i>\nSupervisor Providing Retroactive Feedback.\n</i>\n</p>\n\n<p>A supervisor had to manually move the pink overlay to tell the robot what it\nshould have done after execution. When we tried to retrain the robot with\ndifferent supervisors, we found it was very challenging to provide this feedback\nfor most people. You can think of a human supervisor as a controller that needs\nto constantly adjust their actions to obtain the desired effect. However, with\nretroactive feedback the human must simulate what the action would be without\nseeing the outcome, which is quite unnatural.</p>\n\n<p>To test this hypothesis, we performed a human study with 10 participants to\ncompare DAgger against Behavior Cloning, where each participant was asked to\ntrain a robot to perform planar part singulation. We found that Behavior Cloning\nout-performed DAgger, suggesting that while DAgger mitigates the shift, in\npractice it may add systematic noise to the supervisor’s signal [2].</p>\n\n<h2 id=""limitation-2-safety"">Limitation 2: Safety</h2>\n\n<p>On-Policy methods have the additional burden of needing to roll-out the current\nrobot’s policy during execution. While our robot was able to perform the task at\nthe end of training, for most of learning it wasn’t successful:</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/blog/assets/dart/policy_gif.gif"" alt=""Mechnical Search 3."" width=""250"" height=""250"" /><br />\n<i>\nRobot Rolling Out Unsuccessful Policy.\n</i>\n</p>\n\n<p>In unstructured environments, such as a self-driving car or home robotics, this\ncan be problematic. Ideally, we would like to collect data with the robot while\nmaintaining high performance throughout the entire process.</p>\n\n<h2 id=""limitation-3-computation"">Limitation 3: Computation</h2>\n\n<p>Finally, when building systems either in simulation or the real world, we want\nto collect large amounts of data in parallel and update our policy sparingly.\nNeural networks can require significant computation time for retraining.\nHowever, On-Policy methods suffer when the policy is not updated frequently\nduring data collection. Training on a large batch size of new data can cause\nsignificant changes to the current policy, which can push the robot’s\ndistribution away from the previously collected data and make the aggregate\ndataset stale.</p>\n\n<p>Variants of On-Policy methods have been proposed to solve each of these problems\nindividually. For example, Ho et al. got rid of the retroactive feedback by\nproposing, GAIL, which uses Reinforcement Learning to reduce covariate shift\n[8].  Zhang et al. examined how to detect when the policy is about to deviate to\na risky state and asks the supervisor to take over [4]. Sun et al. has explored\nincremental gradient updates to the model instead of a full retrain, which is\ncomputationally cheaper [5].</p>\n\n<p>While these methods can each solve some of these problems, ideally we want a\nsolution to address all three. Off-Policy algorithms like Behavior Cloning do\nnot exhibit these problems because they passively sample from the supervisor’s\npolicy. Thus, we decided instead of extending On-Policy methods it might be more\nbeneficial to make Off-Policy methods more robust.</p>\n\n<h1 id=""off-policy-with-noise-injection"">Off-Policy with Noise Injection</h1>\n\n<p>Off-Policy methods, like Behavior Cloning, can in fact have low covariate shift.\nIf the robot is able to learn the supervisor’s policy perfectly, then it should\nvisit the same states as the supervisor. In prior work we empirically found in\nsimulation that with sufficient data and expressive learners, such as deep\nneural networks, Behavior Cloning is at parity with DAgger [2].</p>\n\n<p>In real world domains, though, it is unlikely that a robot can perfectly match a\nsupervisor. Machine Learning algorithms generally have a long tail in terms of\nsample complexity, so the amount of data and computation needed to perfectly\nmatch a supervisor may be unreasonable. However, it is likely that we can\nachieve small non-zero test error.</p>\n\n<p>Instead of attempting to perfectly learn the supervisor, we propose simulating\nsmall amounts of error in the supervisor’s policy to better mimic the trained\nrobot. Injecting noise into the supervisor’s policy during teleoperation is one\nway to simulate this small test error during data collection. Noise injection\nforces the supervisor to provide corrective examples to these small disturbances\nas they try to perform the task.  Shown below is the intuition of how noise\ninjection creates a funnel of corrective examples around the supervisor’s\ndistribution.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/blog/assets/dart/dart_intuition.png"" alt=""DART intuition."" width=""300"" />\n<br />\n<i>\nNoise Injection forces the supervisor to provide corrective examples,<br />\nso that the robot can learn to recover.\n</i>\n</p>\n\n<p>Additionally, because we are only injecting small noise levels, we don’t suffer\nas many limitations compared to On-Policy methods. A supervisor can normally be\nrobust to small random disturbances that are concentrated around their current\naction. We will now formalize noise injection a bit to help understand its\neffect more.</p>\n\n<p>Denote by <script type=""math/tex"">p(\\xi|\\pi_{\\theta^*},\\psi)</script> a distribution over trajectories with\nnoise injected into the supervisor’s distribution\n<script type=""math/tex"">\\pi_{\\theta^*}(\\mathbf{u}|\\mathbf{x},\\psi)</script>. The parameter $\\psi$ represents\nthe sufficient statistics that define the noise distribution. For example, if\nGaussian noise is injected parameterized by $\\psi$, then\n<script type=""math/tex"">\\pi_{\\theta^*}(\\mathbf{u}|\\mathbf{x},\\psi) =\n\\mathcal{N}(\\pi_{\\theta^*}(\\mathbf{x}), \\Sigma)</script>.  Note, the stochastic\nsupervisor’s distribution is a slight abuse of notation.\n<script type=""math/tex"">\\pi_{\\theta^*}(\\mathbf{u}|\\mathbf{x},\\psi)</script> is a distribution over actions,\nwhere as <script type=""math/tex"">\\pi_{\\theta^*}(\\mathbf{x})</script> is a deterministic function mapping to a\nsingle action.</p>\n\n<p>Similar to Behavior Cloning, we can sample demonstrations from the\nnoise-injected supervisor and minimize the expected loss via standard supervised\nlearning techniques:</p>\n\n<script type=""math/tex; mode=display"">\\theta^R = \\underset{\\theta}{\\mbox{argmin }} E_{p(\\xi|\\pi_{\\theta^*},\\psi)}\nJ(\\theta,\\theta^* | \\xi)</script>\n\n<p>This equation, though, does not explicitly minimize the covariate shift for\narbitrary choices of $\\psi$; the $\\psi$ needs to be chosen to best simulate the\nerror of the final robot’s policy, which may be complex for high dimensional\naction spaces.  One approach to choose $\\psi$ is grid-search, but this requires\nexpensive data collection, which can be prohibitive in the physical world or in\nhigh fidelity simulation.</p>\n\n<p>Instead of grid-search, we can formulate the selection of $\\psi$ as a maximum\nlikelihood problem. The objective is to increase the probability of the\nsupervisor applying the robot’s control.</p>\n\n<script type=""math/tex; mode=display"">\\underset{\\psi}{\\mbox{min}} \\: E_{p(\\xi|\\pi_{\\theta^R})} -\\sum^{T-1}_{t=0} \\:\n\\mbox{log} [\\pi_{\\theta^*}(\\pi_{\\theta^R}(\\mathbf{x_t})|\\mathbf{x_t},\\psi)]</script>\n\n<p>This objective states that we want the noise injected supervisor to try and\nmatch the final robot’s policy. In the paper, we show that this explicitly\nminimizes the distance between the supervisor and robot’s distribution. A clear\nlimitation of this optimization problem though is that it requires knowing the\nfinal robot’s distribution $p(\\xi|\\pi_{\\theta^R})$, which is determined only\nafter the data is collected.  In the next section, we present DART, which\napplies an iterative approach to the optimization.</p>\n\n<h2 id=""dart-disturbances-for-augmenting-robot-trajectories"">DART: Disturbances for Augmenting Robot Trajectories</h2>\n\n<p>The above objective cannot be solved because $p(\\xi|\\pi_{\\theta^R})$ is not\nknown until after the robot has been trained.  We can instead iteratively sample\nfrom the supervisor’s distribution with the current noise parameter, $\\psi_k$,\nand minimize the negative log-likelihood of the noise-injected supervisor taking\nthe current robot’s, $\\pi_{\\hat{\\theta}}$, control.</p>\n\n<script type=""math/tex; mode=display"">\\hat{\\psi}_{k+1} = \\underset{\\psi}{\\mbox{argmin}} \\: E_{p(\\xi|\\pi_{\\theta^*},\n\\psi_k)} -\\sum^{T-1}_{t=0}\\mbox{log} \\:\n[\\pi_{\\theta^*}(\\pi_{\\hat{\\theta}}(\\mathbf{x_t})|\\mathbf{x_t},\\psi)]</script>\n\n<p>The above iterative process can be slow to converge because it is optimizing the\nnoise with respect to the current robot’s policy. We can obtain a better\nestimate by observing that the supervisor should simulate as much expected error\nas the final robot policy, <script type=""math/tex"">E_{p(\\xi|\\pi_{\\theta^R})}\nJ(\\theta^R,\\theta^*|\\xi)</script>.  It is possible that we have some knowledge of this\nquantity from previously training on similar domains. In the paper, we show how\nto incorporate this knowledge in the form of a prior. For some common noise\ndistributions, the objective can be solved in closed form, as detailed in the\npaper. Thus, the optimization problem determines the shape of the noise injected\nand the prior helps determine the magnitude.</p>\n\n<p>Our algorithm DART, iteratively solves this optimization problem to best set the\nnoise term. DART is still an iterative algorithm like On-Policy methods.\n<em>Through the iterative process, DART optimizes $\\psi$ to better simulate the\nerror in the final robot’s policy.</em></p>\n\n<h1 id=""evaluating-dart"">Evaluating DART</h1>\n\n<p>To understand how effectively DART reduces covariate shift and to determine if\nit suffers from similar limitations as On-Policy methods, we ran experiments in\n4 MuJoco domains, as shown below. The supervisor was a policy trained with TRPO\nand the noise we injected was Gaussian.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/blog/assets/dart/ni_a_results-eps-converted-to.png"" alt=""DART results."" /><br />\n</p>\n\n<p>To test if DART suffers from updating the policy after larger batches, we only\nupdated the model after every $K$ demonstrations for all experiments. DAgger was\nupdated after every demonstration and DAgger-B was updated after every $K$.  The\nresults show that DART is able to have the same performance as DAgger, but is\nsignificantly faster in terms of computation.  DAgger-B is relatively similar in\ncomputation time, but suffers significantly in performance, suggesting DART can\nsignificantly reduce computation time.</p>\n\n<p>We finally compared DART to Behavior Cloning in a human study for the task of\ngrasping in clutter, shown below.  In the task, a Toyota HSR robot was trained to\nreach a goal object by pushing objects away with its gripper.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/blog/assets/dart/hsr.gif"" alt=""DART results."" /><br />\n<i>\nToyota HSR Trained with DART for Grasping in Clutter.\n</i>\n</p>\n\n<p>The task is more complex than the one above because the robot now sees images of\nthe world taken from an eye-in-hand camera. We compared 4 humans subjects and\nsaw that by injecting noise in the controller, we were able to receive a win\nover Behavior Cloning of 62%. DART was able to reduce the shift on the task with\nhuman supervisors.</p>\n\n<h1 id=""robotic-bed-making-a-testbed-for-covariate-shift"">Robotic Bed Making: A Testbed for Covariate Shift</h1>\n\n<p>To better understand how errors compound in real world robotic systems, we built\na literal test bed. Robotic Bed Making has been a challenging task in robotics\ndue to it requiring mobile manipulation of deformable objects and sequential\nplanning. Imitation Learning is one way to sidestep some of the challenges of\ndeformable object manipulation because it doesn’t require modeling the bed\nsheets.</p>\n\n<p>The goal of our bed making system was to have a robot learn to stretch the\nsheets over the bed frame. The task was designed so that the robot must learn\none policy to decide where to grasp the bed sheet and another transition policy\nto decide whether the robot should try again or switch to the other bed side.\nWe trained the bed making policy with 50 demonstrations.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/blog/assets/dart/bed_system.png"" alt=""DART results, bed-making."" /><br />\n<i>\nBed Making System.\n</i>\n</p>\n\n<p>DART was applied to inject Gaussian noise into grasping policy because we\nassumed there would be considerable error in determining where to grasp. The\noptimized covariance matrix decided to inject more noise in the horizontal\ndirection of the bed, because that is where the edge of the sheet varied more\nsignificantly and subsequently the robot had higher error.</p>\n\n<p>In order to test how large covariate shift was in the system, we can take our\ntrained policy $\\pi_{\\theta^R}$ and write its performance with the following\ndecomposition.</p>\n\n<script type=""math/tex; mode=display"">% <![CDATA[\n\\begin{align}\nE_{p(\\xi |\\pi_{\\theta^R})} J(\\theta^R,\\theta^*|\\xi) &= \\underbrace{E_{p(\\xi |\\pi_{\\theta^R})}  \\sum^T_{t=1} l(\\pi_{\\theta}(x_t), \\pi_{\\theta^*}(x_t)) -  E_{p(\\xi |\\pi_{\\theta^*}, \\psi)}  \\sum^T_{t=1} l(\\pi_{\\theta}(x_t), \\pi_{\\theta^*}(x_t))}_{\\text{Shift}} \\\\\n&+ \\underbrace{E_{p(\\xi |\\pi_{\\theta^*},\\psi)}  \\sum^T_{t=1} l(\\pi_{\\theta}(x_t), \\pi_{\\theta^*}(x_t)) }_{\\text{Loss}},\n\\end{align} %]]></script>\n\n<p>where the first term on the right-hand side corresponds to the covariate shift.\nIntuitively, the covariate shift is the difference between the expected error on\nthe robot’s distribution and the supervisor’s distribution. When we measured\nthese quantities on the bed making setup, we observed noticeable covariate shift\nin the transition policy trained with Behavior Cloning.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/blog/assets/dart/cs_graph.png"" alt=""DART results, covariate shift."" width=""500"" /><br />\n<i>\nCovariate Shift in Bed Making Task.\n</i>\n</p>\n\n<p>We attribute this covariate shift due to the fact that with Behavior Cloning the\nrobot rarely saw unsuccessful demonstrations; thus the transition policy never\nknew what failure was. DART gave a more diverse set of states, which allowed the\npolicy to have better class balance. DART was able to train a robust policy that\nallowed it to perform the bed making task even when novel objects were placed on\nthe bed, as shown at the beginning of the blog post. When distractor objects are\nplaced on the bed DART obtained a 97% sheet coverage, whereas Behavior Cloning\nachieved only 63%.</p>\n\n<p>These initial results suggest that covariate shift can occur in modern day\nsystems that use learning components. We will soon release a longer preprint on\nthe Bed Making Setup for more information.</p>\n\n<p>DART presents a way to correct for shift via the injection of small optimized\nnoise. Going forward, we are considering more complex noise models that better\ncapture the temporal structure of the robot’s error.</p>\n\n<p>(For papers and updated information, <a href=""http://autolab.berkeley.edu"">see UC Berkeley’s AUTOLAB website</a>.)</p>\n\n<h2 id=""references"">References</h2>\n\n<ol>\n  <li>\n    <p>Michael Laskey, Jonathan Lee, Roy Fox, Anca Dragan, Ken Goldberg ; DART:\nNoise Injection for Robust Imitation Learning Proceedings of the 1st Annual\nConference on Robot Learning, PMLR 78:143-156, 2017.</p>\n  </li>\n  <li>\n    <p>M. Laskey, C. Chuck, J. Lee, J. Mahler, S. Krishnan, K. Jamieson, A. Dragan,\nand K. Goldberg. Comparing human-centric and robot-centric sampling for robot\ndeep learning from demonstrations. Robotics and Automation (ICRA), 2017 IEEE\nInternational Conference on, pages 358-365. IEEE, 2017</p>\n  </li>\n  <li>\n    <p>M. Laskey, J. Lee, C. Chuck, D. Gealy, W. Hsieh, F. T. Pokorny, A. D. Dragan,\nand K. Goldberg. Robot grasping in clutter: Using a hierarchy of supervisors for\nlearning from demonstrations. In Automation Science and Engineering (CASE), 2016\nIEEE International Conference on, pages 827–834. IEEE, 2016.</p>\n  </li>\n  <li>\n    <p>Zhang, Jiakai, and Kyunghyun Cho. “Query-Efficient Imitation Learning for\nEnd-to-End Simulated Driving.” In AAAI, pp. 2891-2897. 2017.</p>\n  </li>\n  <li>\n    <p>W. Sun, A. Venkatraman, G. J. Gordon, B. Boots, and J. A. Bagnell. Deeply\naggrevated: Differentiable imitation learning for sequential prediction.\nProceedings of the 34th International Conference on Machine Learning, PMLR\n70:3309-3318, 2017.</p>\n  </li>\n  <li>\n    <p>Ross, Stéphane, Geoffrey J. Gordon, and Drew Bagnell. “A reduction of\nimitation learning and structured prediction to no-regret online learning.”\nInternational Conference on Artificial Intelligence and Statistics. 2011.</p>\n  </li>\n  <li>\n    <p>S. Ross and D. Bagnell. Efficient reductions for imitation learning. In\nInternational Conference on Artificial Intelligence and Statistics, pages\n661–668, 2010.</p>\n  </li>\n  <li>\n    <p>Ho, Jonathan, and Stefano Ermon. “Generative adversarial imitation learning.”\nAdvances in Neural Information Processing Systems. 2016.</p>\n  </li>\n</ol>\n', 'DART: Noise Injection for Robust Imitation Learning <p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/static/blog/dart/bed_making_gif.gif"" alt=""Bed-Making GIF"" width=""600"" /><br />\n<i>\nToyota HSR Trained with DART to Make a Bed.\n</i>\n</p>\n\n<p>In Imitation Learning (IL), also known as Learning from Demonstration (LfD), a\nrobot learns a control policy from analyzing demonstrations of the policy\nperformed by an algorithmic or human supervisor. For example, to teach a robot\nmake a bed, a human would tele-operate a robot to perform the task to provide\nexamples.  The robot then learns a control policy, mapping from images/states to\nactions which we hope will generalize to states that were not encountered during\ntraining.</p>\n\n<p>There are two variants of IL: Off-Policy, or Behavior Cloning, where the\ndemonstrations are given independent of the robot’s policy.  However, when the\nrobot encounters novel risky states it may not have learned corrective actions.\nThis occurs because of “covariate shift”  a known challenge, where the states\nencountered during training differ from the states encountered during testing,\nreducing robustness. Common approaches to reduce covariate shift are On-Policy\nmethods, such as DAgger, where the evolving robot’s policy is executed and the\nsupervisor provides corrective feedback. However, On-Policy methods can be\ndifficult for human supervisors, potentially dangerous, and computationally\nexpensive.</p>\n\n<p>This post presents a robust Off-Policy algorithm called DART and summarizes how\ninjecting noise into the supervisor’s actions can improve robustness. The\ninjected noise allows the supervisor to provide corrective examples for the type\nof errors the trained robot is likely to make. However, because the optimized\nnoise is small, it alleviates the difficulties of On-Policy methods. Details on\nDART are in a paper that will be presented at <a href=""http://www.robot-learning.org/"">the 1st Conference on Robot Learning in\nNovember</a>.</p>\n\n<p>We evaluate DART in  simulation with an algorithmic supervisor on MuJoCo tasks\n(Walker, Humanoid, Hopper, Half-Cheetah) and physical experiments with human\nsupervisors training a Toyota HSR robot to perform grasping in clutter, where a\nrobot must search through clutter for a goal object.  Finally, we show how\nDART can be applied in a complex system that leverages both classical robotics\nand learning techniques to teach the first robot to make a bed. For\nresearchers who want to study and use robust Off-Policy approaches, <strong>we\nadditionally announce the release of \n<a href=""https://berkeleyautomation.github.io/DART/"">our codebase</a>\non GitHub</strong>.</p>\n\n<!--more-->\n\n<h1 id=""imitation-learnings-compounding-errors"">Imitation Learning’s Compounding Errors</h1>\n\n<p>In the late 80s, Behavior Cloning was applied to teach cars how to drive, with a\nproject known as ALVINN (Autonomous Land Vehicle in a Neural Network). In\nALVINN, a neural network was trained on driving demonstrations and learned a\npolicy that mapped  images of the road to the supervisor’s steering angle.\nUnfortunately, after learning, the policy was unstable, as indicated in the\nfollowing video:</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/static/blog/dart/alvinn.gif"" alt=""ALVINN."" /><br />\n<i>\nALVINN Suffering from Covariate Shift.\n</i>\n</p>\n\n<p>The car would start drifting to side of the road and not know how to recover.\nThe reason for the car’s instability was that no data was collected on the\nside of the road.  During the data collection the supervisor always drove along\nthe center of the road; however, if the robot began to drift from the\ndemonstrations, it would not know how to recover because it saw no examples.</p>\n\n<p>This example, along with many others that researchers have tried, shows that\nImitation Learning cannot be entirely solved with Behavior Cloning. In\ntraditional Supervised Learning, the training distribution is de-coupled from\nthe learned model, whereas in Imitation Learning, <em>the robot’s policy affects\nwhat state is queried next</em>. Thus the training and testing distributions are no\nlonger equivalent, and this mismatch is known as \n<strong><a href=""http://sifaka.cs.uiuc.edu/jiang4/domain_adaptation/survey/node8.html"">covariate shift</a></strong>.</p>\n\n<p>To reduce covariate shift, the objective of Imitation Learning had to be\nmodified. The robot should now be expected to match the supervisor on the states\nit is likely to visit. Thus, if ALVINN is likely to drift to the side of the\nroad, we expect that it will know what to do in those states.</p>\n\n<p>A robot’s policy and a supervisor’s policy can be denoted as $\\pi_{\\theta}$ and\n<script type=""math/tex"">\\pi_{\\theta^*}</script>, where $\\pi$ is a function mapping state to action and\n$\\theta$ is a parametrization, like weights in a neural network.   We can\nmeasure how close two policies are by what actions they apply at a given state,\nwhich we refer to as the surrogate loss, $l$.  A common surrogate loss is the\nsquared Euclidean distance:</p>\n\n<script type=""math/tex; mode=display"">l(\\pi_{\\theta}(x), \\pi_{\\theta^*}(x)) = \\|\\pi_{\\theta^*}(x)\n-\\pi_{\\theta}(x)\\|^2_2.</script>\n\n<p>Finally, we need a distribution over trajectories  $p(\\xi|\\theta)$, which\nindicate the trajectories, $\\xi$, that are likely under the current policy\n$\\pi_{\\theta}$.  Our objective can then be written as follows:</p>\n\n<script type=""math/tex; mode=display"">\\underset{\\theta}{\\mbox{min}}\\; E_{p(\\xi|\\theta)} \\underbrace{\\sum^T_{t=1}\nl(\\pi_{\\theta}(x_t), \\pi_{\\theta^*}(x_t)) }_{J(\\theta,\\theta^*|\\xi)}.</script>\n\n<p>Hence we want to minimize the expected surrogate loss on the distribution of\nstates induced by the robot’s policy. This objective is challenging to solve\nbecause we don’t know what the robot’s policy is until after data has been\ncollected, which creates a <em>chicken and egg</em> situation. We will now discuss an\niterative On-Policy approach to overcome this problem.</p>\n\n<h1 id=""reducing-shift-with-on-policy-methods"">Reducing Shift with On-Policy Methods</h1>\n\n<p>A large body of work from Ross and Bagnell [6,7], has examined the theoretical\nconsequences of covariate shift. In particular, they proposed the DAgger\nalgorithm to help correct for it. DAgger can be thought of as an On-Policy\nalgorithm — which rolls out the current robot policy during learning.</p>\n\n<p>The key idea of DAgger is to collect data from the current robot policy and\nupdate the model on the aggregate dataset. Implementation of DAgger requires\niteratively rolling out the current robot policy, querying a supervisor for\nfeedback on the states visited by the robot, and then updating the robot on the\naggregate dataset across all iterations.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/static/blog/dart/DAgger.png"" alt=""DAGGER."" width=""600"" /><br />\n<i>\nThe DAgger Algorithm.\n</i>\n</p>\n\n<p>Two years ago, we used DAgger to teach a robot to perform grasping in clutter\n(shown below), which requires a robot to search through objects via pushing to\nreach a desired goal object. Imitation Learning was advantageous in this task\nbecause we didn’t need to explicitly model the collision of multiple non-convex\nobjects.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/static/blog/dart/izzy.gif"" alt=""Mechnical Search 1."" width=""250"" height=""250"" /><br />\n<i>\nPlanar Grasping in Clutter.\n</i>\n</p>\n\n<p>Our planar robot had a neural network policy that mapped images of the workspace\nto a control signal. We trained it with DAgger on 160 expert demonstrations.\nWhile we were able to teach the robot how to perform the task with a 90% success\nrate, we encountered several major hurdles that made it challenging to increase\nthe complexity of the task.</p>\n\n<h1 id=""challenges-with-on-policy-methods"">Challenges with On-Policy Methods</h1>\n\n<p>After applying DAgger to teach our robot, we wanted to study and better\nunderstand 3 key limitations related to On-Policy methods in order to scale up\nto more challenging tasks.</p>\n\n<h2 id=""limitation-1-providing-feedback"">Limitation 1: Providing Feedback</h2>\n\n<p>In order to apply feedback to our robot, we had to do so retroactively with a\nlabeling interface, shown below.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/static/blog/dart/feedback.gif"" alt=""Mechnical Search 2."" width=""250"" height=""250"" /><br />\n<i>\nSupervisor Providing Retroactive Feedback.\n</i>\n</p>\n\n<p>A supervisor had to manually move the pink overlay to tell the robot what it\nshould have done after execution. When we tried to retrain the robot with\ndifferent supervisors, we found it was very challenging to provide this feedback\nfor most people. You can think of a human supervisor as a controller that needs\nto constantly adjust their actions to obtain the desired effect. However, with\nretroactive feedback the human must simulate what the action would be without\nseeing the outcome, which is quite unnatural.</p>\n\n<p>To test this hypothesis, we performed a human study with 10 participants to\ncompare DAgger against Behavior Cloning, where each participant was asked to\ntrain a robot to perform planar part singulation. We found that Behavior Cloning\nout-performed DAgger, suggesting that while DAgger mitigates the shift, in\npractice it may add systematic noise to the supervisor’s signal [2].</p>\n\n<h2 id=""limitation-2-safety"">Limitation 2: Safety</h2>\n\n<p>On-Policy methods have the additional burden of needing to roll-out the current\nrobot’s policy during execution. While our robot was able to perform the task at\nthe end of training, for most of learning it wasn’t successful:</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/static/blog/dart/policy_gif.gif"" alt=""Mechnical Search 3."" width=""250"" height=""250"" /><br />\n<i>\nRobot Rolling Out Unsuccessful Policy.\n</i>\n</p>\n\n<p>In unstructured environments, such as a self-driving car or home robotics, this\ncan be problematic. Ideally, we would like to collect data with the robot while\nmaintaining high performance throughout the entire process.</p>\n\n<h2 id=""limitation-3-computation"">Limitation 3: Computation</h2>\n\n<p>Finally, when building systems either in simulation or the real world, we want\nto collect large amounts of data in parallel and update our policy sparingly.\nNeural networks can require significant computation time for retraining.\nHowever, On-Policy methods suffer when the policy is not updated frequently\nduring data collection. Training on a large batch size of new data can cause\nsignificant changes to the current policy, which can push the robot’s\ndistribution away from the previously collected data and make the aggregate\ndataset stale.</p>\n\n<p>Variants of On-Policy methods have been proposed to solve each of these problems\nindividually. For example, Ho et al. got rid of the retroactive feedback by\nproposing, GAIL, which uses Reinforcement Learning to reduce covariate shift\n[8].  Zhang et al. examined how to detect when the policy is about to deviate to\na risky state and asks the supervisor to take over [4]. Sun et al. has explored\nincremental gradient updates to the model instead of a full retrain, which is\ncomputationally cheaper [5].</p>\n\n<p>While these methods can each solve some of these problems, ideally we want a\nsolution to address all three. Off-Policy algorithms like Behavior Cloning do\nnot exhibit these problems because they passively sample from the supervisor’s\npolicy. Thus, we decided instead of extending On-Policy methods it might be more\nbeneficial to make Off-Policy methods more robust.</p>\n\n<h1 id=""off-policy-with-noise-injection"">Off-Policy with Noise Injection</h1>\n\n<p>Off-Policy methods, like Behavior Cloning, can in fact have low covariate shift.\nIf the robot is able to learn the supervisor’s policy perfectly, then it should\nvisit the same states as the supervisor. In prior work we empirically found in\nsimulation that with sufficient data and expressive learners, such as deep\nneural networks, Behavior Cloning is at parity with DAgger [2].</p>\n\n<p>In real world domains, though, it is unlikely that a robot can perfectly match a\nsupervisor. Machine Learning algorithms generally have a long tail in terms of\nsample complexity, so the amount of data and computation needed to perfectly\nmatch a supervisor may be unreasonable. However, it is likely that we can\nachieve small non-zero test error.</p>\n\n<p>Instead of attempting to perfectly learn the supervisor, we propose simulating\nsmall amounts of error in the supervisor’s policy to better mimic the trained\nrobot. Injecting noise into the supervisor’s policy during teleoperation is one\nway to simulate this small test error during data collection. Noise injection\nforces the supervisor to provide corrective examples to these small disturbances\nas they try to perform the task.  Shown below is the intuition of how noise\ninjection creates a funnel of corrective examples around the supervisor’s\ndistribution.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/static/blog/dart/dart_intuition.png"" alt=""DART intuition."" width=""300"" />\n<br />\n<i>\nNoise Injection forces the supervisor to provide corrective examples,<br />\nso that the robot can learn to recover.\n</i>\n</p>\n\n<p>Additionally, because we are only injecting small noise levels, we don’t suffer\nas many limitations compared to On-Policy methods. A supervisor can normally be\nrobust to small random disturbances that are concentrated around their current\naction. We will now formalize noise injection a bit to help understand its\neffect more.</p>\n\n<p>Denote by <script type=""math/tex"">p(\\xi|\\pi_{\\theta^*},\\psi)</script> a distribution over trajectories with\nnoise injected into the supervisor’s distribution\n<script type=""math/tex"">\\pi_{\\theta^*}(\\mathbf{u}|\\mathbf{x},\\psi)</script>. The parameter $\\psi$ represents\nthe sufficient statistics that define the noise distribution. For example, if\nGaussian noise is injected parameterized by $\\psi$, then\n<script type=""math/tex"">\\pi_{\\theta^*}(\\mathbf{u}|\\mathbf{x},\\psi) =\n\\mathcal{N}(\\pi_{\\theta^*}(\\mathbf{x}), \\Sigma)</script>.  Note, the stochastic\nsupervisor’s distribution is a slight abuse of notation.\n<script type=""math/tex"">\\pi_{\\theta^*}(\\mathbf{u}|\\mathbf{x},\\psi)</script> is a distribution over actions,\nwhere as <script type=""math/tex"">\\pi_{\\theta^*}(\\mathbf{x})</script> is a deterministic function mapping to a\nsingle action.</p>\n\n<p>Similar to Behavior Cloning, we can sample demonstrations from the\nnoise-injected supervisor and minimize the expected loss via standard supervised\nlearning techniques:</p>\n\n<script type=""math/tex; mode=display"">\\theta^R = \\underset{\\theta}{\\mbox{argmin }} E_{p(\\xi|\\pi_{\\theta^*},\\psi)}\nJ(\\theta,\\theta^* | \\xi)</script>\n\n<p>This equation, though, does not explicitly minimize the covariate shift for\narbitrary choices of $\\psi$; the $\\psi$ needs to be chosen to best simulate the\nerror of the final robot’s policy, which may be complex for high dimensional\naction spaces.  One approach to choose $\\psi$ is grid-search, but this requires\nexpensive data collection, which can be prohibitive in the physical world or in\nhigh fidelity simulation.</p>\n\n<p>Instead of grid-search, we can formulate the selection of $\\psi$ as a maximum\nlikelihood problem. The objective is to increase the probability of the\nsupervisor applying the robot’s control.</p>\n\n<script type=""math/tex; mode=display"">\\underset{\\psi}{\\mbox{min}} \\: E_{p(\\xi|\\pi_{\\theta^R})} -\\sum^{T-1}_{t=0} \\:\n\\mbox{log} [\\pi_{\\theta^*}(\\pi_{\\theta^R}(\\mathbf{x_t})|\\mathbf{x_t},\\psi)]</script>\n\n<p>This objective states that we want the noise injected supervisor to try and\nmatch the final robot’s policy. In the paper, we show that this explicitly\nminimizes the distance between the supervisor and robot’s distribution. A clear\nlimitation of this optimization problem though is that it requires knowing the\nfinal robot’s distribution $p(\\xi|\\pi_{\\theta^R})$, which is determined only\nafter the data is collected.  In the next section, we present DART, which\napplies an iterative approach to the optimization.</p>\n\n<h2 id=""dart-disturbances-for-augmenting-robot-trajectories"">DART: Disturbances for Augmenting Robot Trajectories</h2>\n\n<p>The above objective cannot be solved because $p(\\xi|\\pi_{\\theta^R})$ is not\nknown until after the robot has been trained.  We can instead iteratively sample\nfrom the supervisor’s distribution with the current noise parameter, $\\psi_k$,\nand minimize the negative log-likelihood of the noise-injected supervisor taking\nthe current robot’s, $\\pi_{\\hat{\\theta}}$, control.</p>\n\n<script type=""math/tex; mode=display"">\\hat{\\psi}_{k+1} = \\underset{\\psi}{\\mbox{argmin}} \\: E_{p(\\xi|\\pi_{\\theta^*},\n\\psi_k)} -\\sum^{T-1}_{t=0}\\mbox{log} \\:\n[\\pi_{\\theta^*}(\\pi_{\\hat{\\theta}}(\\mathbf{x_t})|\\mathbf{x_t},\\psi)]</script>\n\n<p>The above iterative process can be slow to converge because it is optimizing the\nnoise with respect to the current robot’s policy. We can obtain a better\nestimate by observing that the supervisor should simulate as much expected error\nas the final robot policy, <script type=""math/tex"">E_{p(\\xi|\\pi_{\\theta^R})}\nJ(\\theta^R,\\theta^*|\\xi)</script>.  It is possible that we have some knowledge of this\nquantity from previously training on similar domains. In the paper, we show how\nto incorporate this knowledge in the form of a prior. For some common noise\ndistributions, the objective can be solved in closed form, as detailed in the\npaper. Thus, the optimization problem determines the shape of the noise injected\nand the prior helps determine the magnitude.</p>\n\n<p>Our algorithm DART, iteratively solves this optimization problem to best set the\nnoise term. DART is still an iterative algorithm like On-Policy methods.\n<em>Through the iterative process, DART optimizes $\\psi$ to better simulate the\nerror in the final robot’s policy.</em></p>\n\n<h1 id=""evaluating-dart"">Evaluating DART</h1>\n\n<p>To understand how effectively DART reduces covariate shift and to determine if\nit suffers from similar limitations as On-Policy methods, we ran experiments in\n4 MuJoco domains, as shown below. The supervisor was a policy trained with TRPO\nand the noise we injected was Gaussian.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/static/blog/dart/ni_a_results-eps-converted-to.png"" alt=""DART results."" /><br />\n</p>\n\n<p>To test if DART suffers from updating the policy after larger batches, we only\nupdated the model after every $K$ demonstrations for all experiments. DAgger was\nupdated after every demonstration and DAgger-B was updated after every $K$.  The\nresults show that DART is able to have the same performance as DAgger, but is\nsignificantly faster in terms of computation.  DAgger-B is relatively similar in\ncomputation time, but suffers significantly in performance, suggesting DART can\nsignificantly reduce computation time.</p>\n\n<p>We finally compared DART to Behavior Cloning in a human study for the task of\ngrasping in clutter, shown below.  In the task, a Toyota HSR robot was trained to\nreach a goal object by pushing objects away with its gripper.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/static/blog/dart/hsr.gif"" alt=""DART results."" /><br />\n<i>\nToyota HSR Trained with DART for Grasping in Clutter.\n</i>\n</p>\n\n<p>The task is more complex than the one above because the robot now sees images of\nthe world taken from an eye-in-hand camera. We compared 4 humans subjects and\nsaw that by injecting noise in the controller, we were able to receive a win\nover Behavior Cloning of 62%. DART was able to reduce the shift on the task with\nhuman supervisors.</p>\n\n<h1 id=""robotic-bed-making-a-testbed-for-covariate-shift"">Robotic Bed Making: A Testbed for Covariate Shift</h1>\n\n<p>To better understand how errors compound in real world robotic systems, we built\na literal test bed. Robotic Bed Making has been a challenging task in robotics\ndue to it requiring mobile manipulation of deformable objects and sequential\nplanning. Imitation Learning is one way to sidestep some of the challenges of\ndeformable object manipulation because it doesn’t require modeling the bed\nsheets.</p>\n\n<p>The goal of our bed making system was to have a robot learn to stretch the\nsheets over the bed frame. The task was designed so that the robot must learn\none policy to decide where to grasp the bed sheet and another transition policy\nto decide whether the robot should try again or switch to the other bed side.\nWe trained the bed making policy with 50 demonstrations.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/static/blog/dart/bed_system.png"" alt=""DART results, bed-making."" /><br />\n<i>\nBed Making System.\n</i>\n</p>\n\n<p>DART was applied to inject Gaussian noise into grasping policy because we\nassumed there would be considerable error in determining where to grasp. The\noptimized covariance matrix decided to inject more noise in the horizontal\ndirection of the bed, because that is where the edge of the sheet varied more\nsignificantly and subsequently the robot had higher error.</p>\n\n<p>In order to test how large covariate shift was in the system, we can take our\ntrained policy $\\pi_{\\theta^R}$ and write its performance with the following\ndecomposition.</p>\n\n<script type=""math/tex; mode=display"">% <![CDATA[\n\\begin{align}\nE_{p(\\xi |\\pi_{\\theta^R})} J(\\theta^R,\\theta^*|\\xi) &= \\underbrace{E_{p(\\xi |\\pi_{\\theta^R})}  \\sum^T_{t=1} l(\\pi_{\\theta}(x_t), \\pi_{\\theta^*}(x_t)) -  E_{p(\\xi |\\pi_{\\theta^*}, \\psi)}  \\sum^T_{t=1} l(\\pi_{\\theta}(x_t), \\pi_{\\theta^*}(x_t))}_{\\text{Shift}} \\\\\n&+ \\underbrace{E_{p(\\xi |\\pi_{\\theta^*},\\psi)}  \\sum^T_{t=1} l(\\pi_{\\theta}(x_t), \\pi_{\\theta^*}(x_t)) }_{\\text{Loss}},\n\\end{align} %]]></script>\n\n<p>where the first term on the right-hand side corresponds to the covariate shift.\nIntuitively, the covariate shift is the difference between the expected error on\nthe robot’s distribution and the supervisor’s distribution. When we measured\nthese quantities on the bed making setup, we observed noticeable covariate shift\nin the transition policy trained with Behavior Cloning.</p>\n\n<p style=""text-align:center;"">\n<img src=""http://bair.berkeley.edu/static/blog/dart/cs_graph.png"" alt=""DART results, covariate shift."" width=""500"" /><br />\n<i>\nCovariate Shift in Bed Making Task.\n</i>\n</p>\n\n<p>We attribute this covariate shift due to the fact that with Behavior Cloning the\nrobot rarely saw unsuccessful demonstrations; thus the transition policy never\nknew what failure was. DART gave a more diverse set of states, which allowed the\npolicy to have better class balance. DART was able to train a robust policy that\nallowed it to perform the bed making task even when novel objects were placed on\nthe bed, as shown at the beginning of the blog post. When distractor objects are\nplaced on the bed DART obtained a 97% sheet coverage, whereas Behavior Cloning\nachieved only 63%.</p>\n\n<p>These initial results suggest that covariate shift can occur in modern day\nsystems that use learning components. We will soon release a longer preprint on\nthe Bed Making Setup for more information.</p>\n\n<p>DART presents a way to correct for shift via the injection of small optimized\nnoise. Going forward, we are considering more complex noise models that better\ncapture the temporal structure of the robot’s error.</p>\n\n<p>(For papers and updated information, <a href=""http://autolab.berkeley.edu"">see UC Berkeley’s AUTOLAB website</a>.)</p>\n\n<h2 id=""references"">References</h2>\n\n<ol>\n  <li>\n    <p>Michael Laskey, Jonathan Lee, Roy Fox, Anca Dragan, Ken Goldberg ; DART:\nNoise Injection for Robust Imitation Learning Proceedings of the 1st Annual\nConference on Robot Learning, PMLR 78:143-156, 2017.</p>\n  </li>\n  <li>\n    <p>M. Laskey, C. Chuck, J. Lee, J. Mahler, S. Krishnan, K. Jamieson, A. Dragan,\nand K. Goldberg. Comparing human-centric and robot-centric sampling for robot\ndeep learning from demonstrations. Robotics and Automation (ICRA), 2017 IEEE\nInternational Conference on, pages 358-365. IEEE, 2017</p>\n  </li>\n  <li>\n    <p>M. Laskey, J. Lee, C. Chuck, D. Gealy, W. Hsieh, F. T. Pokorny, A. D. Dragan,\nand K. Goldberg. Robot grasping in clutter: Using a hierarchy of supervisors for\nlearning from demonstrations. In Automation Science and Engineering (CASE), 2016\nIEEE International Conference on, pages 827–834. IEEE, 2016.</p>\n  </li>\n  <li>\n    <p>Zhang, Jiakai, and Kyunghyun Cho. “Query-Efficient Imitation Learning for\nEnd-to-End Simulated Driving.” In AAAI, pp. 2891-2897. 2017.</p>\n  </li>\n  <li>\n    <p>W. Sun, A. Venkatraman, G. J. Gordon, B. Boots, and J. A. Bagnell. Deeply\naggrevated: Differentiable imitation learning for sequential prediction.\nProceedings of the 34th International Conference on Machine Learning, PMLR\n70:3309-3318, 2017.</p>\n  </li>\n  <li>\n    <p>Ross, Stéphane, Geoffrey J. Gordon, and Drew Bagnell. “A reduction of\nimitation learning and structured prediction to no-regret online learning.”\nInternational Conference on Artificial Intelligence and Statistics. 2011.</p>\n  </li>\n  <li>\n    <p>S. Ross and D. Bagnell. Efficient reductions for imitation learning. In\nInternational Conference on Artificial Intelligence and Statistics, pages\n661–668, 2010.</p>\n  </li>\n  <li>\n    <p>Ho, Jonathan, and Stefano Ermon. “Generative adversarial imitation learning.”\nAdvances in Neural Information Processing Systems. 2016.</p>\n  </li>\n</ol>\n']"
207,184,207_grasping_grasp_grasps_robotic,"['grasping', 'grasp', 'grasps', 'robotic', 'robotics', 'articulated', 'robot', 'teleoperation', 'learning', 'manipulation']","['Learning Predictive Representations for Deformable Objects Using Contrastive Estimation Using visual model-based learning for deformable object manipulation is challenging due to difficulties in learning plannable visual representations along with complex dynamic models. In this work, we propose a new learning framework that jointly optimizes both the visual representation model and the dynamics model using contrastive estimation. Using simulation data collected by randomly perturbing deformable objects on a table, we learn latent dynamics models for these objects in an offline fashion. Then, using the learned models, we use simple model-based planning to solve challenging deformable object manipulation tasks such as spreading ropes and cloths. Experimentally, we show substantial improvements in performance over standard model-based learning techniques across our rope and cloth manipulation suite. Finally, we transfer our visual manipulation policies trained on data purely collected in simulation to a real PR2 robot through domain randomization.', 'Jacta: A Versatile Planner for Learning Dexterous and Whole-body Manipulation Robotic manipulation is challenging due to discontinuous dynamics, as well as high-dimensional state and action spaces. Data-driven approaches that succeed in manipulation tasks require large amounts of data and expert demonstrations, typically from humans. Existing planners are restricted to specific systems and often depend on specialized algorithms for using demonstrations. Therefore, we introduce a flexible motion planner tailored to dexterous and whole-body manipulation tasks. Our planner creates readily usable demonstrations for reinforcement learning algorithms, eliminating the need for additional training pipeline complexities. With this approach, we can efficiently learn policies for complex manipulation tasks, where traditional reinforcement learning alone only makes little progress. Furthermore, we demonstrate that learned policies are transferable to real robotic systems for solving complex dexterous manipulation tasks. Project website: https://jacta-manipulation.github.io/', 'Learning Generalizable Dexterous Manipulation from Human Grasp Affordance Dexterous manipulation with a multi-finger hand is one of the most challenging problems in robotics. While recent progress in imitation learning has largely improved the sample efficiency compared to Reinforcement Learning, the learned policy can hardly generalize to manipulate novel objects, given limited expert demonstrations. In this paper, we propose to learn dexterous manipulation using large-scale demonstrations with diverse 3D objects in a category, which are generated from a human grasp affordance model. This generalizes the policy to novel object instances within the same category. To train the policy, we propose a novel imitation learning objective jointly with a geometric representation learning objective using our demonstrations. By experimenting with relocating diverse objects in simulation, we show that our approach outperforms baselines with a large margin when manipulating novel objects. We also ablate the importance of 3D object representation learning for manipulation. We include videos and code on the project website: https://kristery.github.io/ILAD/ .']"
208,36,208_classifiers_imbalanced_imbalance_classifier,"['classifiers', 'imbalanced', 'imbalance', 'classifier', 'ensemble', 'classification', 'ensembles', 'undersampling', 'resamplingbased', 'oversample']","['REBAGG: REsampled BAGGing for Imbalanced Regression The problem of imbalanced domains is important in multiple real world applications. This problem has been thoroughly studied for classification tasks. In particular, the adaptation of ensembles to tackle imbalanced domains has shown important advantages in a classification context. Still, for imbalanced regression problems only a few solutions exist. Moreover, the capabilities of ensembles for dealing with imbalanced regression tasks is yet to be explored. In this paper we present the REsampled BAGGing (REBAGG) algorithm, a bagging-based ensemble method that incorporates data pre-processing strategies for addressing imbalanced domains in regression tasks. The extensive experimental evaluation conducted shows the advantage of our proposal in a diverse set of domains and learning algorithms.', 'Influence of minority class instance types on SMOTE imbalanced data oversampling Despite more than two decades of intense research, learning from imbalanced data still remains as one of the major difficulties posed for computational intelligence systems. Among plethora of techniques dedicated to alleviating this problem, preprocessing algorithms are considered among the most efficient ones. They aim at re-balancing the training set by either undersampling of the majority class, or oversampling of the minority one. Here, Synthetic Minority Oversampling Technique, commonly known as SMOTE, stands as the most popular solution that introduces artificial instances on the basis of minority class neighborhood distribution. However, many recent works point out to the fact that the imbalanced ratio itself is not the sole source of learning difficulties in such scenarios. One should take a deeper look into the minority class structure in order to identify which instances influence the performance of classifiers in most significant manner. In this paper, we propose to investigate the role of minority class instance types on the performance of SMOTE. To achieve this, instead of oversampling uniformly the minority class, we preprocess only selected subsets of instances, based on their individual difficulties. Experimental study proves that such a selective oversampling leads to improved classification performance.', 'Oversampling Tabular Data with Deep Generative Models: Is it worth the effort? In practice, machine learning experts are often confronted with imbalanced data. Without accounting for the imbalance, common classifiers perform poorly, and standard evaluation metrics mislead the practitioners on the model’s performance. A standard method to treat imbalanced datasets is under- and oversampling. In this process, samples are removed from the majority class, or synthetic samples are added to the minority class. In this paper, we follow up on recent developments in deep learning. We take proposals of deep generative models and study these approaches’ ability to provide realistic samples that improve performance on imbalanced classification tasks via oversampling. Across 160K+ experiments, we show that the improvements in terms of performance metric, while shown to be significant when ranking the methods like in the literature, often are minor in absolute terms, especially compared to the required effort. Furthermore, we notice that a large part of the improvement is due to undersampling, not oversampling.']"
209,157,209_tensors_tensor_sparse_tucker,"['tensors', 'tensor', 'sparse', 'tucker', 'factorizations', 'decomposition', 'decompositions', 'factorization', 'multilinear', 'algorithms']","[' Scaling and Scalability: Provable Nonconvex Low-Rank Tensor Completion   Tensors, which provide a powerful and flexible model for representing multi-attribute data and multi-way interactions, play an indispensable role in modern data science across various fields in science and engineering. A fundamental task is tensor completion, which aims to faithfully recover the tensor from a small subset of its entries in a statistically and computationally efficient manner. Harnessing the low-rank structure of tensors in the Tucker decomposition, this paper develops a scaled gradient descent (ScaledGD) algorithm to directly recover the tensor factors with tailored spectral initializations, and shows that it provably converges at a linear rate independent of the condition number of the ground truth tensor for tensor completion as soon as the sample size is above the order of $n^{3/2}$ ignoring other parameter dependencies, where $n$ is the dimension of the tensor. To the best of our knowledge, ScaledGD is the first algorithm that achieves near-optimal statistical and computational complexities simultaneously for low-rank tensor completion with the Tucker decomposition. Our algorithm highlights the power of appropriate preconditioning in accelerating nonconvex statistical estimation, where the iteration-varying preconditioners promote desirable invariance properties of the trajectory with respect to the underlying symmetry in low-rank tensor factorization. ', 'Statistical and Computational Limits for Tensor-on-Tensor Association Detection In this paper, we consider the tensor-on-tensor association detection problem, where the goal is to detect whether there is an association between the tensor responses to tensor covariates linked via a low-rank tensor parameter. We first In this paper, we consider the tensor-on-tensor association detection problem, where the goal is to detect whether there is an association between the tensor responses to tensor covariates linked via a low-rank tensor parameter. We first develop tight bounds on the signal-to-noise ratio (SNR) such that the detection problem is statistically possible. We then provide testing procedures that succeed when the SNR is above the threshold. On the other hand, the statistical optimal tests often require computing the largest singular value of a given tensor, which can be NP-hard in general. To complement that, we develop efficient polynomial-time testing procedures with provable guarantees. We also develop matching lower bounds under the Statistical Query model and show that the SNRs required by the proposed polynomial-time algorithms are essential for computational efficiency. We identify a gap that appears between the SNR requirements of the optimal unconstrained-time tests and polynomial-time tests if and only if the sum of the tensor response order and the tensor covariate order is no less than three. To our best knowledge, this is the first complete characterization of the statistical and computational limits for the general tensor-on-tensor association detection problem. Our findings significantly generalize the results in the literature on signal detection in linear regression and low-rank matrix trace regression. Finally, the connection on the computational hardness of the detection problem and the corresponding estimation problem is discussed.', 'Tensor vs. Matrix Methods: Robust Tensor Decomposition under Block Sparse Perturbations Robust tensor CP decomposition involves decomposing  a tensor into low rank and sparse components. We propose a novel non-convex iterative algorithm with guaranteed recovery.  It alternates  between  low-rank CP decomposition through gradient ascent (a variant of the tensor power method), and  hard thresholding of the residual. We prove convergence to the globally optimal solution   under natural incoherence conditions on the low rank component, and bounded level of sparse perturbations. We compare our method with natural baselines, which apply robust  matrix PCA either to the \\em flattened tensor, or to the matrix slices of the tensor. Our method can provably handle a far greater level of perturbation  when the sparse  tensor is  block-structured. This naturally occurs in many applications such as  the foreground-background separation task in videos. Our experiments validate these findings. Thus, we establish that tensor methods can tolerate  a higher level of gross corruptions compared to matrix methods.']"
210,10,210_generative_models_boltzmann_energy,"['generative', 'models', 'boltzmann', 'energy', 'energybased', 'modelsem', 'modeling', 'likelihoodfree', 'unnormalized', 'model']","['On Energy-Based Models with Overparametrized Shallow Neural Networks Energy-based models (EBMs) are a simple yet powerful framework for generative modeling. They are based on a trainable energy function which defines an associated Gibbs measure, and they can be trained and sampled from via well-established statistical tools, such as MCMC. Neural networks may be used as energy function approximators, providing both a rich class of expressive models as well as a flexible device to incorporate data structure. In this work we focus on shallow neural networks. Building from the incipient theory of overparametrized neural networks, we show that models trained in the so-called ’active’ regime provide a statistical advantage over their associated ’lazy’ or kernel regime, leading to improved adaptivity to hidden low-dimensional structure in the data distribution, as already observed in supervised learning. Our study covers both the maximum likelihood and Stein Discrepancy estimators, and we validate our theoretical results with numerical experiments on synthetic data.', 'Autoregressive Energy Machines Neural density estimators are flexible families of parametric models which have seen widespread use in unsupervised machine learning in recent years. Maximum-likelihood training typically dictates that these models be constrained to specify an explicit density. However, this limitation can be overcome by instead using a neural network to specify an energy function, or unnormalized density, which can subsequently be normalized to obtain a valid distribution. The challenge with this approach lies in accurately estimating the normalizing constant of the high-dimensional energy function. We propose the Autoregressive Energy Machine, an energy-based model which simultaneously learns an unnormalized density and computes an importance-sampling estimate of the normalizing constant for each conditional in an autoregressive decomposition. The Autoregressive Energy Machine achieves state-of-the-art performance on a suite of density-estimation tasks.', 'On Investigating the Conservative Property of Score-Based Generative Models Existing Score-Based Models (SBMs) can be categorized into constrained SBMs (CSBMs) or unconstrained SBMs (USBMs) according to their parameterization approaches. CSBMs model probability density functions as Boltzmann distributions, and assign their predictions as the negative gradients of some scalar-valued energy functions. On the other hand, USBMs employ flexible architectures capable of directly estimating scores without the need to explicitly model energy functions. In this paper, we demonstrate that the architectural constraints of CSBMs may limit their modeling ability. In addition, we show that USBMs’ inability to preserve the property of conservativeness may lead to degraded performance in practice. To address the above issues, we propose Quasi-Conservative Score-Based Models (QCSBMs) for keeping the advantages of both CSBMs and USBMs. Our theoretical derivations demonstrate that the training objective of QCSBMs can be efficiently integrated into the training processes by leveraging the Hutchinson’s trace estimator. In addition, our experimental results on the CIFAR-10, CIFAR-100, ImageNet, and SVHN datasets validate the effectiveness of QCSBMs. Finally, we justify the advantage of QCSBMs using an example of a one-layered autoencoder.']"
211,47,211_classifiers_classifier_classification_misclassification,"['classifiers', 'classifier', 'classification', 'misclassification', 'multiclass', 'supervised', 'boosting', 'prediction', 'loss', 'losses']","['Surrogate regret bounds for generalized classification performance metrics We consider optimization of generalized performance metrics for binary classification by means of surrogate loss.  We focus on a class of metrics, which are linear-fractional functions of the false positive and false negative rates (examples of which include $F_\\\\beta$-measure, Jaccard similarity coefficient, AM measure, and many others). Our analysis concerns the following two-step procedure. First, a real-valued function $f$ is learned by minimizing a surrogate loss for binary classification on the training sample. It is assumed that the surrogate loss is a strongly proper composite loss function (examples of which include logistic loss, squared-error loss, exponential loss, etc.). Then, given $f$, a threshold $\\\\hat{\\\\theta}$ is tuned on a separate validation sample, by direct optimization of the target performance measure. We show that the regret of the resulting classifier (obtained from thresholding $f$ on $\\\\hat{\\\\theta}$  measured with respect to the target metric is upperbounded by the regret of f measured with respect to the surrogate loss.  Our finding is further analyzed in\xa0a\xa0computational study on both synthetic and real data\xa0sets.', 'Cost-sensitive Multiclass Classification Risk Bounds A commonly used approach to multiclass classification is to replace the 0-1 loss with a convex surrogate so as to make empirical risk minimization computationally tractable. Previous work has uncovered sufficient and necessary conditions for the consistency of the resulting procedures. In this paper, we strengthen these results by showing how the 0-1 excess loss of a predictor can be upper bounded as a function of the excess loss of the predictor measured using the convex surrogate. The bound is developed for the case of cost-sensitive multiclass classification and a convex surrogate loss that goes back to the work of  Lee, Lin and Wahba. The bounds are as easy to calculate as in binary classification. Furthermore, we also show that our analysis extends to the analysis of the recently introduced “Simplex Coding” scheme.', 'On the Consistency of Output Code Based Learning Algorithms for Multiclass Learning Problems A popular approach to solving multiclass learning problems is to reduce them to a set of binary classification problems through some output code matrix: the widely used one-vs-all and all-pairs methods, and the error-correcting output code methods of Dietterich and Bakiri (1995), can all be viewed as special cases of this approach. In this paper, we consider the question of statistical consistency of such methods. We focus on settings where the binary problems are solved by minimizing a binary surrogate loss, and derive general conditions on the binary surrogate loss under which the one-vs-all and all-pairs code matrices yield consistent algorithms with respect to the multiclass 0-1 loss. We then consider general multiclass learning problems defined by a general multiclass loss, and derive conditions on the output code matrix and binary surrogates under which the resulting algorithm is consistent with respect to the target multiclass loss. We also consider \\emphprobabilistic code matrices, where one reduces a multiclass problem to a set of \\emphclass probability labeled binary problems, and show that these can yield benefits in the sense of requiring a smaller number of binary problems to achieve overall consistency. Our analysis makes interesting connections with the theory of proper composite losses (Buja et al., 2005; Reid and Williamson, 2010); these play a role in constructing the right ‘decoding’ for converting the predictions on the binary problems to the final multiclass prediction. To our knowledge, this is the first work that comprehensively studies consistency properties of output code based methods for multiclass learning.']"
212,82,212_classifiers_treebased_trees_forests,"['classifiers', 'treebased', 'trees', 'forests', 'tree', 'forest', 'ensembles', 'classifier', 'boosting', 'ensemble']","['Learning Accurate and Interpretable Decision Trees Decision trees are a popular tool in machine learning and yield easy-to-understand models. Several techniques have been proposed in the literature for learning a decision tree classifier, with different techniques working well for data from different domains. In this work, we develop approaches to design decision tree learning algorithms given repeated access to data from the same domain. We propose novel parameterized classes of node splitting criteria in top-down algorithms, which interpolate between popularly used entropy and Gini impurity based criteria, and provide theoretical bounds on the number of samples needed to learn the splitting function appropriate for the data at hand. We also study the sample complexity of tuning prior parameters in Bayesian decision tree learning, and extend our results to decision tree regression. We further consider the problem of tuning hyperparameters in pruning the decision tree for classical pruning algorithms including min-cost complexity pruning. We also study the interpretability of the learned decision trees and introduce a data-driven approach for optimizing the explainability versus accuracy trade-off using decision trees. Finally, we demonstrate the significance of our approach on real world datasets by learning data-specific decision trees which are simultaneously more accurate and interpretable.', 'Smaller, more accurate regression forests using tree alternating optimization Regression forests, based on ensemble approaches such as bagging or boosting, have long been recognized as the leading off-the-shelf method for regression. However, forests rely on a greedy top-down procedure such as CART to learn each tree. We extend a recent algorithm for learning classification trees, Tree Alternating Optimization (TAO), to the regression case, and use it with bagging to construct regression forests of oblique trees, having hyperplane splits at the decision nodes. In a wide range of datasets, we show that the resulting forests exceed the accuracy of state-of-the-art algorithms such as random forests, AdaBoost or gradient boosting, often considerably, while yielding forests that have usually fewer and shallower trees and hence fewer parameters and faster inference overall. This result has an immense practical impact and advocates for the power of optimization in ensemble learning.', 'On Computing Optimal Tree Ensembles Random forests and, more generally, (decision-)tree ensembles are widely used methods for classification and regression. Recent algorithmic advances allow to compute decision trees that are optimal for various measures such as their size or depth. We are not aware of such research for tree ensembles and aim to contribute to this area. Mainly, we provide two novel algorithms and corresponding lower bounds. First, we are able to carry over and substantially improve on tractability results for decision trees, obtaining a $(6\\delta D S)^S \\cdot \\mathrm{poly}$-time algorithm, where $S$ is the number of cuts in the tree ensemble, $D$ the largest domain size, and $\\delta$ is the largest number of features in which two examples differ. To achieve this, we introduce the witness-tree technique which also seems promising for practice. Second, we show that dynamic programming, which has been successful for decision trees, may also be viable for tree ensembles, providing an $\\ell^n \\cdot \\mathrm{poly}$-time algorithm, where $\\ell$ is the number of trees and $n$ the number of examples. Finally, we compare the number of cuts necessary to classify training data sets for decision trees and tree ensembles, showing that ensembles may need exponentially fewer cuts for increasing number of trees.']"
213,36,213_disentangling_disentangled_disentanglement_autoencoders,"['disentangling', 'disentangled', 'disentanglement', 'autoencoders', 'disentangles', 'disentangle', 'representations', 'entangled', 'learning', 'decoder']","['Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look at recent progress in the field and challenge some common assumptions. We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train more than $12000$ models covering most prominent methods and evaluation metrics in a reproducible large-scale experimental study on seven different data sets. We observe that while the different methods successfully enforce properties “encouraged” by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision. Furthermore, increased disentanglement does not seem to lead to a decreased sample complexity of learning for downstream tasks. Our results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets.', 'Disentanglement Learning via Topology We propose TopDis (Topological Disentanglement), a method for learning disentangled representations via adding a multi-scale topological loss term. Disentanglement is a crucial property of data representations substantial for the explainability and robustness of deep learning models and a step towards high-level cognition. The state-of-the-art methods are based on VAE and encourage the joint distribution of latent variables to be factorized. We take a different perspective on disentanglement by analyzing topological properties of data manifolds. In particular, we optimize the topological similarity for data manifolds traversals. To the best of our knowledge, our paper is the first one to propose a differentiable topological loss for disentanglement learning. Our experiments have shown that the proposed TopDis loss improves disentanglement scores such as MIG, FactorVAE score, SAP score, and DCI disentanglement score with respect to state-of-the-art results while preserving the reconstruction quality. Our method works in an unsupervised manner, permitting us to apply it to problems without labeled factors of variation. The TopDis loss works even when factors of variation are correlated. Additionally, we show how to use the proposed topological loss to find disentangled directions in a trained GAN.', 'Orthogonality-Enforced Latent Space in Autoencoders: An Approach to Learning Disentangled Representations Noting the importance of factorizing (or disentangling) the latent space, we propose a novel, non-probabilistic disentangling framework for autoencoders, based on the principles of symmetry transformations that are independent of one another. To the best of our knowledge, this is the first deterministic model that is aiming to achieve disentanglement based on autoencoders using only a reconstruction loss without pairs of images or labels, by explicitly introducing inductive biases into a model architecture through Euler encoding. The proposed model is then compared with a number of state-of-the-art models, relevant to disentanglement, including symmetry-based models and generative models. Our evaluation using six different disentanglement metrics, including the unsupervised disentanglement metric we propose here in this paper, shows that the proposed model can offer better disentanglement, especially when variances of the features are different, where other methods may struggle. We believe that this model opens several opportunities for linear disentangled representation learning based on deterministic autoencoders.']"
214,30,214_autoencoders_autoencoder_generative_encoder,"['autoencoders', 'autoencoder', 'generative', 'encoder', 'decoders', 'decoder', 'decoding', 'variational', 'representations', 'regularization']","['Simple and Effective VAE Training with Calibrated Decoders Variational autoencoders (VAEs) provide an effective and simple method for modeling complex distributions. However, training VAEs often requires considerable hyperparameter tuning to determine the optimal amount of information retained by the latent variable. We study the impact of calibrated decoders, which learn the uncertainty of the decoding distribution and can determine this amount of information automatically, on the VAE performance. While many methods for learning calibrated decoders have been proposed, many of the recent papers that employ VAEs rely on heuristic hyperparameters and ad-hoc modifications instead. We perform the first comprehensive comparative analysis of calibrated decoder and provide recommendations for simple and effective VAE training. Our analysis covers a range of datasets and several single-image and sequential VAE models. We further propose a simple but novel modification to the commonly used Gaussian decoder, which computes the prediction variance analytically. We observe empirically that using heuristic modifications is not necessary with our method.', 'Avoiding Latent Variable Collapse with Generative Skip Models Variational autoencoders (VAEs) learn distributions of high-dimensional data. They model data with a deep latent-variable model and then fit the model by maximizing a lower bound of the log marginal likelihood. VAEs can capture complex distributions, but they can also suffer from an issue known as ""latent variable collapse,"" especially if the likelihood model is powerful. Specifically, the lower bound involves an approximate posterior of the latent variables; this posterior ""collapses"" when it is set equal to the prior, i.e., when the approximate posterior is independent of the data. While VAEs learn good generative models, latent variable collapse prevents them from learning useful representations. In this paper, we propose a simple new way to avoid latent variable collapse by including skip connections in our generative model; these connections enforce strong links between the latent variables and the likelihood function. We study generative skip models both theoretically and empirically. Theoretically, we prove that skip models increase the mutual information between the observations and the inferred latent variables. Empirically, we study images (MNIST and Omniglot) and text (Yahoo). Compared to existing VAE architectures, we show that generative skip models maintain similar predictive performance but lead to less collapse and provide more meaningful representations of the data.', 'Marginalization is not Marginal: No Bad VAE Local Minima when Learning Optimal Sparse Representations Although the variational autoencoder (VAE) represents a widely-used deep generative model, the underlying energy function when applied to continuous data remains poorly understood. In fact, most prior theoretical analysis has assumed a simplified affine decoder such that the model collapses to probabilistic PCA, a restricted regime whereby existing classical algorithms can also be trivially applied to guarantee globally optimal solutions. To push our understanding into more complex, practically-relevant settings, this paper instead adopts a deceptively sophisticated single-layer decoder that nonetheless allows the VAE to address the fundamental challenge of learning optimally sparse representations of continuous data originating from popular multiple-response regression models. In doing so, we can then examine VAE properties within the non-trivial context of solving difficult, NP-hard inverse problems. More specifically, we prove rigorous conditions which guarantee that any minimum of the VAE energy (local or global) will produce the optimally sparse latent representation, meaning zero reconstruction error using a minimal number of active latent dimensions. This is ultimately possible because VAE marginalization over the latent posterior selectively smooths away bad local minima as has been conjectured but not actually proven in prior work. We then discuss how equivalent-capacity deterministic autoencoders, even with appropriate sparsity-promoting regularization of the latent space, maintain bad local minima that do not correspond with such parsimonious representations. Overall, these results serve to elucidate key properties of the VAE loss surface relative to finding low-dimensional structure in data.']"
215,11,215_estimation_estimators_convexity_estimating,"['estimation', 'estimators', 'convexity', 'estimating', 'convex', 'convexitya', 'optimal', 'estimate', 'nonparametric', 'estimator']","['Gaussian Regression with Convex Constraints The focus of this paper is the linear model with Gaussian design under convex constraints. Specifically, we study the performance of the constrained least squares estimate. We derive two general results characterizing its performance - one requiring a tangent cone structure, and one which holds in a general setting. We use our general results to analyze three functional shape constrained problems where the signal is generated from an underlying Lipschitz, monotone or convex function. In each of the examples we show specific classes of functions which achieve fast adaptive estimation rates, and we also provide non-adaptive estimation rates which hold for any function. Our results demonstrate that the Lipschitz, monotone and convex constraints allow one to analyze regression problems even in high-dimensional settings where the dimension may scale as the square or fourth degree of the sample size respectively.', 'An Efficient Minimax Optimal Estimator For Multivariate Convex Regression We study the computational aspects of the task of multivariate convex regression in dimension $d \\geq 5$. We present the first computationally efficient minimax optimal (up to logarithmic factors) estimators for the tasks of $L$-Lipschitz and $\\Gamma$-bounded convex regression under polytopal support. This work is the first to show the existence of efficient minimax optimal estimators for non-Donsker classes whose corresponding Least Squares Estimators are provably minimax suboptimal. The proof of the correctness of these estimators uses a variety of tools from different disciplines, among them empirical process theory, stochastic geometry, and potential theory.', 'Nonparametric Sharpe Ratio Function Estimation in Heteroscedastic Regression Models via Convex Optimization We consider maximum likelihood estimation (MLE) of heteroscedastic regression models based on a new “parametrization” of the likelihood in terms of the Sharpe ratio function, or the ratio of the mean and volatility functions. While with a standard parametrization the MLE problem is not convex and hence hard to solve globally, our parametrization leads to a functional that is jointly convex in the Sharpe ratio and inverse volatility functions. The major difficulty with the resulting infinite-dimensional convex program is the shape constraint on the inverse volatility function. We propose to solve the problem by solving a sequence of finite-dimensional convex programs with increasing dimensions, which can be done globally and efficiently. We demonstrate that, when the goal is to estimate the Sharpe ratio function directly, the finite-sample performance of the proposed estimation method is superior to existing methods that estimate the mean and variance functions separately. When applied to a financial dataset, our method captures a well-known covariate-dependent effect on the Shape ratio.']"
216,32,216_variational_generative_autoencoders_autoencoder,"['variational', 'generative', 'autoencoders', 'autoencoder', 'doublyreparameterized', 'inference', 'models', 'gradient', 'reparameterization', 'divergence']","['Inference Suboptimality in Variational Autoencoders Amortized inference allows latent-variable models trained via variational learning to scale to large datasets. The quality of approximate inference is determined by two factors: a) the capacity of the variational distribution to match the true posterior and b) the ability of the recognition network to produce good variational parameters for each datapoint. We examine approximate inference in variational autoencoders in terms of these factors. We find that divergence from the true posterior is often due to imperfect recognition networks, rather than the limited complexity of the approximating distribution. We show that this is due partly to the generator learning to accommodate the choice of approximation. Furthermore, we show that the parameters used to increase the expressiveness of the approximation play a role in generalizing inference rather than simply improving the complexity of the approximation.', 'Forward Amortized Inference for Likelihood-Free Variational Marginalization In this paper, we introduce a new form of amortized variational inference by using the forward KL divergence in a joint-contrastive variational loss. The resulting forward amortized variational inference is a likelihood-free method as its gradient can be sampled without bias and without requiring any evaluation of either the model joint distribution or its derivatives. We prove that our new variational loss is optimized by the exact posterior marginals in the fully factorized mean-field approximation, a property that is not shared with the more conventional reverse KL inference. Furthermore, we show that forward amortized inference can be easily marginalized over large families of latent variables in order to obtain a marginalized variational posterior. We consider two examples of variational marginalization. In our first example we train a Bayesian forecaster for predicting a simplified chaotic model of atmospheric convection. In the second example we train an amortized variational approximation of a Bayesian optimal classifier by marginalizing over the model space. The result is a powerful meta-classification network that can solve arbitrary classification problems without further training.', 'Training Variational Autoencoders with Buffered Stochastic Variational Inference The recognition network in deep latent variable models such as variational autoencoders (VAEs) relies on amortized inference for efficient posterior approximation that can scale up to large datasets. However, this technique has also been demonstrated to select suboptimal variational parameters, often resulting in considerable additional error called the amortization gap. To close the amortization gap and improve the training of the generative model, recent works have introduced an additional refinement step that applies stochastic variational inference (SVI) to improve upon the variational parameters returned by the amortized inference model. In this paper, we propose the Buffered Stochastic Variational Inference (BSVI), a new refinement procedure that makes use of SVI’s sequence of intermediate variational proposal distributions and their corresponding importance weights to construct a new generalized importance-weighted lower bound. We demonstrate empirically that training the variational autoencoders with BSVI consistently out-performs SVI, yielding an improved training procedure for VAEs.']"
217,60,217_variational_stochastic_inference_probabilistic,"['variational', 'stochastic', 'inference', 'probabilistic', 'bayesian', 'mcmc', 'likelihoods', 'parameterizations', 'posterior', 'models']","['Variational Sequential Monte Carlo Many recent advances in large scale probabilistic inference rely on variational methods. The success of variational approaches depends on (i) formulating a flexible parametric family of distributions, and (ii) optimizing the parameters to find the member of this family that most closely approximates the exact posterior. In this paper we present a new approximating family of distributions, the variational sequential Monte Carlo (VSMC) family, and show how to optimize it in variational inference. VSMC melds variational inference (VI) and sequential Monte Carlo (SMC), providing practitioners with flexible, accurate, and powerful Bayesian inference. The VSMC family is a variational family that can approximate the posterior arbitrarily well, while still allowing for efficient optimization of its parameters. We demonstrate its utility on state space models, stochastic volatility models for financial data, and deep Markov models of brain neural circuits.', 'Trust Region Sequential Variational Inference Stochastic variational inference has emerged as an effective method for performing inference on or learning complex models for data. Yet, one of the challenges in stochastic variational inference is handling high-dimensional data, such as sequential data, and models with non-differentiable densities caused by, for instance, the use of discrete latent variables. In such cases, it is challenging to control the variance of the gradient estimator used in stochastic variational inference, while low variance is often one of the key properties needed for successful inference. In this work, we present a new algorithm for stochastic variational inference of sequential models which trades off bias for variance to tackle this challenge effectively. Our algorithm is inspired by variance reduction techniques in reinforcement learning, yet it uniquely adopts their key ideas in the context of stochastic variational inference. We demonstrate the effectiveness of our approach through formal analysis and experiments on synthetic and real-world datasets.', 'Markov Chain Monte Carlo and Variational Inference: Bridging the Gap Recent advances in stochastic gradient variational inference have made it possible to perform variational Bayesian inference with posterior approximations containing auxiliary random variables. This enables us to explore a new synthesis of variational inference and Monte Carlo methods where we incorporate one or more steps of MCMC into our variational approximation. By doing so we obtain a rich class of inference algorithms bridging the gap between variational methods and MCMC, and offering the best of both worlds: fast posterior approximation through the maximization of an explicit objective, with the option of trading off additional computation for additional accuracy. We describe the theoretical foundations that make this possible and show some promising first results.']"
218,10,218_interpolators_interpolation_interpolator_interpolating,"['interpolators', 'interpolation', 'interpolator', 'interpolating', 'overparameterized', 'classifiers', 'robustness', 'lipschitzness', 'arbitrary', 'bound']","['Fast rates for noisy interpolation require rethinking the effect of inductive bias Good generalization performance on high-dimensional data crucially hinges on a simple structure of the ground truth and a corresponding strong inductive bias of the estimator. Even though this intuition is valid for regularized models, in this paper we caution against a strong inductive bias for interpolation in the presence of noise: While a stronger inductive bias encourages a simpler structure that is more aligned with the ground truth, it also increases the detrimental effect of noise. Specifically, for both linear regression and classification with a sparse ground truth, we prove that minimum $\\ell_p$-norm and maximum $\\ell_p$-margin interpolators achieve fast polynomial rates close to order $1/n$ for $p > 1$ compared to a logarithmic rate for $p = 1$. Finally, we provide preliminary experimental evidence that this trade-off may also play a crucial role in understanding non-linear interpolating models used in practice.', 'Minimum Norm Interpolation Meets The Local Theory of Banach Spaces Minimum-norm interpolators have recently gained attention primarily as an analyzable model to shed light on the double descent phenomenon observed for neural networks. The majority of the work has focused on analyzing interpolators in Hilbert spaces, where typically an effectively low-rank structure of the feature covariance prevents a large bias. More recently, tight vanishing bounds have also been shown for isotropic high-dimensional data for $\\ell_p$-spaces with $p\\in[1,2)$, leveraging sparse structure of the ground truth. However, these proofs are tailored to specific settings and hard to generalize. This paper takes a first step towards establishing a general framework that connects generalization properties of the interpolators to well-known concepts from high-dimensional geometry, specifically, from the local theory of Banach spaces. In particular, we show that under $2$-uniform convexity, the bias of the minimal norm solution is bounded by the Gaussian complexity of the class. We then prove a “reverse” Efron-Stein lower bound on the expected conditional variance of the minimal norm solution under cotype $2$. Finally, we prove that this bound is sharp for $\\ell_p$-linear regression under sub-Gaussian covariates.', 'Exact Gap between Generalization Error and Uniform Convergence in Random Feature Models Recent work showed that there could be a large gap between the classical uniform convergence bound and the actual test error of zero-training-error predictors (interpolators) such as deep neural networks. To better understand this gap, we study the uniform convergence in the nonlinear random feature model and perform a precise theoretical analysis on how uniform convergence depends on the sample size and the number of parameters. We derive and prove analytical expressions for three quantities in this model: 1) classical uniform convergence over norm balls, 2) uniform convergence over interpolators in the norm ball (recently proposed by\xa0\\citet{zhou2021uniform}), and 3) the risk of minimum norm interpolator. We show that, in the setting where the classical uniform convergence bound is vacuous (diverges to $\\infty$), uniform convergence over the interpolators still gives a non-trivial bound of the test error of interpolating solutions. We also showcase a different setting where classical uniform convergence bound is non-vacuous, but uniform convergence over interpolators can give an improved sample complexity guarantee. Our result provides a first exact comparison between the test errors and uniform convergence bounds for interpolators beyond simple linear models.']"
219,44,219_labels_labelnoise_label_classifiers,"['labels', 'labelnoise', 'label', 'classifiers', 'classifier', 'classification', 'noisy', 'multilabel', 'learning', 'labeldependent']","['Estimating Instance-dependent Bayes-label Transition Matrix using a Deep Neural Network In label-noise learning, estimating the transition matrix is a hot topic as the matrix plays an important role in building statistically consistent classifiers. Traditionally, the transition from clean labels to noisy labels (i.e., clean-label transition matrix (CLTM)) has been widely exploited to learn a clean label classifier by employing the noisy data. Motivated by that classifiers mostly output Bayes optimal labels for prediction, in this paper, we study to directly model the transition from Bayes optimal labels to noisy labels (i.e., Bayes-label transition matrix (BLTM)) and learn a classifier to predict Bayes optimal labels. Note that given only noisy data, it is ill-posed to estimate either the CLTM or the BLTM. But favorably, Bayes optimal labels have less uncertainty compared with the clean labels, i.e., the class posteriors of Bayes optimal labels are one-hot vectors while those of clean labels are not. This enables two advantages to estimate the BLTM, i.e., (a) a set of examples with theoretically guaranteed Bayes optimal labels can be collected out of noisy data; (b) the feasible solution space is much smaller. By exploiting the advantages, we estimate the BLTM parametrically by employing a deep neural network, leading to better generalization and superior classification performance.', 'Does label smoothing mitigate label noise? Label smoothing is commonly used in training deep learning models, wherein one-hot training labels are mixed with uniform label vectors. Empirically, smoothing has been shown to improve both predictive performance and model calibration. In this paper, we study whether label smoothing is also effective as a means of coping with label noise. While label smoothing apparently amplifies this problem — being equivalent to injecting symmetric noise to the labels — we show how it relates to a general family of loss-correction techniques from the label noise literature. Building on this connection, we show that label smoothing is competitive with loss-correction under label noise. Further, we show that when distilling models from noisy data, label smoothing of the teacher is beneficial; this is in contrast to recent findings for noise-free problems, and sheds further light on settings where label smoothing is beneficial.', 'Error-Bounded Correction of Noisy Labels To collect large scale annotated data, it is inevitable to introduce label noise, i.e., incorrect class labels. To be robust against label noise, many successful methods rely on the noisy classifiers (i.e., models trained on the noisy training data) to determine whether a label is trustworthy. However, it remains unknown why this heuristic works well in practice. In this paper, we provide the first theoretical explanation for these methods. We prove that the prediction of a noisy classifier can indeed be a good indicator of whether the label of a training data is clean. Based on the theoretical result, we propose a novel algorithm that corrects the labels based on the noisy classifier prediction. The corrected labels are consistent with the true Bayesian optimal classifier with high probability. We incorporate our label correction algorithm into the training of deep neural networks and train models that achieve superior testing performance on multiple public datasets.']"
220,68,220_langevin_stochastic_mcmc_logdensity,"['langevin', 'stochastic', 'mcmc', 'logdensity', 'diffusion', 'diffusions', 'hessian', 'sampling', 'discretization', 'wasserstein']","['Wasserstein Control of Mirror Langevin Monte Carlo  Discretized Langevin diffusions are efficient Monte Carlo methods for sampling from high dimensional target densities that are log-Lipschitz-smooth and (strongly) log-concave. In particular, the Euclidean Langevin Monte Carlo sampling algorithm has received much attention lately, leading to a detailed understanding of its non-asymptotic convergence properties and of the role that smoothness and log-concavity play in the convergence rate. Distributions that do not possess these regularity properties can be addressed by considering a Riemannian Langevin diffusion with a metric capturing the local geometry of the log-density. However, the Monte Carlo algorithms derived from discretizations of such Riemannian Langevin diffusions are notoriously difficult to analyze. In this paper, we consider Langevin diffusions on a Hessian-type manifold and study a discretization that is closely related to the mirror-descent scheme. We establish for the first time a non-asymptotic upper-bound on the sampling error of the resulting Hessian Riemannian Langevin Monte Carlo algorithm. This bound is measured according to a Wasserstein distance induced by a Riemannian metric ground cost capturing the squared Hessian structure and closely related to a self-concordance-like condition. The upper-bound implies, for instance, that the iterates contract toward a Wasserstein ball around the target density whose radius is made explicit. Our theory recovers existing Euclidean results and can cope with a wide variety of Hessian metrics related to highly non-flat geometries. ', 'Resolving the Mixing Time of the Langevin Algorithm to its Stationary Distribution for Log-Concave Sampling Sampling from a high-dimensional distribution is a fundamental task in statistics, engineering, and the sciences. A canonical approach is the Langevin Algorithm, i.e., the Markov chain for the discretized Langevin Diffusion. This is the sampling analog of Gradient Descent. Despite being studied for several decades in multiple communities, tight mixing bounds for this algorithm remain unresolved even in the seemingly simple setting of log-concave distributions over a bounded domain. This paper completely characterizes the mixing time of the Langevin Algorithm to its stationary distribution in this setting (and others). This mixing result can be combined with any bound on the discretization bias in order to sample from the stationary distribution of the Langevin Diffusion. In this way, we disentangle the study of the mixing and bias of the Langevin Algorithm.Our key insight is to analyze the Langevin Algorithm’s convergence by using a new Lyapunov function: the shifted divergence, a quantity studied in the differential privacy literature. Briefly, this Lyapunov function is a version of the Renyi divergence that is smoothed in optimal transport distance, and we use the amount of smoothing to measure the Langevin Algorithm’s progress. In addition to giving a short, simple proof of optimal mixing bounds, this analysis approach also has several additional appealing properties. First, our approach removes all unnecessary assumptions required by other sampling analyses. Second, our approach unifies many settings: it extends if the Langevin Algorithm uses projections, stochastic mini-batch gradients, or strongly convex potentials (whereby our mixing time improves exponentially). Third, our approach unifies many metrics: it proves mixing in the stringent notion of Renyi divergence, which immediately implies mixing in all common metrics via standard comparison inequalities. Fourth, our approach exploits convexity only through the contractivity of a gradient step—reminiscent of how convexity is used in textbook proofs of Gradient Descent. ', 'Further and stronger analogy between sampling and optimization: Langevin Monte Carlo and gradient descent In this paper, we revisit the recently established theoretical guarantees for the convergence of the Langevin Monte Carlo algorithm of sampling from a smooth and (strongly) log-concave density. We improve the existing results when the convergence is measured in the Wasserstein distance and provide further insights on the very tight relations between, on the one hand, the Langevin Monte Carlo for sampling and, on the other hand, the gradient descent for optimization. Finally, we also establish guarantees for the convergence of a version of the Langevin Monte Carlo algorithm that is based on noisy evaluations of the gradient.']"
221,55,221_adaptivemcmc_hamiltonian_mcmc_monte,"['adaptivemcmc', 'hamiltonian', 'mcmc', 'monte', 'metropolishastings', 'sampling', 'stochastic', 'sgmcmc', 'langevin', 'carlo']","['Magnetic Hamiltonian Monte Carlo Hamiltonian Monte Carlo (HMC) exploits Hamiltonian dynamics to construct efficient proposals for Markov chain Monte Carlo (MCMC). In this paper, we present a generalization of HMC which exploits non-canonical Hamiltonian dynamics. We refer to this algorithm as magnetic HMC, since in 3 dimensions a subset of the dynamics map onto the mechanics of a charged particle coupled to a magnetic field. We establish a theoretical basis for the use of non-canonical Hamiltonian dynamics in MCMC, and construct a symplectic, leapfrog-like integrator allowing for the implementation of magnetic HMC. Finally, we exhibit several examples where these non-canonical dynamics can lead to improved mixing of magnetic HMC relative to ordinary HMC.', 'Relativistic Monte Carlo  Hamiltonian Monte Carlo (HMC) is a popular Markov chain Monte Carlo (MCMC) algorithm that generates proposals for a Metropolis-Hastings algorithm by simulating the dynamics of a Hamiltonian system. However, HMC is sensitive to large time discretizations and performs poorly if there is a mismatch between the spatial geometry of the target distribution and the scales of the momentum distribution. In particular the mass matrix of HMC is hard to tune well. In order to alleviate these problems we propose relativistic Hamiltonian Monte Carlo, a version of HMC based on relativistic dynamics that introduces a maximum velocity on particles. We also derive stochastic gradient versions of the algorithm and show that the resulting algorithms bear interesting relationships to gradient clipping, RMSprop, Adagrad and Adam, popular optimisation methods in deep learning. Based on this, we develop relativistic stochastic gradient descent by taking the zero-temperature limit of relativistic stochastic gradient Hamiltonian Monte Carlo. In experiments we show that the relativistic algorithms perform better than classical Newtonian variants and Adam.', 'Hamiltonian Monte Carlo Swindles Hamiltonian Monte Carlo (HMC) is a powerful Markov chain Monte Carlo (MCMC) algorithm for estimating expectations with respect to continuous un-normalized probability distributions. MCMC estimators typically have higher variance than classical Monte Carlo with i.i.d. samples due to autocorrelations; most MCMC research tries to reduce these autocorrelations. In this work, we explore a complementary approach to variance reduction based on two classical Monte Carlo ’swindles’: first, running an auxiliary coupled chain targeting a tractable approximation to the target distribution, and using the auxiliary samples as control variates; and second, generating anti-correlated (""antithetic"") samples by running two chains with flipped randomness. Both ideas have been explored previously in the context of Gibbs samplers and random-walk Metropolis algorithms, but we argue that they are ripe for adaptation to HMC in light of recent coupling results from the HMC theory literature. For many posterior distributions, we find that these swindles generate effective sample sizes orders of magnitude larger than plain HMC, as well as being more efficient than analogous swindles for Metropolis-adjusted Langevin algorithm and random-walk Metropolis. ']"
222,13,222_supervised_labeling_representationlearning_labeled,"['supervised', 'labeling', 'representationlearning', 'labeled', 'learning', 'supervisionsuch', 'supervision', 'weakly', 'scarceannotation', 'labels']","['Constrained labeling for weakly supervised learning Curation of large fully supervised datasets has become one of the major roadblocks for machine learning. Weak supervision provides an alternative to supervised learning by training with cheap, noisy, and possibly correlated labeling functions from varying sources. The key challenge in weakly supervised learning is combining the different weak supervision signals while navigating misleading correlations in their errors. In this paper, we propose a simple data-free approach for combining weak supervision signals by defining a constrained space for the possible labels of the weak signals and training with a random labeling within this constrained space. Our method is efficient and stable, converging after a few iterations of gradient descent. We prove theoretical conditions under which the worst-case error of the randomized label decreases with the rank of the linear constraints. We show experimentally that our method outperforms other weak supervision methods on various text- and image-classification tasks.', 'Fast and Three-rious: Speeding Up Weak Supervision with Triplet Methods Weak supervision is a popular method for building machine learning models without relying on ground truth annotations. Instead, it generates probabilistic training labels by estimating the accuracies of multiple noisy labeling sources (e.g., heuristics, crowd workers). Existing approaches use latent variable estimation to model the noisy sources, but these methods can be computationally expensive, scaling superlinearly in the data. In this work, we show that, for a class of latent variable models highly applicable to weak supervision, we can find a closed-form solution to model parameters, obviating the need for iterative solutions like stochastic gradient descent (SGD). We use this insight to build FlyingSquid, a weak supervision framework that runs orders of magnitude faster than previous weak supervision approaches and requires fewer assumptions. In particular, we prove bounds on generalization error without assuming that the latent variable model can exactly parameterize the underlying data distribution. Empirically, we validate FlyingSquid on benchmark weak supervision datasets and find that it achieves the same or higher quality compared to previous approaches without the need to tune an SGD procedure, recovers model parameters 170 times faster on average, and enables new video analysis and online learning applications.', 'Adversarial Multi Class Learning under Weak Supervision with Performance Guarantees We develop a rigorous approach for using a set of arbitrarily correlated weak supervision sources in order to solve a multiclass classification task when only a very small set of labeled data is available. Our learning algorithm provably converges to a model that has minimum empirical risk with respect to an adversarial choice over feasible labelings for a set of unlabeled data, where the feasibility of a labeling is computed through constraints defined by rigorously estimated statistics of the weak supervision sources. We show theoretical guarantees for this approach that depend on the information provided by the weak supervision sources. Notably, this method does not require the weak supervision sources to have the same labeling space as the multiclass classification task. We demonstrate the effectiveness of our approach with experiments on various image classification tasks.']"
223,82,223_multilabel_labels_labeled_label,"['multilabel', 'labels', 'labeled', 'label', 'complementarylabel', 'classification', 'supervised', 'labelspecific', 'classifiers', 'classifier']","['Variational Label Enhancement Label distribution covers a certain number of labels, representing the degree to which each label describes the instance. When dealing with label ambiguity, label distribution could describe the supervised information in a fine-grained way. Unfortunately, many training sets only contain simple logical labels rather than label distributions due to the difficulty of obtaining label distributions directly. To solve this problem, we consider the label distributions as the latent vectors and infer them from the logical labels in the training datasets by using variational inference. After that, we induce a predictive model to train the label distribution data by employing the multi-output regression technique. The recovery experiment on thirteen real-world LDL datasets and the predictive experiment on ten multi-label learning datasets validate the advantage of our approach over the state-of-the-art approaches.', 'Multiclass-Multilabel Classification with More Classes than Examples We discuss multiclass-multilabel classification problems in which   the set of possible labels is extremely large. Most existing   multiclass-multilabel learning algorithms expect to observe a   reasonably large sample from each class, and fail if they receive   only a handful of examples with a given label. We propose and   analyze the following two-stage approach: first use an arbitrary   (perhaps heuristic) classification algorithm to construct an initial   classifier, then apply a simple but principled method to augment   this classifier by removing harmful labels from its output.  A   careful theoretical analysis allows us to justify our approach under   some reasonable conditions (such as label sparsity and power-law   distribution of label frequencies), even when the training set does   not provide a statistically accurate representation of most   classes. Surprisingly, our theoretical analysis continues to hold   even when the number of classes exceeds the sample size. We   demonstrate the merits of our approach on the ambitious task of   categorizing the entire web using the 1.5 million categories   defined on Wikipedia.', 'Label Filters for Large Scale Multilabel Classification When assigning labels to a test instance, most multilabel and multiclass classifiers systematically evaluate every single label to decide whether it is relevant or not. This linear scan over labels becomes prohibitive when the number of labels is very large. To alleviate this problem we propose a two step approach where computationally efficient label filters pre-select a small set of candidate labels before the base multiclass or multilabel classifier is applied. The label filters select candidate labels by projecting a test instance on a filtering line, and retaining only the labels that have training instances in the vicinity of this projection.  The filter parameters are learned directly from data by solving a constraint optimization problem, and are independent of the base multilabel classifier. The proposed label filters can be used in conjunction with any multiclass or multilabel classifier that requires a linear scan over the labels, and speed up prediction by orders of magnitude without significant impact on performance.']"
224,39,224_expectationmaximization_mixtures_mixture_gaussian,"['expectationmaximization', 'mixtures', 'mixture', 'gaussian', 'mle', 'gaussians', 'estimation', 'mixing', 'learning', 'models']","[' On Learning Mixture Models with Sparse Parameters   Mixture models are widely used to fit complex and multimodal datasets. In this paper we study mixtures with high dimensional sparse latent parameter vectors and consider the problem of support recovery of those vectors. While parameter learning in mixture models is well-studied, the sparsity constraint remains relatively unexplored. Sparsity of parameter vectors is a natural constraint in variety of settings, and support recovery is a major step towards parameter estimation. We provide efficient algorithms for support recovery that have a logarithmic sample complexity dependence on the dimensionality of the latent space. Our algorithms are quite general, namely they are applicable to 1) mixtures of many different canonical distributions including Uniform, Poisson, Laplace, Gaussians, etc. 2) Mixtures of linear regressions and linear classifiers with Gaussian covariates under different assumptions on the unknown parameters. In most of these settings, our results are the first guarantees on this problem while in the rest, we provide significant improvements on existing results in certain regimes. ', 'The More, the Merrier: the Blessing of Dimensionality for Learning Large Gaussian Mixtures In this paper we show that very large mixtures of Gaussians are efficiently learnable in high dimension. More precisely, we prove that a mixture with known identical covariance matrices whose number of components is a polynomial of any fixed degree in the dimension n is polynomially learnable as long as a certain non-degeneracy condition on the means is satisfied. It turns out that this condition is generic in the sense of smoothed complexity, as soon as the dimensionality of the space is high enough. Moreover, we prove that no such condition can possibly exist in low dimension and the problem of learning the parameters is generically hard.  In contrast, much of the existing work on Gaussian Mixtures relies on low-dimensional projections and thus hits an artificial barrier. Our main result on mixture recovery relies on a new “Poissonization""-based technique, which transforms a mixture of Gaussians to a linear map of a product distribution. The problem of learning this map can be efficiently solved using some recent results on tensor decompositions and Independent Component Analysis (ICA), thus giving an  algorithm for recovering the mixture. In addition, we combine our low-dimensional hardness results for Gaussian mixtures with  Poissonization to show how to embed difficult instances of low-dimensional Gaussian mixtures into the ICA setting, thus establishing exponential information-theoretic lower bounds for underdetermined ICA in low dimension. To the best of our knowledge, this is the first such result  in the literature. In addition to contributing to  the problem of Gaussian mixture learning, we believe that this work is among the first steps toward better understanding the rare phenomenon of the “blessing of dimensionality"" in the computational aspects of statistical inference. ', 'Ten Steps of EM Suffice for Mixtures of Two Gaussians The Expectation-Maximization (EM) algorithm is a widely used method for maximum likelihood estimation in models with latent variables. For estimating mixtures of Gaussians, its iteration can be viewed as a soft version of the k-means clustering algorithm. Despite its wide use and applications, there are essentially no known convergence guarantees for this method. We provide global convergence guarantees for  mixtures of two Gaussians with known covariance matrices. We show that the population version of EM, where the algorithm is given access to infinitely many samples from the mixture, converges geometrically to the correct mean vectors, and provide simple, closed-form expressions for the convergence rate. As a simple illustration, we show that, in one dimension, ten steps of the EM algorithm initialized at infinity result in less than $1%$ error estimation of the means. In the finite sample regime, we show that, under a random initialization, $\\tilde{O}(d/ε^2)$ samples suffice to compute the unknown vectors to within $ε$ in Mahalanobis distance, where $d$ is the dimension. In particular, the error rate of the EM based estimator is $\\tilde{O}\\left(\\sqrt{d} \\over n\\right)$ where $n$ is the number of samples, which is optimal up to logarithmic factors.']"
225,284,225_learnability_learnable_complexity_learning,"['learnability', 'learnable', 'complexity', 'learning', 'generalization', 'classes', 'learners', 'class', 'optimal', 'learner']","['Inherent limitations of dimensions for characterizing learnability of distribution classes  We consider the long-standing question of finding a parameter of a class of probability distributions that characterizes its PAC learnability. While for many learning tasks (such as binary classification and online learning) there is a notion of dimension whose finiteness is equivalent to learnability within any level of accuracy, we show, rather surprisingly, that such parameter does not exist for distribution learning. Concretely, our results apply for several general notions of characterizing learnability and for several learning tasks. We show that there is no notion of dimension that characterizes the sample complexity of learning distribution classes. We then consider the weaker requirement of only characterizing learnability (rather than the quantitative sample complexity function). We propose some natural requirements for such a characterization and go on to show that there exists no characterization of learnability that satisfies these requirements for classes of distributions. Furthermore, we show that our results hold for various other learning problems. In particular, we show that there is no notion of dimension characterizing PAC-learnability for any of the tasks: classification learning w.r.t. a restricted set of marginal distributions and learnability of classes of real-valued functions with continuous losses.', 'Open Problem: Information Complexity of VC Learning Uniform convergence approaches learning by studying the complexity of hypothesis classes. In particular, hypothesis classes with bounded Vapnik-Chervonenkis dimension exhibit strong uniform convergence, that is, any hypothesis in the class has low generalization error. On the other hand, a long line of work studies the information complexity of a learning algorithm, as it is connected to several desired properties, including generalization.  We ask whether all VC classes admit a learner with low information complexity which achieves the generalization bounds guaranteed by uniform convergence. Specifically, since we know that this is not possible if we consider proper and consistent learners and measure information complexity in terms of the mutual information (Bassily et al., 2018), we are interested in learners with low information complexity measured in terms of the recently introduced notion of CMI (Steinke and Zakynthinou, 2020). Can we obtain tight bounds on the information complexity of a learning algorithm for a VC class (via CMI), thus exactly retrieving the known generalization bounds implied for this class by uniform convergence?', 'Open Problem: The Statistical Query Complexity of Learning Sparse Halfspaces We consider the long-open problem of attribute-efficient learning of halfspaces. In this problem the learner is given random examples labeled by an unknown halfspace function f on \\mathbbR^n. Further f is r-sparse, that is it depends on at most r out of n variables. An attribute-efficient learning algorithm is an algorithm that can output a hypothesis close to f using a polynomial in r and \\log n number of examples (Blum, 1992). Despite a number of attempts and some partial progress, there are no efficient algorithms or hardness results for the problem. We propose a potentially easier question: what is the query complexity of this learning problem in the statistical query (SQ) model of Kearns (1998). We show that, as in the case of general PAC learning, the query complexity of attribute-efficient SQ learning of any concept class can be characterized by a combinatorial parameter of the concept class. The proposed question is then equivalent to estimating the value of this parameter for the concept class of halfspaces. A potentially simpler problem is to estimate this parameter for the concept class of decision lists, a subclass of halfspaces.']"
226,40,226_complexity_empirical_distributions_testing,"['complexity', 'empirical', 'distributions', 'testing', 'samples', 'uniformity', 'characterization', 'sample', 'distribution', 'testers']","['Open Problem: Tight Characterization of Instance-Optimal Identity Testing In the “instance-optimal” identity testing introduced by Valiant and Valiant (2014), one is given the (succinct) description of a discrete probability distribution $q$, as well as a a parameter $\\varepsilon\\in(0,1]$ and i.i.d. samples from an (unknown, arbitrary) discrete distribution $p$. The goal is to distinguish with high probability between the cases (i)\xa0$p=q$ and (ii)\xa0$\\textrm{TV}(p,q) > \\varepsilon$, using the minimum number of samples possible as a function of (some simple functional of) $q$ and $\\varepsilon$. This is in contrast with the standard formulation of identity testing, where the sample complexity is taken as worst-case over all possible reference distributions $q$. Valiant and Valiant provided upper and lower bounds on this question, where the sample complexity is expressed in terms of the “$\\ell_{2/3}$ norm” of some (truncated version) of the reference distribution $q$. However, these upper and lower bounds do not always match up to constant factors, and can differ by an arbitrary multiplicative gap for some choices of $q$. The question then is: what is the tight characterization of the sample complexity of instance-optimal identity testing? What is the “right” functional $\\Phi(q)$ for it? ', 'Complexity of High-Dimensional Identity Testing with Coordinate Conditional Sampling We study the identity testing problem for high-dimensional distributions. Given as input an explicit distribution $\\mu$, an $\\varepsilon>0$, and access to sampling oracle(s) for a hidden distribution $\\pi$, the goal in identity testing is to distinguish whether the two distributions $\\mu$ and $\\pi$ are identical or are at least $\\varepsilon$-far apart. When there is only access to full samples from the hidden distribution $\\pi$, it is known that exponentially many samples (in the dimension) may be needed for identity testing, and hence previous works have studied identity testing with additional access to various “conditional” sampling oracles. We consider a significantly weaker conditional sampling oracle, which we call the Coordinate Oracle, and provide a computational and statistical characterization of the identity testing problem in this new model.We prove that if an analytic property known as approximate tensorization of entropy holds for an $n$-dimensional visible distribution $\\mu$, then there is an efficient identity testing algorithm for any hidden distribution $\\pi$ using $\\widetilde{O}(n/\\varepsilon)$ queries to the Coordinate Oracle. Approximate tensorization of entropy is a pertinent condition as recent works have established it for a large class of high-dimensional distributions. We also prove a computational phase transition: for a well-studied class of $n$-dimensional distributions, specifically sparse antiferromagnetic Ising models over $\\{+1,-1\\}^n$, we show that in the regime where approximate tensorization of entropy fails, there is no efficient identity testing algorithm unless RP=NP. We complement our results with a matching $\\Omega(n/\\varepsilon)$ statistical lower bound for the sample complexity of identity testing in the $\\coorora$ model.', 'Testing Mixtures of Discrete Distributions  There has been significant study on the sample complexity of testing properties of distributions over large domains.  For many properties, it is known that the sample complexity can be substantially smaller than the domain size. For example, over a domain of size $n$, distinguishing the uniform distribution from distributions that are far from uniform in $\\ell_1$-distance uses only $O(\\sqrt{n})$ samples.  However, the picture is very different in the presence of arbitrary noise, even when the amount of noise is quite small.  In this case, one must distinguish if samples are coming from a distribution that is $\\epsilon$-close to uniform from the case where the distribution is $(1-\\epsilon)$-far from uniform.  The latter task requires nearly linear in $n$ samples (Valiant, 2008; Valiant and Valiant, 2017a). In this work, we present a noise model that on one hand is more tractable for the testing problem, and on the other hand represents a rich class of noise families.   In our model, the noisy distribution is a mixture of the original distribution and noise, where the latter is known to the tester either explicitly or via sample access; the form of the noise is also known \\emph{a priori}.  Focusing on the identity and closeness testing problems leads to the following mixture testing question:  Given samples of distributions $p, q_1,q_2$, can we test if $p$ is a mixture of $q_1$ and $q_2$?  We consider this general question in various scenarios that differ in terms of how the tester can access the distributions, and show that indeed this problem is more tractable.  Our results  show that the sample complexity of our testers are exactly the same as for the classical non-mixture case. ']"
227,22,227_riemannian_accelerated_geodesic_geodesically,"['riemannian', 'accelerated', 'geodesic', 'geodesically', 'optimization', 'gradient', 'manifold', 'manifolds', 'curvature', 'convex']","['Accelerated Gradient Methods for Geodesically Convex Optimization: Tractable Algorithms and Convergence Analysis We propose computationally tractable accelerated first-order methods for Riemannian optimization, extending the Nesterov accelerated gradient (NAG) method. For both geodesically convex and geodesically strongly convex objective functions, our algorithms are shown to have the same iteration complexities as those for the NAG method on Euclidean spaces, under only standard assumptions. To the best of our knowledge, the proposed scheme is the first fully accelerated method for geodesically convex optimization problems. Our convergence analysis makes use of novel metric distortion lemmas as well as carefully designed potential functions. A connection with the continuous-time dynamics for modeling Riemannian acceleration in (Alimisis et al., 2020) is also identified by letting the stepsize tend to zero. We validate our theoretical results through numerical experiments.', 'From Nesterov’s Estimate Sequence to Riemannian Acceleration  We propose the first global accelerated gradient method for Riemannian manifolds. Toward establishing our results, we revisit Nesterov’s estimate sequence technique and develop a conceptually simple alternative from first principles. We then extend our analysis to Riemannian acceleration, localizing the key difficulty into “metric distortion.” We control this distortion via a novel geometric inequality, which enables us to formulate and analyze global Riemannian acceleration.', 'Riemannian Accelerated Gradient Methods via Extrapolation In this paper, we propose a convergence acceleration scheme for general Riemannian optimization problems by extrapolating iterates on manifolds. We show that when the iterates are generated from the Riemannian gradient descent method, the scheme achieves the optimal convergence rate asymptotically and is computationally more favorable than the recently proposed Riemannian Nesterov accelerated gradient methods. A salient feature of our analysis is the convergence guarantees with respect to the use of general retraction and vector transport. Empirically, we verify the practical benefits of the proposed acceleration strategy, including robustness to the choice of different averaging schemes on manifolds.']"
228,21,228_riemannian_manifold_manifolds_optimization,"['riemannian', 'manifold', 'manifolds', 'optimization', 'gradients', 'submanifold', 'gradient', 'algorithms', 'matrices', 'matrix']","['Vector Transport Free Riemannian LBFGS for Optimization on Symmetric Positive Definite Matrix Manifolds This work concentrates on optimization on Riemannian manifolds. The Limited-memory Broyden-Fletcher-Goldfarb-Shanno (LBFGS) algorithm is a commonly used quasi-Newton method for numerical optimization in Euclidean spaces. Riemannian LBFGS (RLBFGS) is an extension of this method to Riemannian manifolds. RLBFGS involves computationally expensive vector transports as well as unfolding recursions using adjoint vector transports. In this article, we propose two mappings in the tangent space using the inverse second root and Cholesky decomposition. These mappings make both vector transport and adjoint vector transport identity and therefore isometric. Identity vector transport makes RLBFGS less computationally expensive and its isometry is also very useful in convergence analysis of RLBFGS. Moreover, under the proposed mappings, the Riemannian metric reduces to Euclidean inner product, which is much less computationally expensive. We focus on the Symmetric Positive Definite (SPD) manifolds which are beneficial in various fields such as data science and statistics. This work opens a research opportunity for extension of the proposed mappings to other well-known manifolds.', 'Riemannian coordinate descent algorithms on matrix manifolds Many machine learning applications are naturally formulated as optimization problems on Riemannian manifolds. The main idea behind Riemannian optimization is to maintain the feasibility of the variables while moving along a descent direction on the manifold. This results in updating all the variables at every iteration. In this work, we provide a general framework for developing computationally efficient coordinate descent (CD) algorithms on matrix manifolds that allows updating only a few variables at every iteration while adhering to the manifold constraint. In particular, we propose CD algorithms for various manifolds such as Stiefel, Grassmann, (generalized) hyperbolic, symplectic, and symmetric positive (semi)definite. While the cost per iteration of the proposed CD algorithms is low, we further develop a more efficient variant via a first-order approximation of the objective function. We analyze their convergence and complexity, and empirically illustrate their efficacy in several applications.', 'Riemannian adaptive stochastic gradient algorithms on matrix manifolds Adaptive stochastic gradient algorithms in the Euclidean space have attracted much attention lately. Such explorations on Riemannian manifolds, on the other hand, are relatively new, limited, and challenging. This is because of the intrinsic non-linear structure of the underlying manifold and the absence of a canonical coordinate system. In machine learning applications, however, most manifolds of interest are represented as matrices with notions of row and column subspaces. In addition, the implicit manifold-related constraints may also lie on such subspaces. For example, the Grassmann manifold is the set of column subspaces. To this end, such a rich structure should not be lost by transforming matrices to just a stack of vectors while developing optimization algorithms on manifolds. We propose novel stochastic gradient algorithms for problems on Riemannian matrix manifolds by adapting the row and column subspaces of gradients. Our algorithms are provably convergent and they achieve the convergence rate of order $O(log(T)/sqrt(T))$, where $T$ is the number of iterations. Our experiments illustrate that the proposed algorithms outperform existing Riemannian adaptive stochastic algorithms.']"
229,22,229_densities_density_nonparametric_estimation,"['densities', 'density', 'nonparametric', 'estimation', 'kernels', 'estimating', 'lowerdimensional', 'estimate', 'distributions', 'embeddings']","['Uniform Convergence Rate of the Kernel Density Estimator Adaptive to Intrinsic Volume Dimension We derive concentration inequalities for the supremum norm of the difference between a kernel density estimator (KDE) and its point-wise expectation that hold uniformly over the selection of the bandwidth and under weaker conditions on the kernel and the data generating distribution than previously used in the literature. We first propose a novel concept, called the volume dimension, to measure the intrinsic dimension of the support of a probability distribution based on the rates of decay of the probability of vanishing Euclidean balls. Our bounds depend on the volume dimension and generalize the existing bounds derived in the literature. In particular, when the data-generating distribution has a bounded Lebesgue density or is supported on a sufficiently well-behaved lower-dimensional manifold, our bound recovers the same convergence rate depending on the intrinsic dimension of the support as ones known in the literature. At the same time, our results apply to more general cases, such as the ones of distribution with unbounded densities or supported on a mixture of manifolds with different dimensions. Analogous bounds are derived for the derivative of the KDE, of any order. Our results are generally applicable but are especially useful for problems in geometric inference and topological data analysis, including level set estimation, density-based clustering, modal clustering and mode hunting, ridge estimation and persistent homology.', 'Estimating Density Ridges by Direct Estimation of Density-Derivative-Ratios Estimation of \\emphdensity ridges has been gathering a great deal of attention since it enables us to reveal lower-dimensional structures hidden in data. Recently, \\emphsubspace constrained mean shift (SCMS) was proposed as a practical algorithm for density ridge estimation. A key technical ingredient in SCMS is to accurately estimate the ratios of the density derivatives to the density. SCMS takes a three-step approach for this purpose — first estimating the data density, then computing its derivatives, and finally taking their ratios. However, this three-step approach can be unreliable because a good density estimator does not necessarily mean a good density derivative estimator and division by an estimated density could significantly magnify the estimation error. To overcome these problems, we propose a novel method that directly estimates the ratios without going through density estimation and division. Our proposed estimator has an analytic-form solution and it can be computed efficiently. We further establish a non-parametric convergence bound for the proposed ratio estimator. Finally, based on this direct ratio estimator, we develop a practical algorithm for density ridge estimation and experimentally demonstrate its usefulness on a variety of datasets.', 'Uniform Convergence Rates for Kernel Density Estimation Kernel density estimation (KDE) is a popular nonparametric density estimation method. We (1) derive finite-sample high-probability density estimation bounds for multivariate KDE under mild density assumptions which hold uniformly in $x \\in \\mathbb{R}^d$ and bandwidth matrices. We apply these results to (2) mode, (3) density level set, and (4) class probability estimation and attain optimal rates up to logarithmic factors. We then (5) provide an extension of our results under the manifold hypothesis. Finally, we (6) give uniform convergence results for local intrinsic dimension estimation.']"
230,43,230_regularization_ridge_ridgeless_regularized,"['regularization', 'ridge', 'ridgeless', 'regularized', 'prediction', 'overparameterized', 'estimators', 'generalization', 'overfitting', 'mles']","[' Asymptotics of Ridge(less) Regression under General Source Condition   We analyze the prediction error of ridge regression in an asymptotic regime where the sample size and dimension go to infinity at a proportional rate. In particular, we consider the role played by the structure of the true regression parameter. We observe that the case of a general deterministic parameter can be reduced to the case of a random parameter from a structured prior. The latter assumption is a natural adaptation of classic smoothness assumptions in nonparametric regression, which are known as source conditions in the the context of regularization theory for inverse problems. Roughly speaking, we assume the large coefficients of the parameter are in correspondence to the principal components. In this setting a precise characterisation of the test error is obtained, depending on the inputs covariance and regression parameter structure. We illustrate this characterisation in a simplified setting to investigate the influence of the true parameter on optimal regularisation for overparameterized models. We show that interpolation (no regularisation) can be optimal even with bounded signal-to-noise ratio (SNR), provided that the parameter coefficients are larger on high-variance directions of the data, corresponding to a more regular function than posited by the regularization term. This contrasts with previous work considering ridge regression with isotropic prior, in which case interpolation is only optimal in the limit of infinite SNR. ', 'Subsample Ridge Ensembles: Equivalences and Generalized Cross-Validation We study subsampling-based ridge ensembles in the proportional asymptotics regime, where the feature size grows proportionally with the sample size such that their ratio converges to a constant. By analyzing the squared prediction risk of ridge ensembles as a function of the explicit penalty $\\lambda$ and the limiting subsample aspect ratio $\\phi_s$ (the ratio of the feature size to the subsample size), we characterize contours in the $(\\lambda, \\phi_s)$-plane at any achievable risk. As a consequence, we prove that the risk of the optimal full ridgeless ensemble (fitted on all possible subsamples) matches that of the optimal ridge predictor. In addition, we prove strong uniform consistency of generalized cross-validation (GCV) over the subsample sizes for estimating the prediction risk of ridge ensembles. This allows for GCV-based tuning of full ridgeless ensembles without sample splitting and yields a predictor whose risk matches optimal ridge risk.', 'Divide and Conquer Kernel Ridge Regression We study a decomposition-based scalable approach to performing kernel ridge regression.  The method is simply described: it randomly partitions a dataset of size N into m subsets of equal size, computes an independent kernel ridge regression estimator for each subset, then averages the local solutions into a global predictor. This partitioning leads to a substantial reduction in computation time versus the standard approach of performing kernel ridge regression on all N samples. Our main theorem establishes that despite the computational speed-up, statistical optimality is retained: that so long as m is not too large, the partition-based estimate achieves optimal rates of convergence for the full sample size N.  As concrete examples, our theory guarantees that m may grow polynomially in N for Sobolev spaces, and nearly linearly for finite-rank kernels and Gaussian kernels. We conclude with simulations complementing our theoretical results and exhibiting the computational and statistical benefits of our approach.']"
231,70,231_kernels_kernel_classification_regularization,"['kernels', 'kernel', 'classification', 'regularization', 'svm', 'learning', 'dimensional', 'spectral', 'empirical', 'randomized']","['On the Complexity of Learning with Kernels A well-recognized limitation of kernel learning is the requirement to handle a kernel matrix, whose size is quadratic in the number of training examples. Many methods have been proposed to reduce this computational cost, mostly by using a subset of the kernel matrix entries, or some form of low-rank matrix approximation, or a random projection method. In this paper, we study lower bounds on the error attainable by such methods as a function of the number of entries observed in the kernel matrix or the rank of an approximate kernel matrix. We show that there are kernel learning problems where no such method will lead to non-trivial computational savings. Our results also quantify how the problem difficulty depends on parameters such as the nature of the loss function, the regularization parameter, the norm of the desired predictor, and the kernel matrix rank. Our results also suggest cases where more efficient kernel learning might be possible.', 'Approximation beats concentration? An approximation view  on inference with smooth radial kernels Positive definite kernels and their associated Reproducing Kernel Hilbert Spaces provide a mathematically compelling and practically competitive framework for learning from data.  In this paper we take the approximation theory point of view to explore various aspects of  smooth kernels related to their inferential properties. We  analyze eigenvalue decay of  kernels operators and  matrices,  properties of eigenfunctions/eigenvectors and “Fourier” coefficients of functions in the kernel space restricted to a discrete set of data points. We also investigate the fitting capacity of kernels,  giving explicit bounds on the fat shattering dimension of the balls in  Reproducing Kernel Hilbert spaces.  Interestingly, the same properties that make kernels  very effective approximators for functions in their “native” kernel space,  also limit their capacity to represent arbitrary functions.  We discuss various implications, including those for gradient descent type methods. It is important to note that most of our  bounds are measure independent.  Moreover,  at least in moderate dimension, the bounds for eigenvalues are much tighter than the bounds which can be obtained from the usual matrix concentration results. For example, we see that  eigenvalues of kernel matrices show nearly exponential decay with constants depending only on the kernel and the domain. We call this “approximation beats concentration” phenomenon as even when the data are sampled from a probability distribution, some of their aspects are better understood in terms of approximation theory.  ', 'Bayesian Nonparametric Kernel-Learning Kernel methods are ubiquitous tools in machine learning. They have proven to be effective in many domains and tasks. Yet, kernel methods often require the user to select a predefined kernel to build an estimator with. However, there is often little reason for the common practice of selecting a kernel a priori. Even if a universal approximating kernel is selected, the quality of the finite sample estimator may be greatly affected by the choice of kernel. Furthermore, when directly applying kernel methods, one typically needs to compute a N \\times N Gram matrix of pairwise kernel evaluations to work with a dataset of N instances. The computation of this Gram matrix precludes the direct application of kernel methods on large datasets, and makes kernel learning especially difficult. In this paper we introduce Bayesian nonparmetric kernel-learning (BaNK), a generic, data-driven framework for scalable learning of kernels. BaNK places a nonparametric prior on the spectral distribution of random frequencies allowing it to both learn kernels and scale to large datasets. We show that this framework can be used for large scale regression and classification tasks. Furthermore, we show that BaNK outperforms several other scalable approaches for kernel learning on a variety of real world datasets.']"
232,27,232_svm_svms_classifiers_kernelized,"['svm', 'svms', 'classifiers', 'kernelized', 'classifier', 'classification', 'kernels', 'csvm', 'vector', 'esvm']","['Clustered Support Vector Machines In many problems of machine learning, the data are distributed nonlinearly. One way to address this kind of data is training a nonlinear classifier such as kernel support vector machine (kernel SVM). However, the computational burden of kernel SVM limits its application to large scale datasets. In this paper, we propose a Clustered Support Vector Machine (CSVM), which tackles the data in a divide and conquer manner. More specifically, CSVM groups the data into several clusters, followed which it trains a linear support vector machine in each cluster to separate the data locally. Meanwhile, CSVM has an additional global regularization, which requires the weight vector of each local linear SVM aligning with a global weight vector. The global regularization leverages the information from one cluster to another, and avoids over-fitting in each cluster. We derive a data-dependent generalization error bound for CSVM, which explains the advantage of CSVM over linear SVM. Experiments on several benchmark datasets show that the proposed method outperforms linear SVM and some other related locally linear classifiers. It is also comparable to a fine-tuned kernel SVM in terms of prediction performance, while it is more efficient than kernel SVM.', 'SVM versus Least Squares SVM We study the relationship between Support Vector Machines (SVM) and Least Squares SVM (LS-SVM). Our main result shows that under mild conditions, LS-SVM for binaryclass classifications is equivalent to the hard margin SVM based on the well-known Mahalanobis distance measure. We further study the asymptotics of the hard margin SVM when the data dimensionality tends to infinity with a fixed sample size. Using recently developed theory on the asymptotics of the distribution of the eigenvalues of the covariance matrix, we show that under mild conditions, the equivalence result holds for the traditional Euclidean distance measure. These equivalence results are further extended to the multi-class case. Experimental results confirm the presented theoretical analysis.', 'Ellipsoidal Support Vector Machines This paper proposes the ellipsoidal SVM (e-SVM) that uses an ellipsoid center, in the version space, to approximate the Bayes point. Since SVM approximates it by a sphere center, e-SVM provides an extension to SVM for better approximation of the Bayes point. Although the idea has been mentioned before (Ruján, 1997), no work has been done for formulating and kernelizing the method. Starting from the maximum volume ellipsoid problem, we successfully formulate and kernelize it by employing relaxations. The resulting e-SVM optimization framework has much similarity to SVM; it is naturally extendable to other loss functions and other problems. A variant of the sequential minimal optimization is provided for efficient batch implementation. Moreover, we provide an online version of linear, or primal, e-SVM to be applicable for large-scale datasets.']"
233,14,233_canonical_sparse_cca_pca,"['canonical', 'sparse', 'cca', 'pca', 'structuredsparsityinducing', 'correlation', 'multidimensional', 'dimensionality', 'covariancebased', 'dca']","['A Simple and Provable Algorithm for Sparse Diagonal CCA Given two sets of variables, derived from a common set of samples, sparse Canonical Correlation Analysis (CCA) seeks linear combinations of a small number of variables in each set, such that the induced \\emphcanonical variables are maximally correlated. Sparse CCA is NP-hard. We propose a novel combinatorial algorithm for sparse diagonal CCA, \\textiti.e., sparse CCA under the additional assumption that variables within each set are standardized and uncorrelated. Our algorithm operates on a low rank approximation of the input data and its computational complexity scales linearly with the number of input variables. It is simple to implement, and parallelizable. In contrast to most existing approaches, our algorithm administers precise control on the sparsity of the extracted canonical vectors, and comes with theoretical data-dependent global approximation guarantees, that hinge on the spectrum of the input data. Finally, it can be straightforwardly adapted to other constrained variants of CCA enforcing structure beyond sparsity. We empirically evaluate the proposed scheme and apply it on a real neuroimaging dataset to investigate associations between brain activity and behavior measurements.', 'Cluster Canonical Correlation Analysis In this paper we present cluster canonical correlation analysis (cluster-CCA) for joint dimensionality reduction of two sets of data points.  Unlike the standard pairwise correspondence between the  data points, in our problem each set is partitioned into multiple clusters or classes, where the class labels define correspondences between the sets. Cluster-CCA is able to learn discriminant low dimensional representations that maximize the correlation between the two sets while segregating the different classes on the learned space.  Furthermore, we present a kernel extension, kernel cluster canonical correlation analysis (cluster-KCCA) that extends cluster-CCA to account for non-linear relationships.  Cluster-(K)CCA is shown to be computationally efficient, the complexity being similar to standard (K)CCA. By means of experimental evaluation on benchmark datasets, cluster-(K)CCA is shown to achieve state of the art performance for cross-modal retrieval tasks.', 'Probabilistic Partial Canonical Correlation Analysis Partial canonical correlation analysis (partial CCA) is a statistical method that estimates a pair of linear projections onto a low dimensional space, where the correlation between two multidimensional variables is maximized after eliminating the influence of a third variable. Partial CCA is known to be closely related to a causality measure between two time series. However, partial CCA requires the inverses of covariance matrices, so the calculation is not stable. This is particularly the case for high-dimensional data or small sample sizes. Additionally, we cannot estimate the optimal dimension of the subspace in the model. In this paper, we have addressed these problems by proposing a probabilistic interpretation of partial CCA and deriving a Bayesian estimation method based on the probabilistic model. Our numerical experiments demonstrated that our methods can stably estimate the model parameters, even in high dimensions or when there are a small number of samples.']"
234,19,234_robustly_robust_outliers_subgaussianity,"['robustly', 'robust', 'outliers', 'subgaussianity', 'inlier', 'estimation', 'sparse', 'subgaussian', 'gaussian', 'estimate']","['On Robust Mean Estimation under Coordinate-level Corruption We study the problem of robust mean estimation and introduce a novel Hamming distance-based measure of distribution shift for coordinate-level corruptions. We show that this measure yields adversary models that capture more realistic corruptions than those used in prior works, and present an information-theoretic analysis of robust mean estimation in these settings. We show that for structured distributions, methods that leverage the structure yield information theoretically more accurate mean estimation. We also focus on practical algorithms for robust mean estimation and study when data cleaning-inspired approaches that first fix corruptions in the input data and then perform robust mean estimation can match the information theoretic bounds of our analysis. We finally demonstrate experimentally that this two-step approach outperforms structure-agnostic robust estimation and provides accurate mean estimation even for high-magnitude corruption.', 'High Dimensional Robust Sparse Regression We provide a novel – and to the best of our knowledge, the first – algorithm for high dimensional sparse regression with constant fraction of corruptions in explanatory and/or response variables. Our algorithm recovers the true sparse parameters with sub-linear sample complexity,in the presence of a constant fraction of arbitrary corruptions. Our main contribution is a robust variant of Iterative Hard Thresholding. Using this, we provide accurate estimators:when the covariance matrix in sparse regression is identity,  our error guarantee is near information-theoretically optimal. We then deal with robust sparse regression with unknown structured covariance matrix. We propose a filtering algorithm whichconsists of a novel randomized outlier removal technique for robust sparse mean estimation that may be of interest in its own right: the filtering algorithm is flexible enough to deal with unknown covariance.Also, it is orderwise more efficient computationally than the ellipsoid algorithm.Using sub-linear sample complexity, our algorithm achieves the best known (and first) error guarantee. We demonstrate the effectiveness on large-scale sparse regression problems with arbitrary corruptions.', 'Private Robust Estimation by Stabilizing Convex Relaxations We give the first polynomial time and sample (epsilon, delta)-differentially private (DP) algorithm to estimate the mean, covariance and higher moments in the presence of a constant fraction of adversarial outliers. Our algorithm succeeds for families of distributions that satisfy two well-studied properties in prior works on robust estimation: certifiable subgaussianity of directional moments and certifiable hypercontractivity of degree 2 polynomials. Our recovery guarantees hold in the “right affine-invariant norms”: Mahalanobis distance for mean, multiplicative spectral and relative Frobenius distance guarantees for covariance and injective norms for higher moments. Prior works obtained private robust algorithms for mean estimation of subgaussian distributions with bounded covariance. For covariance estimation, ours is the first efficient algorithm (even in the absence of outliers) that succeeds without any condition-number assumptions. Our algorithms arise from a new framework that provides a general blueprint for modifying convex relaxations for robust estimation to satisfy strong worst-case stability guarantees in the appropriate parameter norms whenever the algorithms produce witnesses of correctness in their run. We verify such guarantees for a modification of standard sum-of-squares (SoS) semidefinite programming relaxations for robust estimation. Our privacy guarantees are obtained by combining stability guarantees with a new “estimate dependent” noise injection mechanism in which noise scales with the eigenvalues of the estimated covariance. We believe this framework will be useful more generally in obtaining DP counterparts of robust estimators. Independently of our work, Ashtiani and Liaw [AL21] also obtained a polynomial time and sample private robust estimation algorithm for Gaussian distributions.']"
235,14,235_sparse_lasso_regularized_pursuit,"['sparse', 'lasso', 'regularized', 'pursuit', 'thresholding', 'robust', 'sparsity', 'optimal', 'orthogonal', 'regression']","['Signal and Noise Statistics Oblivious Orthogonal Matching Pursuit Orthogonal matching pursuit (OMP) is a widely used algorithm for recovering sparse high dimensional vectors in linear regression models. The optimal performance of OMP requires a priori knowledge of either the sparsity of regression vector or noise statistics. Both these statistics are rarely known a priori and are very difficult to estimate. In this paper, we present a novel technique called residual ratio thresholding (RRT) to operate OMP without any a priori knowledge of sparsity and noise statistics and establish finite sample and large sample support recovery guarantees for the same. Both analytical results and numerical simulations in real and synthetic data sets indicate that RRT has a performance comparable to OMP with a priori knowledge of sparsity and noise statistics.', 'Noisy and Missing Data Regression: Distribution-Oblivious Support Recovery Many models for sparse regression typically assume that the covariates are known completely, and without noise. Particularly in high-dimensional applications, this is often not the case. Worse yet, even estimating statistics of the noise (the noise covariance) can be a central challenge. In this paper we develop a simple variant of orthogonal matching pursuit (OMP) for precisely this setting. We show that without knowledge of the noise covariance, our algorithm recovers the support, and we provide matching lower bounds that show that our algorithm performs at the minimax optimal rate. While simple, this is the first algorithm that (provably) recovers support in a noise-distribution-oblivious manner. When knowledge of the noise-covariance is available, our algorithm matches the best-known \\ell^2-recovery bounds available. We show that these too are min-max optimal. Along the way, we also obtain improved performance guarantees for OMP for the standard sparse regression problem with Gaussian noise.', 'Hardness and Algorithms for Robust and Sparse Optimization We explore algorithms and limitations for sparse optimization problems such as sparse linear regression and robust linear regression. The goal of the sparse linear regression problem is to identify a small number of key features, while the goal of the robust linear regression problem is to identify a small number of erroneous measurements. Specifically, the sparse linear regression problem seeks a $k$-sparse vector $x\\in\\mathbb{R}^d$ to minimize $\\|Ax-b\\|_2$, given an input matrix $A\\in\\mathbb{R}^{n\\times d}$ and a target vector $b\\in\\mathbb{R}^n$, while the robust linear regression problem seeks a set $S$ that ignores at most $k$ rows and a vector $x$ to minimize $\\|(Ax-b)_S\\|_2$. We first show bicriteria, NP-hardness of approximation for robust regression building on the work of \\cite{ODonnellWZ15} which implies a similar result for sparse regression. We further show fine-grained hardness of robust regression through a reduction from the minimum-weight $k$-clique conjecture. On the positive side, we give an algorithm for robust regression that achieves arbitrarily accurate additive error and uses runtime that closely matches the lower bound from the fine-grained hardness result, as well as an algorithm for sparse regression with similar runtime. Both our upper and lower bounds rely on a general reduction from robust linear regression to sparse regression that we introduce. Our algorithms, inspired by the 3SUM problem, use approximate nearest neighbor data structures and may be of independent interest for solving sparse optimization problems. For instance, we demonstrate that our techniques can also be used for the well-studied sparse PCA problem.']"
236,38,236_pca_outliers_outlier_sparse,"['pca', 'outliers', 'outlier', 'sparse', 'inliers', 'outlierrobust', 'robust', 'algorithms', 'eigenvectors', 'matrix']","['Robust Principal Component Analysis with Side Information The robust principal component analysis (robust PCA) problem has been considered in many machine learning applications, where the goal is to decompose the data matrix as a low rank part plus a sparse residual. While current approaches are developed by only considering the low rank plus sparse structure, in many applications, side information of row and/or column entities may also be given, and it is still unclear to what extent could such information help robust PCA. Thus, in this paper, we study the problem of robust PCA with side information, where both prior structure and features of entities are exploited for recovery. We propose a convex problem to incorporate side information in robust PCA and show that the low rank matrix can be exactly recovered via the proposed method under certain conditions. In particular, our guarantee suggests that a substantial amount of low rank matrices, which cannot be recovered by standard robust PCA, become recoverable by our proposed method. The result theoretically justifies the effectiveness of features in robust PCA. In addition, we conduct synthetic experiments as well as a real application on noisy image classification to show that our method also improves the performance in practice by exploiting side information.', 'Streaming Principal Component Analysis in Noisy Setting We study streaming algorithms for principal component analysis (PCA) in noisy settings. We present computationally efficient algorithms with sub-linear regret bounds for PCA in the presence of noise, missing data, and gross outliers.', 'Nearly-Linear Time and Streaming Algorithms for Outlier-Robust PCA We study principal component analysis (PCA), where given a dataset in $\\mathbb R^d$ from a distribution, the task is to find a unit vector $v$ that approximately maximizes the variance of the distribution after being projected along $v$. Despite being a classical task, standard estimators fail drastically if the data contains even a small fraction of outliers, motivating the problem of robust PCA. Recent work has developed computationally-efficient algorithms for robust PCA that either take super-linear time or have sub-optimal error guarantees. Our main contribution is to develop a nearly linear time algorithm for robust PCA with near-optimal error guarantees. We also develop a single-pass streaming algorithm for robust PCA with memory usage nearly-linear in the dimension.']"
237,91,237_matrix_matrices_completion_algorithms,"['matrix', 'matrices', 'completion', 'algorithms', 'minimization', 'semidefinite', 'overparameterized', 'lowrank', 'algorithm', 'spectral']","['Exponential Family Matrix Completion under Structural Constraints We consider the matrix completion problem of recovering a structured matrix from noisy and partial measurements. Recent works have proposed tractable estimators with strong statistical guarantees for the case where the underlying matrix is low–rank, and the measurements consist of a subset, either of the exact individual entries,  or of the entries perturbed by additive Gaussian noise, which is thus implicitly suited for thin–tailed continuous data. Arguably, common applications of matrix completion require estimators for (a) heterogeneous data–types, such as skewed–continuous, count, binary, etc., (b) for heterogeneous noise models (beyond Gaussian), which capture varied uncertainty in the measurements, and (c) heterogeneous structural constraints beyond low–rank, such as block–sparsity, or a superposition structure of low–rank plus elementwise sparseness, among others. In this paper, we provide a vastly unified framework for generalized matrix completion by considering a  matrix completion setting wherein the matrix entries are sampled from any member of the rich family of \\textitexponential family distributions; and impose general structural constraints on the underlying matrix, as captured by a general regularizer \\mathcalR(.). We propose a simple convex regularized M–estimator for the generalized framework, and provide a unified and novel statistical analysis for this general class of estimators. We finally corroborate our theoretical results on simulated datasets.', 'Rank-One Matrix Pursuit for Matrix Completion Low rank matrix completion has been applied successfully in a wide range of machine learning applications, such as collaborative filtering, image inpainting and Microarray data imputation. However, many existing algorithms are not scalable to large-scale problems, as they involve computing singular value decomposition. In this paper, we present an efficient and scalable algorithm for matrix completion. The key idea is to extend the well-known orthogonal matching pursuit from the vector case to the matrix case. In each iteration, we pursue a rank-one matrix basis generated by the top singular vector pair of the current approximation residual and update the weights for all rank-one matrices obtained up to the current iteration. We further propose a novel weight updating rule to reduce the time and storage complexity, making the proposed algorithm scalable to large matrices. We establish the linear convergence of the proposed algorithm. The fast convergence is achieved due to the proposed construction of matrix bases and the estimation of the weights. We empirically evaluate the proposed algorithm on many real-world large scale datasets. Results show that our algorithm is much more efficient than state-of-the-art matrix completion algorithms while achieving similar or better prediction performance.', 'Computational Limits for Matrix Completion Matrix Completion is the problem of recovering an unknown real-valued low-rank matrix from a subsample of its entries. Important recent results show that the problem can be solved efficiently under the assumption that the unknown matrix is incoherent and the subsample is drawn uniformly at random. Are these assumptions necessary? It is well known that Matrix Completion in its full generality is NP-hard. However, little is known if we make additional assumptions such as incoherence and permit the algorithm to output a matrix of slightly higher rank. In this paper we prove that Matrix Completion remains computationally intractable even if the unknown matrix has rank\xa04 but we are allowed to output any constant rank matrix, and even if additionally we assume that the unknown  matrix is incoherent and are shown 90% of the entries. This result relies on the conjectured hardness of the 4-Coloring problem. We also consider the positive semidefinite Matrix Completion problem. Here we show a similar hardness result under the standard assumption that \\mathrmP\\ne \\mathrmNP. Our results greatly narrow the gap between existing feasibility results and computational lower bounds. In particular, we believe that our results give the first complexity-theoretic justification for why distributional assumptions are needed beyond the incoherence assumption in order to obtain positive results. On the technical side, we contribute several new ideas on how to encode hard combinatorial problems in low-rank optimization problems. We hope that these techniques will be helpful in further understanding the computational limits of Matrix Completion and related problems.']"
238,14,238_sparse_compressive_sparsely_dictionaries,"['sparse', 'compressive', 'sparsely', 'dictionaries', 'sparselyused', 'dictionary', 'learning', 'sparsity', 'overcomplete', 'reconstructed']","['Hiding Data Helps: On the Benefits of Masking for Sparse Coding Sparse coding, which refers to modeling a signal as sparse linear combinations of the elements of a learned dictionary, has proven to be a successful (and interpretable) approach in applications such as signal processing, computer vision, and medical imaging. While this success has spurred much work on provable guarantees for dictionary recovery when the learned dictionary is the same size as the ground-truth dictionary, work on the setting where the learned dictionary is larger (or $\\textit{over-realized}$) with respect to the ground truth is comparatively nascent. Existing theoretical results in this setting have been constrained to the case of noise-less data. We show in this work that, in the presence of noise, minimizing the standard dictionary learning objective can fail to recover the elements of the ground-truth dictionary in the over-realized regime, regardless of the magnitude of the signal in the data-generating process. Furthermore, drawing from the growing body of work on self-supervised learning, we propose a novel masking objective for which recovering the ground-truth dictionary is in fact optimal as the signal increases for a large class of data-generating processes. We corroborate our theoretical results with experiments across several parameter regimes showing that our proposed objective also enjoys better empirical performance than the standard reconstruction objective.', 'The Sample Complexity of Dictionary Learning A large set of signals can sometimes be described sparsely using a dictionary, that is, every element can be represented as a linear combination of few elements from the dictionary.  Algorithms for various signal processing applications, including classification, denoising and signal separation, learn a dictionary from a given set of signals to be represented. Can we expect that the error in representing by such a dictionary a previously unseen signal from the same source will be of similar magnitude as those for the given examples? We assume signals are generated from a fixed distribution, and study these questions from a statistical learning theory perspective. We develop generalization bounds on the quality of the learned dictionary for two types of constraints on the coefficient selection, as measured by the expected $L_2$ error in representation when the dictionary is used. For the case of $l_1$ regularized coefficient selection we provide a generalization bound of the order of $O\\left(\\sqrt{np\\ln(m λ)/m}\\right)$, where $n$ is the dimension, $p$ is the number of elements in the dictionary, λis a bound on the $l_1$ norm of the coefficient vector and m is the number of samples, which complements existing results. For the case of representing a new signal as a combination of at most $k$ dictionary elements, we provide a bound ofthe order $O(\\sqrt{np\\ln(m  k)/m})$ under an assumption on the closeness to orthogonality of the dictionary (low Babel function). We further show that this assumption holds for most dictionaries in high dimensions in a strong probabilistic sense. Our results also include bounds that converge as $1/m$, not previously known for this problem. We provide similar results in a general setting using kernels with weak smoothness requirements.', 'Learning Sparsely Used Overcomplete Dictionaries We consider the problem of learning sparsely used overcomplete dictionaries, where each observation is  a sparse combination of elements from an unknown overcomplete dictionary. We establish exact recovery when the dictionary elements are mutually incoherent. Our method consists of a clustering-based initialization step, which provides an approximate estimate   of the true dictionary with guaranteed accuracy. This estimate is then refined via an iterative algorithm with the following alternating steps: 1) estimation of the dictionary coefficients for each observation through \\ell_1 minimization, given the dictionary estimate, and 2) estimation of the dictionary elements through least squares, given the coefficient estimates. We establish that, under a set of sufficient conditions, our method converges at a linear rate to the true dictionary as well as the true coefficients for each observation.']"
239,39,239_compressive_sparse_compressed_decoding,"['compressive', 'sparse', 'compressed', 'decoding', 'sensing', 'sparsity', 'deconvolution', 'signals', 'signal', 'sampling']","['One-Bit Compressed Sensing: Provable Support and Vector Recovery In this paper, we study the problem of one-bit compressed sensing (1-bit CS), where the goal is to design a measurement matrix A and a recovery algorithm s.t. a k-sparse vector \\x^* can be efficiently recovered back from signed linear measurements, i.e., b=\\sign(A\\x^*). This is an important problem in the signal acquisition area and has several learning applications as well, e.g., multi-label classification \\citeHsuKLZ10. We study this problem in two settings: a) support recovery: recover \\supp(\\x^*), b) approximate vector recovery: recover a unit vector \\hx s.t. || \\hatx-\\x^*/||\\x^*|| ||_2≤ε. For support recovery, we propose two novel and efficient solutions based on two combinatorial structures: union free family of sets and expanders. In contrast to  existing methods for  support recovery, our methods are universal i.e. a single measurement matrix A can recover almost all the signals. For approximate recovery, we propose the first  method to recover sparse vector using a near optimal number of measurements.  We also empirically demonstrate  effectiveness of our algorithms; we show that our algorithms are able to recover signals with smaller number of measurements than several existing methods. ', 'Equivariant Priors for compressed sensing with unknown orientation In compressed sensing, the goal is to reconstruct the signal from an underdetermined system of linear measurements. Thus, prior knowledge about the signal of interest and its structure is required. Additionally, in many scenarios, the signal has an unknown orientation prior to measurements. To address such recovery problems, we propose using equivariant generative models as a prior, which encapsulate orientation information in their latent space. Thereby, we show that signals with unknown orientations can be recovered with iterative gradient descent on the latent space of these models and provide additional theoretical recovery guarantees. We construct an equivariant variational autoencoder and use the decoder as generative prior for compressed sensing. We discuss additional potential gains of the proposed approach in terms of convergence and latency.', 'Modeling Sparse Deviations for Compressed Sensing using Generative Models In compressed sensing, a small number of linear measurements can be used to reconstruct an unknown signal. Existing approaches leverage assumptions on the structure of these signals, such as sparsity or the availability of a generative model. A domain-specific generative model can provide a stronger prior and thus allow for recovery with far fewer measurements. However, unlike sparsity-based approaches, existing methods based on generative models guarantee exact recovery only over their support, which is typically only a small subset of the space on which the signals are defined. We propose Sparse-Gen, a framework that allows for sparse deviations from the support set, thereby achieving the best of both worlds by using a domain specific prior and allowing reconstruction over the full space of signals. Theoretically, our framework provides a new class of signals that can be acquired using compressed sensing, reducing classic sparse vector recovery to a special case and avoiding the restrictive support due to a generative model prior. Empirically, we observe consistent improvements in reconstruction accuracy over competing approaches, especially in the more practical setting of transfer compressed sensing where a generative model for a data-rich, source domain aids sensing on a data-scarce, target domain.']"
240,11,240_lasso_feature_featuressamples_features,"['lasso', 'feature', 'featuressamples', 'features', 'lassonet', 'sparse', 'selection', 'penalized', 'predictive', 'predictor']","[' LassoNet: Neural Networks with Feature Sparsity   Much work has been done recently to make neural networks more interpretable, and one approach is to arrange for the network to use only a subset of the available features. In linear models, Lasso (or $\\ell_1$-regularized) regression assigns zero weights to the most irrelevant or redundant features, and is widely used in data science. However the Lasso only applies to linear models. Here we introduce LassoNet, a neural network framework with global feature selection. Our approach achieves feature sparsity by allowing a feature to participate in a hidden unit only if its linear representative is active. Unlike other approaches to feature selection for neural nets, our method uses a modified objective function with constraints, and so integrates feature selection with the parameter learning directly. As a result, it delivers an entire regularization path of solutions with a range of feature sparsity. In experiments with real and simulated data, LassoNet significantly outperforms state-of-the-art methods for feature selection and regression. The LassoNet method uses projected proximal gradient descent, and generalizes directly to deep networks. It can be implemented by adding just a few lines of code to a standard neural network. ', 'Predictive Correlation Screening: Application to Two-stage Predictor Design in High Dimension We introduce a new approach to variable selection, called Predictive Correlation Screening, for predictor design. Predictive Correlation Screening (PCS) implements false positive control on the selected variables, is well suited to small sample sizes, and is scalable to high dimensions. We establish asymptotic bounds for Familywise Error Rate (FWER), and resultant mean square error of a linear predictor on the selected variables. We apply Predictive Correlation Screening to the following two-stage predictor design problem. An experimenter wants to learn a multivariate predictor of gene expressions based on successive biological samples assayed on mRNA arrays. She assays the whole genome on a few samples and from these assays she selects a small number of variables using Predictive Correlation Screening. To reduce assay cost, she subsequently assays only the selected variables on the remaining samples, to learn the predictor coefficients. We show superiority of Predictive Correlation Screening relative to LASSO and correlation learning (sometimes popularly referred to in the literature as marginal regression or simple thresholding) in terms of performance and computational complexity.', ' Parametric Programming Approach for More Powerful and General Lasso Selective Inference   Selective Inference (SI) has been actively studied in the past few years for conducting inference on the features of linear models that are adaptively selected by feature selection methods such as Lasso. The basic idea of SI is to make inference conditional on the selection event. Unfortunately, the main limitation of the original SI approach for Lasso is that the inference is conducted not only conditional on the selected features but also on their signs—this leads to loss of power because of over-conditioning. Although this limitation can be circumvented by considering the union of such selection events for all possible combinations of signs, this is only feasible when the number of selected features is sufficiently small. To address this computational bottleneck, we propose a parametric programming-based method that can conduct SI without conditioning on signs even when we have thousands of active features. The main idea is to compute the continuum path of Lasso solutions in the direction of a test statistic, and identify the subset of the data space corresponding to the feature selection event by following the solution path. The proposed parametric programming-based method not only avoids the aforementioned computational bottleneck but also improves the performance and practicality of SI for Lasso in various respects. We conduct several experiments to demonstrate the effectiveness and efficiency of our proposed method. ']"
241,38,241_lasso_lassos_sparse_regularization,"['lasso', 'lassos', 'sparse', 'regularization', 'lassotype', 'lassobench', 'regularizers', 'sparsity', 'optimization', 'regression']","['Estimation Consistency of the Group Lasso and its Applications We extend the $\\ell_2$-consistency result of (Meinshausen and Yu 2008) from the Lasso to  the group Lasso. Our main theorem shows that the group Lasso  achieves estimation consistency under a mild condition and an asymptotic upper bound on the number of selected variables can be obtained.  As a result, we can apply the nonnegative garrote procedure to the group Lasso result to obtain an estimator which is simultaneously  estimation  and variable selection consistent.  In particular,  our setting allows both the number of groups and the number of variables per  group increase and thus is applicable to high-dimensional problems.  We also provide   estimation consistency analysis for a version of the sparse additive models with increasing dimensions. Some finite-sample results are also reported.', 'Lasso with Latents: Efficient Estimation, Covariate Rescaling, and Computational-Statistical Gaps It is well-known that the statistical performance of Lasso can suffer significantly when the covariates of interest have strong correlations. In particular, the prediction error of Lasso becomes much worse than computationally inefficient alternatives like Best Subset Selection. Due to a large conjectured computational-statistical tradeoff in the problem of sparse linear regression, it may be impossible to close this gap in general. In this work, we propose a natural sparse linear regression setting where strong correlations between covariates arise from unobserved latent variables. In this setting, we analyze the problem caused by strong correlations and design a surprisingly simple fix. While Lasso with standard normalization of covariates fails, there exists a heterogeneous scaling of the covariates with which Lasso will suddenly obtain strong provable guarantees for estimation. Moreover, we design a simple, efficient procedure for computing such a “smart scaling.” The sample complexity of the resulting “rescaled Lasso” algorithm incurs (in the worst case) quadratic dependence on the sparsity of the underlying signal. While this dependence is not information-theoretically necessary, we give evidence that it is optimal among the class of polynomial-time algorithms, via the method of low-degree polynomials. This argument reveals a new connection between sparse linear regression and a special version of sparse PCA with a \\emph{near-critical negative spike}. The latter problem can be thought of as a real-valued analogue of learning a sparse parity. Using it, we also establish the first computational-statistical gap for the closely related problem of learning a Gaussian Graphical Model.', 'The Well-Tempered Lasso We study the complexity of the entire regularization path for least squares regression with 1-norm penalty, known as the Lasso. Every regression parameter in the Lasso changes linearly as a function of the regularization value. The number of changes is regarded as the Lasso’s complexity. Experimental results using exact path following exhibit polynomial complexity of the Lasso in the problem size. Alas, the path complexity of the Lasso on artificially designed regression problems is exponential We use smoothed analysis as a mechanism for bridging the gap between worst case settings and the de facto low complexity. Our analysis assumes that the observed data has a tiny amount of intrinsic noise. We then prove that the Lasso’s complexity is polynomial in the problem size.']"
242,23,242_minimax_gans_gan_minmax,"['minimax', 'gans', 'gan', 'minmax', 'adversarial', 'generative', 'optimality', 'ascent', 'optimization', 'gradient']","['Stochastic Smoothed Gradient Descent Ascent for Federated Minimax Optimization In recent years, federated minimax optimization has attracted growing interest due to its extensive applications in various machine learning tasks. While Smoothed Alternative Gradient Descent Ascent (Smoothed-AGDA) has proved successful in centralized nonconvex minimax optimization, how and whether smoothing techniques could be helpful in a federated setting remains unexplored. In this paper, we propose a new algorithm termed Federated Stochastic Smoothed Gradient Descent Ascent (FESS-GDA), which utilizes the smoothing technique for federated minimax optimization. We prove that FESS-GDA can be uniformly applied to solve several classes of federated minimax problems and prove new or better analytical convergence results for these settings. We showcase the practical efficiency of FESS-GDA in practical federated learning tasks of training generative adversarial networks (GANs) and fair classification.', 'What is Local Optimality in Nonconvex-Nonconcave Minimax Optimization? Minimax optimization has found extensive applications in modern machine learning, in settings such as generative adversarial networks (GANs), adversarial training and multi-agent reinforcement learning. As most of these applications involve continuous nonconvex-nonconcave formulations, a very basic question arises—“what is a proper definition of local optima?” Most previous work answers this question using classical notions of equilibria from simultaneous games, where the min-player and the max-player act simultaneously. In contrast, most applications in machine learning, including GANs and adversarial training, correspond to sequential games, where the order of which player acts first is crucial (since minimax is in general not equal to maximin due to the nonconvex-nonconcave nature of the problems). The main contribution of this paper is to propose a proper mathematical definition of local optimality for this sequential setting—local minimax, as well as to present its properties and existence results. Finally, we establish a strong connection to a basic local search algorithm—gradient descent ascent (GDA): under mild conditions, all stable limit points of GDA are exactly local minimax points up to some degenerate points.', 'Train simultaneously, generalize better: Stability of gradient-based minimax learners The success of minimax learning problems of generative adversarial networks (GANs) has been observed to depend on the minimax optimization algorithm used for their training. This dependence is commonly attributed to the convergence speed and robustness properties of the underlying optimization algorithm. In this paper, we show that the optimization algorithm also plays a key role in the generalization performance of the trained minimax model. To this end, we analyze the generalization properties of standard gradient descent ascent (GDA) and proximal point method (PPM) algorithms through the lens of algorithmic stability as defined by Bousquet & Elisseeff, 2002 under both convex-concave and nonconvex-nonconcave minimax settings. While the GDA algorithm is not guaranteed to have a vanishing excess risk in convex-concave problems, we show the PPM algorithm enjoys a bounded excess risk in the same setup. For nonconvex-nonconcave problems, we compare the generalization performance of stochastic GDA and GDmax algorithms where the latter fully solves the maximization subproblem at every iteration. Our generalization analysis suggests the superiority of GDA provided that the minimization and maximization subproblems are solved simultaneously with similar learning rates. We discuss several numerical results indicating the role of optimization algorithms in the generalization of learned minimax models.']"
243,14,243_sparse_regularization_sparser_lasso,"['sparse', 'regularization', 'sparser', 'lasso', 'regularized', 'regularizations', 'sparsityyielding', 'optimization', 'thresholding', 'regularizer']","['Sparse Convex Optimization via Adaptively Regularized Hard Thresholding The goal of Sparse Convex Optimization is to optimize a convex function $f$ under a sparsity constraint $s\\leq s^*\\gamma$, where $s^*$ is the target number of non-zero entries in a feasible solution (sparsity) and $\\gamma\\geq 1$ is an approximation factor. There has been a lot of work to analyze the sparsity guarantees of various algorithms (LASSO, Orthogonal Matching Pursuit (OMP), Iterative Hard Thresholding (IHT)) in terms of the Restricted Condition Number $\\kappa$. The best known algorithms guarantee to find an approximate solution of value $f(x^*)+\\epsilon$ with the sparsity bound of $\\gamma = O\\left(\\kappa\\min\\left\\{\\log \\frac{f(x^0)-f(x^*)}{\\epsilon}, \\kappa\\right\\}\\right)$, where $x^*$ is the target solution. We present a new Adaptively Regularized Hard Thresholding (ARHT) algorithm that makes significant progress on this problem by bringing the bound down to $\\gamma=O(\\kappa)$, which has been shown to be tight for a general class of algorithms including LASSO, OMP, and IHT. This is achieved without significant sacrifice in the runtime efficiency compared to the fastest known algorithms. We also provide a new analysis of OMP with Replacement (OMPR) for general $f$, under the condition $s > s^* \\frac{\\kappa^2}{4}$, which yields Compressed Sensing bounds under the Restricted Isometry Property (RIP). When compared to other Compressed Sensing approaches, it has the advantage of providing a strong tradeoff between the RIP condition and the solution sparsity, while working for any general function $f$ that meets the RIP condition.', 'A General Iterative Shrinkage and Thresholding Algorithm for Non-convex Regularized Optimization Problems Non-convex sparsity-inducing penalties have recently received considerable attentions in sparse learning. Recent theoretical investigations have demonstrated their superiority over the convex counterparts in several sparse learning settings. However, solving the non-convex optimization problems associated with non-convex penalties remains a big challenge. A commonly used approach is the Multi-Stage (MS) convex relaxation (or DC programming), which relaxes the original non-convex problem to a sequence of convex problems. This approach is usually not very practical for large-scale problems because its computational cost is a multiple of solving a single convex problem. In this paper, we propose a General Iterative Shrinkage and Thresholding (GIST) algorithm to solve the nonconvex optimization problem for a large class of non-convex penalties. The GIST algorithm iteratively solves a proximal operator problem, which in turn has a closed-form solution for many commonly used penalties. At each outer iteration of the algorithm, we use a line search initialized by the Barzilai-Borwein (BB) rule that allows finding an appropriate step size quickly. The paper also presents a detailed convergence analysis of the GIST algorithm. The efficiency of the proposed algorithm is demonstrated by extensive experiments on large-scale data sets.', 'High-dimensional Inference via Lipschitz Sparsity-Yielding Regularizers Non-convex regularizers are more and more applied to high-dimensional inference with sparsity prior knowledge. In general, the non-convex regularizer is superior to the convex ones in inference but it suffers the difficulties brought by local optimums and massive computation. A ""good"" regularizer should perform well in both inference and optimization. In this paper, we prove that some non-convex regularizers can be such ""good"" regularizers. They are a family of sparsity-yielding penalties with proper Lipschitz subgradients. These regularizers keep the superiority of non-convex regularizers in inference. Their estimation conditions based on sparse eigenvalues are weaker than the convex regularizers. Meanwhile, if properly tuned, they behave like convex regularizers since standard proximal methods guarantee to give stationary solutions. These stationary solutions, if sparse enough, are identical to the global solutions. If the solution sequence provided by proximal methods is along a sparse path, the convergence rate to the global optimum is on the order of 1/k where k is the number of iterations.']"
244,198,244_hessian_minimization_optimization_convexity,"['hessian', 'minimization', 'optimization', 'convexity', 'convex', 'gradient', 'optimal', 'accelerated', 'minimizing', 'nonconvex']","['First-Order Algorithms Converge Faster than $O(1/k)$ on Convex Problems It is well known that both gradient descent and stochastic coordinate descent achieve a global convergence rate of $O(1/k)$ in the objective value, when applied to a scheme for minimizing a Lipschitz-continuously differentiable, unconstrained convex function. In this work, we improve this rate to $o(1/k)$. We extend the result to proximal gradient and proximal coordinate descent on regularized problems to show similar $o(1/k)$ convergence rates. The result is tight in the sense that a rate of $O(1/k^{1+\\epsilon})$ is not generally attainable for any $\\epsilon>0$, for any of these methods.', 'Accelerated Gradient Descent Escapes Saddle Points Faster than Gradient Descent Nesterov’s accelerated gradient descent (AGD), an instance of the general family of “momentum methods,” provably achieves faster convergence rate than gradient descent (GD) in the convex setting. While these methods are widely used in modern \\emph{nonconvex} applications, including training of deep neural networks, whether they are provably superior to GD in the nonconvex setting remains open. This paper studies a simple variant of Nesterov’s\xa0AGD, and shows that it escapes saddle points and finds a second-order stationary point in $\\tilde{O}(1/\\epsilon^{7/4})$ iterations, matching the best known convergence rate, which is faster than the $\\tilde{O}(1/\\epsilon^{2})$ iterations required by GD. To the best of our knowledge, this is the first direct acceleration (single-loop) algorithm that is provably faster than GD in general nonconvex setting—all previous nonconvex accelerated algorithms rely on more complex mechanisms such as nested loops and proximal terms. Our analysis is based on two key ideas: (1) the use of a simple Hamiltonian function, inspired by a continuous-time perspective, which AGD monotonically decreases on each step even for nonconvex functions, and (2) a novel framework called\xa0\\emph{improve or localize}, which is useful for tracking the long-term behavior of gradient-based optimization algorithms. We believe that these techniques may deepen our understanding of both acceleration algorithms and nonconvex optimization.', 'Accelerated Stochastic Gradient Descent for Minimizing Finite Sums We propose an optimization method for minimizing the finite sums of smooth convex functions. Our method incorporates an accelerated gradient descent (AGD) and a stochastic variance reduction gradient (SVRG) in a mini-batch setting. An important feature of the method is that it can be directly applied to general convex and semi-strongly convex problems that is a weaker condition than strong convexity. We show that our method achieves a better overall complexity for the general convex problems and linear convergence for optimal strongly convex problems. Moreover we prove the fast iteration complexity of our method. Our experiments show the effectiveness of our method.']"
245,163,245_sgd_stochastic_gradients_gradient,"['sgd', 'stochastic', 'gradients', 'gradient', 'convexity', 'generalization', 'convex', 'optimization', 'optimal', 'descent']","['SGD and Hogwild! Convergence Without the Bounded Gradients Assumption Stochastic gradient descent (SGD) is the optimization algorithm of choice in many machine learning applications such as regularized empirical risk minimization and training deep neural networks. The classical convergence analysis of SGD is carried out under the assumption that the norm of the stochastic gradient is uniformly bounded. While this might hold for some loss functions, it is always violated for cases where the objective function is strongly convex. In (Bottou et al.,2016), a new analysis of convergence of SGD is performed under the assumption that stochastic gradients are bounded with respect to the true gradient norm. Here we show that for stochastic problems arising in machine learning such bound always holds; and we also propose an alternative convergence analysis of SGD with diminishing learning rate regime, which results in more relaxed conditions than those in (Bottou et al.,2016). We then move on the asynchronous parallel setting, and prove convergence of Hogwild! algorithm in the same regime, obtaining the first convergence results for this method in the case of diminished learning rate.', 'Information-Theoretic Generalization Bounds for Stochastic Gradient Descent We study the generalization properties of the popular stochastic optimization method known as  stochastic gradient descent (SGD) for optimizing general non-convex loss functions. Our main contribution is providing upper bounds on the generalization error that depend on local statistics of the stochastic gradients evaluated along the path of iterates calculated by SGD. The key factors our bounds depend on are the variance of the gradients (with respect to the data distribution) and the local smoothness of the objective function along the SGD path, and the sensitivity of the loss function to perturbations to the final output. Our key technical tool is combining the information-theoretic generalization bounds previously used for analyzing randomized variants of SGD with a perturbation analysis of the iterates.', 'Fast and Faster Convergence of SGD for Over-Parameterized Models and an Accelerated Perceptron Modern machine learning focuses on highly expressive models that are able to fit or interpolate the data completely,  resulting in zero training loss. For such models, we show that the stochastic gradients of common loss functions satisfy a strong growth condition. Under this condition, we prove that constant step-size stochastic gradient descent (SGD) with Nesterov acceleration matches the convergence rate of the deterministic accelerated method for both convex and strongly-convex functions. We also show that this condition implies that SGD can find a first-order stationary point as efficiently as full gradient descent in non-convex settings. Under interpolation, we further show that all smooth loss functions with a finite-sum structure satisfy a weaker growth condition. Given this weaker condition, we prove that SGD with a constant step-size attains the deterministic convergence rate in both the strongly-convex and convex settings. Under additional assumptions, the above results enable us to prove an $O(1/k^2)$ mistake bound for $k$ iterations of a stochastic perceptron algorithm using the squared-hinge loss. Finally, we validate our theoretical findings with experiments on synthetic and real datasets.']"
