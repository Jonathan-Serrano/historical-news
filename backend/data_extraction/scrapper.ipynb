{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyter in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: requests in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (8.1.6)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: notebook in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jupyter) (7.4.0)\n",
      "Requirement already satisfied: jupyter-console in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jupyter) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jupyter) (7.16.6)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jupyter) (6.29.5)\n",
      "Requirement already satisfied: jupyterlab in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jupyter) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from ipywidgets) (9.1.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.14 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from ipywidgets) (3.0.14)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: decorator in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from ipykernel->jupyter) (1.8.14)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from ipykernel->jupyter) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from ipykernel->jupyter) (5.7.2)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from ipykernel->jupyter) (1.6.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from ipykernel->jupyter) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from ipykernel->jupyter) (7.0.0)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from ipykernel->jupyter) (26.4.0)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from ipykernel->jupyter) (6.4.2)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jupyterlab->jupyter) (2.0.5)\n",
      "Requirement already satisfied: httpx>=0.25.0 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jupyterlab->jupyter) (0.28.1)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jupyterlab->jupyter) (3.1.6)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jupyterlab->jupyter) (2.2.5)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jupyterlab->jupyter) (2.15.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jupyterlab->jupyter) (2.27.3)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jupyterlab->jupyter) (0.2.4)\n",
      "Requirement already satisfied: setuptools>=41.1.0 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jupyterlab->jupyter) (78.1.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from nbconvert->jupyter) (4.13.4)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter) (6.2.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from nbconvert->jupyter) (0.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from nbconvert->jupyter) (3.0.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from nbconvert->jupyter) (3.1.3)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from nbconvert->jupyter) (0.10.2)\n",
      "Requirement already satisfied: nbformat>=5.7 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from nbconvert->jupyter) (5.10.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from nbconvert->jupyter) (1.5.1)\n",
      "Requirement already satisfied: webencodings in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter) (1.4.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->jupyter) (0.14.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.3.7)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (310)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (23.1.0)\n",
      "Requirement already satisfied: jupyter-events>=0.11.0 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.12.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.5.3)\n",
      "Requirement already satisfied: overrides>=5.0 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (7.7.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.21.1)\n",
      "Requirement already satisfied: pywinpty>=2.0.1 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.0.15)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.18.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.8.0)\n",
      "Requirement already satisfied: babel>=2.10 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.17.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.12.0)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (4.23.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from nbformat>=5.7->nbconvert->jupyter) (2.21.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from beautifulsoup4->nbconvert->jupyter) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from beautifulsoup4->nbconvert->jupyter) (4.13.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from anyio->httpx>=0.25.0->jupyterlab->jupyter) (1.3.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (21.2.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.24.0)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (3.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (6.0.2)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.1.1)\n",
      "Requirement already satisfied: fqdn in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.5.1)\n",
      "Requirement already satisfied: isoduration in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (3.0.0)\n",
      "Requirement already satisfied: uri-template in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (24.11.1)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.22)\n",
      "Requirement already satisfied: arrow>=0.15.0 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in c:\\users\\tan vo\\documents\\github\\historical-news\\backend\\venv\\lib\\site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.9.0.20241206)\n",
      "Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 5.8/11.5 MB 35.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 45.0 MB/s eta 0:00:00\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade jupyter requests tqdm ipywidgets pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter labextension enable widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "import xml.etree.ElementTree as ET\n",
    "import html\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_mementos_for_feed(feed_url, limit=10, retries=3, backoff=10, timeout=10):\n",
    "    \"\"\"\n",
    "    Query the Wayback Machine TimeMap Link API for a given feed URL,\n",
    "    parse the returned Link-format lines, and return up to `limit` memento URIs.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (compatible; ArchiveFetcher/1.0; +https://example.com)\"\n",
    "    }\n",
    "    timemap_url = f\"https://web.archive.org/web/timemap/link/{feed_url}\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            resp = requests.get(timemap_url, headers=headers, timeout=timeout)\n",
    "            resp.raise_for_status()\n",
    "            lines = resp.text.splitlines()\n",
    "            mementos = []\n",
    "            for line in lines:\n",
    "                parts = line.split(';', 1)\n",
    "                if not parts or not parts[0].startswith('<') or '>' not in parts[0]:\n",
    "                    continue\n",
    "                uri = parts[0].strip()[1:-1]\n",
    "                mementos.append(uri)\n",
    "                if len(mementos) >= limit:\n",
    "                    break\n",
    "            return mementos\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"[ERROR] Attempt {attempt + 1} failed for {timemap_url}: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                sleep_time = backoff**(attempt + 1)\n",
    "                print(f\"[INFO] Retrying in {sleep_time}s...\")\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                print(f\"[ERROR] All attempts failed for {timemap_url}\")\n",
    "                return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_xml_from_memento(memento_url, retries=3, backoff=10, timeout=10):\n",
    "    \"\"\"\n",
    "    Query the Memento URL, parse it as XML. Retries on failure with backoff.\n",
    "    Returns an ElementTree root element, or None on failure.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (compatible; ArchiveFetcher/1.0; +https://example.com)\"\n",
    "    }\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            resp = requests.get(memento_url, headers=headers, timeout=timeout)\n",
    "            resp.raise_for_status()\n",
    "            try:\n",
    "                root = ET.fromstring(resp.content)\n",
    "                return root\n",
    "            except ET.ParseError as e:\n",
    "                print(f\"[ERROR] Failed to parse XML from memento {memento_url}: {e}\")\n",
    "                return None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"[ERROR] Attempt {attempt + 1} failed for {memento_url}: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                sleep_time = backoff**(attempt + 1)\n",
    "                print(f\"[INFO] Retrying in {sleep_time}s...\")\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                print(f\"[ERROR] All attempts failed for {memento_url}\")\n",
    "                return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_memento_data(source_url, content):\n",
    "    \"\"\"\n",
    "    Extracts relevant data from the XML content of a Memento response.\n",
    "    \"\"\"\n",
    "    # Get title and description of channel\n",
    "    channel = content.find(\"channel\")\n",
    "    if channel is not None:\n",
    "        channel_title = channel.find(\"title\").text if channel.find(\"title\") is not None else None\n",
    "        channel_description = (\n",
    "            channel.find(\"description\").text\n",
    "            if channel.find(\"description\") is not None\n",
    "            else None\n",
    "        )\n",
    "    else:\n",
    "        channel_title = None\n",
    "        channel_description = None\n",
    "    \n",
    "    # Parse the XML content\n",
    "    items = content.findall(\".//item\")\n",
    "\n",
    "    memento_data = []\n",
    "\n",
    "    for item in items:\n",
    "        # Extract required fields from each item\n",
    "        title = item.find(\"title\").text if item.find(\"title\") is not None else None\n",
    "        description = (\n",
    "            item.find(\"description\").text\n",
    "            if item.find(\"description\") is not None\n",
    "            else None\n",
    "        )\n",
    "        link = item.find(\"link\").text if item.find(\"link\") is not None else None\n",
    "        pubDate = (\n",
    "            item.find(\"pubDate\").text if item.find(\"pubDate\") is not None else None\n",
    "        )\n",
    "\n",
    "        author_tags = [\"author\", \"{http://purl.org/dc/elements/1.1/}creator\", \"dc:creator\"]\n",
    "        author = None\n",
    "        for tag in author_tags:\n",
    "            elem = item.find(tag)\n",
    "            if elem is not None and elem.text:\n",
    "                author = elem.text\n",
    "                break\n",
    "\n",
    "        memento_data.append(\n",
    "            {\n",
    "                \"source\": source_url,\n",
    "                \"channel_title\": channel_title,\n",
    "                \"channel_description\": channel_description,\n",
    "                \"title\": title,\n",
    "                \"description\": description,\n",
    "                \"link\": link,\n",
    "                \"pubDate\": pubDate,\n",
    "                \"author\": author,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return memento_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_urls = [\n",
    "    # For enthusiasts\n",
    "    \"https://machinelearningmastery.com/feed\",\n",
    "    \"https://transferlab.ai/index.xml\",\n",
    "    # \"http://feeds.feedburner.com/FeaturedBlogPosts-DataScienceCentral?format=xml\", <--- Give error, not format as xml RSS\n",
    "    # For company\n",
    "    \"https://eng.uber.com/tag/machine-learning/feed\",\n",
    "    \"https://aws.amazon.com/blogs/machine-learning/feed\",\n",
    "    \"http://news.mit.edu/rss/topic/artificial-intelligence2\",\n",
    "    \"http://feeds.feedburner.com/nvidiablog\",\n",
    "    \"https://openai.com/news/rss.xml\",\n",
    "    \"http://feeds.feedburner.com/blogspot/gJZg\",\n",
    "    # For researchers\n",
    "    \"http://arxiv.org/rss/cs.LG\",\n",
    "    \"http://arxiv.org/rss/stat.ML\",\n",
    "    \"https://distill.pub/rss.xml\",\n",
    "    \"https://bair.berkeley.edu/blog/feed.xml\",\n",
    "    \"https://becominghuman.ai/feed\",\n",
    "    \"https://www.microsoft.com/en-us/research/feed\",\n",
    "]\n",
    "\n",
    "# Max number of mementos per feed\n",
    "limit_per_feed = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the feed URLs and their corresponding memento URIs\n",
    "feed_mementos = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "797695b4a74945829746c29dbac408c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing feeds:   0%|          | 0/14 [00:00<?, ?feed/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 20 mementos for https://machinelearningmastery.com/feed\n",
      "[INFO] Found 13 mementos for https://transferlab.ai/index.xml\n",
      "[INFO] Found 15 mementos for https://eng.uber.com/tag/machine-learning/feed\n",
      "[INFO] Found 20 mementos for https://aws.amazon.com/blogs/machine-learning/feed\n",
      "[INFO] Found 20 mementos for http://news.mit.edu/rss/topic/artificial-intelligence2\n",
      "[INFO] Found 20 mementos for http://feeds.feedburner.com/nvidiablog\n",
      "[INFO] Found 9 mementos for https://openai.com/news/rss.xml\n",
      "[INFO] Found 20 mementos for http://feeds.feedburner.com/blogspot/gJZg\n",
      "[INFO] Found 20 mementos for http://arxiv.org/rss/cs.LG\n",
      "[INFO] Found 20 mementos for http://arxiv.org/rss/stat.ML\n",
      "[INFO] Found 20 mementos for https://distill.pub/rss.xml\n",
      "[INFO] Found 20 mementos for https://bair.berkeley.edu/blog/feed.xml\n",
      "[INFO] Found 20 mementos for https://becominghuman.ai/feed\n",
      "[INFO] Found 20 mementos for https://www.microsoft.com/en-us/research/feed\n"
     ]
    }
   ],
   "source": [
    "for feed in tqdm(feed_urls, desc=\"Processing feeds\", unit=\"feed\"):\n",
    "    # Skip if already processed\n",
    "    if feed in feed_mementos and len(feed_mementos[feed]) == limit_per_feed:\n",
    "        print(f\"[INFO] Already processed {feed}\")\n",
    "        continue\n",
    "    try:\n",
    "        mementos = fetch_mementos_for_feed(feed, limit=limit_per_feed + 2)\n",
    "    except requests.HTTPError as e:\n",
    "        print(f\"[ERROR] Failed to fetch for {feed}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Remove the first two mementos (if they exist)\n",
    "    # Which is the original feed URL and the timemap URL\n",
    "    if len(mementos) > 2:\n",
    "        mementos = mementos[2:]\n",
    "\n",
    "    # Store the memento URIs in the dictionary\n",
    "    feed_mementos[feed] = mementos\n",
    "    print(f\"[INFO] Found {len(mementos)} mementos for {feed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feed: https://machinelearningmastery.com/feed\n",
      "- https://web.archive.org/web/https://machinelearningmastery.com/feed\n",
      "- https://web.archive.org/web/20150912134829/http://machinelearningmastery.com/feed/\n",
      "- https://web.archive.org/web/20160315034406/http://machinelearningmastery.com/feed/\n",
      "- https://web.archive.org/web/20170606124339/http://machinelearningmastery.com/feed/\n",
      "- https://web.archive.org/web/20171021225645/http://machinelearningmastery.com/feed/\n",
      "\n",
      "Feed: https://transferlab.ai/index.xml\n",
      "- https://web.archive.org/web/https://transferlab.ai/index.xml\n",
      "- https://web.archive.org/web/20240212185349/https://transferlab.ai/index.xml\n",
      "- https://web.archive.org/web/20240421141756/https://transferlab.ai/index.xml\n",
      "- https://web.archive.org/web/20240725170059/https://transferlab.ai/index.xml\n",
      "- https://web.archive.org/web/20240910065617/https://transferlab.ai/index.xml\n",
      "\n",
      "Feed: https://eng.uber.com/tag/machine-learning/feed\n",
      "- https://web.archive.org/web/https://eng.uber.com/tag/machine-learning/feed\n",
      "- https://web.archive.org/web/20180125144550/https://eng.uber.com/tag/machine-learning/feed/\n",
      "- https://web.archive.org/web/20181116163344/https://eng.uber.com/tag/machine-learning/feed/\n",
      "- https://web.archive.org/web/20190330131559/https://eng.uber.com/tag/machine-learning/feed/\n",
      "- https://web.archive.org/web/20190714151649/https://eng.uber.com/tag/machine-learning/feed/\n",
      "\n",
      "Feed: https://aws.amazon.com/blogs/machine-learning/feed\n",
      "- https://web.archive.org/web/https://aws.amazon.com/blogs/machine-learning/feed\n",
      "- https://web.archive.org/web/20180522183012/https://aws.amazon.com/blogs/machine-learning/feed/\n",
      "- https://web.archive.org/web/20180522223136/https://aws.amazon.com/blogs/machine-learning/feed/\n",
      "- https://web.archive.org/web/20180523083816/https://aws.amazon.com/blogs/machine-learning/feed/\n",
      "- https://web.archive.org/web/20180523190221/https://aws.amazon.com/blogs/machine-learning/feed/\n",
      "\n",
      "Feed: http://news.mit.edu/rss/topic/artificial-intelligence2\n",
      "- https://web.archive.org/web/http://news.mit.edu/rss/topic/artificial-intelligence2\n",
      "- https://web.archive.org/web/20150905204516/http://news.mit.edu/rss/topic/artificial-intelligence2\n",
      "- https://web.archive.org/web/20160304031213/http://news.mit.edu:80/rss/topic/artificial-intelligence2\n",
      "- https://web.archive.org/web/20160521114724/http://news.mit.edu:80/rss/topic/artificial-intelligence2\n",
      "- https://web.archive.org/web/20160621170220/http://news.mit.edu:80/rss/topic/artificial-intelligence2\n",
      "\n",
      "Feed: http://feeds.feedburner.com/nvidiablog\n",
      "- https://web.archive.org/web/http://feeds.feedburner.com/nvidiablog\n",
      "- https://web.archive.org/web/20101212100419/http://feeds.feedburner.com:80/nvidiablog\n",
      "- https://web.archive.org/web/20110311223355/http://feeds.feedburner.com:80/nvidiablog\n",
      "- https://web.archive.org/web/20110403044422/http://feeds.feedburner.com:80/nvidiablog?\n",
      "- https://web.archive.org/web/20110907220144/http://feeds.feedburner.com:80/nvidiablog\n",
      "\n",
      "Feed: https://openai.com/news/rss.xml\n",
      "- https://web.archive.org/web/https://openai.com/news/rss.xml\n",
      "- https://web.archive.org/web/20241204101815/https://openai.com/news/rss.xml\n",
      "- https://web.archive.org/web/20250105004615/http://openai.com/news/rss.xml\n",
      "- https://web.archive.org/web/20250213133337/https://openai.com/news/rss.xml\n",
      "- https://web.archive.org/web/20250213133359/https://openai.com/news/rss.xml\n",
      "\n",
      "Feed: http://feeds.feedburner.com/blogspot/gJZg\n",
      "- https://web.archive.org/web/http://feeds.feedburner.com/blogspot/gJZg\n",
      "- https://web.archive.org/web/20071029185815/http://feeds.feedburner.com:80/blogspot/gJZg\n",
      "- https://web.archive.org/web/20080415193603/http://feeds.feedburner.com:80/blogspot/gJZg\n",
      "- https://web.archive.org/web/20080524023148/http://feeds.feedburner.com:80/blogspot/gJZg\n",
      "- https://web.archive.org/web/20080618223319/http://feeds.feedburner.com:80/blogspot/gJZg?\n",
      "\n",
      "Feed: http://arxiv.org/rss/cs.LG\n",
      "- https://web.archive.org/web/http://arxiv.org/rss/cs.LG\n",
      "- https://web.archive.org/web/20060205081224/http://arxiv.org:80/rss/cs.LG\n",
      "- https://web.archive.org/web/20090506052826/http://arxiv.org/rss/cs.LG\n",
      "- https://web.archive.org/web/20090506052826/http://arxiv.org/rss/cs.LG\n",
      "- https://web.archive.org/web/20090506052826/http://arxiv.org/rss/cs.LG\n",
      "\n",
      "Feed: http://arxiv.org/rss/stat.ML\n",
      "- https://web.archive.org/web/http://arxiv.org/rss/stat.ML\n",
      "- https://web.archive.org/web/20090114032246/http://arxiv.org/rss/stat.ML\n",
      "- https://web.archive.org/web/20090506053346/http://arxiv.org/rss/stat.ML\n",
      "- https://web.archive.org/web/20090506053346/http://arxiv.org/rss/stat.ML\n",
      "- https://web.archive.org/web/20090506053346/http://arxiv.org/rss/stat.ML\n",
      "\n",
      "Feed: https://distill.pub/rss.xml\n",
      "- https://web.archive.org/web/https://distill.pub/rss.xml\n",
      "- https://web.archive.org/web/20161031193317/http://distill.pub:80/rss.xml\n",
      "- https://web.archive.org/web/20161209231314/http://distill.pub:80/rss.xml\n",
      "- https://web.archive.org/web/20170108002607/http://distill.pub/rss.xml\n",
      "- https://web.archive.org/web/20170306050047/http://distill.pub/rss.xml\n",
      "\n",
      "Feed: https://bair.berkeley.edu/blog/feed.xml\n",
      "- https://web.archive.org/web/https://bair.berkeley.edu/blog/feed.xml\n",
      "- https://web.archive.org/web/20170624072123/http://bair.berkeley.edu:80/blog/feed.xml\n",
      "- https://web.archive.org/web/20170824091901/http://bair.berkeley.edu:80/blog/feed.xml\n",
      "- https://web.archive.org/web/20170830170301/http://bair.berkeley.edu/blog/feed.xml\n",
      "- https://web.archive.org/web/20170901190406/http://bair.berkeley.edu/blog/feed.xml\n",
      "\n",
      "Feed: https://becominghuman.ai/feed\n",
      "- https://web.archive.org/web/https://becominghuman.ai/feed\n",
      "- https://web.archive.org/web/20170526193329/https://becominghuman.ai/feed\n",
      "- https://web.archive.org/web/20170630013818/https://becominghuman.ai/feed\n",
      "- https://web.archive.org/web/20180815093604/https://becominghuman.ai/feed\n",
      "- https://web.archive.org/web/20180821080415/https://becominghuman.ai/feed\n",
      "\n",
      "Feed: https://www.microsoft.com/en-us/research/feed\n",
      "- https://web.archive.org/web/https://www.microsoft.com/en-us/research/feed\n",
      "- https://web.archive.org/web/20160702215923/https://www.microsoft.com/en-us/research/feed\n",
      "- https://web.archive.org/web/20160708091441/https://www.microsoft.com/en-us/research/feed/\n",
      "- https://web.archive.org/web/20160723065431/https://www.microsoft.com/en-us/research/feed/\n",
      "- https://web.archive.org/web/20160812095616/https://www.microsoft.com/en-us/research/feed/\n"
     ]
    }
   ],
   "source": [
    "# Print the first 5 memento URIs for each feed\n",
    "for feed, mementos in feed_mementos.items():\n",
    "    print(f\"\\nFeed: {feed}\")\n",
    "    for memento in mementos[:5]:\n",
    "        print(f\"- {memento}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feed: https://machinelearningmastery.com/feed\n",
      "Fetching memento: https://web.archive.org/web/https://machinelearningmastery.com/feed\n",
      "Found 10 items in memento https://web.archive.org/web/https://machinelearningmastery.com/feed\n",
      "- Title: 3 Ways Vibe Coding and AI-Assisted Development Are 2 Different Things\n",
      "  Description: Vibe coding and AI-assisted development are two trendy terms in today's tech jargon.\n",
      "  Link: https://machinelearningmastery.com/3-ways-vibe-coding-and-ai-assisted-development-are-2-different-things/\n",
      "  PubDate: Mon, 31 Mar 2025 11:00:41 +0000\n",
      "  Author: Iván Palomares Carrascosa\n",
      "\n",
      "\n",
      "Feed: https://transferlab.ai/index.xml\n",
      "Fetching memento: https://web.archive.org/web/https://transferlab.ai/index.xml\n",
      "[ERROR] Attempt 1 failed for https://web.archive.org/web/https://transferlab.ai/index.xml: HTTPSConnectionPool(host='web.archive.org', port=443): Max retries exceeded with url: /web/https://transferlab.ai/index.xml (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000020028E07FB0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n",
      "[INFO] Retrying in 10s...\n",
      "[ERROR] Attempt 2 failed for https://web.archive.org/web/https://transferlab.ai/index.xml: HTTPSConnectionPool(host='web.archive.org', port=443): Max retries exceeded with url: /web/https://transferlab.ai/index.xml (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000020028E06D50>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n",
      "[INFO] Retrying in 100s...\n",
      "Found 30 items in memento https://web.archive.org/web/https://transferlab.ai/index.xml\n",
      "- Title: Model Misspecification in Simulation-Based Inference - Recent Advances and Open Challenges\n",
      "  Description: Model misspecification is a critical challenge in simulation-based inference (SBI), particularly in neural SBI, where methods rely on simulated data to train neural networks. These methods often assume that simulators accurately represent the true data-generating process, but in practice, this assumption is frequently violated. Such discrepancies can result in observed data that are out-of-distribution relative to the simulations, leading to biased posterior distributions and unreliable inferences. This blog reviews recent work on model misspecification in SBI, discussing its definitions, methods for detection and mitigation, and open challenges. The aim is to emphasize the importance of developing robust SBI methods that can accommodate the complexities of real-world applications.\n",
      "  Link: https://transferlab.ai/blog/model-misspecification-in-sbi/\n",
      "  PubDate: Fri, 07 Feb 2025 00:00:00 +0000\n",
      "  Author: Jan Teusen\n",
      "\n",
      "\n",
      "Feed: https://eng.uber.com/tag/machine-learning/feed\n",
      "Fetching memento: https://web.archive.org/web/https://eng.uber.com/tag/machine-learning/feed\n",
      "[ERROR] Attempt 1 failed for https://web.archive.org/web/https://eng.uber.com/tag/machine-learning/feed: HTTPSConnectionPool(host='web.archive.org', port=443): Max retries exceeded with url: /web/https://eng.uber.com/tag/machine-learning/feed (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000020028E04410>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n",
      "[INFO] Retrying in 10s...\n",
      "[ERROR] Attempt 2 failed for https://web.archive.org/web/https://eng.uber.com/tag/machine-learning/feed: HTTPSConnectionPool(host='web.archive.org', port=443): Max retries exceeded with url: /web/https://eng.uber.com/tag/machine-learning/feed (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000020028DFA5D0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n",
      "[INFO] Retrying in 100s...\n",
      "Found 40 items in memento https://web.archive.org/web/https://eng.uber.com/tag/machine-learning/feed\n",
      "- Title: Elastic Distributed Training with XGBoost on Ray\n",
      "  Description: <h3><span style=\"font-weight: 400;\">Introduction</span></h3>\n",
      "<p><span style=\"font-weight: 400;\">Since we </span><a href=\"https://eng.uber.com/productionizing-distributed-xgboost/\"><span style=\"font-weight: 400;\">productionized distributed XGBoost on </span><span style=\"font-weight: 400;\">Apache Spark™</span><span style=\"font-weight: 400;\"> at Uber</span></a><span style=\"font-weight: 400;\"> in 2017, XGBoost has powered a wide spectrum of machine learning (ML) use cases at Uber, spanning from </span><a href=\"https://eng.uber.com/freight-markov/\"><span style=\"font-weight: 400;\">optimizing marketplace dynamic pricing policies for Freight</span></a><span style=\"font-weight: 400;\">, </span><a href=\"https://eng.uber.com/engineering-an-efficient-route/\"><span style=\"font-weight: 400;\">improving times of </span></a>&#8230;</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://eng.uber.com/elastic-xgboost-ray/\">Elastic Distributed Training with XGBoost on Ray</a> appeared first on <a rel=\"nofollow\" href=\"https://eng.uber.com\">Uber Engineering Blog</a>.</p>\n",
      "\n",
      "  Link: https://eng.uber.com/elastic-xgboost-ray/\n",
      "  PubDate: Wed, 07 Jul 2021 16:00:26 +0000\n",
      "  Author: Michael Mui\n",
      "\n",
      "\n",
      "Feed: https://aws.amazon.com/blogs/machine-learning/feed\n",
      "Fetching memento: https://web.archive.org/web/https://aws.amazon.com/blogs/machine-learning/feed\n",
      "Found 20 items in memento https://web.archive.org/web/https://aws.amazon.com/blogs/machine-learning/feed\n",
      "- Title: Reduce ML training costs with Amazon SageMaker HyperPod\n",
      "  Description: In this post, we explore the challenges of large-scale frontier model training, focusing on hardware failures and the benefits of Amazon SageMaker HyperPod - a solution that minimizes disruptions, enhances efficiency, and reduces training costs.\n",
      "  Link: https://aws.amazon.com/blogs/machine-learning/reduce-ml-training-costs-with-amazon-sagemaker-hyperpod/\n",
      "  PubDate: Thu, 10 Apr 2025 20:11:51 +0000\n",
      "  Author: Anoop Saha\n",
      "\n",
      "\n",
      "Feed: http://news.mit.edu/rss/topic/artificial-intelligence2\n",
      "Fetching memento: https://web.archive.org/web/http://news.mit.edu/rss/topic/artificial-intelligence2\n",
      "Found 50 items in memento https://web.archive.org/web/http://news.mit.edu/rss/topic/artificial-intelligence2\n",
      "- Title: Making higher education more accessible to students in Pakistan\n",
      "  Description: EduFi, founded by an MIT alumna, provides low-interest student loans to families in Pakistan so more can attend college.\n",
      "  Link: https://news.mit.edu/2025/edufi-makes-higher-education-more-accessible-students-pakistan-0327\n",
      "  PubDate: Thu, 27 Mar 2025 14:30:00 -0400\n",
      "  Author: Zach Winn | MIT News\n",
      "\n",
      "\n",
      "Feed: http://feeds.feedburner.com/nvidiablog\n",
      "Fetching memento: https://web.archive.org/web/http://feeds.feedburner.com/nvidiablog\n",
      "Found 18 items in memento https://web.archive.org/web/http://feeds.feedburner.com/nvidiablog\n",
      "- Title: From Browsing to Buying: How AI Agents Enhance Online Shopping\n",
      "  Description: Online shopping puts a world of choices at people’s fingertips, making it convenient for them to purchase and receive orders — all from the comfort of their homes.\n",
      "  Link: https://blogs.nvidia.com/blog/ai-agents-online-shopping/\n",
      "  PubDate: Thu, 03 Apr 2025 15:00:51 +0000\n",
      "  Author: Allison Siu\n",
      "\n",
      "\n",
      "Feed: https://openai.com/news/rss.xml\n",
      "Fetching memento: https://web.archive.org/web/https://openai.com/news/rss.xml\n",
      "Found 1018 items in memento https://web.archive.org/web/https://openai.com/news/rss.xml\n",
      "- Title: Sub-processor list—April 2025 update\n",
      "  Description: This page provides information about the Sub-processors OpenAI has engaged to provide processing activities on Customer Data as defined in the OpenAI Data Processing Agreement.\n",
      "  Link: https://openai.com/policies/sub-processor-list-april-2025-update\n",
      "  PubDate: Wed, 30 Apr 2025 00:00:00 GMT\n",
      "  Author: None\n",
      "\n",
      "\n",
      "Feed: http://feeds.feedburner.com/blogspot/gJZg\n",
      "Fetching memento: https://web.archive.org/web/http://feeds.feedburner.com/blogspot/gJZg\n",
      "Found 0 items in memento https://web.archive.org/web/http://feeds.feedburner.com/blogspot/gJZg\n",
      "\n",
      "Feed: http://arxiv.org/rss/cs.LG\n",
      "Fetching memento: https://web.archive.org/web/http://arxiv.org/rss/cs.LG\n",
      "Found 159 items in memento https://web.archive.org/web/http://arxiv.org/rss/cs.LG\n",
      "- Title: PAODING: A High-fidelity Data-free Pruning Toolkit for Debloating Pre-trained Neural Networks\n",
      "  Description: arXiv:2405.00074v1 Announce Type: new \n",
      "Abstract: We present PAODING, a toolkit to debloat pretrained neural network models through the lens of data-free pruning. To preserve the model fidelity, PAODING adopts an iterative process, which dynamically measures the effect of deleting a neuron to identify candidates that have the least impact to the output layer. Our evaluation shows that PAODING can significantly reduce the model size, generalize on different datasets and models, and meanwhile preserve the model fidelity in terms of test accuracy and adversarial robustness. PAODING is publicly available on PyPI via https://pypi.org/project/paoding-dl.\n",
      "  Link: https://arxiv.org/abs/2405.00074\n",
      "  PubDate: None\n",
      "  Author: Mark Huasong Meng, Hao Guan, Liuhuo Wan, Sin Gee Teo, Guangdong Bai, Jin Song Dong\n",
      "\n",
      "\n",
      "Feed: http://arxiv.org/rss/stat.ML\n",
      "Fetching memento: https://web.archive.org/web/http://arxiv.org/rss/stat.ML\n",
      "Found 32 items in memento https://web.archive.org/web/http://arxiv.org/rss/stat.ML\n",
      "- Title: Variational Bayesian Methods for a Tree-Structured Stick-Breaking Process Mixture of Gaussians\n",
      "  Description: arXiv:2405.00385v1 Announce Type: new \n",
      "Abstract: The Bayes coding algorithm for context tree source is a successful example of Bayesian tree estimation in text compression in information theory. This algorithm provides an efficient parametric representation of the posterior tree distribution and exact updating of its parameters. We apply this algorithm to a clustering task in machine learning. More specifically, we apply it to Bayesian estimation of the tree-structured stick-breaking process (TS-SBP) mixture models. For TS-SBP mixture models, only Markov chain Monte Carlo methods have been proposed so far, but any variational Bayesian methods have not been proposed yet. In this paper, we propose a variational Bayesian method that has a subroutine similar to the Bayes coding algorithm for context tree sources. We confirm its behavior by a numerical experiment on a toy example.\n",
      "  Link: https://arxiv.org/abs/2405.00385\n",
      "  PubDate: None\n",
      "  Author: Yuta Nakahara\n",
      "\n",
      "\n",
      "Feed: https://distill.pub/rss.xml\n",
      "Fetching memento: https://web.archive.org/web/https://distill.pub/rss.xml\n",
      "Found 52 items in memento https://web.archive.org/web/https://distill.pub/rss.xml\n",
      "- Title: Understanding Convolutions on Graphs\n",
      "  Description: Understanding the building blocks and design choices of graph neural networks.\n",
      "  Link: https://distill.pub/2021/understanding-gnns\n",
      "  PubDate: Thu, 02 Sep 2021 20:0:0 Z\n",
      "  Author: None\n",
      "\n",
      "\n",
      "Feed: https://bair.berkeley.edu/blog/feed.xml\n",
      "Fetching memento: https://web.archive.org/web/https://bair.berkeley.edu/blog/feed.xml\n",
      "Found 10 items in memento https://web.archive.org/web/https://bair.berkeley.edu/blog/feed.xml\n",
      "- Title: Scaling Up Reinforcement Learning for Traffic Smoothing: A 100-AV Highway Deployment\n",
      "  Description: <!-- twitter -->\n",
      "<meta name=\"twitter:title\" content=\"Scaling Up Reinforcement Learning for Traffic Smoothing: A 100-AV Highway Deployment\" />\n",
      "\n",
      "<meta name=\"twitter:card\" content=\"summary_large_image\" />\n",
      "\n",
      "<meta name=\"twitter:image\" content=\"https://bair.berkeley.edu/static/blog/rl_av_smoothing/megavandertest.png\" />\n",
      "\n",
      "<meta name=\"keywords\" content=\"reinforcement learning, RL, autonomous vehicles, AV, traffic\" />\n",
      "\n",
      "<meta name=\"description\" content=\"The BAIR Blog\" />\n",
      "\n",
      "<meta name=\"author\" content=\"Nathan Lichtlé, Kathy Jang, Eugene Vinitsky, Adit Shah, Jonathan W. Lee, Alexandre M. Bayen\" />\n",
      "\n",
      "<title>Training Diffusion Models with Reinforcement Learning</title>\n",
      "\n",
      "<video autoplay=\"\" muted=\"\" playsinline=\"\" disableRemotePlayback=\"\" loop=\"\" style=\"width: 100%; margin: 0; padding: 0; outline: none; border: none; background: transparent; display: block; border-radius: 5px\" cover=\"https://bair.berkeley.edu/static/blog/rl_av_smoothing/megavandertest.png\">\n",
      "    <source src=\"https://bair.berkeley.edu/static/blog/rl_av_smoothing/megavandertest.mp4\" type=\"video/mp4\" />\n",
      "</video>\n",
      "\n",
      "<p style=\"margin-top: 20px;\">\n",
      "    <b>We deployed 100 reinforcement learning (RL)-controlled cars into rush-hour highway traffic to smooth congestion and reduce fuel consumption for everyone.</b> Our goal is to tackle <a href=\"https://www.youtube.com/watch?v=TNokBgtSUvQ\" target=\"_blank\">\"stop-and-go\" waves</a>, those frustrating slowdowns and speedups that usually have no clear cause but lead to congestion and significant energy waste. To train efficient flow-smoothing controllers, we built fast, data-driven simulations that RL agents interact with, learning to maximize energy efficiency while maintaining throughput and operating safely around human drivers.\n",
      "</p>\n",
      "<p>    \n",
      "    Overall, a small proportion of well-controlled autonomous vehicles (AVs) is enough to significantly improve traffic flow and fuel efficiency for all drivers on the road. Moreover, the trained controllers are designed to be deployable on most modern vehicles, operating in a decentralized manner and relying on standard radar sensors. In our <a href=\"https://ieeexplore.ieee.org/document/10858625\" target=\"_blank\">latest paper</a>, we explore the challenges of deploying RL controllers on a large-scale, from simulation to the field, during this 100-car experiment.\n",
      "</p>\n",
      "\n",
      "<!--more-->\n",
      "\n",
      "<h2 id=\"the-challenges-of-phantom-jams\">The challenges of phantom jams</h2>\n",
      "\n",
      "<p style=\"text-align: center; margin-top: 50px;\">\n",
      "    <img src=\"https://bair.berkeley.edu/static/blog/rl_av_smoothing/highway_wave.gif\" width=\"80%\" style=\"width: 80%; border-radius: 5px;\" />\n",
      "    <br />\n",
      "    <i>A stop-and-go wave moving backwards through highway traffic.</i>\n",
      "</p>\n",
      "\n",
      "<p>If you drive, you’ve surely experienced the frustration of stop-and-go waves, those seemingly inexplicable traffic slowdowns that appear out of nowhere and then suddenly clear up. These waves are often caused by small fluctuations in our driving behavior that get amplified through the flow of traffic. We naturally adjust our speed based on the vehicle in front of us. If the gap opens, we speed up to keep up. If they brake, we also slow down. But due to our nonzero reaction time, we might brake just a bit harder than the vehicle in front. The next driver behind us does the same, and this keeps amplifying. Over time, what started as an insignificant slowdown turns into a full stop further back in traffic. These waves move backward through the traffic stream, leading to significant drops in energy efficiency due to frequent accelerations, accompanied by increased CO<sub>2</sub> emissions and accident risk.</p>\n",
      "\n",
      "<p>And this isn’t an isolated phenomenon! These waves are ubiquitous on busy roads when the traffic density exceeds a critical threshold. So how can we address this problem? Traditional approaches like ramp metering and variable speed limits attempt to manage traffic flow, but they often require costly infrastructure and centralized coordination. A more scalable approach is to use AVs, which can dynamically adjust their driving behavior in real-time. However, simply inserting AVs among human drivers isn’t enough: they must also drive in a smarter way that makes traffic better for everyone, which is where RL comes in.</p>\n",
      "\n",
      "<p style=\"text-align: justify; margin-top: 50px;\">\n",
      "    <img src=\"https://bair.berkeley.edu/static/blog/rl_av_smoothing/fundamental_diagram.png\" width=\"80%\" style=\"display: block; margin: auto\" />\n",
      "    <br />\n",
      "    <i><b>Fundamental diagram of traffic flow.</b> The number of cars on the road (density) affects how much traffic is moving forward (flow). At low density, adding more cars increases flow because more vehicles can pass through. But beyond a critical threshold, cars start blocking each other, leading to congestion, where adding more cars actually slows down overall movement.</i>\n",
      "</p>\n",
      "\n",
      "<h2 id=\"reinforcement-learning-for-wave-smoothing-avs\">Reinforcement learning for wave-smoothing AVs</h2>\n",
      "\n",
      "<p>RL is a powerful control approach where an agent learns to maximize a reward signal through interactions with an environment. The agent collects experience through trial and error, learns from its mistakes, and improves over time. In our case, the environment is a mixed-autonomy traffic scenario, where AVs learn driving strategies to dampen stop-and-go waves and reduce fuel consumption for both themselves and nearby human-driven vehicles.</p>\n",
      "\n",
      "<p>Training these RL agents requires fast simulations with realistic traffic dynamics that can replicate highway stop-and-go behavior. To achieve this, we leveraged experimental data collected on Interstate 24 (I-24) near Nashville, Tennessee, and used it to build simulations where vehicles replay highway trajectories, creating unstable traffic that AVs driving behind them learn to smooth out.</p>\n",
      "\n",
      "<p style=\"text-align: center; margin-top: 50px;\">\n",
      "    <video autoplay=\"\" muted=\"\" playsinline=\"\" disableRemotePlayback=\"\" loop=\"\" style=\"width: 100%; margin: 0; padding: 0; outline: none; border: none; background: transparent; display: block; border-radius: 5px\" cover=\"https://bair.berkeley.edu/static/blog/rl_av_smoothing/simulation.png\">\n",
      "        <source src=\"https://bair.berkeley.edu/static/blog/rl_av_smoothing/simulation.mp4\" type=\"video/mp4\" />\n",
      "    </video>\n",
      "    <br />\n",
      "    <i>Simulation replaying a highway trajectory that exhibits several stop-and-go waves.</i>\n",
      "</p>\n",
      "\n",
      "<p>We designed the AVs with deployment in mind, ensuring that they can operate using only basic sensor information about themselves and the vehicle in front. The observations consist of the AV’s speed, the speed of the leading vehicle, and the space gap between them. Given these inputs, the RL agent then prescribes either an instantaneous acceleration or a desired speed for the AV. The key advantage of using only these local measurements is that the RL controllers can be deployed on most modern vehicles in a decentralized way, without requiring additional infrastructure.</p>\n",
      "\n",
      "<h3 id=\"reward-design\">Reward design</h3>\n",
      "\n",
      "<p>The most challenging part is designing a reward function that, when maximized, aligns with the different objectives that we desire the AVs to achieve:</p>\n",
      "\n",
      "<ul>\n",
      "  <li><strong>Wave smoothing:</strong> Reduce stop-and-go oscillations.</li>\n",
      "  <li><strong>Energy efficiency:</strong> Lower fuel consumption for all vehicles, not just AVs.</li>\n",
      "  <li><strong>Safety:</strong> Ensure reasonable following distances and avoid abrupt braking.</li>\n",
      "  <li><strong>Driving comfort:</strong> Avoid aggressive accelerations and decelerations.</li>\n",
      "  <li><strong>Adherence to human driving norms:</strong> Ensure a “normal” driving behavior that doesn’t make surrounding drivers uncomfortable.</li>\n",
      "</ul>\n",
      "\n",
      "<p>Balancing these objectives together is difficult, as suitable coefficients for each term must be found. For instance, if minimizing fuel consumption dominates the reward, RL AVs learn to come to a stop in the middle of the highway because that is energy optimal. To prevent this, we introduced dynamic minimum and maximum gap thresholds to ensure safe and reasonable behavior while optimizing fuel efficiency. We also penalized the fuel consumption of human-driven vehicles behind the AV to discourage it from learning a selfish behavior that optimizes energy savings for the AV at the expense of surrounding traffic. Overall, we aim to strike a balance between energy savings and having a reasonable and safe driving behavior.</p>\n",
      "\n",
      "<h3 id=\"simulation-results\">Simulation results</h3>\n",
      "\n",
      "<p style=\"text-align: center; margin-top: 0;\">\n",
      "    <img src=\"https://bair.berkeley.edu/static/blog/rl_av_smoothing/gap_thresholds.png\" width=\"80%\" />\n",
      "    <br />\n",
      "    <i>Illustration of the dynamic minimum and maximum gap thresholds, within which the AV can operate freely to smooth traffic as efficiently as possible.</i>\n",
      "</p>\n",
      "\n",
      "<p>The typical behavior learned by the AVs is to maintain slightly larger gaps than human drivers, allowing them to absorb upcoming, possibly abrupt, traffic slowdowns more effectively. In simulation, this approach resulted in significant fuel savings of up to 20% across all road users in the most congested scenarios, with fewer than 5% of AVs on the road. And these AVs don’t have to be special vehicles! They can simply be standard consumer cars equipped with a smart adaptive cruise control (ACC), which is what we tested at scale.</p>\n",
      "\n",
      "<p style=\"text-align: justify; margin-top: 50px;\">\n",
      "    <img src=\"https://bair.berkeley.edu/static/blog/rl_av_smoothing/wave_smoothing.png\" width=\"100%\" style=\"display: block; margin: auto;\" />\n",
      "    <i>\n",
      "    <b>Smoothing behavior of RL AVs.</b> Red: a human trajectory from the dataset. Blue: successive AVs in the platoon, where AV 1 is the closest behind the human trajectory. There is typically between 20 and 25 human vehicles between AVs. Each AV doesn’t slow down as much or accelerate as fast as its leader, leading to decreasing wave amplitude over time and thus energy savings. \n",
      "    </i>\n",
      "</p>\n",
      "\n",
      "<h2 id=\"100-av-field-test-deploying-rl-at-scale\">100 AV field test: deploying RL at scale</h2>\n",
      "\n",
      "<div style=\"display: flex; justify-content: center; width: 100%; margin-top: 30px;\">\n",
      "    <img src=\"https://bair.berkeley.edu/static/blog/rl_av_smoothing/parking_lot.png\" style=\"height: 300px; object-fit: cover; width: 50%; border-top-left-radius: 5px; border-bottom-left-radius: 5px;\" />\n",
      "    <img src=\"https://bair.berkeley.edu/static/blog/rl_av_smoothing/parking_lot_drone.png\" style=\"height: 300px; object-fit: cover; width: 50%; border-top-right-radius: 5px; border-bottom-right-radius: 5px;\" />\n",
      "</div>\n",
      "<p style=\"text-align: center; margin-top: 10px;\">\n",
      "    <i style=\"font-size: 0.9rem;\">Our 100 cars parked at our operational center during the experiment week.</i>\n",
      "</p>\n",
      "\n",
      "<p>Given the promising simulation results, the natural next step was to bridge the gap from simulation to the highway. We took the trained RL controllers and deployed them on 100 vehicles on the I-24 during peak traffic hours over several days. This large-scale experiment, which we called the MegaVanderTest, is the largest mixed-autonomy traffic-smoothing experiment ever conducted.</p>\n",
      "\n",
      "<p>Before deploying RL controllers in the field, we trained and evaluated them extensively in simulation and validated them on the hardware. Overall, the steps towards deployment involved:</p>\n",
      "\n",
      "<ul>\n",
      "  <li><strong>Training in data-driven simulations:</strong> We used highway traffic data from I-24 to create a training environment with realistic wave dynamics, then validate the trained agent’s performance and robustness in a variety of new traffic scenarios.</li>\n",
      "  <li><strong>Deployment on hardware:</strong> After being validated in robotics software, the trained controller is uploaded onto the car and is able to control the set speed of the vehicle. We operate through the vehicle’s on-board cruise control, which acts as a lower-level safety controller.</li>\n",
      "  <li><strong>Modular control framework:</strong> One key challenge during the test was not having access to the leading vehicle information sensors. To overcome this, the RL controller was integrated into a hierarchical system, the MegaController, which combines a speed planner guide that accounts for downstream traffic conditions, with the RL controller as the final decision maker.</li>\n",
      "  <li><strong>Validation on hardware:</strong> The RL agents were designed to operate in an environment where most vehicles were human-driven, requiring robust policies that adapt to unpredictable behavior. We verify this by driving the RL-controlled vehicles on the road under careful human supervision, making changes to the control based on feedback.</li>\n",
      "</ul>\n",
      "\n",
      "<div style=\"display: flex; justify-content: space-around; width: 100%; margin: 30px 0;\">\n",
      "    <div style=\"display: flex; flex-direction: column; align-items: center; width: 48%;\">\n",
      "        <img src=\"https://bair.berkeley.edu/static/blog/rl_av_smoothing/raspberry_pi.png\" style=\"height: 200px; object-fit: cover; width: 100%; border-radius: 5px;\" />\n",
      "        <i style=\"font-size: 0.9rem; display: block; text-align: center; margin-top: 5px;\">Each of the 100 cars is connected to a Raspberry Pi, on which the RL controller (a small neural network) is deployed.</i>\n",
      "    </div>\n",
      "    <div style=\"display: flex; flex-direction: column; align-items: center; width: 48%;\">\n",
      "        <img src=\"https://bair.berkeley.edu/static/blog/rl_av_smoothing/acc.png\" style=\"height: 200px; object-fit: cover; width: 100%; border-radius: 5px;\" />\n",
      "        <i style=\"font-size: 0.9rem; display: block; text-align: center; margin-top: 5px;\">The RL controller directly controls the onboard adaptive cruise control (ACC) system, setting its speed and desired following distance.</i>\n",
      "    </div>\n",
      "</div>\n",
      "\n",
      "<p>Once validated, the RL controllers were deployed on 100 cars and driven on I-24 during morning rush hour. Surrounding traffic was unaware of the experiment, ensuring unbiased driver behavior. Data was collected during the experiment from dozens of overhead cameras placed along the highway, which led to the extraction of millions of individual vehicle trajectories through a computer vision pipeline. Metrics computed on these trajectories indicate a trend of reduced fuel consumption around AVs, as expected from simulation results and previous smaller validation deployments. For instance, we can observe that the closer people are driving behind our AVs, the less fuel they appear to consume on average (which is calculated using a calibrated energy model):</p>\n",
      "\n",
      "<p style=\"text-align: center; margin-top: 0;\">\n",
      "    <img src=\"https://bair.berkeley.edu/static/blog/rl_av_smoothing/fuel_data.png\" width=\"80%\" />\n",
      "    <br />\n",
      "    <i>Average fuel consumption as a function of distance behind the nearest engaged RL-controlled AV in the downstream traffic. As human drivers get further away behind AVs, their average fuel consumption increases.</i>\n",
      "</p>\n",
      "\n",
      "<p>Another way to measure the impact is to measure the variance of the speeds and accelerations: the lower the variance, the less amplitude the waves should have, which is what we observe from the field test data. Overall, although getting precise measurements from a large amount of camera video data is complicated, we observe a trend of 15 to 20% of energy savings around our controlled cars.</p>\n",
      "\n",
      "<p style=\"text-align: center; margin-top: 0;\">\n",
      "    <img src=\"https://bair.berkeley.edu/static/blog/rl_av_smoothing/data_scatter.png\" width=\"50%\" />\n",
      "    <br />\n",
      "    <i>Data points from all vehicles on the highway over a single day of the experiment, plotted in speed-acceleration space. The cluster to the left of the red line represents congestion, while the one on the right corresponds to free flow. We observe that the congestion cluster is smaller when AVs are present, as measured by computing the area of a soft convex envelope or by fitting a Gaussian kernel.</i>\n",
      "</p>\n",
      "\n",
      "<h2 id=\"final-thoughts\">Final thoughts</h2>\n",
      "\n",
      "<p>The 100-car field operational test was decentralized, with no explicit cooperation or communication between AVs, reflective of current autonomy deployment, and bringing us one step closer to smoother, more energy-efficient highways. Yet, there is still vast potential for improvement. Scaling up simulations to be faster and more accurate with better human-driving models is crucial for bridging the simulation-to-reality gap. Equipping AVs with additional traffic data, whether through advanced sensors or centralized planning, could further improve the performance of the controllers. For instance, while multi-agent RL is promising for improving cooperative control strategies, it remains an open question how enabling explicit communication between AVs over 5G networks could further improve stability and further mitigate stop-and-go waves. Crucially, our controllers integrate seamlessly with existing adaptive cruise control (ACC) systems, making field deployment feasible at scale. The more vehicles equipped with smart traffic-smoothing control, the fewer waves we’ll see on our roads, meaning less pollution and fuel savings for everyone!</p>\n",
      "\n",
      "<hr />\n",
      "\n",
      "<p><i>Many contributors took part in making the MegaVanderTest happen!  The full list is available on the <a href=\"https://circles-consortium.github.io/\" target=\"_blank\">CIRCLES project</a> page, along with more details about the project.</i></p>\n",
      "\n",
      "<p><i><b>Read more: <a href=\"https://ieeexplore.ieee.org/document/10858625\" target=\"_blank\">[paper]</a></b></i></p>\n",
      "\n",
      "  Link: http://bair.berkeley.edu/blog/2025/03/25/rl-av-smoothing/\n",
      "  PubDate: Tue, 25 Mar 2025 02:00:00 -0700\n",
      "  Author: None\n",
      "\n",
      "\n",
      "Feed: https://becominghuman.ai/feed\n",
      "Fetching memento: https://web.archive.org/web/https://becominghuman.ai/feed\n",
      "Found 10 items in memento https://web.archive.org/web/https://becominghuman.ai/feed\n",
      "- Title: AGI in 2025 |Do you think what matters today will still matter in the coming months? TL;DR: No!\n",
      "  Description: None\n",
      "  Link: https://becominghuman.ai/agi-in-2025-do-you-think-what-matters-today-will-still-matter-in-the-coming-months-tl-dr-no-5f22fc93c221?source=rss----5e5bef33608a---4\n",
      "  PubDate: Mon, 03 Feb 2025 16:58:24 GMT\n",
      "  Author: M. Pajuhaan\n",
      "\n",
      "\n",
      "Feed: https://www.microsoft.com/en-us/research/feed\n",
      "Fetching memento: https://web.archive.org/web/https://www.microsoft.com/en-us/research/feed\n",
      "Found 10 items in memento https://web.archive.org/web/https://www.microsoft.com/en-us/research/feed\n",
      "- Title: Research Focus: Week of March 24, 2025\n",
      "  Description: <p>In this issue, we examine a new conversation segmentation method that delivers more coherent and personalized agent conversation, and we review efforts to improve MLLMs’ understanding of geologic maps. Check out the latest research and other updates.</p>\n",
      "<p>The post <a href=\"https://www.microsoft.com/en-us/research/blog/research-focus-week-of-march-24-2025/\">Research Focus: Week of March 24, 2025</a> appeared first on <a href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "\n",
      "  Link: https://www.microsoft.com/en-us/research/blog/research-focus-week-of-march-24-2025/\n",
      "  PubDate: Wed, 26 Mar 2025 16:00:00 +0000\n",
      "  Author: Microsoft Research Team\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the first momento content for each feed\n",
    "for feed, mementos in feed_mementos.items():\n",
    "    print(f\"\\nFeed: {feed}\")\n",
    "    memento = mementos[0]\n",
    "    print(f\"Fetching memento: {memento}\")\n",
    "    content = fetch_xml_from_memento(memento)\n",
    "    if content is None:\n",
    "        print(f\"[ERROR] Failed to fetch memento {memento}\")\n",
    "        continue\n",
    "    memento_data = extract_memento_data(feed, content)\n",
    "    print(f\"Found {len(memento_data)} items in memento {memento}\")\n",
    "    for item in memento_data[:1]:\n",
    "        print(f\"- Title: {item['title']}\")\n",
    "        print(f\"  Description: {item['description']}\")\n",
    "        print(f\"  Link: {item['link']}\")\n",
    "        print(f\"  PubDate: {item['pubDate']}\")\n",
    "        print(f\"  Author: {item['author']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_datas = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f12185c2b94c40a5b6f27278f5c429c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing mementos for https://machinelearningmastery.com/feed:   0%|          | 0/20 [00:00<?, ?memento/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d252ca60fde41cfa0950b46704abff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing mementos for https://transferlab.ai/index.xml:   0%|          | 0/13 [00:00<?, ?memento/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec23b80afb884d0190e600509f9b8656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing mementos for https://eng.uber.com/tag/machine-learning/feed:   0%|          | 0/15 [00:00<?, ?memen…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b794248e6bcf498eb832430201edd547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing mementos for https://aws.amazon.com/blogs/machine-learning/feed:   0%|          | 0/20 [00:00<?, ?m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ee70e85ad443c9bf9903b22ef987e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing mementos for http://news.mit.edu/rss/topic/artificial-intelligence2:   0%|          | 0/20 [00:00<?…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7c744e74b046ea99f57d93db7057c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing mementos for http://feeds.feedburner.com/nvidiablog:   0%|          | 0/20 [00:00<?, ?memento/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d351329461ce4bd5883ec3a60bdf8a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing mementos for https://openai.com/news/rss.xml:   0%|          | 0/9 [00:00<?, ?memento/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb348cc5565d4007a5e0a9f6dc3a5dd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing mementos for http://feeds.feedburner.com/blogspot/gJZg:   0%|          | 0/20 [00:00<?, ?memento/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Attempt 1 failed for https://web.archive.org/web/20080724154728/http://feeds.feedburner.com/blogspot/gJZg: ('Received response with content-encoding: gzip, but failed to decode it.', error('Error -3 while decompressing data: incorrect header check'))\n",
      "[INFO] Retrying in 10s...\n",
      "[ERROR] Attempt 2 failed for https://web.archive.org/web/20080724154728/http://feeds.feedburner.com/blogspot/gJZg: ('Received response with content-encoding: gzip, but failed to decode it.', error('Error -3 while decompressing data: incorrect header check'))\n",
      "[INFO] Retrying in 100s...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31merror\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tan Vo\\Documents\\GitHub\\historical-news\\backend\\venv\\Lib\\site-packages\\urllib3\\response.py:485\u001b[39m, in \u001b[36mBaseHTTPResponse._decode\u001b[39m\u001b[34m(self, data, decode_content, flush_decoder)\u001b[39m\n\u001b[32m    484\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decoder:\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m     \u001b[38;5;28mself\u001b[39m._has_decoded_content = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tan Vo\\Documents\\GitHub\\historical-news\\backend\\venv\\Lib\\site-packages\\urllib3\\response.py:128\u001b[39m, in \u001b[36mGzipDecoder.decompress\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     ret += \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m zlib.error:\n",
      "\u001b[31merror\u001b[39m: Error -3 while decompressing data: incorrect header check",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mDecodeError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tan Vo\\Documents\\GitHub\\historical-news\\backend\\venv\\Lib\\site-packages\\requests\\models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    819\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tan Vo\\Documents\\GitHub\\historical-news\\backend\\venv\\Lib\\site-packages\\urllib3\\response.py:1063\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1062\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.supports_chunked_reads():\n\u001b[32m-> \u001b[39m\u001b[32m1063\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.read_chunked(amt, decode_content=decode_content)\n\u001b[32m   1064\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tan Vo\\Documents\\GitHub\\historical-news\\backend\\venv\\Lib\\site-packages\\urllib3\\response.py:1223\u001b[39m, in \u001b[36mHTTPResponse.read_chunked\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1222\u001b[39m chunk = \u001b[38;5;28mself\u001b[39m._handle_chunk(amt)\n\u001b[32m-> \u001b[39m\u001b[32m1223\u001b[39m decoded = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflush_decoder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m   1225\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m decoded:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tan Vo\\Documents\\GitHub\\historical-news\\backend\\venv\\Lib\\site-packages\\urllib3\\response.py:489\u001b[39m, in \u001b[36mBaseHTTPResponse._decode\u001b[39m\u001b[34m(self, data, decode_content, flush_decoder)\u001b[39m\n\u001b[32m    488\u001b[39m     content_encoding = \u001b[38;5;28mself\u001b[39m.headers.get(\u001b[33m\"\u001b[39m\u001b[33mcontent-encoding\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).lower()\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DecodeError(\n\u001b[32m    490\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReceived response with content-encoding: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m, but \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    491\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfailed to decode it.\u001b[39m\u001b[33m\"\u001b[39m % content_encoding,\n\u001b[32m    492\u001b[39m         e,\n\u001b[32m    493\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m flush_decoder:\n",
      "\u001b[31mDecodeError\u001b[39m: ('Received response with content-encoding: gzip, but failed to decode it.', error('Error -3 while decompressing data: incorrect header check'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mContentDecodingError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mfetch_xml_from_memento\u001b[39m\u001b[34m(memento_url, retries, backoff, timeout)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     resp = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemento_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     resp.raise_for_status()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tan Vo\\Documents\\GitHub\\historical-news\\backend\\venv\\Lib\\site-packages\\requests\\api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m:rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tan Vo\\Documents\\GitHub\\historical-news\\backend\\venv\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tan Vo\\Documents\\GitHub\\historical-news\\backend\\venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tan Vo\\Documents\\GitHub\\historical-news\\backend\\venv\\Lib\\site-packages\\requests\\sessions.py:746\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m     \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tan Vo\\Documents\\GitHub\\historical-news\\backend\\venv\\Lib\\site-packages\\requests\\models.py:902\u001b[39m, in \u001b[36mResponse.content\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    901\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m902\u001b[39m         \u001b[38;5;28mself\u001b[39m._content = \u001b[33;43mb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    904\u001b[39m \u001b[38;5;28mself\u001b[39m._content_consumed = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Tan Vo\\Documents\\GitHub\\historical-news\\backend\\venv\\Lib\\site-packages\\requests\\models.py:824\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ContentDecodingError(e)\n\u001b[32m    825\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ReadTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mContentDecodingError\u001b[39m: ('Received response with content-encoding: gzip, but failed to decode it.', error('Error -3 while decompressing data: incorrect header check'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m memento \u001b[38;5;129;01min\u001b[39;00m feed_datas:\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m content = \u001b[43mfetch_xml_from_memento\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemento\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m content \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mfetch_xml_from_memento\u001b[39m\u001b[34m(memento_url, retries, backoff, timeout)\u001b[39m\n\u001b[32m     23\u001b[39m     sleep_time = backoff**(attempt + \u001b[32m1\u001b[39m)\n\u001b[32m     24\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[INFO] Retrying in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msleep_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msleep_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     27\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[ERROR] All attempts failed for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmemento_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for feed, mementos in feed_mementos.items():\n",
    "    for memento in tqdm(mementos, desc=f\"Processing mementos for {feed}\", unit=\"memento\"):\n",
    "        if memento in feed_datas:\n",
    "            continue\n",
    "        content = fetch_xml_from_memento(memento)\n",
    "        if content is None:\n",
    "            continue\n",
    "        data = extract_memento_data(feed, content)\n",
    "        if data is None:\n",
    "            continue\n",
    "        feed_datas[memento] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Memento: https://web.archive.org/web/https://machinelearningmastery.com/feed\n",
      "- Source: https://machinelearningmastery.com/feed\n",
      "  Channel Title: MachineLearningMastery.com\n",
      "  Channel Description: Making developers awesome at machine learning\n",
      "- Title: 3 Ways Vibe Coding and AI-Assisted Development Are 2 Different Things\n",
      "  Description: Vibe coding and AI-assisted development are two trendy terms in today's tech jargon.\n",
      "  Link: https://machinelearningmastery.com/3-ways-vibe-coding-and-ai-assisted-development-are-2-different-things/\n",
      "  PubDate: Mon, 31 Mar 2025 11:00:41 +0000\n",
      "  Author: Iván Palomares Carrascosa\n",
      "\n",
      "Memento: https://web.archive.org/web/20150912134829/http://machinelearningmastery.com/feed/\n",
      "- Source: https://machinelearningmastery.com/feed\n",
      "  Channel Title: Machine Learning Mastery\n",
      "  Channel Description: Making programmers awesome at machine learning\n",
      "- Title: How Do I Get Started In Machine Learning? (the short version)\n",
      "  Description: <p>I get daily emails asking the question: How do I get started in machine learning? This post provides my quick answer. Here is my long answer. So here is how to get started in machine learning, the quick version. Practice Creating Predictive Models You&#8217;re interested in machine learning but you&#8217;re not sure of the specific outcome [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"http://machinelearningmastery.com/how-do-i-get-started-in-machine-learning/\">How Do I Get Started In Machine Learning? (the short version)</a> appeared first on <a rel=\"nofollow\" href=\"http://machinelearningmastery.com\">Machine Learning Mastery</a>.</p>\n",
      "\n",
      "  Link: http://machinelearningmastery.com/how-do-i-get-started-in-machine-learning/\n",
      "  PubDate: Wed, 09 Sep 2015 19:00:23 +0000\n",
      "  Author: Jason Brownlee\n",
      "\n",
      "Memento: https://web.archive.org/web/20160315034406/http://machinelearningmastery.com/feed/\n",
      "- Source: https://machinelearningmastery.com/feed\n",
      "  Channel Title: Machine Learning Mastery\n",
      "  Channel Description: Making programmers awesome at machine learning\n",
      "- Title: Parametric and Nonparametric Machine Learning Algorithms\n",
      "  Description: <p>What is a parametric machine learning algorithm and how is it different from a nonparametric machine learning algorithm? In this post you will discover the difference between parametric and nonparametric machine learning algorithms. Let&#8217;s get started. Learning a Function Machine learning can be summarized as learning a function (f) that maps input variables (X) to output [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"http://machinelearningmastery.com/parametric-and-nonparametric-machine-learning-algorithms/\">Parametric and Nonparametric Machine Learning Algorithms</a> appeared first on <a rel=\"nofollow\" href=\"http://machinelearningmastery.com\">Machine Learning Mastery</a>.</p>\n",
      "\n",
      "  Link: http://machinelearningmastery.com/parametric-and-nonparametric-machine-learning-algorithms/\n",
      "  PubDate: Sun, 13 Mar 2016 18:00:14 +0000\n",
      "  Author: Jason Brownlee\n",
      "\n",
      "Memento: https://web.archive.org/web/20170606124339/http://machinelearningmastery.com/feed/\n",
      "- Source: https://machinelearningmastery.com/feed\n",
      "  Channel Title: Machine Learning Mastery\n",
      "  Channel Description: Making developers awesome at machine learning\n",
      "- Title: How to Calculate Bootstrap Confidence Intervals For Machine Learning Results in Python\n",
      "  Description: <p>It is important to both present the expected skill of a machine learning model a well as confidence intervals for that model skill. Confidence intervals provide a range of model skills and a likelihood that the model skill will fall between the ranges when making predictions on new data. For example, a 95% likelihood of [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"http://machinelearningmastery.com/calculate-bootstrap-confidence-intervals-machine-learning-results-python/\">How to Calculate Bootstrap Confidence Intervals For Machine Learning Results in Python</a> appeared first on <a rel=\"nofollow\" href=\"http://machinelearningmastery.com\">Machine Learning Mastery</a>.</p>\n",
      "\n",
      "  Link: http://machinelearningmastery.com/calculate-bootstrap-confidence-intervals-machine-learning-results-python/\n",
      "  PubDate: Sun, 04 Jun 2017 19:00:39 +0000\n",
      "  Author: Jason Brownlee\n",
      "\n",
      "Memento: https://web.archive.org/web/20171021225645/http://machinelearningmastery.com/feed/\n",
      "- Source: https://machinelearningmastery.com/feed\n",
      "  Channel Title: Machine Learning Mastery\n",
      "  Channel Description: Making developers awesome at machine learning\n",
      "- Title: How to Develop a Deep Learning Bag-of-Words Model for Predicting Movie Review Sentiment\n",
      "  Description: <p>Movie reviews can be classified as either favorable or not. The evaluation of movie review text is a classification problem often called sentiment analysis. A popular technique for developing sentiment analysis models is to use a bag-of-words model that transforms documents into vectors where each word in the document is assigned a score. In this [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://machinelearningmastery.com/deep-learning-bag-of-words-model-sentiment-analysis/\">How to Develop a Deep Learning Bag-of-Words Model for Predicting Movie Review Sentiment</a> appeared first on <a rel=\"nofollow\" href=\"https://machinelearningmastery.com\">Machine Learning Mastery</a>.</p>\n",
      "\n",
      "  Link: https://machinelearningmastery.com/deep-learning-bag-of-words-model-sentiment-analysis/\n",
      "  PubDate: Thu, 19 Oct 2017 18:00:01 +0000\n",
      "  Author: Jason Brownlee\n",
      "\n",
      "Memento: https://web.archive.org/web/20190607215113/http://machinelearningmastery.com/feed/\n",
      "- Source: https://machinelearningmastery.com/feed\n",
      "  Channel Title: Machine Learning Mastery\n",
      "  Channel Description: Making developers awesome at machine learning\n",
      "- Title: How to Develop a Face Recognition System Using FaceNet in Keras\n",
      "  Description: <p>Face recognition is a computer vision task of identifying and verifying a person based on a photograph of their face. FaceNet is a face recognition system developed in 2015 by researchers at Google that achieved then state-of-the-art results on a range of face recognition benchmark datasets. The FaceNet system can be used broadly thanks to [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://machinelearningmastery.com/how-to-develop-a-face-recognition-system-using-facenet-in-keras-and-an-svm-classifier/\">How to Develop a Face Recognition System Using FaceNet in Keras</a> appeared first on <a rel=\"nofollow\" href=\"https://machinelearningmastery.com\">Machine Learning Mastery</a>.</p>\n",
      "\n",
      "  Link: https://machinelearningmastery.com/how-to-develop-a-face-recognition-system-using-facenet-in-keras-and-an-svm-classifier/\n",
      "  PubDate: Thu, 06 Jun 2019 19:00:22 +0000\n",
      "  Author: Jason Brownlee\n",
      "\n",
      "Memento: https://web.archive.org/web/20190616171613/https://machinelearningmastery.com/feed/\n",
      "- Source: https://machinelearningmastery.com/feed\n",
      "  Channel Title: Machine Learning Mastery\n",
      "  Channel Description: Making developers awesome at machine learning\n",
      "- Title: 18 Impressive Applications of Generative Adversarial Networks (GANs)\n",
      "  Description: <p>A Generative Adversarial Network, or GAN, is a type of neural network architecture for generative modeling. Generative modeling involves using a model to generate new examples that plausibly come from an existing distribution of samples, such as generating new photographs that are similar but specifically different from a dataset of existing photographs. A GAN is [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://machinelearningmastery.com/impressive-applications-of-generative-adversarial-networks/\">18 Impressive Applications of Generative Adversarial Networks (GANs)</a> appeared first on <a rel=\"nofollow\" href=\"https://machinelearningmastery.com\">Machine Learning Mastery</a>.</p>\n",
      "\n",
      "  Link: https://machinelearningmastery.com/impressive-applications-of-generative-adversarial-networks/\n",
      "  PubDate: Thu, 13 Jun 2019 19:00:52 +0000\n",
      "  Author: Jason Brownlee\n",
      "\n",
      "Memento: https://web.archive.org/web/20190619234501/https://machinelearningmastery.com/feed/\n",
      "- Source: https://machinelearningmastery.com/feed\n",
      "  Channel Title: Machine Learning Mastery\n",
      "  Channel Description: Making developers awesome at machine learning\n",
      "- Title: Tips for Training Stable Generative Adversarial Networks\n",
      "  Description: <p>The Empirical Heuristics, Tips, and Tricks That You Need to Know to Train Stable Generative Adversarial Networks (GANs). Generative Adversarial Networks, or GANs for short, are an approach to generative modeling using deep learning methods such as deep convolutional neural networks. Although the results generated by GANs can be remarkable, it can be challenging to [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://machinelearningmastery.com/how-to-train-stable-generative-adversarial-networks/\">Tips for Training Stable Generative Adversarial Networks</a> appeared first on <a rel=\"nofollow\" href=\"https://machinelearningmastery.com\">Machine Learning Mastery</a>.</p>\n",
      "\n",
      "  Link: https://machinelearningmastery.com/how-to-train-stable-generative-adversarial-networks/\n",
      "  PubDate: Tue, 18 Jun 2019 19:00:44 +0000\n",
      "  Author: Jason Brownlee\n",
      "\n",
      "Memento: https://web.archive.org/web/20200924190109/https://machinelearningmastery.com/feed/\n",
      "- Source: https://machinelearningmastery.com/feed\n",
      "  Channel Title: Machine Learning Mastery\n",
      "  Channel Description: Making developers awesome at machine learning\n",
      "- Title: How to Hill Climb the Test Set for Machine Learning\n",
      "  Description: <p>Hill climbing the test set is an approach to achieving good or perfect predictions on a machine learning competition without touching the training set or even developing a predictive model. As an approach to machine learning competitions, it is rightfully frowned upon, and most competition platforms impose limitations to prevent it, which is important. Nevertheless, [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://machinelearningmastery.com/hill-climb-the-test-set-for-machine-learning/\">How to Hill Climb the Test Set for Machine Learning</a> appeared first on <a rel=\"nofollow\" href=\"https://machinelearningmastery.com\">Machine Learning Mastery</a>.</p>\n",
      "\n",
      "  Link: https://machinelearningmastery.com/hill-climb-the-test-set-for-machine-learning/\n",
      "  PubDate: Thu, 24 Sep 2020 19:00:48 +0000\n",
      "  Author: Jason Brownlee\n",
      "\n",
      "Memento: https://web.archive.org/web/20200928023154/https://machinelearningmastery.com/feed/\n",
      "- Source: https://machinelearningmastery.com/feed\n",
      "  Channel Title: Machine Learning Mastery\n",
      "  Channel Description: Making developers awesome at machine learning\n",
      "- Title: Linear Discriminant Analysis With Python\n",
      "  Description: <p>Linear Discriminant Analysis is a linear classification machine learning algorithm. The algorithm involves developing a probabilistic model per class based on the specific distribution of observations for each input variable. A new example is then classified by calculating the conditional probability of it belonging to each class and selecting the class with the highest probability. [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://machinelearningmastery.com/linear-discriminant-analysis-with-python/\">Linear Discriminant Analysis With Python</a> appeared first on <a rel=\"nofollow\" href=\"https://machinelearningmastery.com\">Machine Learning Mastery</a>.</p>\n",
      "\n",
      "  Link: https://machinelearningmastery.com/linear-discriminant-analysis-with-python/\n",
      "  PubDate: Sun, 27 Sep 2020 19:00:20 +0000\n",
      "  Author: Jason Brownlee\n",
      "\n",
      "Memento: https://web.archive.org/web/20201027061454/http://machinelearningmastery.com/feed/\n",
      "- Source: https://machinelearningmastery.com/feed\n",
      "  Channel Title: Machine Learning Mastery\n",
      "  Channel Description: Making developers awesome at machine learning\n",
      "- Title: Why Use Ensemble Learning?\n",
      "  Description: <p>What are the Benefits of Ensemble Methods for Machine Learning? Ensembles are predictive models that combine predictions from two or more other models. Ensemble learning methods are popular and the go-to technique when the best performance on a predictive modeling project is the most important outcome. Nevertheless, they are not always the most appropriate technique [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://machinelearningmastery.com/why-use-ensemble-learning/\">Why Use Ensemble Learning?</a> appeared first on <a rel=\"nofollow\" href=\"https://machinelearningmastery.com\">Machine Learning Mastery</a>.</p>\n",
      "\n",
      "  Link: https://machinelearningmastery.com/why-use-ensemble-learning/\n",
      "  PubDate: Sun, 25 Oct 2020 18:00:15 +0000\n",
      "  Author: Jason Brownlee\n",
      "\n",
      "Memento: https://web.archive.org/web/20201116230444/https://machinelearningmastery.com/feed/\n",
      "- Source: https://machinelearningmastery.com/feed\n",
      "  Channel Title: Machine Learning Mastery\n",
      "  Channel Description: Making developers awesome at machine learning\n",
      "- Title: Develop a Bagging Ensemble with Different Data Transformations\n",
      "  Description: <p>Bootstrap aggregation, or bagging, is an ensemble where each model is trained on a different sample of the training dataset. The idea of bagging can be generalized to other techniques for changing the training dataset and fitting the same model on each changed version of the data. One approach is to use data transforms that [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://machinelearningmastery.com/bagging-ensemble-with-different-data-transformations/\">Develop a Bagging Ensemble with Different Data Transformations</a> appeared first on <a rel=\"nofollow\" href=\"https://machinelearningmastery.com\">Machine Learning Mastery</a>.</p>\n",
      "\n",
      "  Link: https://machinelearningmastery.com/bagging-ensemble-with-different-data-transformations/\n",
      "  PubDate: Sun, 15 Nov 2020 18:00:52 +0000\n",
      "  Author: Jason Brownlee\n",
      "\n",
      "Memento: https://web.archive.org/web/20210117154720/https://machinelearningmastery.com/feed/\n",
      "- Source: https://machinelearningmastery.com/feed\n",
      "  Channel Title: Machine Learning Mastery\n",
      "  Channel Description: Making developers awesome at machine learning\n",
      "- Title: Visualization for Function Optimization in Python\n",
      "  Description: <p>Function optimization involves finding the input that results in the optimal value from an objective function. Optimization algorithms navigate the search space of input variables in order to locate the optima, and both the shape of the objective function and behavior of the algorithm in the search space are opaque on real-world problems. As such, [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://machinelearningmastery.com/visualization-for-function-optimization-in-python/\">Visualization for Function Optimization in Python</a> appeared first on <a rel=\"nofollow\" href=\"https://machinelearningmastery.com\">Machine Learning Mastery</a>.</p>\n",
      "\n",
      "  Link: https://machinelearningmastery.com/visualization-for-function-optimization-in-python/\n",
      "  PubDate: Thu, 14 Jan 2021 18:00:23 +0000\n",
      "  Author: Jason Brownlee\n",
      "\n",
      "Memento: https://web.archive.org/web/20210118231426/http://machinelearningmastery.com/feed/\n",
      "- Source: https://machinelearningmastery.com/feed\n",
      "  Channel Title: Machine Learning Mastery\n",
      "  Channel Description: Making developers awesome at machine learning\n",
      "- Title: How to Choose an Activation Function for Deep Learning\n",
      "  Description: <p>Last Updated on January 19, 2021 Activation functions are a critical part of the design of a neural network. The choice of activation function in the hidden layer will control how well the network model learns the training dataset. The choice of activation function in the output layer will define the type of predictions the [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/\">How to Choose an Activation Function for Deep Learning</a> appeared first on <a rel=\"nofollow\" href=\"https://machinelearningmastery.com\">Machine Learning Mastery</a>.</p>\n",
      "\n",
      "  Link: https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/\n",
      "  PubDate: Sun, 17 Jan 2021 18:00:15 +0000\n",
      "  Author: Jason Brownlee\n",
      "\n",
      "Memento: https://web.archive.org/web/20210413215557/http://machinelearningmastery.com/feed/\n",
      "- Source: https://machinelearningmastery.com/feed\n",
      "  Channel Title: Machine Learning Mastery\n",
      "  Channel Description: Making developers awesome at machine learning\n",
      "- Title: What Is a Gradient in Machine Learning?\n",
      "  Description: <p>Gradient is a commonly used term in optimization and machine learning. For example, deep learning neural networks are fit using [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://machinelearningmastery.com/gradient-in-machine-learning/\">What Is a Gradient in Machine Learning?</a> appeared first on <a rel=\"nofollow\" href=\"https://machinelearningmastery.com\">Machine Learning Mastery</a>.</p>\n",
      "\n",
      "  Link: https://machinelearningmastery.com/gradient-in-machine-learning/\n",
      "  PubDate: Tue, 13 Apr 2021 19:00:53 +0000\n",
      "  Author: Jason Brownlee\n",
      "\n",
      "Memento: https://web.archive.org/web/20210415101818/https://machinelearningmastery.com/feed/\n",
      "- Source: https://machinelearningmastery.com/feed\n",
      "  Channel Title: Machine Learning Mastery\n",
      "  Channel Description: Making developers awesome at machine learning\n",
      "- Title: What Is a Gradient in Machine Learning?\n",
      "  Description: <p>Gradient is a commonly used term in optimization and machine learning. For example, deep learning neural networks are fit using [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://machinelearningmastery.com/gradient-in-machine-learning/\">What Is a Gradient in Machine Learning?</a> appeared first on <a rel=\"nofollow\" href=\"https://machinelearningmastery.com\">Machine Learning Mastery</a>.</p>\n",
      "\n",
      "  Link: https://machinelearningmastery.com/gradient-in-machine-learning/\n",
      "  PubDate: Tue, 13 Apr 2021 19:00:53 +0000\n",
      "  Author: Jason Brownlee\n",
      "\n",
      "Memento: https://web.archive.org/web/20210421230102/https://machinelearningmastery.com/feed/\n",
      "- Source: https://machinelearningmastery.com/feed\n",
      "  Channel Title: Machine Learning Mastery\n",
      "  Channel Description: Making developers awesome at machine learning\n",
      "- Title: Gradient Descent With Adadelta from Scratch\n",
      "  Description: <p>Gradient descent is an optimization algorithm that follows the negative gradient of an objective function in order to locate the [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://machinelearningmastery.com/gradient-descent-with-adadelta-from-scratch/\">Gradient Descent With Adadelta from Scratch</a> appeared first on <a rel=\"nofollow\" href=\"https://machinelearningmastery.com\">Machine Learning Mastery</a>.</p>\n",
      "\n",
      "  Link: https://machinelearningmastery.com/gradient-descent-with-adadelta-from-scratch/\n",
      "  PubDate: Sun, 11 Apr 2021 19:00:46 +0000\n",
      "  Author: Jason Brownlee\n",
      "\n",
      "Memento: https://web.archive.org/web/20210919042148/https://machinelearningmastery.com/feed/\n",
      "- Source: https://machinelearningmastery.com/feed\n",
      "  Channel Title: Machine Learning Mastery\n",
      "  Channel Description: Making developers awesome at machine learning\n",
      "- Title: What is Attention?\n",
      "  Description: <p>Last Updated on September 15, 2021 Attention is becoming increasingly popular in machine learning, but what makes it such an [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://machinelearningmastery.com/what-is-attention/\">What is Attention?</a> appeared first on <a rel=\"nofollow\" href=\"https://machinelearningmastery.com\">Machine Learning Mastery</a>.</p>\n",
      "\n",
      "  Link: https://machinelearningmastery.com/what-is-attention/\n",
      "  PubDate: Tue, 14 Sep 2021 16:40:45 +0000\n",
      "  Author: Stefania Cristina\n",
      "\n",
      "Memento: https://web.archive.org/web/20211130204530/https://machinelearningmastery.com/feed/\n",
      "- Source: https://machinelearningmastery.com/feed\n",
      "  Channel Title: Machine Learning Mastery\n",
      "  Channel Description: Making developers awesome at machine learning\n",
      "- Title: Application of differentiations in neural networks\n",
      "  Description: <p>Last Updated on November 26, 2021 Differential calculus is an important tool in machine learning algorithms. Neural networks in particular, [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://machinelearningmastery.com/application-of-differentiations-in-neural-networks/\">Application of differentiations in neural networks</a> appeared first on <a rel=\"nofollow\" href=\"https://machinelearningmastery.com\">Machine Learning Mastery</a>.</p>\n",
      "\n",
      "  Link: https://machinelearningmastery.com/application-of-differentiations-in-neural-networks/\n",
      "  PubDate: Thu, 25 Nov 2021 18:39:53 +0000\n",
      "  Author: Adrian Tam\n",
      "\n",
      "Memento: https://web.archive.org/web/https://transferlab.ai/index.xml\n",
      "- Source: https://transferlab.ai/index.xml\n",
      "  Channel Title: Content feed of the TransferLab — appliedAI Institute\n",
      "  Channel Description: All updates by the TransferLab team\n",
      "- Title: Model Misspecification in Simulation-Based Inference - Recent Advances and Open Challenges\n",
      "  Description: Model misspecification is a critical challenge in simulation-based inference (SBI), particularly in neural SBI, where methods rely on simulated data to train neural networks. These methods often assume that simulators accurately represent the true data-generating process, but in practice, this assumption is frequently violated. Such discrepancies can result in observed data that are out-of-distribution relative to the simulations, leading to biased posterior distributions and unreliable inferences. This blog reviews recent work on model misspecification in SBI, discussing its definitions, methods for detection and mitigation, and open challenges. The aim is to emphasize the importance of developing robust SBI methods that can accommodate the complexities of real-world applications.\n",
      "  Link: https://transferlab.ai/blog/model-misspecification-in-sbi/\n",
      "  PubDate: Fri, 07 Feb 2025 00:00:00 +0000\n",
      "  Author: Jan Teusen\n",
      "\n",
      "Memento: https://web.archive.org/web/20240212185349/https://transferlab.ai/index.xml\n",
      "- Source: https://transferlab.ai/index.xml\n",
      "  Channel Title: Content feed of the TransferLab — appliedAI Institute\n",
      "  Channel Description: All updates by the TransferLab team\n",
      "- Title: Mind the Gap: Methods and Applicability of Simulation-Based Inference\n",
      "  Description: Jan Teusen (née Boelts), AI Researcher at appliedAI Institute, will give a gentle introduction to simulation-based inference (SBI), along with a presentation of his own research on new SBI methods for computational neuroscience and new software tools.\n",
      "  Link: https://transferlab.ai/seminar/2024/mind-the-gap-methods-and-applicability-of-sbi/\n",
      "  PubDate: Thu, 14 Mar 2024 16:00:00 +0100\n",
      "  Author: Jan Teusen\n",
      "\n",
      "Memento: https://web.archive.org/web/20240421141756/https://transferlab.ai/index.xml\n",
      "- Source: https://transferlab.ai/index.xml\n",
      "  Channel Title: Content feed of the TransferLab — appliedAI Institute\n",
      "  Channel Description: All updates by the TransferLab team\n",
      "- Title: LAVA: Data Valuation Without Pre-Specified Learning Algorithms\n",
      "  Description: Today&amp;rsquo;s talk is about LAVA, an Optimal-Transport-based approach to data valuation that dispenses with training of a model to compute values.\n",
      "  Link: https://transferlab.ai/seminar/2024/lava-data-valuation-without-pre-specified-learning-algorithms/\n",
      "  PubDate: Thu, 02 May 2024 16:00:00 +0200\n",
      "  Author: Feiyang Kang\n",
      "\n",
      "Memento: https://web.archive.org/web/20240725170059/https://transferlab.ai/index.xml\n",
      "- Source: https://transferlab.ai/index.xml\n",
      "  Channel Title: Content feed of the TransferLab — appliedAI Institute\n",
      "  Channel Description: All updates by the TransferLab team\n",
      "- Title: Generalized Stability Guaranteed Quadratic Embeddings for Nonlinear Dynamical Systems\n",
      "  Description: Pawan Goyal, Senior AI Engineer at appliedAI, will present a recent work in physics-enhanced machine learning on generalized quadratic embeddings for nonlinear dynamics.\n",
      "  Link: https://transferlab.ai/seminar/2024/generalized-stability-guaranteed-quadratic-embeddings-for-nonlinear-dynamical-systems/\n",
      "  PubDate: Thu, 12 Sep 2024 10:00:00 +0200\n",
      "  Author: Pawan Goyal\n",
      "\n",
      "Memento: https://web.archive.org/web/20240910065617/https://transferlab.ai/index.xml\n",
      "- Source: https://transferlab.ai/index.xml\n",
      "  Channel Title: Content feed of the TransferLab — appliedAI Institute\n",
      "  Channel Description: All updates by the TransferLab team\n",
      "- Title: Introduction to Reduced Order Modeling\n",
      "  Description: Sridhar Chellappa will introduce the concept of reduced order modeling (ROM), a technique used in the field of simulation and AI to reduce the complexity of mathematical models. The seminar will cover the basics of ROM, its applications, and a lead up to more ML-flavoured approaches.\n",
      "  Link: https://transferlab.ai/seminar/2024/introduction-to-reduced-order-modeling/\n",
      "  PubDate: Thu, 17 Oct 2024 16:00:00 +0200\n",
      "  Author: Sridhar Chellappa\n",
      "\n",
      "Memento: https://web.archive.org/web/20241113161441/https://transferlab.ai/index.xml\n",
      "- Source: https://transferlab.ai/index.xml\n",
      "  Channel Title: Content feed of the TransferLab — appliedAI Institute\n",
      "  Channel Description: All updates by the TransferLab team\n",
      "- Title: Towards a statistical theory of data selection under weak supervision\n",
      "  Description: Pulkit Tandon, research engineer at Granica, will present his work on data selection, showing how using surrogate models to select subsamples of a data set for labeling can improve training efficiency and performance.\n",
      "  Link: https://transferlab.ai/seminar/2024/towards-a-statistical-theory-of-data-selection-under-weak-supervision/\n",
      "  PubDate: Wed, 30 Oct 2024 19:00:00 +0300\n",
      "  Author: Pulkit Tandon\n",
      "\n",
      "Memento: https://web.archive.org/web/20241207195439/https://transferlab.ai/index.xml\n",
      "- Source: https://transferlab.ai/index.xml\n",
      "  Channel Title: Content feed of the TransferLab — appliedAI Institute\n",
      "  Channel Description: All updates by the TransferLab team\n",
      "- Title: Towards a statistical theory of data selection under weak supervision\n",
      "  Description: Pulkit Tandon, research engineer at Granica, will present his work on data selection, showing how using surrogate models to select subsamples of a data set for labeling can improve training efficiency and performance.\n",
      "  Link: https://transferlab.ai/seminar/2024/towards-a-statistical-theory-of-data-selection-under-weak-supervision/\n",
      "  PubDate: Wed, 30 Oct 2024 19:00:00 +0300\n",
      "  Author: Pulkit Tandon\n",
      "\n",
      "Memento: https://web.archive.org/web/20241207195521/https://transferlab.ai/index.xml\n",
      "- Source: https://transferlab.ai/index.xml\n",
      "  Channel Title: Content feed of the TransferLab — appliedAI Institute\n",
      "  Channel Description: All updates by the TransferLab team\n",
      "- Title: Towards a statistical theory of data selection under weak supervision\n",
      "  Description: Pulkit Tandon, research engineer at Granica, will present his work on data selection, showing how using surrogate models to select subsamples of a data set for labeling can improve training efficiency and performance.\n",
      "  Link: https://transferlab.ai/seminar/2024/towards-a-statistical-theory-of-data-selection-under-weak-supervision/\n",
      "  PubDate: Wed, 30 Oct 2024 19:00:00 +0300\n",
      "  Author: Pulkit Tandon\n",
      "\n",
      "Memento: https://web.archive.org/web/20241207214251/https://transferlab.ai/index.xml\n",
      "- Source: https://transferlab.ai/index.xml\n",
      "  Channel Title: Content feed of the TransferLab — appliedAI Institute\n",
      "  Channel Description: All updates by the TransferLab team\n",
      "- Title: Towards a statistical theory of data selection under weak supervision\n",
      "  Description: Pulkit Tandon, research engineer at Granica, will present his work on data selection, showing how using surrogate models to select subsamples of a data set for labeling can improve training efficiency and performance.\n",
      "  Link: https://transferlab.ai/seminar/2024/towards-a-statistical-theory-of-data-selection-under-weak-supervision/\n",
      "  PubDate: Wed, 30 Oct 2024 19:00:00 +0300\n",
      "  Author: Pulkit Tandon\n",
      "\n",
      "Memento: https://web.archive.org/web/20250126220407/https://transferlab.ai/index.xml\n",
      "- Source: https://transferlab.ai/index.xml\n",
      "  Channel Title: Content feed of the TransferLab — appliedAI Institute\n",
      "  Channel Description: All updates by the TransferLab team\n",
      "- Title: Towards a statistical theory of data selection under weak supervision\n",
      "  Description: Pulkit Tandon, research engineer at Granica, will present his work on data selection, showing how using surrogate models to select subsamples of a data set for labeling can improve training efficiency and performance.\n",
      "  Link: https://transferlab.ai/seminar/2024/towards-a-statistical-theory-of-data-selection-under-weak-supervision/\n",
      "  PubDate: Wed, 30 Oct 2024 19:00:00 +0300\n",
      "  Author: Pulkit Tandon\n",
      "\n",
      "Memento: https://web.archive.org/web/20250127072134/https://transferlab.ai/index.xml\n",
      "- Source: https://transferlab.ai/index.xml\n",
      "  Channel Title: Content feed of the TransferLab — appliedAI Institute\n",
      "  Channel Description: All updates by the TransferLab team\n",
      "- Title: Towards a statistical theory of data selection under weak supervision\n",
      "  Description: Pulkit Tandon, research engineer at Granica, will present his work on data selection, showing how using surrogate models to select subsamples of a data set for labeling can improve training efficiency and performance.\n",
      "  Link: https://transferlab.ai/seminar/2024/towards-a-statistical-theory-of-data-selection-under-weak-supervision/\n",
      "  PubDate: Wed, 30 Oct 2024 19:00:00 +0300\n",
      "  Author: Pulkit Tandon\n",
      "\n",
      "Memento: https://web.archive.org/web/20250214203043/https://transferlab.ai/index.xml\n",
      "- Source: https://transferlab.ai/index.xml\n",
      "  Channel Title: Content feed of the TransferLab — appliedAI Institute\n",
      "  Channel Description: All updates by the TransferLab team\n",
      "- Title: Model Misspecification in Simulation-Based Inference - Recent Advances and Open Challenges\n",
      "  Description: Model misspecification is a critical challenge in simulation-based inference (SBI), particularly in neural SBI, where methods rely on simulated data to train neural networks. These methods often assume that simulators accurately represent the true data-generating process, but in practice, this assumption is frequently violated. Such discrepancies can result in observed data that are out-of-distribution relative to the simulations, leading to biased posterior distributions and unreliable inferences. This blog reviews recent work on model misspecification in SBI, discussing its definitions, methods for detection and mitigation, and open challenges. The aim is to emphasize the importance of developing robust SBI methods that can accommodate the complexities of real-world applications.\n",
      "  Link: https://transferlab.ai/blog/model-misspecification-in-sbi/\n",
      "  PubDate: Fri, 07 Feb 2025 00:00:00 +0000\n",
      "  Author: Jan Teusen\n",
      "\n",
      "Memento: https://web.archive.org/web/20250306125406/https://transferlab.ai/index.xml\n",
      "- Source: https://transferlab.ai/index.xml\n",
      "  Channel Title: Content feed of the TransferLab — appliedAI Institute\n",
      "  Channel Description: All updates by the TransferLab team\n",
      "- Title: Model Misspecification in Simulation-Based Inference - Recent Advances and Open Challenges\n",
      "  Description: Model misspecification is a critical challenge in simulation-based inference (SBI), particularly in neural SBI, where methods rely on simulated data to train neural networks. These methods often assume that simulators accurately represent the true data-generating process, but in practice, this assumption is frequently violated. Such discrepancies can result in observed data that are out-of-distribution relative to the simulations, leading to biased posterior distributions and unreliable inferences. This blog reviews recent work on model misspecification in SBI, discussing its definitions, methods for detection and mitigation, and open challenges. The aim is to emphasize the importance of developing robust SBI methods that can accommodate the complexities of real-world applications.\n",
      "  Link: https://transferlab.ai/blog/model-misspecification-in-sbi/\n",
      "  PubDate: Fri, 07 Feb 2025 00:00:00 +0000\n",
      "  Author: Jan Teusen\n",
      "\n",
      "Memento: https://web.archive.org/web/https://eng.uber.com/tag/machine-learning/feed\n",
      "- Source: https://eng.uber.com/tag/machine-learning/feed\n",
      "  Channel Title: Machine Learning Archives - Uber Engineering Blog\n",
      "  Channel Description: Software engineering and technologies that set the world in motion\n",
      "- Title: Elastic Distributed Training with XGBoost on Ray\n",
      "  Description: <h3><span style=\"font-weight: 400;\">Introduction</span></h3>\n",
      "<p><span style=\"font-weight: 400;\">Since we </span><a href=\"https://eng.uber.com/productionizing-distributed-xgboost/\"><span style=\"font-weight: 400;\">productionized distributed XGBoost on </span><span style=\"font-weight: 400;\">Apache Spark™</span><span style=\"font-weight: 400;\"> at Uber</span></a><span style=\"font-weight: 400;\"> in 2017, XGBoost has powered a wide spectrum of machine learning (ML) use cases at Uber, spanning from </span><a href=\"https://eng.uber.com/freight-markov/\"><span style=\"font-weight: 400;\">optimizing marketplace dynamic pricing policies for Freight</span></a><span style=\"font-weight: 400;\">, </span><a href=\"https://eng.uber.com/engineering-an-efficient-route/\"><span style=\"font-weight: 400;\">improving times of </span></a>&#8230;</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://eng.uber.com/elastic-xgboost-ray/\">Elastic Distributed Training with XGBoost on Ray</a> appeared first on <a rel=\"nofollow\" href=\"https://eng.uber.com\">Uber Engineering Blog</a>.</p>\n",
      "\n",
      "  Link: https://eng.uber.com/elastic-xgboost-ray/\n",
      "  PubDate: Wed, 07 Jul 2021 16:00:26 +0000\n",
      "  Author: Michael Mui\n",
      "\n",
      "Memento: https://web.archive.org/web/20180125144550/https://eng.uber.com/tag/machine-learning/feed/\n",
      "- Source: https://eng.uber.com/tag/machine-learning/feed\n",
      "  Channel Title: Machine Learning – Uber Engineering Blog\n",
      "  Channel Description: Engineering that helps move people places.\n",
      "- Title: Omphalos, Uber’s Parallel and Language-Extensible Time Series Backtesting Tool\n",
      "  Description: <p>Uber Engineering created Omphalos, our new backtesting framework, to enable efficient and reliable comparison of forecasting models across languages.</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://eng.uber.com/omphalos/\">Omphalos, Uber’s Parallel and Language-Extensible Time Series Backtesting Tool</a> appeared first on <a rel=\"nofollow\" href=\"https://eng.uber.com\">Uber Engineering Blog</a>.</p>\n",
      "\n",
      "  Link: https://eng.uber.com/omphalos/#utm_source=rss&utm_medium=rss&utm_campaign=omphalos\n",
      "  PubDate: Wed, 24 Jan 2018 16:30:09 +0000\n",
      "  Author: Roy Yang &#38; Calvin Worsnup\n",
      "\n",
      "Memento: https://web.archive.org/web/20181116163344/https://eng.uber.com/tag/machine-learning/feed/\n",
      "- Source: https://eng.uber.com/tag/machine-learning/feed\n",
      "  Channel Title: Machine Learning – Uber Engineering Blog\n",
      "  Channel Description: Engineering that helps move people places.\n",
      "- Title: Experience in AI: Uber Hires Jan Pedersen\n",
      "  Description: <p><span style=\"font-weight: 400;\">Whenever a rider gets dropped off at their location, one of our driver-partners finishes a session laden with trips, or an eater gets food delivered to their door, data underlies these interactions on the Uber platform. And our teams could </span>&#8230;</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://eng.uber.com/uber-hires-jan-pedersen/\">Experience in AI: Uber Hires Jan Pedersen</a> appeared first on <a rel=\"nofollow\" href=\"https://eng.uber.com\">Uber Engineering Blog</a>.</p>\n",
      "\n",
      "  Link: https://eng.uber.com/uber-hires-jan-pedersen/#utm_source=rss&utm_medium=rss&utm_campaign=uber-hires-jan-pedersen\n",
      "  PubDate: Thu, 15 Nov 2018 17:00:34 +0000\n",
      "  Author: Wayne Cunningham\n",
      "\n",
      "Memento: https://web.archive.org/web/20190330131559/https://eng.uber.com/tag/machine-learning/feed/\n",
      "- Source: https://eng.uber.com/tag/machine-learning/feed\n",
      "  Channel Title: Machine Learning – Uber Engineering Blog\n",
      "  Channel Description: Engineering that helps move people places.\n",
      "- Title: Accessible Machine Learning through Data Workflow Management\n",
      "  Description: <p><span style=\"font-weight: 400;\">Machine learning (ML) pervades many aspect of Uber&#8217;s business. From </span><a href=\"https://eng.uber.com/nlp-deep-learning-uber-maps/\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"font-weight: 400;\">responding to customer support tickets</span></a><span style=\"font-weight: 400;\">, </span><a href=\"https://eng.uber.com/uber-eats-query-understanding/\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"font-weight: 400;\">optimizing queries</span></a><span style=\"font-weight: 400;\">, and </span><a href=\"https://eng.uber.com/forecasting-introduction/\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"font-weight: 400;\">forecasting demand</span></a><span style=\"font-weight: 400;\">, ML provides critical insights for many of our teams. </span></p>\n",
      "<p><span style=\"font-weight: 400;\">Our teams encountered many different challenges while incorporating </span>&#8230;</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://eng.uber.com/machine-learning-data-workflow-management/\">Accessible Machine Learning through Data Workflow Management</a> appeared first on <a rel=\"nofollow\" href=\"https://eng.uber.com\">Uber Engineering Blog</a>.</p>\n",
      "\n",
      "  Link: https://eng.uber.com/machine-learning-data-workflow-management/\n",
      "  PubDate: Mon, 18 Mar 2019 16:00:59 +0000\n",
      "  Author: Jianyong Zhang\n",
      "\n",
      "Memento: https://web.archive.org/web/20190714151649/https://eng.uber.com/tag/machine-learning/feed/\n",
      "- Source: https://eng.uber.com/tag/machine-learning/feed\n",
      "  Channel Title: Machine Learning – Uber Engineering Blog\n",
      "  Channel Description: Engineering that helps move people places.\n",
      "- Title: Gaining Insights in a Simulated Marketplace with Machine Learning at Uber\n",
      "  Description: <p><span style=\"font-weight: 400;\">At Uber, we use marketplace algorithms to connect drivers and riders. Before the algorithms roll out globally, Uber fully tests and evaluates them to create an optimal user experience that maps to </span><a href=\"https://marketplace.uber.com/?utm=\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"font-weight: 400;\">our core marketplace principles</span></a><span style=\"font-weight: 400;\">. </span></p>\n",
      "<p><span style=\"font-weight: 400;\">To make product </span>&#8230;</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://eng.uber.com/simulated-marketplace/\">Gaining Insights in a Simulated Marketplace with Machine Learning at Uber</a> appeared first on <a rel=\"nofollow\" href=\"https://eng.uber.com\">Uber Engineering Blog</a>.</p>\n",
      "\n",
      "  Link: https://eng.uber.com/simulated-marketplace/\n",
      "  PubDate: Mon, 24 Jun 2019 15:30:58 +0000\n",
      "  Author: Haoyang Chen\n",
      "\n",
      "Memento: https://web.archive.org/web/20191228050923/https://eng.uber.com/tag/machine-learning/feed/\n",
      "- Source: https://eng.uber.com/tag/machine-learning/feed\n",
      "  Channel Title: Machine Learning – Uber Engineering Blog\n",
      "  Channel Description: Software engineering and technologies that set the world in motion\n",
      "- Title: Uber AI in 2019: Advancing Mobility with Artificial Intelligence\n",
      "  Description: <p><a href=\"https://www.youtube.com/watch?v=al8VjHVd7TM\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"font-weight: 400;\">Artificial intelligence</span></a><span style=\"font-weight: 400;\"> powers many of the technologies and services underpinning Uber’s platform, allowing engineering and data science teams to make informed decisions that help improve user experiences for products across our lines of business. </span></p>\n",
      "<p><span style=\"font-weight: 400;\">At the forefront of this effort </span>&#8230;</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://eng.uber.com/uber-ai-2019/\">Uber AI in 2019: Advancing Mobility with Artificial Intelligence</a> appeared first on <a rel=\"nofollow\" href=\"https://eng.uber.com\">Uber Engineering Blog</a>.</p>\n",
      "\n",
      "  Link: https://eng.uber.com/uber-ai-2019/\n",
      "  PubDate: Wed, 18 Dec 2019 17:34:37 +0000\n",
      "  Author: Zoubin Ghahramani\n",
      "\n",
      "Memento: https://web.archive.org/web/20200401021219/https://eng.uber.com/tag/machine-learning/feed/\n",
      "- Source: https://eng.uber.com/tag/machine-learning/feed\n",
      "  Channel Title: Machine Learning – Uber Engineering Blog\n",
      "  Channel Description: Software engineering and technologies that set the world in motion\n",
      "- Title: Under the Hood of Uber ATG’s Machine Learning Infrastructure and Versioning Control Platform for Self-Driving Vehicles\n",
      "  Description: <p><span style=\"font-weight: 400;\">As Uber experienced exponential growth over the last few years, now supporting 14 million trips each day, our engineers proved they could build for scale. That value extends to other areas, including </span><a href=\"https://www.uber.com/us/en/atg/\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"font-weight: 400;\">Uber ATG</span></a><span style=\"font-weight: 400;\"> (Advanced Technologies Group) and its quest </span>&#8230;</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://eng.uber.com/machine-learning-model-life-cycle-version-control/\">Under the Hood of Uber ATG’s Machine Learning Infrastructure and Versioning Control Platform for Self-Driving Vehicles</a> appeared first on <a rel=\"nofollow\" href=\"https://eng.uber.com\">Uber Engineering Blog</a>.</p>\n",
      "\n",
      "  Link: https://eng.uber.com/machine-learning-model-life-cycle-version-control/\n",
      "  PubDate: Wed, 04 Mar 2020 17:00:38 +0000\n",
      "  Author: Yu Guo\n",
      "\n",
      "Memento: https://web.archive.org/web/20200427022331/https://eng.uber.com/tag/machine-learning/feed/\n",
      "- Source: https://eng.uber.com/tag/machine-learning/feed\n",
      "  Channel Title: Machine Learning – Uber Engineering Blog\n",
      "  Channel Description: Software engineering and technologies that set the world in motion\n",
      "- Title: Under the Hood of Uber ATG’s Machine Learning Infrastructure and Versioning Control Platform for Self-Driving Vehicles\n",
      "  Description: <p><span style=\"font-weight: 400;\">As Uber experienced exponential growth over the last few years, now supporting 14 million trips each day, our engineers proved they could build for scale. That value extends to other areas, including </span><a href=\"https://www.uber.com/us/en/atg/\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"font-weight: 400;\">Uber ATG</span></a><span style=\"font-weight: 400;\"> (Advanced Technologies Group) and its quest </span>&#8230;</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://eng.uber.com/machine-learning-model-life-cycle-version-control/\">Under the Hood of Uber ATG’s Machine Learning Infrastructure and Versioning Control Platform for Self-Driving Vehicles</a> appeared first on <a rel=\"nofollow\" href=\"https://eng.uber.com\">Uber Engineering Blog</a>.</p>\n",
      "\n",
      "  Link: https://eng.uber.com/machine-learning-model-life-cycle-version-control/\n",
      "  PubDate: Wed, 04 Mar 2020 17:00:38 +0000\n",
      "  Author: Yu Guo\n",
      "\n",
      "Memento: https://web.archive.org/web/20200502043919/https://eng.uber.com/tag/machine-learning/feed/\n",
      "- Source: https://eng.uber.com/tag/machine-learning/feed\n",
      "  Channel Title: Machine Learning – Uber Engineering Blog\n",
      "  Channel Description: Software engineering and technologies that set the world in motion\n",
      "- Title: Under the Hood of Uber ATG’s Machine Learning Infrastructure and Versioning Control Platform for Self-Driving Vehicles\n",
      "  Description: <p><span style=\"font-weight: 400;\">As Uber experienced exponential growth over the last few years, now supporting 14 million trips each day, our engineers proved they could build for scale. That value extends to other areas, including </span><a href=\"https://www.uber.com/us/en/atg/\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"font-weight: 400;\">Uber ATG</span></a><span style=\"font-weight: 400;\"> (Advanced Technologies Group) and its quest </span>&#8230;</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://eng.uber.com/machine-learning-model-life-cycle-version-control/\">Under the Hood of Uber ATG’s Machine Learning Infrastructure and Versioning Control Platform for Self-Driving Vehicles</a> appeared first on <a rel=\"nofollow\" href=\"https://eng.uber.com\">Uber Engineering Blog</a>.</p>\n",
      "\n",
      "  Link: https://eng.uber.com/machine-learning-model-life-cycle-version-control/\n",
      "  PubDate: Wed, 04 Mar 2020 17:00:38 +0000\n",
      "  Author: Yu Guo\n",
      "\n",
      "Memento: https://web.archive.org/web/20201030082537/https://eng.uber.com/tag/machine-learning/feed/\n",
      "- Source: https://eng.uber.com/tag/machine-learning/feed\n",
      "  Channel Title: Machine Learning – Uber Engineering Blog\n",
      "  Channel Description: Software engineering and technologies that set the world in motion\n",
      "- Title: Fiber: Distributed Computing for AI Made  Simple\n",
      "  Description: <p><i><span style=\"font-weight: 400;\">Project Homepage:</span></i> <a href=\"https://github.com/uber/fiber\"><span style=\"font-weight: 400;\">GitHub</span></a></p>\n",
      "<p><span style=\"font-weight: 400;\">Over the past several years, </span><a href=\"https://openai.com/blog/ai-and-compute/\"><span style=\"font-weight: 400;\">increasing processing power</span></a><span style=\"font-weight: 400;\"> of computing machines has led to an increase in machine learning advances. More and more, algorithms exploit parallelism and rely on distributed training to process an enormous amount of </span>&#8230;</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://eng.uber.com/fiberdistributed/\">Fiber: Distributed Computing for AI Made  Simple</a> appeared first on <a rel=\"nofollow\" href=\"https://eng.uber.com\">Uber Engineering Blog</a>.</p>\n",
      "\n",
      "  Link: https://eng.uber.com/fiberdistributed/\n",
      "  PubDate: Tue, 30 Jun 2020 16:15:50 +0000\n",
      "  Author: Jiale Zhi\n",
      "\n",
      "Memento: https://web.archive.org/web/20210319134324/https://eng.uber.com/tag/machine-learning/feed/\n",
      "- Source: https://eng.uber.com/tag/machine-learning/feed\n",
      "  Channel Title: Machine Learning – Uber Engineering Blog\n",
      "  Channel Description: Software engineering and technologies that set the world in motion\n",
      "- Title: Fiber: Distributed Computing for AI Made  Simple\n",
      "  Description: <p><i><span style=\"font-weight: 400;\">Project Homepage:</span></i> <a href=\"https://github.com/uber/fiber\"><span style=\"font-weight: 400;\">GitHub</span></a></p>\n",
      "<p><span style=\"font-weight: 400;\">Over the past several years, </span><a href=\"https://openai.com/blog/ai-and-compute/\"><span style=\"font-weight: 400;\">increasing processing power</span></a><span style=\"font-weight: 400;\"> of computing machines has led to an increase in machine learning advances. More and more, algorithms exploit parallelism and rely on distributed training to process an enormous amount of </span>&#8230;</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://eng.uber.com/fiberdistributed/\">Fiber: Distributed Computing for AI Made  Simple</a> appeared first on <a rel=\"nofollow\" href=\"https://eng.uber.com\">Uber Engineering Blog</a>.</p>\n",
      "\n",
      "  Link: https://eng.uber.com/fiberdistributed/\n",
      "  PubDate: Tue, 30 Jun 2020 16:15:50 +0000\n",
      "  Author: Jiale Zhi\n",
      "\n",
      "Memento: https://web.archive.org/web/20210512184213/https://eng.uber.com/tag/machine-learning/feed/\n",
      "- Source: https://eng.uber.com/tag/machine-learning/feed\n",
      "  Channel Title: Machine Learning – Uber Engineering Blog\n",
      "  Channel Description: Software engineering and technologies that set the world in motion\n",
      "- Title: Fiber: Distributed Computing for AI Made  Simple\n",
      "  Description: <p><i><span style=\"font-weight: 400;\">Project Homepage:</span></i> <a href=\"https://github.com/uber/fiber\"><span style=\"font-weight: 400;\">GitHub</span></a></p>\n",
      "<p><span style=\"font-weight: 400;\">Over the past several years, </span><a href=\"https://openai.com/blog/ai-and-compute/\"><span style=\"font-weight: 400;\">increasing processing power</span></a><span style=\"font-weight: 400;\"> of computing machines has led to an increase in machine learning advances. More and more, algorithms exploit parallelism and rely on distributed training to process an enormous amount of </span>&#8230;</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://eng.uber.com/fiberdistributed/\">Fiber: Distributed Computing for AI Made  Simple</a> appeared first on <a rel=\"nofollow\" href=\"https://eng.uber.com\">Uber Engineering Blog</a>.</p>\n",
      "\n",
      "  Link: https://eng.uber.com/fiberdistributed/\n",
      "  PubDate: Tue, 30 Jun 2020 16:15:50 +0000\n",
      "  Author: Jiale Zhi\n",
      "\n",
      "Memento: https://web.archive.org/web/20211104114346/https://eng.uber.com/tag/machine-learning/feed/\n",
      "- Source: https://eng.uber.com/tag/machine-learning/feed\n",
      "  Channel Title: Machine Learning – Uber Engineering Blog\n",
      "  Channel Description: Software engineering and technologies that set the world in motion\n",
      "- Title: Elastic Distributed Training with XGBoost on Ray\n",
      "  Description: <h3><span style=\"font-weight: 400;\">Introduction</span></h3>\n",
      "<p><span style=\"font-weight: 400;\">Since we </span><a href=\"https://eng.uber.com/productionizing-distributed-xgboost/\"><span style=\"font-weight: 400;\">productionized distributed XGBoost on </span><span style=\"font-weight: 400;\">Apache Spark™</span><span style=\"font-weight: 400;\"> at Uber</span></a><span style=\"font-weight: 400;\"> in 2017, XGBoost has powered a wide spectrum of machine learning (ML) use cases at Uber, spanning from </span><a href=\"https://eng.uber.com/freight-markov/\"><span style=\"font-weight: 400;\">optimizing marketplace dynamic pricing policies for Freight</span></a><span style=\"font-weight: 400;\">, </span><a href=\"https://eng.uber.com/engineering-an-efficient-route/\"><span style=\"font-weight: 400;\">improving times of </span></a>&#8230;</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://eng.uber.com/elastic-xgboost-ray/\">Elastic Distributed Training with XGBoost on Ray</a> appeared first on <a rel=\"nofollow\" href=\"https://eng.uber.com\">Uber Engineering Blog</a>.</p>\n",
      "\n",
      "  Link: https://eng.uber.com/elastic-xgboost-ray/\n",
      "  PubDate: Wed, 07 Jul 2021 16:00:26 +0000\n",
      "  Author: Michael Mui\n",
      "\n",
      "Memento: https://web.archive.org/web/20220520213604/https://eng.uber.com/tag/machine-learning/feed/\n",
      "- Source: https://eng.uber.com/tag/machine-learning/feed\n",
      "  Channel Title: Machine Learning Archives - Uber Engineering Blog\n",
      "  Channel Description: Software engineering and technologies that set the world in motion\n",
      "- Title: Elastic Distributed Training with XGBoost on Ray\n",
      "  Description: <h3><span style=\"font-weight: 400;\">Introduction</span></h3>\n",
      "<p><span style=\"font-weight: 400;\">Since we </span><a href=\"https://eng.uber.com/productionizing-distributed-xgboost/\"><span style=\"font-weight: 400;\">productionized distributed XGBoost on </span><span style=\"font-weight: 400;\">Apache Spark™</span><span style=\"font-weight: 400;\"> at Uber</span></a><span style=\"font-weight: 400;\"> in 2017, XGBoost has powered a wide spectrum of machine learning (ML) use cases at Uber, spanning from </span><a href=\"https://eng.uber.com/freight-markov/\"><span style=\"font-weight: 400;\">optimizing marketplace dynamic pricing policies for Freight</span></a><span style=\"font-weight: 400;\">, </span><a href=\"https://eng.uber.com/engineering-an-efficient-route/\"><span style=\"font-weight: 400;\">improving times of </span></a>&#8230;</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://eng.uber.com/elastic-xgboost-ray/\">Elastic Distributed Training with XGBoost on Ray</a> appeared first on <a rel=\"nofollow\" href=\"https://eng.uber.com\">Uber Engineering Blog</a>.</p>\n",
      "\n",
      "  Link: https://eng.uber.com/elastic-xgboost-ray/\n",
      "  PubDate: Wed, 07 Jul 2021 16:00:26 +0000\n",
      "  Author: Michael Mui\n",
      "\n",
      "Memento: https://web.archive.org/web/https://aws.amazon.com/blogs/machine-learning/feed\n",
      "- Source: https://aws.amazon.com/blogs/machine-learning/feed\n",
      "  Channel Title: AWS Machine Learning Blog\n",
      "  Channel Description: Official Machine Learning Blog of Amazon Web Services\n",
      "- Title: Reduce ML training costs with Amazon SageMaker HyperPod\n",
      "  Description: In this post, we explore the challenges of large-scale frontier model training, focusing on hardware failures and the benefits of Amazon SageMaker HyperPod - a solution that minimizes disruptions, enhances efficiency, and reduces training costs.\n",
      "  Link: https://aws.amazon.com/blogs/machine-learning/reduce-ml-training-costs-with-amazon-sagemaker-hyperpod/\n",
      "  PubDate: Thu, 10 Apr 2025 20:11:51 +0000\n",
      "  Author: Anoop Saha\n",
      "\n",
      "Memento: https://web.archive.org/web/20180522183012/https://aws.amazon.com/blogs/machine-learning/feed/\n",
      "- Source: https://aws.amazon.com/blogs/machine-learning/feed\n",
      "  Channel Title: AWS Machine Learning Blog\n",
      "  Channel Description: Official Machine Learning Blog of Amazon Web Services\n",
      "- Title: Apache MXNet (incubating) adds support for Keras 2\n",
      "  Description: The Keras-MXNet deep learning backend is available now, thanks to contributors to the Keras and Apache MXNet (incubating) open source projects. Keras is a high-level neural network API written in Python. It’s popular for its fast and easy prototyping of CNNs and RNNs. Keras developers can now use the high-performance MXNet deep learning engine for […]\n",
      "  Link: https://aws.amazon.com/blogs/machine-learning/apache-mxnet-incubating-adds-support-for-keras-2/\n",
      "  PubDate: Mon, 21 May 2018 19:51:21 +0000\n",
      "  Author: Lai Wei\n",
      "\n",
      "Memento: https://web.archive.org/web/20180522223136/https://aws.amazon.com/blogs/machine-learning/feed/\n",
      "- Source: https://aws.amazon.com/blogs/machine-learning/feed\n",
      "  Channel Title: AWS Machine Learning Blog\n",
      "  Channel Description: Official Machine Learning Blog of Amazon Web Services\n",
      "- Title: Apache MXNet (incubating) adds support for Keras 2\n",
      "  Description: The Keras-MXNet deep learning backend is available now, thanks to contributors to the Keras and Apache MXNet (incubating) open source projects. Keras is a high-level neural network API written in Python. It’s popular for its fast and easy prototyping of CNNs and RNNs. Keras developers can now use the high-performance MXNet deep learning engine for […]\n",
      "  Link: https://aws.amazon.com/blogs/machine-learning/apache-mxnet-incubating-adds-support-for-keras-2/\n",
      "  PubDate: Mon, 21 May 2018 19:51:21 +0000\n",
      "  Author: Lai Wei\n",
      "\n",
      "Memento: https://web.archive.org/web/20180523083816/https://aws.amazon.com/blogs/machine-learning/feed/\n",
      "- Source: https://aws.amazon.com/blogs/machine-learning/feed\n",
      "  Channel Title: AWS Machine Learning Blog\n",
      "  Channel Description: Official Machine Learning Blog of Amazon Web Services\n",
      "- Title: Apache MXNet (incubating) adds support for Keras 2\n",
      "  Description: The Keras-MXNet deep learning backend is available now, thanks to contributors to the Keras and Apache MXNet (incubating) open source projects. Keras is a high-level neural network API written in Python. It’s popular for its fast and easy prototyping of CNNs and RNNs. Keras developers can now use the high-performance MXNet deep learning engine for […]\n",
      "  Link: https://aws.amazon.com/blogs/machine-learning/apache-mxnet-incubating-adds-support-for-keras-2/\n",
      "  PubDate: Mon, 21 May 2018 19:51:21 +0000\n",
      "  Author: Lai Wei\n",
      "\n",
      "Memento: https://web.archive.org/web/20180523190221/https://aws.amazon.com/blogs/machine-learning/feed/\n",
      "- Source: https://aws.amazon.com/blogs/machine-learning/feed\n",
      "  Channel Title: AWS Machine Learning Blog\n",
      "  Channel Description: Official Machine Learning Blog of Amazon Web Services\n",
      "- Title: Apache MXNet (incubating) adds support for Keras 2\n",
      "  Description: The Keras-MXNet deep learning backend is available now, thanks to contributors to the Keras and Apache MXNet (incubating) open source projects. Keras is a high-level neural network API written in Python. It’s popular for its fast and easy prototyping of CNNs and RNNs. Keras developers can now use the high-performance MXNet deep learning engine for […]\n",
      "  Link: https://aws.amazon.com/blogs/machine-learning/apache-mxnet-incubating-adds-support-for-keras-2/\n",
      "  PubDate: Mon, 21 May 2018 19:51:21 +0000\n",
      "  Author: Lai Wei\n",
      "\n",
      "Memento: https://web.archive.org/web/20180524041229/https://aws.amazon.com/blogs/machine-learning/feed/\n",
      "- Source: https://aws.amazon.com/blogs/machine-learning/feed\n",
      "  Channel Title: AWS Machine Learning Blog\n",
      "  Channel Description: Official Machine Learning Blog of Amazon Web Services\n",
      "- Title: Using Pipe input mode for Amazon SageMaker algorithms\n",
      "  Description: Today, we are introducing Pipe input mode support for the Amazon SageMaker built-in algorithms. With Pipe input mode, your dataset is streamed directly to your training instances instead of being downloaded first. This means that your training jobs start sooner, finish quicker, and need less disk space. Amazon SageMaker algorithms have been engineered to be […]\n",
      "  Link: https://aws.amazon.com/blogs/machine-learning/using-pipe-input-mode-for-amazon-sagemaker-algorithms/\n",
      "  PubDate: Wed, 23 May 2018 22:06:36 +0000\n",
      "  Author: Can Balioglu\n",
      "\n",
      "Memento: https://web.archive.org/web/20180525023522/https://aws.amazon.com/blogs/machine-learning/feed/\n",
      "- Source: https://aws.amazon.com/blogs/machine-learning/feed\n",
      "  Channel Title: AWS Machine Learning Blog\n",
      "  Channel Description: Official Machine Learning Blog of Amazon Web Services\n",
      "- Title: Using Pipe input mode for Amazon SageMaker algorithms\n",
      "  Description: Today, we are introducing Pipe input mode support for the Amazon SageMaker built-in algorithms. With Pipe input mode, your dataset is streamed directly to your training instances instead of being downloaded first. This means that your training jobs start sooner, finish quicker, and need less disk space. Amazon SageMaker algorithms have been engineered to be […]\n",
      "  Link: https://aws.amazon.com/blogs/machine-learning/using-pipe-input-mode-for-amazon-sagemaker-algorithms/\n",
      "  PubDate: Wed, 23 May 2018 22:06:36 +0000\n",
      "  Author: Can Balioglu\n",
      "\n",
      "Memento: https://web.archive.org/web/20180526031514/https://aws.amazon.com/blogs/machine-learning/feed/\n",
      "- Source: https://aws.amazon.com/blogs/machine-learning/feed\n",
      "  Channel Title: AWS Machine Learning Blog\n",
      "  Channel Description: Official Machine Learning Blog of Amazon Web Services\n",
      "- Title: Amazon Translate is now supported in AWS Mobile SDK for Android and iOS\n",
      "  Description: Amazon Translate is a neural machine translation service that delivers fast, high-quality, and affordable language translation. Support for Amazon Translate API is now available in the AWS Mobile SDK for Android and iOS. Now, you can use the AWS Mobile SDK to develop and publish multilingual mobile apps quickly and easily with Amazon Translate. By […]\n",
      "  Link: https://aws.amazon.com/blogs/machine-learning/amazon-translate-is-now-supported-in-aws-mobile-sdk-for-android-and-ios/\n",
      "  PubDate: Fri, 25 May 2018 20:51:08 +0000\n",
      "  Author: Woo Kim\n",
      "\n",
      "Memento: https://web.archive.org/web/20180526032413/https://aws.amazon.com/blogs/machine-learning/feed/\n",
      "- Source: https://aws.amazon.com/blogs/machine-learning/feed\n",
      "  Channel Title: AWS Machine Learning Blog\n",
      "  Channel Description: Official Machine Learning Blog of Amazon Web Services\n",
      "- Title: Amazon Translate is now supported in AWS Mobile SDK for Android and iOS\n",
      "  Description: Amazon Translate is a neural machine translation service that delivers fast, high-quality, and affordable language translation. Support for Amazon Translate API is now available in the AWS Mobile SDK for Android and iOS. Now, you can use the AWS Mobile SDK to develop and publish multilingual mobile apps quickly and easily with Amazon Translate. By […]\n",
      "  Link: https://aws.amazon.com/blogs/machine-learning/amazon-translate-is-now-supported-in-aws-mobile-sdk-for-android-and-ios/\n",
      "  PubDate: Fri, 25 May 2018 20:51:08 +0000\n",
      "  Author: Woo Kim\n",
      "\n",
      "Memento: https://web.archive.org/web/20180527025807/https://aws.amazon.com/blogs/machine-learning/feed/\n",
      "- Source: https://aws.amazon.com/blogs/machine-learning/feed\n",
      "  Channel Title: AWS Machine Learning Blog\n",
      "  Channel Description: Official Machine Learning Blog of Amazon Web Services\n",
      "- Title: Amazon Translate is now supported in AWS Mobile SDK for Android and iOS\n",
      "  Description: Amazon Translate is a neural machine translation service that delivers fast, high-quality, and affordable language translation. Support for Amazon Translate API is now available in the AWS Mobile SDK for Android and iOS. Now, you can use the AWS Mobile SDK to develop and publish multilingual mobile apps quickly and easily with Amazon Translate. By […]\n",
      "  Link: https://aws.amazon.com/blogs/machine-learning/amazon-translate-is-now-supported-in-aws-mobile-sdk-for-android-and-ios/\n",
      "  PubDate: Fri, 25 May 2018 20:51:08 +0000\n",
      "  Author: Woo Kim\n",
      "\n",
      "Memento: https://web.archive.org/web/20180527054105/https://aws.amazon.com/blogs/machine-learning/feed/\n",
      "- Source: https://aws.amazon.com/blogs/machine-learning/feed\n",
      "  Channel Title: AWS Machine Learning Blog\n",
      "  Channel Description: Official Machine Learning Blog of Amazon Web Services\n",
      "- Title: Amazon Translate is now supported in AWS Mobile SDK for Android and iOS\n",
      "  Description: Amazon Translate is a neural machine translation service that delivers fast, high-quality, and affordable language translation. Support for Amazon Translate API is now available in the AWS Mobile SDK for Android and iOS. Now, you can use the AWS Mobile SDK to develop and publish multilingual mobile apps quickly and easily with Amazon Translate. By […]\n",
      "  Link: https://aws.amazon.com/blogs/machine-learning/amazon-translate-is-now-supported-in-aws-mobile-sdk-for-android-and-ios/\n",
      "  PubDate: Fri, 25 May 2018 20:51:08 +0000\n",
      "  Author: Woo Kim\n",
      "\n",
      "Memento: https://web.archive.org/web/20180528032926/https://aws.amazon.com/blogs/machine-learning/feed/\n",
      "- Source: https://aws.amazon.com/blogs/machine-learning/feed\n",
      "  Channel Title: AWS Machine Learning Blog\n",
      "  Channel Description: Official Machine Learning Blog of Amazon Web Services\n",
      "- Title: Amazon Translate is now supported in AWS Mobile SDK for Android and iOS\n",
      "  Description: Amazon Translate is a neural machine translation service that delivers fast, high-quality, and affordable language translation. Support for Amazon Translate API is now available in the AWS Mobile SDK for Android and iOS. Now, you can use the AWS Mobile SDK to develop and publish multilingual mobile apps quickly and easily with Amazon Translate. By […]\n",
      "  Link: https://aws.amazon.com/blogs/machine-learning/amazon-translate-is-now-supported-in-aws-mobile-sdk-for-android-and-ios/\n",
      "  PubDate: Fri, 25 May 2018 20:51:08 +0000\n",
      "  Author: Woo Kim\n",
      "\n",
      "Memento: https://web.archive.org/web/20180528063026/https://aws.amazon.com/blogs/machine-learning/feed/\n",
      "- Source: https://aws.amazon.com/blogs/machine-learning/feed\n",
      "  Channel Title: AWS Machine Learning Blog\n",
      "  Channel Description: Official Machine Learning Blog of Amazon Web Services\n",
      "- Title: Amazon Translate is now supported in AWS Mobile SDK for Android and iOS\n",
      "  Description: Amazon Translate is a neural machine translation service that delivers fast, high-quality, and affordable language translation. Support for Amazon Translate API is now available in the AWS Mobile SDK for Android and iOS. Now, you can use the AWS Mobile SDK to develop and publish multilingual mobile apps quickly and easily with Amazon Translate. By […]\n",
      "  Link: https://aws.amazon.com/blogs/machine-learning/amazon-translate-is-now-supported-in-aws-mobile-sdk-for-android-and-ios/\n",
      "  PubDate: Fri, 25 May 2018 20:51:08 +0000\n",
      "  Author: Woo Kim\n",
      "\n",
      "Memento: https://web.archive.org/web/20180530040901/https://aws.amazon.com/blogs/machine-learning/feed/\n",
      "- Source: https://aws.amazon.com/blogs/machine-learning/feed\n",
      "  Channel Title: AWS Machine Learning Blog\n",
      "  Channel Description: Official Machine Learning Blog of Amazon Web Services\n",
      "- Title: Using R with Amazon SageMaker\n",
      "  Description: This blog post describes how to train, deploy, and retrieve predictions from a machine learning (ML) model using&nbsp;Amazon SageMaker&nbsp;and R. The model predicts abalone age as measured by the number of rings in the shell. The reticulate package will be used as an R interface to&nbsp;Amazon SageMaker Python SDK to make API calls to Amazon […]\n",
      "  Link: https://aws.amazon.com/blogs/machine-learning/using-r-with-amazon-sagemaker/\n",
      "  PubDate: Tue, 29 May 2018 16:29:18 +0000\n",
      "  Author: Ryan Garner\n",
      "\n",
      "Memento: https://web.archive.org/web/20180530080929/https://aws.amazon.com/blogs/machine-learning/feed/\n",
      "- Source: https://aws.amazon.com/blogs/machine-learning/feed\n",
      "  Channel Title: AWS Machine Learning Blog\n",
      "  Channel Description: Official Machine Learning Blog of Amazon Web Services\n",
      "- Title: Using R with Amazon SageMaker\n",
      "  Description: This blog post describes how to train, deploy, and retrieve predictions from a machine learning (ML) model using&nbsp;Amazon SageMaker&nbsp;and R. The model predicts abalone age as measured by the number of rings in the shell. The reticulate package will be used as an R interface to&nbsp;Amazon SageMaker Python SDK to make API calls to Amazon […]\n",
      "  Link: https://aws.amazon.com/blogs/machine-learning/using-r-with-amazon-sagemaker/\n",
      "  PubDate: Tue, 29 May 2018 16:29:18 +0000\n",
      "  Author: Ryan Garner\n",
      "\n",
      "Memento: https://web.archive.org/web/20180531065757/https://aws.amazon.com/blogs/machine-learning/feed/\n",
      "- Source: https://aws.amazon.com/blogs/machine-learning/feed\n",
      "  Channel Title: AWS Machine Learning Blog\n",
      "  Channel Description: Official Machine Learning Blog of Amazon Web Services\n",
      "- Title: Load test and optimize an Amazon SageMaker endpoint using automatic scaling\n",
      "  Description: Once you have trained, optimized and deployed your machine learning (ML) model, the next challenge is to host it in such a way that consumers can easily invoke and get predictions from it. Many customers have consumers who are either external or internal to their organizations and want to use the model for predictions (ML […]\n",
      "  Link: https://aws.amazon.com/blogs/machine-learning/load-test-and-optimize-an-amazon-sagemaker-endpoint-using-automatic-scaling/\n",
      "  PubDate: Wed, 30 May 2018 22:30:13 +0000\n",
      "  Author: BK Chaurasiya\n",
      "\n",
      "Memento: https://web.archive.org/web/20180607043324/https://aws.amazon.com/blogs/machine-learning/feed/\n",
      "- Source: https://aws.amazon.com/blogs/machine-learning/feed\n",
      "  Channel Title: AWS Machine Learning Blog\n",
      "  Channel Description: Official Machine Learning Blog of Amazon Web Services\n",
      "- Title: Amazon SageMaker console now supports training job cloning\n",
      "  Description: Today we are launching the training job cloning feature on the Amazon SageMaker console, which makes it much easier for you to create training jobs based on existing ones. When you use Amazon SageMaker, it’s common to run multiple training jobs using different training sets and identical configuration. It’s also common to adjust a specific […]\n",
      "  Link: https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-console-now-supports-training-job-cloning/\n",
      "  PubDate: Wed, 06 Jun 2018 21:04:32 +0000\n",
      "  Author: Fan Li\n",
      "\n",
      "Memento: https://web.archive.org/web/20180608064939/https://aws.amazon.com/blogs/machine-learning/feed/\n",
      "- Source: https://aws.amazon.com/blogs/machine-learning/feed\n",
      "  Channel Title: AWS Machine Learning Blog\n",
      "  Channel Description: Official Machine Learning Blog of Amazon Web Services\n",
      "- Title: VidMob combines computer vision and language AI services for data-driven creative asset production\n",
      "  Description: VidMob is a social video creation platform that marketers of all sizes can use to develop personalized advertising communications at scale. VidMob uses machine learning (ML) to power its SaaS application. This application uses metadata extraction and sentiment analysis to provide marketers with actionable insights into which creative assets resonate with their intended audience, and […]\n",
      "  Link: https://aws.amazon.com/blogs/machine-learning/vidmob-combines-computer-vision-and-language-ai-services-for-data-driven-creative-asset-production/\n",
      "  PubDate: Thu, 07 Jun 2018 21:45:53 +0000\n",
      "  Author: Kaiser Larsen\n",
      "\n",
      "Memento: https://web.archive.org/web/20180609094514/https://aws.amazon.com/blogs/machine-learning/feed/\n",
      "- Source: https://aws.amazon.com/blogs/machine-learning/feed\n",
      "  Channel Title: AWS Machine Learning Blog\n",
      "  Channel Description: Official Machine Learning Blog of Amazon Web Services\n",
      "- Title: VidMob combines computer vision and language AI services for data-driven creative asset production\n",
      "  Description: VidMob is a social video creation platform that marketers of all sizes can use to develop personalized advertising communications at scale. VidMob uses machine learning (ML) to power its SaaS application. This application uses metadata extraction and sentiment analysis to provide marketers with actionable insights into which creative assets resonate with their intended audience, and […]\n",
      "  Link: https://aws.amazon.com/blogs/machine-learning/vidmob-combines-computer-vision-and-language-ai-services-for-data-driven-creative-asset-production/\n",
      "  PubDate: Thu, 07 Jun 2018 21:45:53 +0000\n",
      "  Author: Kaiser Larsen\n",
      "\n",
      "Memento: https://web.archive.org/web/http://news.mit.edu/rss/topic/artificial-intelligence2\n",
      "- Source: http://news.mit.edu/rss/topic/artificial-intelligence2\n",
      "  Channel Title: MIT News - Artificial intelligence\n",
      "  Channel Description: MIT news feed about: Artificial intelligence\n",
      "- Title: Making higher education more accessible to students in Pakistan\n",
      "  Description: EduFi, founded by an MIT alumna, provides low-interest student loans to families in Pakistan so more can attend college.\n",
      "  Link: https://news.mit.edu/2025/edufi-makes-higher-education-more-accessible-students-pakistan-0327\n",
      "  PubDate: Thu, 27 Mar 2025 14:30:00 -0400\n",
      "  Author: Zach Winn | MIT News\n",
      "\n",
      "Memento: https://web.archive.org/web/20150905204516/http://news.mit.edu/rss/topic/artificial-intelligence2\n",
      "- Source: http://news.mit.edu/rss/topic/artificial-intelligence2\n",
      "  Channel Title: MIT News - Artificial intelligence\n",
      "  Channel Description: MIT News is dedicated to communicating to the media and the public the news and achievements of the students, faculty, staff and the greater MIT community.\n",
      "- Title: CSAIL joins with Toyota on $25 million research center for autonomous cars\n",
      "  Description: Seeking to reduce traffic casualties, center will focus on robotics and artificial intelligence systems. \n",
      "  Link: http://news.mit.edu/2015/csail-toyota-25-million-research-center-autonomous-cars-0904\n",
      "  PubDate: Fri, 04 Sep 2015 13:00:00 -0400\n",
      "  Author: Adam Conner-Simons | MIT News Office\n",
      "\n",
      "Memento: https://web.archive.org/web/20160304031213/http://news.mit.edu:80/rss/topic/artificial-intelligence2\n",
      "- Source: http://news.mit.edu/rss/topic/artificial-intelligence2\n",
      "  Channel Title: MIT News - Artificial intelligence\n",
      "  Channel Description: MIT News is dedicated to communicating to the media and the public the news and achievements of the students, faculty, staff and the greater MIT community.\n",
      "- Title: Enabling human-robot rescue teams\n",
      "  Description: System could help prevent robots from overwhelming human teammates with information.\n",
      "  Link: http://news.mit.edu/2016/human-robot-rescue-teams-0217\n",
      "  PubDate: Wed, 17 Feb 2016 00:00:00 -0500\n",
      "  Author: Larry Hardesty | MIT News Office\n",
      "\n",
      "Memento: https://web.archive.org/web/20160521114724/http://news.mit.edu:80/rss/topic/artificial-intelligence2\n",
      "- Source: http://news.mit.edu/rss/topic/artificial-intelligence2\n",
      "  Channel Title: MIT News - Artificial intelligence\n",
      "  Channel Description: MIT News is dedicated to communicating to the media and the public the news and achievements of the students, faculty, staff and the greater MIT community.\n",
      "- Title: Robotic consensus\n",
      "  Description: Control algorithm for teams of robots factors in moving obstacles.\n",
      "  Link: http://news.mit.edu/2016/algorithm-robot-teams-moving-obstacles-0421\n",
      "  PubDate: Thu, 21 Apr 2016 00:00:00 -0400\n",
      "  Author: Larry Hardesty | MIT News Office\n",
      "\n",
      "Memento: https://web.archive.org/web/20160621170220/http://news.mit.edu:80/rss/topic/artificial-intelligence2\n",
      "- Source: http://news.mit.edu/rss/topic/artificial-intelligence2\n",
      "  Channel Title: MIT News - Artificial intelligence\n",
      "  Channel Description: MIT News is dedicated to communicating to the media and the public the news and achievements of the students, faculty, staff and the greater MIT community.\n",
      "- Title: Teaching machines to predict the future\n",
      "  Description: Deep-learning vision system from the Computer Science and Artificial Intelligence Lab anticipates human interactions using videos of TV shows.\n",
      "  Link: http://news.mit.edu/2016/teaching-machines-to-predict-the-future-0621\n",
      "  PubDate: Tue, 21 Jun 2016 08:00:00 -0400\n",
      "  Author: Adam Conner-Simons | Rachel Gordon | CSAIL\n",
      "\n",
      "Memento: https://web.archive.org/web/20160723212358/http://news.mit.edu:80/rss/topic/artificial-intelligence2\n",
      "- Source: http://news.mit.edu/rss/topic/artificial-intelligence2\n",
      "  Channel Title: MIT News - Artificial intelligence\n",
      "  Channel Description: MIT News is dedicated to communicating to the media and the public the news and achievements of the students, faculty, staff and the greater MIT community.\n",
      "- Title: Robert Fano, computing pioneer and founder of CSAIL, dies at 98\n",
      "  Description: Professor emeritus helped launch field of information theory and developed early time-sharing computers.\n",
      "  Link: http://news.mit.edu/2016/robert-fano-obituary-0715\n",
      "  PubDate: Fri, 15 Jul 2016 10:30:00 -0400\n",
      "  Author: Adam Conner-Simons and Rachel Gordon | CSAIL\n",
      "\n",
      "Memento: https://web.archive.org/web/20161229075800/http://news.mit.edu/rss/topic/artificial-intelligence2\n",
      "- Source: http://news.mit.edu/rss/topic/artificial-intelligence2\n",
      "  Channel Title: MIT News - Artificial intelligence\n",
      "  Channel Description: MIT News is dedicated to communicating to the media and the public the news and achievements of the students, faculty, staff and the greater MIT community.\n",
      "- Title: Data diversity\n",
      "  Description: Preserving variety in subsets of unmanageably large data sets should aid machine learning.\n",
      "  Link: http://news.mit.edu/2016/variety-subsets-large-data-sets-machine-learning-1216\n",
      "  PubDate: Fri, 16 Dec 2016 00:00:00 -0500\n",
      "  Author: Larry Hardesty | MIT News Office\n",
      "\n",
      "Memento: https://web.archive.org/web/20171112232953/http://news.mit.edu:80/rss/topic/artificial-intelligence2\n",
      "- Source: http://news.mit.edu/rss/topic/artificial-intelligence2\n",
      "  Channel Title: MIT News - Artificial intelligence\n",
      "  Channel Description: MIT News is dedicated to communicating to the media and the public the news and achievements of the students, faculty, staff and the greater MIT community.\n",
      "- Title: President Reif: We must build a future in which technology works for everyone\n",
      "  Description: Boston Globe op-ed by MIT president calls for a “whole-society effort” to address profound challenges posed by automation.\n",
      "\n",
      "  Link: http://news.mit.edu/2017/president-reif-we-must-build-future-technology-works-everyone-1110\n",
      "  PubDate: Fri, 10 Nov 2017 09:04:32 -0500\n",
      "  Author: MIT News Office\n",
      "\n",
      "Memento: https://web.archive.org/web/20171218143319/http://news.mit.edu:80/rss/topic/artificial-intelligence2\n",
      "- Source: http://news.mit.edu/rss/topic/artificial-intelligence2\n",
      "  Channel Title: MIT News - Artificial intelligence\n",
      "  Channel Description: MIT News is dedicated to communicating to the media and the public the news and achievements of the students, faculty, staff and the greater MIT community.\n",
      "- Title: Unlocking marine mysteries with artificial intelligence\n",
      "  Description: Students put their AI software for underwater vehicles to the test on the Charles River. \n",
      "  Link: http://news.mit.edu/2017/unlocking-marine-mysteries-artificial-intelligence-1215\n",
      "  PubDate: Thu, 14 Dec 2017 23:59:59 -0500\n",
      "  Author: Mary Beth O'Leary | Department of Mechanical Engineering\n",
      "\n",
      "Memento: https://web.archive.org/web/20180120010444/http://news.mit.edu:80/rss/topic/artificial-intelligence2\n",
      "- Source: http://news.mit.edu/rss/topic/artificial-intelligence2\n",
      "  Channel Title: MIT News - Artificial intelligence\n",
      "  Channel Description: MIT News is dedicated to communicating to the media and the public the news and achievements of the students, faculty, staff and the greater MIT community.\n",
      "- Title: Robo-picker grasps and packs \n",
      "  Description: New robotic system could lend a hand with warehouse sorting and other picking or clearing tasks.\n",
      "  Link: http://news.mit.edu/2018/robo-picker-grasps-and-packs-0220\n",
      "  PubDate: Mon, 19 Feb 2018 23:59:59 -0500\n",
      "  Author: Jennifer Chu | MIT News Office\n",
      "\n",
      "Memento: https://web.archive.org/web/20180221084913/http://news.mit.edu:80/rss/topic/artificial-intelligence2\n",
      "- Source: http://news.mit.edu/rss/topic/artificial-intelligence2\n",
      "  Channel Title: MIT News - Artificial intelligence\n",
      "  Channel Description: MIT News is dedicated to communicating to the media and the public the news and achievements of the students, faculty, staff and the greater MIT community.\n",
      "- Title: Robo-picker grasps and packs \n",
      "  Description: New robotic system could lend a hand with warehouse sorting and other picking or clearing tasks.\n",
      "  Link: http://news.mit.edu/2018/robo-picker-grasps-and-packs-0220\n",
      "  PubDate: Mon, 19 Feb 2018 23:59:59 -0500\n",
      "  Author: Jennifer Chu | MIT News Office\n",
      "\n",
      "Memento: https://web.archive.org/web/20180403101647/http://news.mit.edu:80/rss/topic/artificial-intelligence2\n",
      "- Source: http://news.mit.edu/rss/topic/artificial-intelligence2\n",
      "  Channel Title: MIT News - Artificial intelligence\n",
      "  Channel Description: MIT News is dedicated to communicating to the media and the public the news and achievements of the students, faculty, staff and the greater MIT community.\n",
      "- Title: Computer searches telescope data for evidence of distant planets\n",
      "  Description: Machine-learning system uses physics principles to augment data from NASA crowdsourcing project.\n",
      "  Link: http://news.mit.edu/2018/computer-searches-telescope-data-evidence-distant-planets-0330\n",
      "  PubDate: Thu, 29 Mar 2018 23:59:59 -0400\n",
      "  Author: Larry Hardesty | MIT News Office\n",
      "\n",
      "Memento: https://web.archive.org/web/20180511124343/http://news.mit.edu:80/rss/topic/artificial-intelligence2\n",
      "- Source: http://news.mit.edu/rss/topic/artificial-intelligence2\n",
      "  Channel Title: MIT News - Artificial intelligence\n",
      "  Channel Description: MIT News is dedicated to communicating to the media and the public the news and achievements of the students, faculty, staff and the greater MIT community.\n",
      "- Title: Celebrating 21st century statistics\n",
      "  Description: MIT Statistics and Data Science Conference highlights new approaches and varied applications.\n",
      "  Link: http://news.mit.edu/2018/mit-conference-celebrates-21st-century-statistics-0508\n",
      "  PubDate: Tue, 08 May 2018 16:20:00 -0400\n",
      "  Author: Scott Murray | Institute for Data, Systems, and Society\n",
      "\n",
      "Memento: https://web.archive.org/web/20180518165443/http://news.mit.edu:80/rss/topic/artificial-intelligence2\n",
      "- Source: http://news.mit.edu/rss/topic/artificial-intelligence2\n",
      "  Channel Title: MIT News - Artificial intelligence\n",
      "  Channel Description: MIT News is dedicated to communicating to the media and the public the news and achievements of the students, faculty, staff and the greater MIT community.\n",
      "- Title: Researchers develop virtual-reality testing ground for drones\n",
      "  Description: With new system, drones navigate through an empty room, avoiding crashes while “seeing” a virtual world.\n",
      "  Link: http://news.mit.edu/2018/virtual-reality-testing-ground-drones-0517\n",
      "  PubDate: Thu, 17 May 2018 00:00:00 -0400\n",
      "  Author: Jennifer Chu | MIT News Office\n",
      "\n",
      "Memento: https://web.archive.org/web/20180617105300/http://news.mit.edu:80/rss/topic/artificial-intelligence2\n",
      "- Source: http://news.mit.edu/rss/topic/artificial-intelligence2\n",
      "  Channel Title: MIT News - Artificial intelligence\n",
      "  Channel Description: MIT News is dedicated to communicating to the media and the public the news and achievements of the students, faculty, staff and the greater MIT community.\n",
      "- Title: CSAIL launches new five-year collaboration with iFlyTek\n",
      "  Description: Lab will work with Chinese company on research in artificial intelligence, language processing, and human-computer interaction.\n",
      "  Link: http://news.mit.edu/2018/csail-launches-five-year-collaboration-with-iflytek-0615\n",
      "  PubDate: Fri, 15 Jun 2018 13:00:01 -0400\n",
      "  Author: Adam Conner-Simons | CSAIL\n",
      "\n",
      "Memento: https://web.archive.org/web/20180726153018/http://news.mit.edu:80/rss/topic/artificial-intelligence2\n",
      "- Source: http://news.mit.edu/rss/topic/artificial-intelligence2\n",
      "  Channel Title: MIT News - Artificial intelligence\n",
      "  Channel Description: MIT News is dedicated to communicating to the media and the public the news and achievements of the students, faculty, staff and the greater MIT community.\n",
      "- Title: CSAIL launches new initiative for financial technology\n",
      "  Description: FinTech@CSAIL industry collaboration will work to improve business models, access to data, and security in the finance sector.\n",
      "  Link: http://news.mit.edu/2018/mit-csail-launches-new-initiative-financial-technology-0726\n",
      "  PubDate: Thu, 26 Jul 2018 10:40:00 -0400\n",
      "  Author: Adam Conner-Simons | Rachel Gordon | CSAIL\n",
      "\n",
      "Memento: https://web.archive.org/web/20180826130210/http://news.mit.edu:80/rss/topic/artificial-intelligence2\n",
      "- Source: http://news.mit.edu/rss/topic/artificial-intelligence2\n",
      "  Channel Title: MIT News - Artificial intelligence\n",
      "  Channel Description: MIT News is dedicated to communicating to the media and the public the news and achievements of the students, faculty, staff and the greater MIT community.\n",
      "- Title: MIT-SenseTime Alliance funds projects from all five schools\n",
      "  Description: Interdisciplinary work will advance research in human and machine intelligence.\n",
      "  Link: http://news.mit.edu/2018/mit-sensetime-alliance-funds-projects-human-machine-intelligence-0824\n",
      "  PubDate: Fri, 24 Aug 2018 09:00:01 -0400\n",
      "  Author: Meg Murphy | School of Engineering\n",
      "\n",
      "Memento: https://web.archive.org/web/20181002144822/http://news.mit.edu:80/rss/topic/artificial-intelligence2\n",
      "- Source: http://news.mit.edu/rss/topic/artificial-intelligence2\n",
      "  Channel Title: MIT News - Artificial intelligence\n",
      "  Channel Description: MIT News is dedicated to communicating to the media and the public the news and achievements of the students, faculty, staff and the greater MIT community.\n",
      "- Title: Ideas abound at Quest for Intelligence workshop\n",
      "  Description: Community event generates ideas for sparking innovative and ambitious plans to advance research in human and machine intelligence.\n",
      "  Link: http://news.mit.edu/2018/mit-quest-for-intelligence-holds-ideas-workshop-1001\n",
      "  PubDate: Mon, 01 Oct 2018 11:40:00 -0400\n",
      "  Author: Brittany Flaherty | School of Science\n",
      "\n",
      "Memento: https://web.archive.org/web/20181108132120/http://news.mit.edu:80/rss/topic/artificial-intelligence2\n",
      "- Source: http://news.mit.edu/rss/topic/artificial-intelligence2\n",
      "  Channel Title: MIT News - Artificial intelligence\n",
      "  Channel Description: MIT News is dedicated to communicating to the media and the public the news and achievements of the students, faculty, staff and the greater MIT community.\n",
      "- Title: Highlighting new research opportunities in civil and environmental engineering\n",
      "  Description: At its annual alumni reception, CEE faculty shared innovative research projects ranging from machine learning to regional impacts of climate change.\n",
      "  Link: http://news.mit.edu/2018/mit-faculty-discuss-research-opportunities-in-cee-1107\n",
      "  PubDate: Wed, 07 Nov 2018 10:50:00 -0500\n",
      "  Author: Taylor De Leon | Department of Civil and Environmental Engineering\n",
      "\n",
      "Memento: https://web.archive.org/web/20181114101646/https://news.mit.edu/rss/topic/artificial-intelligence2\n",
      "- Source: http://news.mit.edu/rss/topic/artificial-intelligence2\n",
      "  Channel Title: MIT News - Artificial intelligence\n",
      "  Channel Description: MIT News is dedicated to communicating to the media and the public the news and achievements of the students, faculty, staff and the greater MIT community.\n",
      "- Title: Bridge to the future of engineering\n",
      "  Description: The School of Engineering’s faculty leadership weigh in on what the MIT Stephen A. Schwarzman College of Computing will mean for their students and faculty. \n",
      "\n",
      "  Link: https://news.mit.edu/2018/bridge-to-the-future-of-engineering-1111\n",
      "  PubDate: Sun, 11 Nov 2018 00:01:00 -0500\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/http://feeds.feedburner.com/nvidiablog\n",
      "- Source: http://feeds.feedburner.com/nvidiablog\n",
      "  Channel Title: NVIDIA Blog\n",
      "  Channel Description: None\n",
      "- Title: From Browsing to Buying: How AI Agents Enhance Online Shopping\n",
      "  Description: Online shopping puts a world of choices at people’s fingertips, making it convenient for them to purchase and receive orders — all from the comfort of their homes.\n",
      "  Link: https://blogs.nvidia.com/blog/ai-agents-online-shopping/\n",
      "  PubDate: Thu, 03 Apr 2025 15:00:51 +0000\n",
      "  Author: Allison Siu\n",
      "\n",
      "Memento: https://web.archive.org/web/20101212100419/http://feeds.feedburner.com:80/nvidiablog\n",
      "- Source: http://feeds.feedburner.com/nvidiablog\n",
      "  Channel Title: NVIDIA\n",
      "  Channel Description: None\n",
      "- Title: Superfans’ Passion More Than Skin Deep\n",
      "  Description: NVIDIA isn’t your typical company. We have a unique style and bring lots of enthusiasm as we work to deliver inspiring products to about the most demanding and opinionated communities on the planet. If I had to use one word to describe the people at NVIDIA, I’d call them passionate. We seek out employees who....\n",
      "  Link: http://feedproxy.google.com/~r/nvidiablog/~3/7_Uj-lGqYXs/\n",
      "  PubDate: Fri, 10 Dec 2010 21:20:01 +0000\n",
      "  Author: Lee Hirsch\n",
      "\n",
      "Memento: https://web.archive.org/web/20110311223355/http://feeds.feedburner.com:80/nvidiablog\n",
      "- Source: http://feeds.feedburner.com/nvidiablog\n",
      "  Channel Title: NVIDIA\n",
      "  Channel Description: None\n",
      "- Title: We’re at PAX East 2011\n",
      "  Description: Setup for PAX East 2011 is well under way and we have lots of exciting content lined up for the show. A major element in our booth is the PC gaming eco-system, featuring twelve of our technology partners. They&#8217;ll be walking you through all the elements of creating and enjoying your own gaming setup. What....\n",
      "  Link: http://feedproxy.google.com/~r/nvidiablog/~3/NqdfzPJK7cw/\n",
      "  PubDate: Thu, 10 Mar 2011 17:25:24 +0000\n",
      "  Author: Kris Rey\n",
      "\n",
      "Memento: https://web.archive.org/web/20110403044422/http://feeds.feedburner.com:80/nvidiablog?\n",
      "- Source: http://feeds.feedburner.com/nvidiablog\n",
      "  Channel Title: NVIDIA\n",
      "  Channel Description: None\n",
      "- Title: Release 270 Beta Drivers Are Available For Download\n",
      "  Description: Gamers, are you ready for performance upgrades and cool new features? The first Release 270 beta driver, version 270.51, is now available for download on GeForce.com. Check out the highlights below or read up in more detail about Release 270 specifications on GeForce.com. Release 270 highlights: Boosts gaming performance by as much as 6x. Enables....\n",
      "  Link: http://feedproxy.google.com/~r/nvidiablog/~3/FAioUo0F4-8/\n",
      "  PubDate: Wed, 30 Mar 2011 13:00:12 +0000\n",
      "  Author: Chris Daniel\n",
      "\n",
      "Memento: https://web.archive.org/web/20110907220144/http://feeds.feedburner.com:80/nvidiablog\n",
      "- Source: http://feeds.feedburner.com/nvidiablog\n",
      "  Channel Title: NVIDIA\n",
      "  Channel Description: None\n",
      "- Title: Serving up the 2011 US Open Tennis Finals in 3D\n",
      "  Description: Put on your 3D Vision glasses because the Women’s Singles Final and Men’s Singles Final US Open matches will be streaming in full 3D via the official US Open website. Last year’s 3D production of the 2010 US Open Tennis Championships – the first world-wide 3D broadcast of a major tennis tournament – was honored with a....\n",
      "  Link: http://feedproxy.google.com/~r/nvidiablog/~3/PBbzPyYEU5g/\n",
      "  PubDate: Tue, 06 Sep 2011 18:26:48 +0000\n",
      "  Author: Jon Barad\n",
      "\n",
      "Memento: https://web.archive.org/web/20111003190300/http://feeds.feedburner.com:80/nvidiablog\n",
      "- Source: http://feeds.feedburner.com/nvidiablog\n",
      "  Channel Title: NVIDIA\n",
      "  Channel Description: None\n",
      "- Title: Inner Geek – Adam Pintek’s Project GT90 Supercar\n",
      "  Description: My name is Adam Pintek. Welcome to my Project GT90 Supercar. Ever since I can remember, I have been addicted to supercars. Unfortunately, the price tag has gotten in the way of fulfilling my childhood dream of owning one. Then one day not too long ago, I thought to myself, “Why not just make my....\n",
      "  Link: http://feedproxy.google.com/~r/nvidiablog/~3/OUL9ScQ0N7g/\n",
      "  PubDate: Mon, 03 Oct 2011 17:07:15 +0000\n",
      "  Author: Adam Pintek\n",
      "\n",
      "Memento: https://web.archive.org/web/20111103080420/http://feeds.feedburner.com:80/nvidiablog\n",
      "- Source: http://feeds.feedburner.com/nvidiablog\n",
      "  Channel Title: NVIDIA\n",
      "  Channel Description: None\n",
      "- Title: We’re Listening: NVIDIA’s Social Media Command Center\n",
      "  Description: Fans are NVIDIA’s life blood. They’re why we come into work. They’re why we innovate amazing products. And they’re why we put on events like last month’s GeForce LAN 6 on the USS Hornet aircraft carrier. It’s vital for us to know what fans are saying – so we can understand how we’re doing and....\n",
      "  Link: http://feedproxy.google.com/~r/nvidiablog/~3/X1vSJvevBKg/\n",
      "  PubDate: Wed, 02 Nov 2011 22:37:43 +0000\n",
      "  Author: Shanee Ben-Zur\n",
      "\n",
      "Memento: https://web.archive.org/web/20111205050901/http://feeds.feedburner.com:80/nvidiablog?\n",
      "- Source: http://feeds.feedburner.com/nvidiablog\n",
      "  Channel Title: NVIDIA\n",
      "  Channel Description: None\n",
      "- Title: Looking For A Few Good Codenames\n",
      "  Description: We need your help choosing a codename for our upcoming CUDA on ARM development kit. Recently announced at the SuperComputing 2011 conference in Seattle, Wash., the CUDA on ARM development kit will enable programmers to easily write code for the next generation of powerful, energy efficient supercomputers. Fitting, as this same hardware – powered by....\n",
      "  Link: http://feedproxy.google.com/~r/nvidiablog/~3/SswohwDwnmA/\n",
      "  PubDate: Fri, 02 Dec 2011 20:08:21 +0000\n",
      "  Author: Devang Sachdev\n",
      "\n",
      "Memento: https://web.archive.org/web/20111211141707/http://feeds.feedburner.com:80/nvidiablog\n",
      "- Source: http://feeds.feedburner.com/nvidiablog\n",
      "  Channel Title: NVIDIA\n",
      "  Channel Description: None\n",
      "- Title: Shadowgun Flexes Tegra 3 Quad-Core Muscles\n",
      "  Description: The popular Android game Shadowgun has been enhanced to take advantage of the Tegra 3 quad-core mobile processor. This demo shows Shadowgun’s graphics performance when played on Tegra 3 devices. In Shadowgun, the year is 2350 and you play the role of infamous bounty hunter John Slade. Your mission: neutralize mutant baddies. Over the course of....\n",
      "  Link: http://feedproxy.google.com/~r/nvidiablog/~3/ge50CUA15M8/\n",
      "  PubDate: Thu, 08 Dec 2011 22:37:29 +0000\n",
      "  Author: Hassan Anjum\n",
      "\n",
      "Memento: https://web.archive.org/web/20120117210527/http://feeds.feedburner.com/nvidiablog\n",
      "\n",
      "Memento: https://web.archive.org/web/20120317210939/http://feeds.feedburner.com/nvidiablog\n",
      "\n",
      "Memento: https://web.archive.org/web/20120329190016/http://feeds.feedburner.com/nvidiablog\n",
      "\n",
      "Memento: https://web.archive.org/web/20120329190158/http://feeds.feedburner.com/nvidiablog\n",
      "\n",
      "Memento: https://web.archive.org/web/20120819111444/http://feeds.feedburner.com:80/nvidiablog\n",
      "\n",
      "Memento: https://web.archive.org/web/20121102205309/http://feeds.feedburner.com:80/nvidiablog\n",
      "\n",
      "Memento: https://web.archive.org/web/20121104155322/http://feeds.feedburner.com:80/nvidiablog?\n",
      "\n",
      "Memento: https://web.archive.org/web/20130109181154/http://feeds.feedburner.com/nvidiablog\n",
      "\n",
      "Memento: https://web.archive.org/web/20131021002509/http://feeds.feedburner.com/nvidiablog/\n",
      "\n",
      "Memento: https://web.archive.org/web/20131109171016/http://feeds.feedburner.com/nvidiablog\n",
      "\n",
      "Memento: https://web.archive.org/web/20131228171330/http://feeds.feedburner.com/nvidiablog\n",
      "\n",
      "Memento: https://web.archive.org/web/https://openai.com/news/rss.xml\n",
      "- Source: https://openai.com/news/rss.xml\n",
      "  Channel Title: OpenAI News\n",
      "  Channel Description: The OpenAI blog\n",
      "- Title: Sub-processor list—April 2025 update\n",
      "  Description: This page provides information about the Sub-processors OpenAI has engaged to provide processing activities on Customer Data as defined in the OpenAI Data Processing Agreement.\n",
      "  Link: https://openai.com/policies/sub-processor-list-april-2025-update\n",
      "  PubDate: Wed, 30 Apr 2025 00:00:00 GMT\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20241204101815/https://openai.com/news/rss.xml\n",
      "- Source: https://openai.com/news/rss.xml\n",
      "  Channel Title: OpenAI News\n",
      "  Channel Description: The OpenAI blog\n",
      "- Title: Advancing red teaming with people and AI \n",
      "  Description: Advancing red teaming with people and AI \n",
      "  Link: https://openai.com/index/advancing-red-teaming-with-people-and-ai\n",
      "  PubDate: Thu, 21 Nov 2024 10:30:00 GMT\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20250105004615/http://openai.com/news/rss.xml\n",
      "- Source: https://openai.com/news/rss.xml\n",
      "  Channel Title: OpenAI News\n",
      "  Channel Description: The OpenAI blog\n",
      "- Title: Advancing red teaming with people and AI \n",
      "  Description: Advancing red teaming with people and AI \n",
      "  Link: https://openai.com/index/advancing-red-teaming-with-people-and-ai\n",
      "  PubDate: Thu, 21 Nov 2024 10:30:00 GMT\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20250213133337/https://openai.com/news/rss.xml\n",
      "- Source: https://openai.com/news/rss.xml\n",
      "  Channel Title: OpenAI News\n",
      "  Channel Description: The OpenAI blog\n",
      "- Title: Sharing the latest Model Spec\n",
      "  Description: We’ve made updates to the Model Spec based on external feedback and our continued research in shaping desired model behavior.\n",
      "  Link: https://openai.com/index/sharing-the-latest-model-spec\n",
      "  PubDate: Wed, 12 Feb 2025 13:00:00 GMT\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20250213133359/https://openai.com/news/rss.xml\n",
      "- Source: https://openai.com/news/rss.xml\n",
      "  Channel Title: OpenAI News\n",
      "  Channel Description: The OpenAI blog\n",
      "- Title: Sharing the latest Model Spec\n",
      "  Description: We’ve made updates to the Model Spec based on external feedback and our continued research in shaping desired model behavior.\n",
      "  Link: https://openai.com/index/sharing-the-latest-model-spec\n",
      "  PubDate: Wed, 12 Feb 2025 13:00:00 GMT\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20250213133417/https://openai.com/news/rss.xml\n",
      "- Source: https://openai.com/news/rss.xml\n",
      "  Channel Title: OpenAI News\n",
      "  Channel Description: The OpenAI blog\n",
      "- Title: Sharing the latest Model Spec\n",
      "  Description: We’ve made updates to the Model Spec based on external feedback and our continued research in shaping desired model behavior.\n",
      "  Link: https://openai.com/index/sharing-the-latest-model-spec\n",
      "  PubDate: Wed, 12 Feb 2025 13:00:00 GMT\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20250313191614/https://openai.com/news/rss.xml\n",
      "- Source: https://openai.com/news/rss.xml\n",
      "  Channel Title: OpenAI News\n",
      "  Channel Description: The OpenAI blog\n",
      "- Title: OpenAI’s proposals for the U.S. AI Action Plan\n",
      "  Description: Recommendations build on OpenAI’s Economic Blueprint to strengthen America’s AI leadership.\n",
      "  Link: https://openai.com/global-affairs/openai-proposals-for-the-us-ai-action-plan\n",
      "  PubDate: Thu, 13 Mar 2025 03:00:00 GMT\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20250331232851/https://openai.com/news/rss.xml\n",
      "- Source: https://openai.com/news/rss.xml\n",
      "  Channel Title: OpenAI News\n",
      "  Channel Description: The OpenAI blog\n",
      "- Title: New funding to build towards AGI\n",
      "  Description: Today we’re announcing new funding—$40B at a $300B post-money valuation, which enables us to push the frontiers of AI research even further, scale our compute infrastructure, and deliver increasingly powerful tools for the 500 million people who use ChatGPT every week.\n",
      "  Link: https://openai.com/index/march-funding-updates\n",
      "  PubDate: Mon, 31 Mar 2025 15:00:00 GMT\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20250407072746/https://openai.com/news/rss.xml\n",
      "- Source: https://openai.com/news/rss.xml\n",
      "  Channel Title: OpenAI News\n",
      "  Channel Description: The OpenAI blog\n",
      "- Title: Sub-processor list—April 2025 update\n",
      "  Description: This page provides information about the Sub-processors OpenAI has engaged to provide processing activities on Customer Data as defined in the OpenAI Data Processing Agreement.\n",
      "  Link: https://openai.com/policies/sub-processor-list-april-2025-update\n",
      "  PubDate: Wed, 30 Apr 2025 00:00:00 GMT\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/http://feeds.feedburner.com/blogspot/gJZg\n",
      "\n",
      "Memento: https://web.archive.org/web/20071029185815/http://feeds.feedburner.com:80/blogspot/gJZg\n",
      "\n",
      "Memento: https://web.archive.org/web/20080415193603/http://feeds.feedburner.com:80/blogspot/gJZg\n",
      "\n",
      "Memento: https://web.archive.org/web/20080524023148/http://feeds.feedburner.com:80/blogspot/gJZg\n",
      "\n",
      "Memento: https://web.archive.org/web/20080618223319/http://feeds.feedburner.com:80/blogspot/gJZg?\n",
      "\n",
      "Memento: https://web.archive.org/web/20080623134407/http://feeds.feedburner.com:80/blogspot/gJZg\n",
      "\n",
      "Memento: https://web.archive.org/web/20080714123153/http://feeds.feedburner.com:80/blogspot/gJZg\n",
      "\n",
      "Memento: https://web.archive.org/web/20080804195831/http://feeds.feedburner.com:80/blogspot/gJZg\n",
      "\n",
      "Memento: https://web.archive.org/web/20081110050201/http://feeds.feedburner.com/blogspot/gJZg\n",
      "\n",
      "Memento: https://web.archive.org/web/20081116190533/http://feeds.feedburner.com/blogspot/gJZg\n",
      "\n",
      "Memento: https://web.archive.org/web/20081130124453/http://feeds.feedburner.com/blogspot/gJZg\n",
      "\n",
      "Memento: https://web.archive.org/web/20090127022540/http://feeds.feedburner.com:80/blogspot/gJZg\n",
      "\n",
      "Memento: https://web.archive.org/web/20090328191340/http://feeds.feedburner.com:80/blogspot/gJZg\n",
      "\n",
      "Memento: https://web.archive.org/web/20090521081515/http://feeds.feedburner.com:80/blogspot/gJZg\n",
      "\n",
      "Memento: https://web.archive.org/web/http://arxiv.org/rss/cs.LG\n",
      "- Source: http://arxiv.org/rss/cs.LG\n",
      "  Channel Title: cs.LG updates on arXiv.org\n",
      "  Channel Description: cs.LG updates on the arXiv.org e-print archive.\n",
      "- Title: PAODING: A High-fidelity Data-free Pruning Toolkit for Debloating Pre-trained Neural Networks\n",
      "  Description: arXiv:2405.00074v1 Announce Type: new \n",
      "Abstract: We present PAODING, a toolkit to debloat pretrained neural network models through the lens of data-free pruning. To preserve the model fidelity, PAODING adopts an iterative process, which dynamically measures the effect of deleting a neuron to identify candidates that have the least impact to the output layer. Our evaluation shows that PAODING can significantly reduce the model size, generalize on different datasets and models, and meanwhile preserve the model fidelity in terms of test accuracy and adversarial robustness. PAODING is publicly available on PyPI via https://pypi.org/project/paoding-dl.\n",
      "  Link: https://arxiv.org/abs/2405.00074\n",
      "  PubDate: None\n",
      "  Author: Mark Huasong Meng, Hao Guan, Liuhuo Wan, Sin Gee Teo, Guangdong Bai, Jin Song Dong\n",
      "\n",
      "Memento: https://web.archive.org/web/20090506052826/http://arxiv.org/rss/cs.LG\n",
      "\n",
      "Memento: https://web.archive.org/web/20111126112445/http://arxiv.org:80/rss/cs.LG\n",
      "\n",
      "Memento: https://web.archive.org/web/20131216171709/http://arxiv.org/rss/cs.LG\n",
      "\n",
      "Memento: https://web.archive.org/web/20150520003833/http://arxiv.org/rss/cs.LG\n",
      "\n",
      "Memento: https://web.archive.org/web/20151202211800/http://arxiv.org/rss/cs.LG\n",
      "\n",
      "Memento: https://web.archive.org/web/20160629221840/http://arxiv.org/rss/cs.LG\n",
      "\n",
      "Memento: https://web.archive.org/web/20160930051356/http://arxiv.org/rss/cs.LG\n",
      "\n",
      "Memento: https://web.archive.org/web/20161007054337/http://arxiv.org/rss/cs.LG\n",
      "\n",
      "Memento: https://web.archive.org/web/20161014061723/http://arxiv.org/rss/cs.LG\n",
      "\n",
      "Memento: https://web.archive.org/web/20161108002458/http://arxiv.org/rss/cs.LG\n",
      "\n",
      "Memento: https://web.archive.org/web/20161115004940/http://arxiv.org/rss/cs.LG\n",
      "\n",
      "Memento: https://web.archive.org/web/20161122012544/http://arxiv.org/rss/cs.LG\n",
      "\n",
      "Memento: https://web.archive.org/web/20161129014441/http://arxiv.org/rss/cs.LG\n",
      "\n",
      "Memento: https://web.archive.org/web/20161206020750/http://arxiv.org/rss/cs.LG\n",
      "\n",
      "Memento: https://web.archive.org/web/20161209012647/http://arxiv.org/rss/cs.LG\n",
      "\n",
      "Memento: https://web.archive.org/web/20161210022121/http://arxiv.org/rss/cs.LG\n",
      "\n",
      "Memento: https://web.archive.org/web/http://arxiv.org/rss/stat.ML\n",
      "- Source: http://arxiv.org/rss/stat.ML\n",
      "  Channel Title: stat.ML updates on arXiv.org\n",
      "  Channel Description: stat.ML updates on the arXiv.org e-print archive.\n",
      "- Title: Variational Bayesian Methods for a Tree-Structured Stick-Breaking Process Mixture of Gaussians\n",
      "  Description: arXiv:2405.00385v1 Announce Type: new \n",
      "Abstract: The Bayes coding algorithm for context tree source is a successful example of Bayesian tree estimation in text compression in information theory. This algorithm provides an efficient parametric representation of the posterior tree distribution and exact updating of its parameters. We apply this algorithm to a clustering task in machine learning. More specifically, we apply it to Bayesian estimation of the tree-structured stick-breaking process (TS-SBP) mixture models. For TS-SBP mixture models, only Markov chain Monte Carlo methods have been proposed so far, but any variational Bayesian methods have not been proposed yet. In this paper, we propose a variational Bayesian method that has a subroutine similar to the Bayes coding algorithm for context tree sources. We confirm its behavior by a numerical experiment on a toy example.\n",
      "  Link: https://arxiv.org/abs/2405.00385\n",
      "  PubDate: None\n",
      "  Author: Yuta Nakahara\n",
      "\n",
      "Memento: https://web.archive.org/web/20090114032246/http://arxiv.org/rss/stat.ML\n",
      "\n",
      "Memento: https://web.archive.org/web/20090506053346/http://arxiv.org/rss/stat.ML\n",
      "\n",
      "Memento: https://web.archive.org/web/20131216173215/http://arxiv.org/rss/stat.ML\n",
      "\n",
      "Memento: https://web.archive.org/web/20150521134138/http://arxiv.org/rss/stat.ML\n",
      "\n",
      "Memento: https://web.archive.org/web/20160324004042/http://arxiv.org/rss/stat.ML\n",
      "\n",
      "Memento: https://web.archive.org/web/20160930055814/http://arxiv.org/rss/stat.ML\n",
      "\n",
      "Memento: https://web.archive.org/web/20161007062431/http://arxiv.org/rss/stat.ML\n",
      "\n",
      "Memento: https://web.archive.org/web/20161014070232/http://arxiv.org/rss/stat.ML\n",
      "\n",
      "Memento: https://web.archive.org/web/20161108011452/http://arxiv.org/rss/stat.ML\n",
      "\n",
      "Memento: https://web.archive.org/web/20161115013738/http://arxiv.org/rss/stat.ML\n",
      "\n",
      "Memento: https://web.archive.org/web/20161122022036/http://arxiv.org/rss/stat.ML\n",
      "\n",
      "Memento: https://web.archive.org/web/20161129022701/http://arxiv.org/rss/stat.ML\n",
      "\n",
      "Memento: https://web.archive.org/web/20161206025149/http://arxiv.org/rss/stat.ML\n",
      "\n",
      "Memento: https://web.archive.org/web/20161209021103/http://arxiv.org/rss/stat.ML\n",
      "\n",
      "Memento: https://web.archive.org/web/20161210031013/http://arxiv.org/rss/stat.ML\n",
      "\n",
      "Memento: https://web.archive.org/web/20161211015508/http://arxiv.org/rss/stat.ML\n",
      "\n",
      "Memento: https://web.archive.org/web/20161212015409/http://arxiv.org/rss/stat.ML\n",
      "\n",
      "Memento: https://web.archive.org/web/https://distill.pub/rss.xml\n",
      "- Source: https://distill.pub/rss.xml\n",
      "  Channel Title: Distill\n",
      "  Channel Description: Homepage articles from Distill\n",
      "- Title: Understanding Convolutions on Graphs\n",
      "  Description: Understanding the building blocks and design choices of graph neural networks.\n",
      "  Link: https://distill.pub/2021/understanding-gnns\n",
      "  PubDate: Thu, 02 Sep 2021 20:0:0 Z\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20161031193317/http://distill.pub:80/rss.xml\n",
      "- Source: https://distill.pub/rss.xml\n",
      "  Channel Title: Distill\n",
      "  Channel Description: Homepage articles from Distill\n",
      "- Title: Deconvolution and Checkerboard Artifacts\n",
      "  Description: When we look very closely at images generated by neural networks, we often see a strange checkerboard pattern of artifacts.\n",
      "  Link: http://distill.pub/2016/deconv-checkerboard\n",
      "  PubDate: Thu, 08 Sep 2016 17:27:05 GMT\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20161209231314/http://distill.pub:80/rss.xml\n",
      "- Source: https://distill.pub/rss.xml\n",
      "  Channel Title: Distill\n",
      "  Channel Description: Homepage articles from Distill\n",
      "- Title: Experiments in Handwriting with a Neural Network\n",
      "  Description: Several interactive visualizations of a generative model of handwriting. Some are fun, some are serious.\n",
      "  Link: http://distill.pub/2016/handwriting\n",
      "  PubDate: Wed, 07 Dec 2016 04:39:33 GMT\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20170108002607/http://distill.pub/rss.xml\n",
      "- Source: https://distill.pub/rss.xml\n",
      "  Channel Title: Distill\n",
      "  Channel Description: Homepage articles from Distill\n",
      "- Title: Experiments in Handwriting with a Neural Network\n",
      "  Description: Several interactive visualizations of a generative model of handwriting. Some are fun, some are serious.\n",
      "  Link: http://distill.pub/2016/handwriting\n",
      "  PubDate: Wed, 07 Dec 2016 04:39:33 GMT\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20170306050047/http://distill.pub/rss.xml\n",
      "- Source: https://distill.pub/rss.xml\n",
      "  Channel Title: Distill\n",
      "  Channel Description: Homepage articles from Distill\n",
      "- Title: Experiments in Handwriting with a Neural Network\n",
      "  Description: Several interactive visualizations of a generative model of handwriting. Some are fun, some are serious.\n",
      "  Link: http://distill.pub/2016/handwriting\n",
      "  PubDate: Tue, 06 Dec 2016 00:00:00 -0800\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20170320230721/http://distill.pub/rss.xml\n",
      "- Source: https://distill.pub/rss.xml\n",
      "  Channel Title: Distill\n",
      "  Channel Description: Homepage articles from Distill\n",
      "- Title: Experiments in Handwriting with a Neural Network\n",
      "  Description: Several interactive visualizations of a generative model of handwriting. Some are fun, some are serious.\n",
      "  Link: http://distill.pub/2016/handwriting\n",
      "  PubDate: Tue, 06 Dec 2016 00:00:00 -0800\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20170321113234/http://distill.pub/rss.xml\n",
      "- Source: https://distill.pub/rss.xml\n",
      "  Channel Title: Distill\n",
      "  Channel Description: Homepage articles from Distill\n",
      "- Title: Experiments in Handwriting with a Neural Network\n",
      "  Description: Several interactive visualizations of a generative model of handwriting. Some are fun, some are serious.\n",
      "  Link: http://distill.pub/2016/handwriting\n",
      "  PubDate: Tue, 06 Dec 2016 00:00:00 -0800\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20170328004235/http://distill.pub:80/rss.xml\n",
      "- Source: https://distill.pub/rss.xml\n",
      "  Channel Title: Distill\n",
      "  Channel Description: Homepage articles from Distill\n",
      "- Title: Research Debt\n",
      "  Description: Science is a human activity. When we fail to distill and explain research, we accumulate a kind of debt...\n",
      "  Link: http://distill.pub/2017/research-debt\n",
      "  PubDate: Wed, 22 Mar 2017 00:00:00 -0700\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20170512203733/http://distill.pub/rss.xml\n",
      "- Source: https://distill.pub/rss.xml\n",
      "  Channel Title: Distill\n",
      "  Channel Description: Homepage articles from Distill\n",
      "- Title: Why Momentum Really Works\n",
      "  Description: We often think of optimization with momentum as a ball rolling down a hill. This isn't wrong, but there is much more to the story.\n",
      "  Link: http://distill.pub/2017/momentum\n",
      "  PubDate: Tue, 04 Apr 2017 00:00:00 -0700\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20170519094419/http://distill.pub/rss.xml\n",
      "- Source: https://distill.pub/rss.xml\n",
      "  Channel Title: Distill\n",
      "  Channel Description: Homepage articles from Distill\n",
      "- Title: Why Momentum Really Works\n",
      "  Description: We often think of optimization with momentum as a ball rolling down a hill. This isn't wrong, but there is much more to the story.\n",
      "  Link: http://distill.pub/2017/momentum\n",
      "  PubDate: Tue, 04 Apr 2017 00:00:00 -0700\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20170519095712/http://distill.pub:80/rss.xml\n",
      "- Source: https://distill.pub/rss.xml\n",
      "  Channel Title: Distill\n",
      "  Channel Description: Homepage articles from Distill\n",
      "- Title: Why Momentum Really Works\n",
      "  Description: We often think of optimization with momentum as a ball rolling down a hill. This isn't wrong, but there is much more to the story.\n",
      "  Link: http://distill.pub/2017/momentum\n",
      "  PubDate: Tue, 04 Apr 2017 00:00:00 -0700\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20170603213400/http://distill.pub/rss.xml\n",
      "- Source: https://distill.pub/rss.xml\n",
      "  Channel Title: Distill\n",
      "  Channel Description: Homepage articles from Distill\n",
      "- Title: Why Momentum Really Works\n",
      "  Description: We often think of optimization with momentum as a ball rolling down a hill. This isn't wrong, but there is much more to the story.\n",
      "  Link: http://distill.pub/2017/momentum\n",
      "  PubDate: Tue, 04 Apr 2017 00:00:00 -0700\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20170607183730/http://distill.pub/rss.xml\n",
      "- Source: https://distill.pub/rss.xml\n",
      "  Channel Title: Distill\n",
      "  Channel Description: Homepage articles from Distill\n",
      "- Title: Why Momentum Really Works\n",
      "  Description: We often think of optimization with momentum as a ball rolling down a hill. This isn't wrong, but there is much more to the story.\n",
      "  Link: http://distill.pub/2017/momentum\n",
      "  PubDate: Tue, 04 Apr 2017 00:00:00 -0700\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20170626171017/http://distill.pub/rss.xml\n",
      "- Source: https://distill.pub/rss.xml\n",
      "  Channel Title: Distill\n",
      "  Channel Description: Homepage articles from Distill\n",
      "- Title: Why Momentum Really Works\n",
      "  Description: We often think of optimization with momentum as a ball rolling down a hill. This isn't wrong, but there is much more to the story.\n",
      "  Link: http://distill.pub/2017/momentum\n",
      "  PubDate: Tue, 04 Apr 2017 00:00:00 -0700\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20170719020644/http://distill.pub:80/rss.xml\n",
      "- Source: https://distill.pub/rss.xml\n",
      "  Channel Title: Distill\n",
      "  Channel Description: Homepage articles from Distill\n",
      "- Title: Why Momentum Really Works\n",
      "  Description: We often think of optimization with momentum as a ball rolling down a hill. This isn't wrong, but there is much more to the story.\n",
      "  Link: http://distill.pub/2017/momentum\n",
      "  PubDate: Tue, 04 Apr 2017 00:00:00 -0700\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20170823003332/https://distill.pub/rss.xml\n",
      "- Source: https://distill.pub/rss.xml\n",
      "  Channel Title: Distill\n",
      "  Channel Description: Homepage articles from Distill\n",
      "- Title: Why Momentum Really Works\n",
      "  Description: We often think of optimization with momentum as a ball rolling down a hill. This isn't wrong, but there is much more to the story.\n",
      "  Link: http://distill.pub/2017/momentum\n",
      "  PubDate: Tue, 04 Apr 2017 00:00:00 +0000\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20170831085408/http://distill.pub/rss.xml\n",
      "- Source: https://distill.pub/rss.xml\n",
      "  Channel Title: Distill\n",
      "  Channel Description: Homepage articles from Distill\n",
      "- Title: Why Momentum Really Works\n",
      "  Description: We often think of optimization with momentum as a ball rolling down a hill. This isn't wrong, but there is much more to the story.\n",
      "  Link: http://distill.pub/2017/momentum\n",
      "  PubDate: Tue, 04 Apr 2017 00:00:00 +0000\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20170831102730/https://distill.pub/rss.xml\n",
      "- Source: https://distill.pub/rss.xml\n",
      "  Channel Title: Distill\n",
      "  Channel Description: Homepage articles from Distill\n",
      "- Title: Why Momentum Really Works\n",
      "  Description: We often think of optimization with momentum as a ball rolling down a hill. This isn't wrong, but there is much more to the story.\n",
      "  Link: http://distill.pub/2017/momentum\n",
      "  PubDate: Tue, 04 Apr 2017 00:00:00 +0000\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20170913141506/https://distill.pub/rss.xml\n",
      "- Source: https://distill.pub/rss.xml\n",
      "  Channel Title: Distill\n",
      "  Channel Description: Homepage articles from Distill\n",
      "- Title: Why Momentum Really Works\n",
      "  Description: We often think of optimization with momentum as a ball rolling down a hill. This isn't wrong, but there is much more to the story.\n",
      "  Link: http://distill.pub/2017/momentum\n",
      "  PubDate: Tue, 04 Apr 2017 00:00:00 +0000\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20170914180218/https://distill.pub/rss.xml\n",
      "- Source: https://distill.pub/rss.xml\n",
      "  Channel Title: Distill\n",
      "  Channel Description: Homepage articles from Distill\n",
      "- Title: Why Momentum Really Works\n",
      "  Description: We often think of optimization with momentum as a ball rolling down a hill. This isn't wrong, but there is much more to the story.\n",
      "  Link: http://distill.pub/2017/momentum\n",
      "  PubDate: Tue, 04 Apr 2017 00:00:00 +0000\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/https://bair.berkeley.edu/blog/feed.xml\n",
      "- Source: https://bair.berkeley.edu/blog/feed.xml\n",
      "  Channel Title: The Berkeley Artificial Intelligence Research Blog\n",
      "  Channel Description: The BAIR Blog\n",
      "- Title: Scaling Up Reinforcement Learning for Traffic Smoothing: A 100-AV Highway Deployment\n",
      "  Description: <!-- twitter -->\n",
      "<meta name=\"twitter:title\" content=\"Scaling Up Reinforcement Learning for Traffic Smoothing: A 100-AV Highway Deployment\" />\n",
      "\n",
      "<meta name=\"twitter:card\" content=\"summary_large_image\" />\n",
      "\n",
      "<meta name=\"twitter:image\" content=\"https://bair.berkeley.edu/static/blog/rl_av_smoothing/megavandertest.png\" />\n",
      "\n",
      "<meta name=\"keywords\" content=\"reinforcement learning, RL, autonomous vehicles, AV, traffic\" />\n",
      "\n",
      "<meta name=\"description\" content=\"The BAIR Blog\" />\n",
      "\n",
      "<meta name=\"author\" content=\"Nathan Lichtlé, Kathy Jang, Eugene Vinitsky, Adit Shah, Jonathan W. Lee, Alexandre M. Bayen\" />\n",
      "\n",
      "<title>Training Diffusion Models with Reinforcement Learning</title>\n",
      "\n",
      "<video autoplay=\"\" muted=\"\" playsinline=\"\" disableRemotePlayback=\"\" loop=\"\" style=\"width: 100%; margin: 0; padding: 0; outline: none; border: none; background: transparent; display: block; border-radius: 5px\" cover=\"https://bair.berkeley.edu/static/blog/rl_av_smoothing/megavandertest.png\">\n",
      "    <source src=\"https://bair.berkeley.edu/static/blog/rl_av_smoothing/megavandertest.mp4\" type=\"video/mp4\" />\n",
      "</video>\n",
      "\n",
      "<p style=\"margin-top: 20px;\">\n",
      "    <b>We deployed 100 reinforcement learning (RL)-controlled cars into rush-hour highway traffic to smooth congestion and reduce fuel consumption for everyone.</b> Our goal is to tackle <a href=\"https://www.youtube.com/watch?v=TNokBgtSUvQ\" target=\"_blank\">\"stop-and-go\" waves</a>, those frustrating slowdowns and speedups that usually have no clear cause but lead to congestion and significant energy waste. To train efficient flow-smoothing controllers, we built fast, data-driven simulations that RL agents interact with, learning to maximize energy efficiency while maintaining throughput and operating safely around human drivers.\n",
      "</p>\n",
      "<p>    \n",
      "    Overall, a small proportion of well-controlled autonomous vehicles (AVs) is enough to significantly improve traffic flow and fuel efficiency for all drivers on the road. Moreover, the trained controllers are designed to be deployable on most modern vehicles, operating in a decentralized manner and relying on standard radar sensors. In our <a href=\"https://ieeexplore.ieee.org/document/10858625\" target=\"_blank\">latest paper</a>, we explore the challenges of deploying RL controllers on a large-scale, from simulation to the field, during this 100-car experiment.\n",
      "</p>\n",
      "\n",
      "<!--more-->\n",
      "\n",
      "<h2 id=\"the-challenges-of-phantom-jams\">The challenges of phantom jams</h2>\n",
      "\n",
      "<p style=\"text-align: center; margin-top: 50px;\">\n",
      "    <img src=\"https://bair.berkeley.edu/static/blog/rl_av_smoothing/highway_wave.gif\" width=\"80%\" style=\"width: 80%; border-radius: 5px;\" />\n",
      "    <br />\n",
      "    <i>A stop-and-go wave moving backwards through highway traffic.</i>\n",
      "</p>\n",
      "\n",
      "<p>If you drive, you’ve surely experienced the frustration of stop-and-go waves, those seemingly inexplicable traffic slowdowns that appear out of nowhere and then suddenly clear up. These waves are often caused by small fluctuations in our driving behavior that get amplified through the flow of traffic. We naturally adjust our speed based on the vehicle in front of us. If the gap opens, we speed up to keep up. If they brake, we also slow down. But due to our nonzero reaction time, we might brake just a bit harder than the vehicle in front. The next driver behind us does the same, and this keeps amplifying. Over time, what started as an insignificant slowdown turns into a full stop further back in traffic. These waves move backward through the traffic stream, leading to significant drops in energy efficiency due to frequent accelerations, accompanied by increased CO<sub>2</sub> emissions and accident risk.</p>\n",
      "\n",
      "<p>And this isn’t an isolated phenomenon! These waves are ubiquitous on busy roads when the traffic density exceeds a critical threshold. So how can we address this problem? Traditional approaches like ramp metering and variable speed limits attempt to manage traffic flow, but they often require costly infrastructure and centralized coordination. A more scalable approach is to use AVs, which can dynamically adjust their driving behavior in real-time. However, simply inserting AVs among human drivers isn’t enough: they must also drive in a smarter way that makes traffic better for everyone, which is where RL comes in.</p>\n",
      "\n",
      "<p style=\"text-align: justify; margin-top: 50px;\">\n",
      "    <img src=\"https://bair.berkeley.edu/static/blog/rl_av_smoothing/fundamental_diagram.png\" width=\"80%\" style=\"display: block; margin: auto\" />\n",
      "    <br />\n",
      "    <i><b>Fundamental diagram of traffic flow.</b> The number of cars on the road (density) affects how much traffic is moving forward (flow). At low density, adding more cars increases flow because more vehicles can pass through. But beyond a critical threshold, cars start blocking each other, leading to congestion, where adding more cars actually slows down overall movement.</i>\n",
      "</p>\n",
      "\n",
      "<h2 id=\"reinforcement-learning-for-wave-smoothing-avs\">Reinforcement learning for wave-smoothing AVs</h2>\n",
      "\n",
      "<p>RL is a powerful control approach where an agent learns to maximize a reward signal through interactions with an environment. The agent collects experience through trial and error, learns from its mistakes, and improves over time. In our case, the environment is a mixed-autonomy traffic scenario, where AVs learn driving strategies to dampen stop-and-go waves and reduce fuel consumption for both themselves and nearby human-driven vehicles.</p>\n",
      "\n",
      "<p>Training these RL agents requires fast simulations with realistic traffic dynamics that can replicate highway stop-and-go behavior. To achieve this, we leveraged experimental data collected on Interstate 24 (I-24) near Nashville, Tennessee, and used it to build simulations where vehicles replay highway trajectories, creating unstable traffic that AVs driving behind them learn to smooth out.</p>\n",
      "\n",
      "<p style=\"text-align: center; margin-top: 50px;\">\n",
      "    <video autoplay=\"\" muted=\"\" playsinline=\"\" disableRemotePlayback=\"\" loop=\"\" style=\"width: 100%; margin: 0; padding: 0; outline: none; border: none; background: transparent; display: block; border-radius: 5px\" cover=\"https://bair.berkeley.edu/static/blog/rl_av_smoothing/simulation.png\">\n",
      "        <source src=\"https://bair.berkeley.edu/static/blog/rl_av_smoothing/simulation.mp4\" type=\"video/mp4\" />\n",
      "    </video>\n",
      "    <br />\n",
      "    <i>Simulation replaying a highway trajectory that exhibits several stop-and-go waves.</i>\n",
      "</p>\n",
      "\n",
      "<p>We designed the AVs with deployment in mind, ensuring that they can operate using only basic sensor information about themselves and the vehicle in front. The observations consist of the AV’s speed, the speed of the leading vehicle, and the space gap between them. Given these inputs, the RL agent then prescribes either an instantaneous acceleration or a desired speed for the AV. The key advantage of using only these local measurements is that the RL controllers can be deployed on most modern vehicles in a decentralized way, without requiring additional infrastructure.</p>\n",
      "\n",
      "<h3 id=\"reward-design\">Reward design</h3>\n",
      "\n",
      "<p>The most challenging part is designing a reward function that, when maximized, aligns with the different objectives that we desire the AVs to achieve:</p>\n",
      "\n",
      "<ul>\n",
      "  <li><strong>Wave smoothing:</strong> Reduce stop-and-go oscillations.</li>\n",
      "  <li><strong>Energy efficiency:</strong> Lower fuel consumption for all vehicles, not just AVs.</li>\n",
      "  <li><strong>Safety:</strong> Ensure reasonable following distances and avoid abrupt braking.</li>\n",
      "  <li><strong>Driving comfort:</strong> Avoid aggressive accelerations and decelerations.</li>\n",
      "  <li><strong>Adherence to human driving norms:</strong> Ensure a “normal” driving behavior that doesn’t make surrounding drivers uncomfortable.</li>\n",
      "</ul>\n",
      "\n",
      "<p>Balancing these objectives together is difficult, as suitable coefficients for each term must be found. For instance, if minimizing fuel consumption dominates the reward, RL AVs learn to come to a stop in the middle of the highway because that is energy optimal. To prevent this, we introduced dynamic minimum and maximum gap thresholds to ensure safe and reasonable behavior while optimizing fuel efficiency. We also penalized the fuel consumption of human-driven vehicles behind the AV to discourage it from learning a selfish behavior that optimizes energy savings for the AV at the expense of surrounding traffic. Overall, we aim to strike a balance between energy savings and having a reasonable and safe driving behavior.</p>\n",
      "\n",
      "<h3 id=\"simulation-results\">Simulation results</h3>\n",
      "\n",
      "<p style=\"text-align: center; margin-top: 0;\">\n",
      "    <img src=\"https://bair.berkeley.edu/static/blog/rl_av_smoothing/gap_thresholds.png\" width=\"80%\" />\n",
      "    <br />\n",
      "    <i>Illustration of the dynamic minimum and maximum gap thresholds, within which the AV can operate freely to smooth traffic as efficiently as possible.</i>\n",
      "</p>\n",
      "\n",
      "<p>The typical behavior learned by the AVs is to maintain slightly larger gaps than human drivers, allowing them to absorb upcoming, possibly abrupt, traffic slowdowns more effectively. In simulation, this approach resulted in significant fuel savings of up to 20% across all road users in the most congested scenarios, with fewer than 5% of AVs on the road. And these AVs don’t have to be special vehicles! They can simply be standard consumer cars equipped with a smart adaptive cruise control (ACC), which is what we tested at scale.</p>\n",
      "\n",
      "<p style=\"text-align: justify; margin-top: 50px;\">\n",
      "    <img src=\"https://bair.berkeley.edu/static/blog/rl_av_smoothing/wave_smoothing.png\" width=\"100%\" style=\"display: block; margin: auto;\" />\n",
      "    <i>\n",
      "    <b>Smoothing behavior of RL AVs.</b> Red: a human trajectory from the dataset. Blue: successive AVs in the platoon, where AV 1 is the closest behind the human trajectory. There is typically between 20 and 25 human vehicles between AVs. Each AV doesn’t slow down as much or accelerate as fast as its leader, leading to decreasing wave amplitude over time and thus energy savings. \n",
      "    </i>\n",
      "</p>\n",
      "\n",
      "<h2 id=\"100-av-field-test-deploying-rl-at-scale\">100 AV field test: deploying RL at scale</h2>\n",
      "\n",
      "<div style=\"display: flex; justify-content: center; width: 100%; margin-top: 30px;\">\n",
      "    <img src=\"https://bair.berkeley.edu/static/blog/rl_av_smoothing/parking_lot.png\" style=\"height: 300px; object-fit: cover; width: 50%; border-top-left-radius: 5px; border-bottom-left-radius: 5px;\" />\n",
      "    <img src=\"https://bair.berkeley.edu/static/blog/rl_av_smoothing/parking_lot_drone.png\" style=\"height: 300px; object-fit: cover; width: 50%; border-top-right-radius: 5px; border-bottom-right-radius: 5px;\" />\n",
      "</div>\n",
      "<p style=\"text-align: center; margin-top: 10px;\">\n",
      "    <i style=\"font-size: 0.9rem;\">Our 100 cars parked at our operational center during the experiment week.</i>\n",
      "</p>\n",
      "\n",
      "<p>Given the promising simulation results, the natural next step was to bridge the gap from simulation to the highway. We took the trained RL controllers and deployed them on 100 vehicles on the I-24 during peak traffic hours over several days. This large-scale experiment, which we called the MegaVanderTest, is the largest mixed-autonomy traffic-smoothing experiment ever conducted.</p>\n",
      "\n",
      "<p>Before deploying RL controllers in the field, we trained and evaluated them extensively in simulation and validated them on the hardware. Overall, the steps towards deployment involved:</p>\n",
      "\n",
      "<ul>\n",
      "  <li><strong>Training in data-driven simulations:</strong> We used highway traffic data from I-24 to create a training environment with realistic wave dynamics, then validate the trained agent’s performance and robustness in a variety of new traffic scenarios.</li>\n",
      "  <li><strong>Deployment on hardware:</strong> After being validated in robotics software, the trained controller is uploaded onto the car and is able to control the set speed of the vehicle. We operate through the vehicle’s on-board cruise control, which acts as a lower-level safety controller.</li>\n",
      "  <li><strong>Modular control framework:</strong> One key challenge during the test was not having access to the leading vehicle information sensors. To overcome this, the RL controller was integrated into a hierarchical system, the MegaController, which combines a speed planner guide that accounts for downstream traffic conditions, with the RL controller as the final decision maker.</li>\n",
      "  <li><strong>Validation on hardware:</strong> The RL agents were designed to operate in an environment where most vehicles were human-driven, requiring robust policies that adapt to unpredictable behavior. We verify this by driving the RL-controlled vehicles on the road under careful human supervision, making changes to the control based on feedback.</li>\n",
      "</ul>\n",
      "\n",
      "<div style=\"display: flex; justify-content: space-around; width: 100%; margin: 30px 0;\">\n",
      "    <div style=\"display: flex; flex-direction: column; align-items: center; width: 48%;\">\n",
      "        <img src=\"https://bair.berkeley.edu/static/blog/rl_av_smoothing/raspberry_pi.png\" style=\"height: 200px; object-fit: cover; width: 100%; border-radius: 5px;\" />\n",
      "        <i style=\"font-size: 0.9rem; display: block; text-align: center; margin-top: 5px;\">Each of the 100 cars is connected to a Raspberry Pi, on which the RL controller (a small neural network) is deployed.</i>\n",
      "    </div>\n",
      "    <div style=\"display: flex; flex-direction: column; align-items: center; width: 48%;\">\n",
      "        <img src=\"https://bair.berkeley.edu/static/blog/rl_av_smoothing/acc.png\" style=\"height: 200px; object-fit: cover; width: 100%; border-radius: 5px;\" />\n",
      "        <i style=\"font-size: 0.9rem; display: block; text-align: center; margin-top: 5px;\">The RL controller directly controls the onboard adaptive cruise control (ACC) system, setting its speed and desired following distance.</i>\n",
      "    </div>\n",
      "</div>\n",
      "\n",
      "<p>Once validated, the RL controllers were deployed on 100 cars and driven on I-24 during morning rush hour. Surrounding traffic was unaware of the experiment, ensuring unbiased driver behavior. Data was collected during the experiment from dozens of overhead cameras placed along the highway, which led to the extraction of millions of individual vehicle trajectories through a computer vision pipeline. Metrics computed on these trajectories indicate a trend of reduced fuel consumption around AVs, as expected from simulation results and previous smaller validation deployments. For instance, we can observe that the closer people are driving behind our AVs, the less fuel they appear to consume on average (which is calculated using a calibrated energy model):</p>\n",
      "\n",
      "<p style=\"text-align: center; margin-top: 0;\">\n",
      "    <img src=\"https://bair.berkeley.edu/static/blog/rl_av_smoothing/fuel_data.png\" width=\"80%\" />\n",
      "    <br />\n",
      "    <i>Average fuel consumption as a function of distance behind the nearest engaged RL-controlled AV in the downstream traffic. As human drivers get further away behind AVs, their average fuel consumption increases.</i>\n",
      "</p>\n",
      "\n",
      "<p>Another way to measure the impact is to measure the variance of the speeds and accelerations: the lower the variance, the less amplitude the waves should have, which is what we observe from the field test data. Overall, although getting precise measurements from a large amount of camera video data is complicated, we observe a trend of 15 to 20% of energy savings around our controlled cars.</p>\n",
      "\n",
      "<p style=\"text-align: center; margin-top: 0;\">\n",
      "    <img src=\"https://bair.berkeley.edu/static/blog/rl_av_smoothing/data_scatter.png\" width=\"50%\" />\n",
      "    <br />\n",
      "    <i>Data points from all vehicles on the highway over a single day of the experiment, plotted in speed-acceleration space. The cluster to the left of the red line represents congestion, while the one on the right corresponds to free flow. We observe that the congestion cluster is smaller when AVs are present, as measured by computing the area of a soft convex envelope or by fitting a Gaussian kernel.</i>\n",
      "</p>\n",
      "\n",
      "<h2 id=\"final-thoughts\">Final thoughts</h2>\n",
      "\n",
      "<p>The 100-car field operational test was decentralized, with no explicit cooperation or communication between AVs, reflective of current autonomy deployment, and bringing us one step closer to smoother, more energy-efficient highways. Yet, there is still vast potential for improvement. Scaling up simulations to be faster and more accurate with better human-driving models is crucial for bridging the simulation-to-reality gap. Equipping AVs with additional traffic data, whether through advanced sensors or centralized planning, could further improve the performance of the controllers. For instance, while multi-agent RL is promising for improving cooperative control strategies, it remains an open question how enabling explicit communication between AVs over 5G networks could further improve stability and further mitigate stop-and-go waves. Crucially, our controllers integrate seamlessly with existing adaptive cruise control (ACC) systems, making field deployment feasible at scale. The more vehicles equipped with smart traffic-smoothing control, the fewer waves we’ll see on our roads, meaning less pollution and fuel savings for everyone!</p>\n",
      "\n",
      "<hr />\n",
      "\n",
      "<p><i>Many contributors took part in making the MegaVanderTest happen!  The full list is available on the <a href=\"https://circles-consortium.github.io/\" target=\"_blank\">CIRCLES project</a> page, along with more details about the project.</i></p>\n",
      "\n",
      "<p><i><b>Read more: <a href=\"https://ieeexplore.ieee.org/document/10858625\" target=\"_blank\">[paper]</a></b></i></p>\n",
      "\n",
      "  Link: http://bair.berkeley.edu/blog/2025/03/25/rl-av-smoothing/\n",
      "  PubDate: Tue, 25 Mar 2025 02:00:00 -0700\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20170624072123/http://bair.berkeley.edu:80/blog/feed.xml\n",
      "- Source: https://bair.berkeley.edu/blog/feed.xml\n",
      "  Channel Title: The Berkeley Artificial Intelligence Research Blog\n",
      "  Channel Description: The BAIR Blog\n",
      "- Title: Learning to Reason with Neural Module Networks\n",
      "  Description: <p>(Joint work with Ronghang Hu, Marcus Rohrbach, Trevor Darrell, Dan Klein and\n",
      "Kate Saenko.)</p>\n",
      "\n",
      "<p>Suppose we’re building a household robot, and want it to be able to answer\n",
      "questions about its surroundings. We might ask questions like these:</p>\n",
      "\n",
      "<p style=\"text-align:center;\"> \n",
      "<img src=\"http://bair.berkeley.edu/blog/assets/nmns/examples.jpg\" width=\"600\" /> \n",
      "</p>\n",
      "\n",
      "<p>How can we ensure that the robot can answer these questions correctly? The\n",
      "standard approach in deep learning is to collect a large dataset of questions,\n",
      "images, and answers, and train a single neural network to map directly from\n",
      "questions and images to answers.  If most questions look like the one on the\n",
      "left, we have a familiar image recognition problem, and these kinds of\n",
      "monolithic approaches are quite effective:</p>\n",
      "\n",
      "<p style=\"text-align:center;\"> \n",
      "<img src=\"http://bair.berkeley.edu/blog/assets/nmns/cat_pred.jpg\" width=\"600\" /> \n",
      "</p>\n",
      "\n",
      "<p>But things don’t work quite so well for questions like the one on the\n",
      "right:</p>\n",
      "\n",
      "<p style=\"text-align:center;\"> \n",
      "<img src=\"http://bair.berkeley.edu/blog/assets/nmns/clevr_pred.jpg\" width=\"600\" /> \n",
      "</p>\n",
      "\n",
      "<p>Here the network we trained has given up and guessed the most common color in\n",
      "the image.  What makes this question so much harder? Even though the image is\n",
      "cleaner, the question requires many steps of <em>reasoning</em>: rather than\n",
      "simply recognizing the main object in the image, the model must first find the\n",
      "blue cylinder, locate the other object with the same size, and then determine\n",
      "its color. This is a complicated computation, and it’s a computation\n",
      "<em>specific to the question that was asked</em>. Different questions require\n",
      "different sequences of steps to solve.</p>\n",
      "\n",
      "<p>The dominant paradigm in deep learning is a \"one size fits all\" approach: for\n",
      "whatever problem we’re trying to solve, we write down a fixed model architecture\n",
      "that we hope can capture everything about the relationship between the input and\n",
      "output, and learn parameters for that fixed model from labeled training\n",
      "data.</p>\n",
      "\n",
      "<p>But real-world reasoning doesn’t work this way: it involves a variety of\n",
      "different capabilities, combined and synthesized in new ways for every new \n",
      "challenge we encounter in the wild. What we need is a model that can\n",
      "<em>dynamically</em> determine how to reason about the problem in front of it—a\n",
      "network that can choose its own structure on the fly. In this post, we’ll talk\n",
      "about a new class of models we call <strong>neural module networks</strong>\n",
      "(NMNs), which incorporate this more flexible approach to problem-solving while\n",
      "preserving the expressive power that makes deep learning so effective.</p>\n",
      "\n",
      "<!--more-->\n",
      "<hr />\n",
      "\n",
      "<p>Earlier, we noticed that there are three different steps involved in answering\n",
      "the question above: finding a blue cylinder, finding something else the same\n",
      "size, and determining its color. We can draw this schematically like:</p>\n",
      "\n",
      "<p style=\"text-align:center;\"> \n",
      "<img src=\"http://bair.berkeley.edu/blog/assets/nmns/layout1.jpg\" width=\"600\" /> \n",
      "</p>\n",
      "\n",
      "<p>A different question might involve a different series of steps. If we ask \"how\n",
      "many things are the same size as the ball?\", we might have something like:</p>\n",
      "\n",
      "<p style=\"text-align:center;\"> \n",
      "<img src=\"http://bair.berkeley.edu/blog/assets/nmns/layout2.jpg\" width=\"600\" /> \n",
      "</p>\n",
      "\n",
      "<p>Basic operations like \"compare size\" are shared between questions, but they\n",
      "get used in different ways. The key idea behind NMNs is to make this sharing\n",
      "explicit: we use two different network structures to answer the two questions\n",
      "above, but we share weights between pieces of networks that involve the same\n",
      "basic operations:</p>\n",
      "\n",
      "<p style=\"text-align:center;\"> \n",
      "<img src=\"http://bair.berkeley.edu/blog/assets/nmns/tying.jpg\" width=\"600\" /> \n",
      "</p>\n",
      "\n",
      "<p>How do we learn a model like this? Rather than training a single large network\n",
      "on lots of input/output pairs, we actually train a huge number of different\n",
      "networks at the same time, while tying their parameters together where\n",
      "appropriate:</p>\n",
      "\n",
      "<p style=\"text-align:center;\"> \n",
      "<img src=\"http://bair.berkeley.edu/blog/assets/nmns/training.jpg\" width=\"600\" /> \n",
      "</p>\n",
      "\n",
      "<p>(Several recent deep learning frameworks, including DyNet and TensorFlow Fold,\n",
      "were explicitly designed with this kind of dynamic computation in mind.)</p>\n",
      "\n",
      "<p>What we get at the end of the training process is not a single deep network,\n",
      "but rather a collection of neural \"modules\", each of which implements a single\n",
      "step of reasoning. When we want to use our trained model on a new problem\n",
      "instance, we can assemble these modules dynamically to produce a new network\n",
      "structure tailored to that problem.</p>\n",
      "\n",
      "<p>One of the remarkable things about this process is that we don’t need to provide\n",
      "any low-level supervision for individual modules: the model never sees an\n",
      "isolated example of blue object or a \"left-of\" relationship. Modules are learned\n",
      "only inside larger composed structures, with only (question, answer) pairs as\n",
      "supervision. But the training procedure is able to <em>automatically</em> infer the\n",
      "correct relationship between pieces of structure and the computations they’re\n",
      "responsible for:</p>\n",
      "\n",
      "<p style=\"text-align:center;\"> \n",
      "<img src=\"http://bair.berkeley.edu/blog/assets/nmns/exploded.jpg\" width=\"600\" /> \n",
      "</p>\n",
      "\n",
      "<p>This same process works for answering questions about more realistic\n",
      "photographs, and even other knowledge sources like databases:</p>\n",
      "\n",
      "<p>\n",
      "<table>\n",
      "<tr>\n",
      "<td style=\"border: 0; text-align: center\">\n",
      "<img src=\"http://bair.berkeley.edu/blog/assets/nmns/vqa.jpg\" width=\"300\" />\n",
      "</td>\n",
      "<td style=\"border: 0; text-align: center\">\n",
      "<img src=\"http://bair.berkeley.edu/blog/assets/nmns/geo.jpg\" width=\"320\" />\n",
      "</td>\n",
      "</tr>\n",
      "</table>\n",
      "</p>\n",
      "\n",
      "<hr />\n",
      "\n",
      "<p>The key ingredient in this whole process is a collection of high-level\n",
      "\"reasoning blueprints\" like the ones above. These blueprints tell us how the\n",
      "network for each question should be laid out, and how different questions relate\n",
      "to one another. But where do the blueprints come from?</p>\n",
      "\n",
      "<p>In our initial work on these models (1, 2), we drew on a surprising\n",
      "connection between the problem of designing question-specific neural networks\n",
      "and the problem of analyzing grammatical structure. Linguists have long observed\n",
      "that the grammar of a question is closely related to the sequence of\n",
      "computational steps needed to answer it. Thanks to recent advances in natural\n",
      "language processing, we can use off-the-shelf tools for grammatical analysis to\n",
      "provide approximate versions of these blueprints automatically.</p>\n",
      "\n",
      "<p>But finding exactly the right mapping from linguistic structure to network\n",
      "structure is still a challenging problem, and the conversion process is prone to\n",
      "errors. In later work, rather than relying on this kind of linguistic analysis,\n",
      "we instead turned to data produced by human experts who directly labeled a\n",
      "collection of questions with idealized reasoning blueprints (3). By learning to\n",
      "imitate these humans, our model was able to improve the quality of its\n",
      "predictions substantially. Most surprisingly, when we took a model trained to\n",
      "imitate experts, but allowed it to explore its own modifications to these expert\n",
      "predictions, it was able to find even better solutions than experts on a \n",
      "variety of questions.</p>\n",
      "\n",
      "<hr />\n",
      "\n",
      "<p>Despite the remarkable success of deep learning methods in recent years, many\n",
      "problems&mdash;including few-shot learning and complex reasoning&mdash;remain a\n",
      "challenge. But these are exactly the sorts of problems where more structured\n",
      "classical techniques like semantic parsing and program induction really shine.\n",
      "Neural module networks give us the best of both worlds: the flexibility and data\n",
      "efficiency of discrete compositionality, combined with the representational\n",
      "power of deep networks. NMNs have already seen a number of successes for visual\n",
      "and textual reasoning tasks, and we’re excited to start applying them to other\n",
      "AI problems as well.</p>\n",
      "\n",
      "<hr />\n",
      "\n",
      "<p>This post is based on the following papers:</p>\n",
      "\n",
      "<ol>\n",
      "  <li>\n",
      "    <p>Neural Module Networks. Jacob Andreas, Marcus Rohrbach, Trevor Darrell and\n",
      "Dan Klein.  CVPR 2016. (<a href=\"https://arxiv.org/abs/1511.02799\">arXiv</a>)</p>\n",
      "  </li>\n",
      "  <li>\n",
      "    <p>Learning to Compose Neural Networks for Question Answering.\n",
      "Jacob Andreas, Marcus Rohrbach, Trevor Darrell and Dan Klein.\n",
      "NAACL 2016. (<a href=\"https://arxiv.org/abs/1601.01705\">arXiv</a>)</p>\n",
      "  </li>\n",
      "  <li>\n",
      "    <p>Modeling Relationships in Referential Expressions with Compositional Modular\n",
      "Networks. Ronghang Hu, Marcus Rohrbach, Jacob Andreas, Trevor Darrell and Kate\n",
      "Saenko. CVPR 2017. (<a href=\"https://arxiv.org/abs/1611.09978\">arXiv</a>)</p>\n",
      "  </li>\n",
      "</ol>\n",
      "\n",
      "<p>Images are from the <a href=\"http://www.visualqa.org/\">VQA</a> and <a href=\"http://cs.stanford.edu/people/jcjohns/clevr/\">CLEVR</a> datasets.</p>\n",
      "\n",
      "\n",
      "  Link: http://bair.berkeley.edu/blog/2017/06/20/learning-to-reason-with-neural-module-networks/\n",
      "  PubDate: Tue, 20 Jun 2017 10:00:00 +0000\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20170824091901/http://bair.berkeley.edu:80/blog/feed.xml\n",
      "- Source: https://bair.berkeley.edu/blog/feed.xml\n",
      "  Channel Title: The Berkeley Artificial Intelligence Research Blog\n",
      "  Channel Description: The BAIR Blog\n",
      "- Title: High Quality 3D Object Reconstruction from a Single Color Image\n",
      "  Description: <p>Digitally reconstructing 3D geometry from images is a core problem in computer vision. There are various applications, such as movie productions, content generation for video games, virtual and augmented reality, 3D printing and many more. The task discussed in this blog post is reconstructing high quality 3D geometry from a single color image of an object as shown in the figure below.</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/blog/assets/hsp/image_0.png\" width=\"600\" />\n",
      "</p>\n",
      "\n",
      "<p>Humans have the ability to effortlessly reason about the shapes of objects and scenes even if we only see a single image. Note that the binocular arrangement of our eyes allows us to perceive depth, but it is not required to understand 3D geometry. Even if we only see a photograph of an object we have a good understanding of its shape. Moreover, we are also able to reason about the unseen parts of objects such as the back, which is an important ability for grasping objects. The question which immediately arises is how are humans able to reason about geometry from a single image? And in terms of artificial intelligence: how can we teach machines this ability?</p>\n",
      "\n",
      "<!--more-->\n",
      "\n",
      "<h1 id=\"shape-spaces\">Shape Spaces</h1>\n",
      "\n",
      "<p>The basic principle used to reconstruct geometry from ambiguous input is the fact that shapes are not arbitrary, and hence some shapes are likely, and some very unlikely. In general surfaces tend to be smooth. In man-made environments they are often piece-wise planar. For objects high level rules apply. For example airplanes very commonly have a fuselage with two main wings attached on each side and on the back a vertical stabilizer. Humans are able to acquire this knowledge by observing the world with their eyes and interacting with the world using their hands. In computer vision the fact that shapes are not arbitrary allows us to describe all possible shapes of an object class or multiple object classes as a low dimensional shape space, which is acquired from large collections of example shapes.</p>\n",
      "\n",
      "<h2 id=\"voxel-prediction-using-cnns\">Voxel Prediction Using CNNs</h2>\n",
      "\n",
      "<p>One of the most recent lines of work for 3D reconstruction [<a href=\"https://arxiv.org/abs/1604.00449\">Choy et al. ECCV 2016</a>, <a href=\"https://arxiv.org/abs/1603.08637\">Girdhar et al. ECCV 2016</a>] utilizes convolutional neural networks (CNNs) to predict the shape of objects as a 3D occupancy volume. The 3D output volume is subdivided into volume elements, called voxels, and for each voxel an assignment to be either occupied or free space, i.e. the interior or exterior of the object respectively, is determined. The input is commonly given as a single color image which depicts the object, and the CNN predicts an occupancy volume using an up-convolutional decoder architecture. The network is trained end-to-end and supervised with known ground truth occupancy volumes which are acquired from synthetic CAD model datasets. Using this 3D representation and CNNs, models which are able to fit into a variety of object classes can be learned.</p>\n",
      "\n",
      "<h1 id=\"hierarchical-surface-prediction\">Hierarchical Surface Prediction</h1>\n",
      "\n",
      "<p><img src=\"http://bair.berkeley.edu/blog/assets/hsp/image_1.png\" class=\"stretch-center\" /></p>\n",
      "\n",
      "<p>The main shortcoming with predicting occupancy volumes using a CNN is that the output space is three dimensional and hence has cubic growth with respect to increased resolution. This problem prevents the works mentioned above from predicting high quality geometry and is therefore restricted to coarse resolution voxel grids, e.g. 32<sup>3</sup> (c.f. figure above). In our work we argue that this is an unnecessary restriction given that surfaces are actually only two dimensional. We exploit the two dimensional nature of surfaces by hierarchically predicting fine resolution voxels only where a surface is expected judging from the low resolution prediction. The basic idea is closely related to octree representations which are often used in multi-view stereo and depth map fusion to represent high resolution geometry.</p>\n",
      "\n",
      "<h2 id=\"method\">Method</h2>\n",
      "\n",
      "<p>The basic 3D prediction pipeline takes a color image as input which gets first encoded into a low dimensional representation using a convolutional encoder. This low dimensional representation then gets decoded into a 3D occupancy volume. The main idea of our method, called hierarchical surface prediction (HSP), is to start decoding by predicting low resolution voxels. However, in contrast to the standard approach where each voxel would get classified into either free or occupied space, we use three classes: free space, occupied space, and boundary. This allows us to analyze the outputs at low resolution and only predict a higher resolution of the parts of the volume where there is evidence that it contains the surface. By iterating the refinement procedure we hierarchically predict high resolution voxel grids (see figure below). For more details about the method we refer the reader to our tech report [<a href=\"https://arxiv.org/abs/1704.00710\">Häne et al. arXiv 2017</a>].</p>\n",
      "\n",
      "<p><img src=\"http://bair.berkeley.edu/blog/assets/hsp/image_2.png\" class=\"stretch-center\" /></p>\n",
      "\n",
      "<h2 id=\"experiments\">Experiments</h2>\n",
      "\n",
      "<p>Our experiments are mainly conducted on the synthetic <a href=\"https://shapenet.org/\">ShapeNet</a> dataset [<a href=\"https://arxiv.org/abs/1512.03012\">Chang et al. arXiv 2015</a>]. The main task we studied is predicting high resolution geometry from a single color image. We compare our method to two baselines which we call low resolution hard (LR hard) and low resolution soft (LR soft). These baselines predict at the same coarse resolution of 32<sup>3</sup> but differ in how the training data is generated. The LR hard baseline uses binary assignments for the voxels. All voxels are labeled as occupied if at least one of the corresponding high resolution voxels is occupied. The LR soft baseline uses fractional assignments reflecting the percentage of occupied voxels in the corresponding high resolution voxels. Our method, HSP predicts at a resolution of 256<sup>3</sup>. The results in the figures below show the benefits in terms of surface quality and completeness of the high resolution prediction compared to the low resolution baselines. Quantitative results and more experiments can be found in our tech report.</p>\n",
      "\n",
      "<p><img src=\"http://bair.berkeley.edu/blog/assets/hsp/image_3.png\" class=\"stretch-center\" /></p>\n",
      "\n",
      "<p><img src=\"http://bair.berkeley.edu/blog/assets/hsp/image_4.png\" class=\"stretch-center\" /></p>\n",
      "\n",
      "<p>I would like to thank Shubham Tulsiani and Jitendra Malik for their valuable feedback.</p>\n",
      "\n",
      "<p><strong>This blog post is based on the tech report:</strong></p>\n",
      "\n",
      "<ul>\n",
      "  <li>Hierarchical Surface Prediction for 3D Object Reconstruction, C. Häne, S.Tulsiani, J.Malik, ArXiv 2017</li>\n",
      "</ul>\n",
      "\n",
      "\n",
      "  Link: http://bair.berkeley.edu/blog/2017/08/23/high-quality-3d-obj-reconstruction/\n",
      "  PubDate: Wed, 23 Aug 2017 09:00:00 +0000\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20170830170301/http://bair.berkeley.edu/blog/feed.xml\n",
      "- Source: https://bair.berkeley.edu/blog/feed.xml\n",
      "  Channel Title: The Berkeley Artificial Intelligence Research Blog\n",
      "  Channel Description: The BAIR Blog\n",
      "- Title: High Quality 3D Object Reconstruction from a Single Color Image\n",
      "  Description: <p>Digitally reconstructing 3D geometry from images is a core problem in computer vision. There are various applications, such as movie productions, content generation for video games, virtual and augmented reality, 3D printing and many more. The task discussed in this blog post is reconstructing high quality 3D geometry from a single color image of an object as shown in the figure below.</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/blog/assets/hsp/image_0.png\" width=\"600\" />\n",
      "</p>\n",
      "\n",
      "<p>Humans have the ability to effortlessly reason about the shapes of objects and scenes even if we only see a single image. Note that the binocular arrangement of our eyes allows us to perceive depth, but it is not required to understand 3D geometry. Even if we only see a photograph of an object we have a good understanding of its shape. Moreover, we are also able to reason about the unseen parts of objects such as the back, which is an important ability for grasping objects. The question which immediately arises is how are humans able to reason about geometry from a single image? And in terms of artificial intelligence: how can we teach machines this ability?</p>\n",
      "\n",
      "<!--more-->\n",
      "\n",
      "<h1 id=\"shape-spaces\">Shape Spaces</h1>\n",
      "\n",
      "<p>The basic principle used to reconstruct geometry from ambiguous input is the fact that shapes are not arbitrary, and hence some shapes are likely, and some very unlikely. In general surfaces tend to be smooth. In man-made environments they are often piece-wise planar. For objects high level rules apply. For example airplanes very commonly have a fuselage with two main wings attached on each side and on the back a vertical stabilizer. Humans are able to acquire this knowledge by observing the world with their eyes and interacting with the world using their hands. In computer vision the fact that shapes are not arbitrary allows us to describe all possible shapes of an object class or multiple object classes as a low dimensional shape space, which is acquired from large collections of example shapes.</p>\n",
      "\n",
      "<h2 id=\"voxel-prediction-using-cnns\">Voxel Prediction Using CNNs</h2>\n",
      "\n",
      "<p>One of the most recent lines of work for 3D reconstruction [<a href=\"https://arxiv.org/abs/1604.00449\">Choy et al. ECCV 2016</a>, <a href=\"https://arxiv.org/abs/1603.08637\">Girdhar et al. ECCV 2016</a>] utilizes convolutional neural networks (CNNs) to predict the shape of objects as a 3D occupancy volume. The 3D output volume is subdivided into volume elements, called voxels, and for each voxel an assignment to be either occupied or free space, i.e. the interior or exterior of the object respectively, is determined. The input is commonly given as a single color image which depicts the object, and the CNN predicts an occupancy volume using an up-convolutional decoder architecture. The network is trained end-to-end and supervised with known ground truth occupancy volumes which are acquired from synthetic CAD model datasets. Using this 3D representation and CNNs, models which are able to fit into a variety of object classes can be learned.</p>\n",
      "\n",
      "<h1 id=\"hierarchical-surface-prediction\">Hierarchical Surface Prediction</h1>\n",
      "\n",
      "<p><img src=\"http://bair.berkeley.edu/blog/assets/hsp/image_1.png\" class=\"stretch-center\" /></p>\n",
      "\n",
      "<p>The main shortcoming with predicting occupancy volumes using a CNN is that the output space is three dimensional and hence has cubic growth with respect to increased resolution. This problem prevents the works mentioned above from predicting high quality geometry and is therefore restricted to coarse resolution voxel grids, e.g. 32<sup>3</sup> (c.f. figure above). In our work we argue that this is an unnecessary restriction given that surfaces are actually only two dimensional. We exploit the two dimensional nature of surfaces by hierarchically predicting fine resolution voxels only where a surface is expected judging from the low resolution prediction. The basic idea is closely related to octree representations which are often used in multi-view stereo and depth map fusion to represent high resolution geometry.</p>\n",
      "\n",
      "<h2 id=\"method\">Method</h2>\n",
      "\n",
      "<p>The basic 3D prediction pipeline takes a color image as input which gets first encoded into a low dimensional representation using a convolutional encoder. This low dimensional representation then gets decoded into a 3D occupancy volume. The main idea of our method, called hierarchical surface prediction (HSP), is to start decoding by predicting low resolution voxels. However, in contrast to the standard approach where each voxel would get classified into either free or occupied space, we use three classes: free space, occupied space, and boundary. This allows us to analyze the outputs at low resolution and only predict a higher resolution of the parts of the volume where there is evidence that it contains the surface. By iterating the refinement procedure we hierarchically predict high resolution voxel grids (see figure below). For more details about the method we refer the reader to our tech report [<a href=\"https://arxiv.org/abs/1704.00710\">Häne et al. arXiv 2017</a>].</p>\n",
      "\n",
      "<p><img src=\"http://bair.berkeley.edu/blog/assets/hsp/image_2.png\" class=\"stretch-center\" /></p>\n",
      "\n",
      "<h2 id=\"experiments\">Experiments</h2>\n",
      "\n",
      "<p>Our experiments are mainly conducted on the synthetic <a href=\"https://shapenet.org/\">ShapeNet</a> dataset [<a href=\"https://arxiv.org/abs/1512.03012\">Chang et al. arXiv 2015</a>]. The main task we studied is predicting high resolution geometry from a single color image. We compare our method to two baselines which we call low resolution hard (LR hard) and low resolution soft (LR soft). These baselines predict at the same coarse resolution of 32<sup>3</sup> but differ in how the training data is generated. The LR hard baseline uses binary assignments for the voxels. All voxels are labeled as occupied if at least one of the corresponding high resolution voxels is occupied. The LR soft baseline uses fractional assignments reflecting the percentage of occupied voxels in the corresponding high resolution voxels. Our method, HSP predicts at a resolution of 256<sup>3</sup>. The results in the figures below show the benefits in terms of surface quality and completeness of the high resolution prediction compared to the low resolution baselines. Quantitative results and more experiments can be found in our tech report.</p>\n",
      "\n",
      "<p><img src=\"http://bair.berkeley.edu/blog/assets/hsp/image_3.png\" class=\"stretch-center\" /></p>\n",
      "\n",
      "<p><img src=\"http://bair.berkeley.edu/blog/assets/hsp/image_4.png\" class=\"stretch-center\" /></p>\n",
      "\n",
      "<p>I would like to thank Shubham Tulsiani and Jitendra Malik for their valuable feedback.</p>\n",
      "\n",
      "<p><strong>This blog post is based on the tech report:</strong></p>\n",
      "\n",
      "<ul>\n",
      "  <li>Hierarchical Surface Prediction for 3D Object Reconstruction, C. Häne, S.Tulsiani, J.Malik, ArXiv 2017</li>\n",
      "</ul>\n",
      "\n",
      "\n",
      "  Link: http://bair.berkeley.edu/blog/2017/08/23/high-quality-3d-obj-reconstruction/\n",
      "  PubDate: Wed, 23 Aug 2017 09:00:00 +0000\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20170901190406/http://bair.berkeley.edu/blog/feed.xml\n",
      "- Source: https://bair.berkeley.edu/blog/feed.xml\n",
      "  Channel Title: The Berkeley Artificial Intelligence Research Blog\n",
      "  Channel Description: The BAIR Blog\n",
      "- Title: How to Escape Saddle Points Efficiently\n",
      "  Description: <p><em>This post was initially published on <a href=\"http://www.offconvex.org/2017/07/19/saddle-efficiency/\">Off the Convex Path</a>. It is reposted here with authors’ permission.</em></p>\n",
      "\n",
      "<p>A core, emerging problem in nonconvex optimization involves the escape of saddle points.  While recent research has shown that gradient descent (GD) generically escapes saddle points asymptotically (see <a href=\"http://www.offconvex.org/2016/03/22/saddlepoints/\">Rong Ge’s</a> and <a href=\"http://www.offconvex.org/2016/03/24/saddles-again/\">Ben Recht’s</a> blog posts), the critical open problem is one of <strong>efficiency</strong> — is GD able to move past saddle points quickly, or can it be slowed down significantly?  How does the rate of escape scale with the ambient dimensionality?  In this post, we describe <a href=\"https://arxiv.org/abs/1703.00887\">our recent work with Rong Ge, Praneeth Netrapalli and Sham Kakade</a>, that provides the first provable <em>positive</em> answer to the efficiency question, showing that, rather surprisingly, GD augmented with suitable perturbations escapes saddle points efficiently; indeed, in terms of rate and dimension dependence it is almost as if the saddle points aren’t there!</p>\n",
      "\n",
      "<!--more-->\n",
      "\n",
      "<h2 id=\"perturbing-gradient-descent\">Perturbing Gradient Descent</h2>\n",
      "<p>We are in the realm of classical gradient descent (GD) — given a function $f:\\mathbb{R}^d \\to \\mathbb{R}$ we aim to minimize the function by moving in the direction of the negative gradient:</p>\n",
      "\n",
      "<script type=\"math/tex; mode=display\">x_{t+1} = x_t - \\eta \\nabla f(x_t),</script>\n",
      "\n",
      "<p>where $x_t$ are the iterates and $\\eta$ is the step size. GD is well understood theorietically in the case of convex optimization, but the general case of nonconvex optimization has been far less studied. We know that GD converges quickly to the neighborhood of stationary points (points where $\\nabla f(x) = 0$) in the nonconvex setting, but these stationary points may be local minima or, unhelpfully, local maxima or saddle points.</p>\n",
      "\n",
      "<p>Clearly GD will never move away from a stationary point if started there (even a local maximum); thus, to provide general guarantees, it is necessary to modify GD slightly to incorporate some degree of randomness. Two simple methods have been studied in the literature:</p>\n",
      "\n",
      "<ol>\n",
      "  <li>\n",
      "    <p><strong>Intermittent Perturbations</strong>: <a href=\"http://arxiv.org/abs/1503.02101\">Ge, Huang, Jin and Yuan 2015</a> considered adding occasional random perturbations to GD, and were able to provide the first <em>polynomial time</em> guarantee for GD to escape saddle points.  (See also <a href=\"http://www.offconvex.org/2016/03/22/saddlepoints/\">Rong Ge’s post</a> )</p>\n",
      "  </li>\n",
      "  <li>\n",
      "    <p><strong>Random Initialization</strong>: <a href=\"http://arxiv.org/abs/1602.04915\">Lee et al. 2016</a> showed that with only random initialization, GD provably avoids saddle points asymptotically (i.e., as the number of steps goes to infinity). (see also <a href=\"http://www.offconvex.org/2016/03/24/saddles-again/\">Ben Recht’s post</a>)</p>\n",
      "  </li>\n",
      "</ol>\n",
      "\n",
      "<p>Asymptotic — and even polynomial time —results are important for the general theory, but they stop short of explaining the success of gradient-based algorithms in practical nonconvex problems.  And they fail to provide reassurance that runs of GD can be trusted — that we won’t find ourselves in a situation in which the learning curve flattens out for an indefinite amount of time, with the user having no way of knowing that the asymptotics have not yet kicked in. Lastly, they fail to provide reassurance that GD has the kind of favorable properties in high dimensions that it is known to have for convex problems.</p>\n",
      "\n",
      "<p>One reasonable approach to this issue is to consider second-order (Hessian-based) algorithms.  Although these algorithms are generally (far) more expensive per iteration than GD, and can be more complicated to implement, they do provide the kind of geometric information around saddle points that allows for efficient escape. Accordingly, a reasonable understanding of Hessian-based algorithms has emerged in the literature, and positive efficiency results have been obtained.</p>\n",
      "\n",
      "<p><strong><em>Is GD also efficient? Or is the Hessian necessary for fast escape of saddle points?</em></strong></p>\n",
      "\n",
      "<p>A negative result emerges to this first question if one considers the random initialization strategy discussed. Indeed, this approach is provably <em>inefficient</em> in general, taking exponential time to escape saddle points in the worst case (see “On the Necessity of Adding Perturbations” section).</p>\n",
      "\n",
      "<p>Somewhat surprisingly, it turns out that we obtain a rather different — and <em>positive</em> — result if we consider the perturbation strategy.  To be able to state this result, let us be clear on the algorithm that we analyze:</p>\n",
      "\n",
      "<blockquote>\n",
      "  <p><strong>Perturbed gradient descent (PGD)</strong></p>\n",
      "  <ol>\n",
      "    <li><strong>for</strong> $~t = 1, 2, \\ldots ~$ <strong>do</strong></li>\n",
      "    <li>$\\quad\\quad x_{t} \\leftarrow x_{t-1} - \\eta \\nabla f (x_{t-1})$</li>\n",
      "    <li>$\\quad\\quad$ <strong>if</strong> $~$<em>perturbation condition holds</em>$~$ <strong>then</strong></li>\n",
      "    <li>$\\quad\\quad\\quad\\quad x_t \\leftarrow x_t + \\xi_t$</li>\n",
      "  </ol>\n",
      "</blockquote>\n",
      "\n",
      "<p>Here the perturbation $\\xi_t$ is sampled uniformly from a ball centered at zero with a suitably small radius, and is added to the iterate when the gradient is suitably small. These particular choices are made for analytic convenience; we do not believe that uniform noise is necessary. nor do we believe it essential that noise be added only when the gradient is small.</p>\n",
      "\n",
      "<h2 id=\"strict-saddle-and-second-order-stationary-points\">Strict-Saddle and Second-order Stationary Points</h2>\n",
      "<p>We define <em>saddle points</em> in this post to include both classical saddle points as well as local maxima.  They are stationary points which are locally maximized along <em>at least one direction</em>.  Saddle points and local minima can be categorized according to the minimum eigenvalue of Hessian:</p>\n",
      "\n",
      "<script type=\"math/tex; mode=display\">% <![CDATA[\n",
      "\\lambda_{\\min}(\\nabla^2 f(x))  \\begin{cases}\n",
      "                        > 0 \\quad\\quad  \\text{local minimum} \\\\\n",
      "                        = 0 \\quad\\quad  \\text{local minimum or saddle point} \\\\\n",
      "                        < 0 \\quad\\quad  \\text{saddle point}\n",
      "                    \\end{cases} %]]></script>\n",
      "\n",
      "<p>We further call the saddle points in the last category, where $\\lambda_{\\min}(\\nabla^2 f(x)) &lt; 0$, <strong>strict saddle points</strong>.</p>\n",
      "\n",
      "<p><img src=\"http://bair.berkeley.edu/blog/assets/saddle_eff/strictsaddle.png\" class=\"stretch-center\" /></p>\n",
      "\n",
      "<p>While non-strict saddle points can be flat in the valley, strict saddle points require that there is <em>at least one direction</em> along which the curvature is strictly negative. The presence of such a direction gives a gradient-based algorithm the possibility of escaping the saddle point.  In general, distinguishing local minima and non-strict saddle points is <em>NP-hard</em>; therefore, we — and previous authors — focus on escaping <em>strict</em> saddle points.</p>\n",
      "\n",
      "<p>Formally, we make the following two standard assumptions regarding smoothness.</p>\n",
      "\n",
      "<blockquote>\n",
      "  <p><strong>Assumption 1</strong>: $f$ is $\\ell$-gradient-Lipschitz, i.e. <br />\n",
      "$\\quad\\quad\\quad\\quad \\forall x_1, x_2, |\\nabla f(x_1) - \\nabla f(x_2)| \\le \\ell |x_1 - x_2|$. <br />\n",
      "$~$<br />\n",
      " <strong>Assumption 2</strong>: $f$ is $\\rho$-Hessian-Lipschitz, i.e. <br />\n",
      "$\\quad\\quad\\quad\\quad \\forall x_1, x_2$, $|\\nabla^2 f(x_1) - \\nabla^2 f(x_2)| \\le \\rho |x_1 - x_2|$.</p>\n",
      "</blockquote>\n",
      "\n",
      "<p>Similarly to classical theory, which studies convergence to a first-order stationary point, $\\nabla f(x) = 0$, by bounding the number of iterations to find a <strong>$\\epsilon$-first-order stationary point</strong>,  $|\\nabla f(x)| \\le \\epsilon$, we formulate the speed of escape of strict saddle points and the ensuing convergence to a second-order stationary point, $\\nabla f(x) = 0, \\lambda_{\\min}(\\nabla^2 f(x)) \\ge 0$, with an $\\epsilon$-version of the definition:</p>\n",
      "\n",
      "<blockquote>\n",
      "  <p><strong>Definition</strong>: A point $x$ is an <strong>$\\epsilon$-second-order stationary point</strong> if:<br />\n",
      "$\\quad\\quad\\quad\\quad |\\nabla f(x)|\\le \\epsilon$, and $\\lambda_{\\min}(\\nabla^2 f(x)) \\ge -\\sqrt{\\rho \\epsilon}$.</p>\n",
      "</blockquote>\n",
      "\n",
      "<p>In this definition, $\\rho$ is the Hessian Lipschitz constant introduced above. This scaling follows the convention of <a href=\"http://rd.springer.com/article/10.1007%2Fs10107-006-0706-8\">Nesterov and Polyak 2006</a>.</p>\n",
      "\n",
      "<h3 id=\"applications\">Applications</h3>\n",
      "<p>In a wide range of practical nonconvex problems it has been proved that <strong>all saddle points are strict</strong> — such problems include, but not are limited to, principal components analysis, canonical correlation analysis,\n",
      "<a href=\"http://arxiv.org/abs/1503.02101\">orthogonal tensor decomposition</a>,\n",
      "<a href=\"http://arxiv.org/abs/1602.06664\">phase retrieval</a>,\n",
      "<a href=\"http://arxiv.org/abs/1504.06785\">dictionary learning</a>,\n",
      "<!-- matrix factorization,  -->\n",
      "<a href=\"http://arxiv.org/abs/1605.07221\">matrix sensing</a>,\n",
      "<a href=\"http://arxiv.org/abs/1605.07272\">matrix completion</a>,\n",
      "and <a href=\"http://arxiv.org/abs/1704.00708\">other nonconvex low-rank problems</a>.</p>\n",
      "\n",
      "<p>Furthermore, in all of these nonconvex problems, it also turns out that <strong>all local minima are global minima</strong>. Thus, in these cases, any general efficient algorithm for finding $\\epsilon$-second-order stationary points immediately becomes an efficient algorithm for solving those nonconvex problem with global guarantees.</p>\n",
      "\n",
      "<h2 id=\"escaping-saddle-point-with-negligible-overhead\">Escaping Saddle Point with Negligible Overhead</h2>\n",
      "<p>In the classical case of first-order stationary points, GD is known to have very favorable theoretical properties:</p>\n",
      "\n",
      "<blockquote>\n",
      "  <p><strong>Theorem (<a href=\"http://rd.springer.com/book/10.1007%2F978-1-4419-8853-9\">Nesterov 1998</a>)</strong>: If Assumption 1 holds, then GD, with $\\eta = 1/\\ell$, finds an $\\epsilon$-<strong>first</strong>-order stationary point in $2\\ell (f(x_0) - f^\\star)/\\epsilon^2$ iterations.</p>\n",
      "</blockquote>\n",
      "\n",
      "<p>In this theorem, $x_0$ is the initial point and $f^\\star$ is the function value of the global minimum. The theorem says for that any gradient-Lipschitz function, a stationary point can be found by GD in $O(1/\\epsilon^2)$ steps, with no explicit dependence on $d$. This is called “dimension-free optimization” in the literature; of course the cost of a gradient computation is $O(d)$, and thus the overall runtime of GD scales as $O(d)$. The linear scaling in $d$ is especially important for modern high-dimensional nonconvex problems such as deep learning.</p>\n",
      "\n",
      "<p>We now wish to address the corresponding problem for second-order stationary points.\n",
      "What is the best we can hope for? Can we also achieve</p>\n",
      "<ol>\n",
      "  <li>A dimension-free number of iterations;</li>\n",
      "  <li>An $O(1/\\epsilon^2)$ convergence rate;</li>\n",
      "  <li>The same dependence on $\\ell$ and $(f(x_0) - f^\\star)$ as in (Nesterov 1998)?</li>\n",
      "</ol>\n",
      "\n",
      "<p>Rather surprisingly, the answer is <em>Yes</em> to all three questions (up to small log factors).</p>\n",
      "\n",
      "<blockquote>\n",
      "  <p><strong>Main Theorem</strong>: If Assumptions 1 and 2 hold, then PGD, with $\\eta = O(1/\\ell)$, finds an $\\epsilon$-<strong>second</strong>-order stationary point in $\\tilde{O}(\\ell (f(x_0) - f^\\star)/\\epsilon^2)$ iterations with high probability.</p>\n",
      "</blockquote>\n",
      "\n",
      "<p>Here $\\tilde{O}(\\cdot)$ hides only logarithmic factors; indeed, the dimension dependence in our result is only $\\log^4(d)$. The theorem thus asserts that a perturbed form of GD, under an additional Hessian-Lipschitz condition, <strong><em>converges to a second-order-stationary point in almost the same time required for GD to converge to a first-order-stationary point.</em></strong> In this sense, we claim that PGD can escape strict saddle points almost for free.</p>\n",
      "\n",
      "<p>We turn to a discussion of some of the intuitions underlying these results.</p>\n",
      "\n",
      "<h3 id=\"why-do-polylogd-iterations-suffice\">Why do polylog(d) iterations suffice?</h3>\n",
      "<p>Our strict-saddle assumption means that there is only, in the worst case, one direction in $d$ dimensions along which we can escape. A naive search for the descent direction intuitively should take at least $\\text{poly}(d)$ iterations, so why should only $\\text{polylog}(d)$ suffice?</p>\n",
      "\n",
      "<p>Consider a simple case in which we assume that the function is quadratic in the neighborhood of the saddle point.  That is, let the objective function be $f(x) = x^\\top H x$, a saddle point at zero, with constant Hessian $H = \\text{diag}(-1, 1, \\cdots, 1)$. In this case, only the first direction is an escape direction (with negative eigenvalue $-1$).</p>\n",
      "\n",
      "<p>It is straightforward to work out the general form of the iterates in this case:</p>\n",
      "\n",
      "<script type=\"math/tex; mode=display\">x_t = x_{t-1} - \\eta \\nabla f(x_{t-1}) = (I - \\eta H)x_{t-1} = (I - \\eta H)^t x_0.</script>\n",
      "\n",
      "<p>Assume that we start at the saddle point at zero, then add a perturbation so that $x_0$ is sampled uniformly from a ball $\\mathcal{B}_0(1)$ centered at zero with radius one.\n",
      "The decrease in the function value can be expressed as:</p>\n",
      "\n",
      "<script type=\"math/tex; mode=display\">f(x_t) - f(0) = x_t^\\top H x_t  = x_0^\\top (I - \\eta H)^t H (I - \\eta H)^t x_0.</script>\n",
      "\n",
      "<p>Set the step size to be $1/2$, let $\\lambda_i$ denote the $i$-th eigenvalue of the Hessian $H$ and let $\\alpha_i = e_i^\\top x_0$ denote the component in the $i$th direction of the initial point $x_0$. We have $\\sum_{i=1}^d \\alpha_i^2  = | x_0|^2 = 1$, thus:</p>\n",
      "\n",
      "<script type=\"math/tex; mode=display\">f(x_t) - f(0) = \\sum_{i=1}^d \\lambda_i (1-\\eta\\lambda_i)^{2t} \\alpha_i^2 \\le  -1.5^{2t} \\alpha_1^2 + 0.5^{2t}.</script>\n",
      "\n",
      "<p>A simple probability argument shows that sampling uniformly in $\\mathcal{B}_0(1)$ will result in at least a $\\Omega(1/d)$ component in the first direction with high probability. That is, $\\alpha^2_1 = \\Omega(1/d)$. Substituting $\\alpha_1$ in the above equation, we see that it takes at most $O(\\log d)$ steps for the function value to decrease by a constant amount.</p>\n",
      "\n",
      "<h3 id=\"pancake-shape-stuck-region-for-general-hessian\">Pancake-shape stuck region for general Hessian</h3>\n",
      "<p>We can conclude that for the case of a constant Hessian, only when the perturbation $x_0$ lands in the set $\\{x | ~ |e_1^\\top x|^2 \\le O(1/d)\\}$ $\\cap \\mathcal{B}_0 (1)$, can we take a very long time to escape the saddle point. We call this set the <strong>stuck region</strong>; in this case it is a flat disk. In general, when the Hessian is no longer constant, the stuck region becomes a non-flat pancake, depicted as a green object in the left graph. In general this region will not have an analytic expression.</p>\n",
      "\n",
      "<p>Earlier attempts to analyze the dynamics around saddle points tried to the approximate stuck region by a flat set.  This results in a requirement of an extremely small step size and a correspondingly very large runtime complexity.  Our sharp rate depends on a key observation — <em>although we don’t know the shape of the stuck region, we know it is very thin</em>.</p>\n",
      "\n",
      "<p><img src=\"http://bair.berkeley.edu/blog/assets/saddle_eff/flow.png\" class=\"stretch-center\" /></p>\n",
      "\n",
      "<p>In order to characterize the “thinness” of this pancake, we studied pairs of hypothetical perturbation points $w, u$ separated by $O(1/\\sqrt{d})$ along an escaping direction. We claim that if we run GD starting at $w$ and $u$, at least one of the resulting trajectories will escape the saddle point very quickly. This implies that the thickness of the stuck region can be at most $O(1/\\sqrt{d})$, so a random perturbation has very little chance to land in the stuck region.</p>\n",
      "\n",
      "<h2 id=\"on-the-necessity-of-adding-perturbations\">On the Necessity of Adding Perturbations</h2>\n",
      "<p>We have discussed two possible ways to modify the standard gradient descent algorithm, the first by adding intermittent perturbations, and the second by relying on random initialization. Although the latter exhibits asymptotic convergence, it does not yield efficient convergence in general; in recent <a href=\"http://arxiv.org/abs/1705.10412\">joint work with Simon Du, Jason Lee, Barnabas Poczos, and Aarti Singh</a>, we have shown that even with fairly natural random initialization schemes and non-pathological functions, <strong>GD with only random initialization can be significantly slowed by saddle points, taking exponential time to escape. The behavior of PGD is strikingingly different — it can generically escape saddle points in polynomial time.</strong></p>\n",
      "\n",
      "<p>To establish this result, we considered random initializations from a very general class including Gaussians and uniform distributions over the hypercube, and we constructed a smooth objective function that satisfies both Assumptions 1 and 2. This function is constructed such that, even with random initialization, with high probability both GD and PGD have to travel sequentially in the vicinity of $d$ strict saddle points before reaching a local minimum. All strict saddle points have only one direction of escape. (See the left graph for the case of $d=2$).</p>\n",
      "\n",
      "<p><img src=\"http://bair.berkeley.edu/blog/assets/saddle_eff/necesperturbation.png\" class=\"stretch-center\" /></p>\n",
      "\n",
      "<p>When GD travels in the vicinity of a sequence of saddle points, it can get closer and closer to the later saddle points, and thereby take longer and longer to escape. Indeed, the time to escape the $i$th saddle point scales as $e^{i}$. On the other hand, PGD is always able to escape any saddle point in a small number of steps independent of the history. This phenomenon is confirmed by our experiments; see, for example, an experiment with $d=10$ in the right graph.</p>\n",
      "\n",
      "<h2 id=\"conclusion\">Conclusion</h2>\n",
      "<p>In this post, we have shown that a perturbed form of gradient descent can converge to a second-order-stationary point at almost the same rate as standard gradient descent converges to a first-order-stationary point. This implies that Hessian information is not necessary for to escape saddle points efficiently, and helps to explain why basic gradient-based algorithms such as GD (and SGD) work surprisingly well in the nonconvex setting. This new line of sharp convergence results can be directly applied to nonconvex problem such as matrix sensing/completion to establish efficient global convergence rates.</p>\n",
      "\n",
      "<p>There are of course still many open problems in general nonconvex optimization. To name a few: will adding momentum improve the convergence rate to a second-order stationary point? What type of local minima are tractable and are there useful structural assumptions that we can impose on local minima so as to avoid local minima efficiently? We are making slow but steady progress on nonconvex optimization, and there is the hope that at some point we will transition from “black art” to “science”.</p>\n",
      "\n",
      "  Link: http://bair.berkeley.edu/blog/2017/08/31/saddle-efficiency/\n",
      "  PubDate: Thu, 31 Aug 2017 02:00:00 -0700\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20170909155232/http://bair.berkeley.edu/blog/feed.xml\n",
      "- Source: https://bair.berkeley.edu/blog/feed.xml\n",
      "  Channel Title: The Berkeley Artificial Intelligence Research Blog\n",
      "  Channel Description: The BAIR Blog\n",
      "- Title: Learning a Multi-View Stereo Machine\n",
      "  Description: <p>Consider looking at a photograph of a chair.\n",
      "We humans have the remarkable capacity of inferring properties about the 3D shape of the chair from this single photograph even if we might not have seen such a chair ever before.\n",
      "A more representative example of our experience though is being in the same physical space as the chair and accumulating information from various viewpoints around it to build up our hypothesis of the chair’s 3D shape.\n",
      "How do we solve this complex 2D to 3D inference task? What kind of cues do we use?<br />\n",
      "How do we seamlessly integrate information from just a few views to build up a holistic 3D model of the scene?</p>\n",
      "\n",
      "<p><img src=\"http://bair.berkeley.edu/blog/assets/unified-3d/problem_fig.png\" alt=\"Problem Statement\" /></p>\n",
      "\n",
      "<p>A vast body of work in computer vision has been devoted to developing algorithms which leverage various cues from images that enable this task of 3D reconstruction.\n",
      "They range from monocular <a href=\"http://www.eruptingmind.com/depth-perception-cues-other-forms-of-perception/\">cues</a> such as shading, linear perspective, size constancy etc. to binocular and even multi-view stereopsis.\n",
      "The dominant paradigm for integrating multiple views has been to leverage stereopsis, i.e. if a point in the 3D world is viewed from multiple viewpoints, its location in 3D can be determined by triangulating its projections in the respective views.\n",
      "This family of algorithms has led to work on  Structure from Motion (SfM) and Multi-view Stereo (MVS) and have been used to produce <a href=\"https://grail.cs.washington.edu/rome/\">city-scale</a> <a href=\"http://www.di.ens.fr/pmvs/\">3D models</a> and enable rich visual experiences such as <a href=\"http://mashable.com/2017/06/28/apple-maps-flyover/\">3D flyover</a> <a href=\"https://vr.google.com/earth/\">maps</a>.\n",
      "With the advent of deep neural networks and their immense power in modelling visual data, the focus has recently shifted to modelling monocular cues implicitly with a CNN and predicting 3D from a single image as <a href=\"http://www.cs.nyu.edu/~deigen/dnl/\">depth</a>/<a href=\"http://www.cs.cmu.edu/~xiaolonw/deep3d.html\">surface orientation</a> maps or 3D <a href=\"http://3d-r2n2.stanford.edu/\">voxel</a> <a href=\"https://rohitgirdhar.github.io/GenerativePredictableVoxels/\">grids</a>.</p>\n",
      "\n",
      "<p>In our <a href=\"https://arxiv.org/abs/1708.05375\">recent work</a>, we tried to unify these paradigms of single and multi-view 3D reconstruction.\n",
      "We proposed a novel system called a Learnt Stereo Machine (LSM) that can leverage monocular/semantic cues for single-view 3D reconstruction while also being able to integrate information from multiple viewpoints using stereopsis - all within a single end-to-end learnt deep neural network.</p>\n",
      "\n",
      "<!--more-->\n",
      "\n",
      "<h2 id=\"learnt-stereo-machines\">Learnt Stereo Machines</h2>\n",
      "<p><img src=\"http://bair.berkeley.edu/blog/assets/unified-3d/Network.png\" alt=\"Learnt Stereo Machine\" />\n",
      "LSMs are designed to solve the task of multi-view stereo. Given a set of images with <em>known camera poses</em>, they produce a 3D model for the underlying scene - specifically either a voxel occupancy grid or a dense point cloud of the scene in the form of a pixel-wise depth map per input view. While designing LSMs, we drew inspiration from classic works on MVS. These methods first <em>extract features</em> from the images for finding correspondences between them. By comparing the features between images, a matching cost volume is formed. These (typically noisy) matching costs are then <em>filtered/regularized</em> by aggregating information across multiple scales and incorporating priors on shape such as local smoothness, piecewise planarity etc. The final filtered cost volume is then decoded into the desired shape representation such as a 3D volume/surface/disparity maps.</p>\n",
      "\n",
      "<p><img src=\"http://bair.berkeley.edu/blog/assets/unified-3d/proj_gif.gif\" style=\"width:45%; margin-left:4%; border-right:solid; border-width:1px; border-color:rgba(0,0,0,0.42);\" />\n",
      "<img src=\"http://bair.berkeley.edu/blog/assets/unified-3d/unproj_gif.gif\" style=\"width:45%; margin-right:4%\" /></p>\n",
      "\n",
      "<p>The key ingredients here are a differentiable feature <strong>projection</strong> and <strong>unprojection</strong> modules which allow LSMs to move between 2D image and 3D world spaces in a geometrically consistent manner. The unprojection operation places features from a 2D image (extracted by a feedforward CNN) into a 3D world grid such that features from multiple such images align in the 3D grid according to epipolar constraints. This simplifies feature matching as now a search along an epipolar line to compute matching costs reduces to just looking up all features which map to a given location in the 3D world grid. This feature matching is modeled using a 3D recurrent unit which performs sequential matching of the unprojected grids while maintaining a running estimate of the matching scores. Once we filter the local matching cost volume using a 3D CNN,  we either decode it directly into a 3D voxel occupancy grid for the voxel prediction task or project it back into 2D image space using a differentiable projection operation. The projection operation can be thought of as the inverse of the unprojection operation where we take a 3D feature grid and sample features along viewing rays at equal depth intervals to place them in a 2D feature map. These projected feature maps are then decoded into per view depth maps by a series of convolution operations. As every step in our network is completely differentiable, we can train the system end-to-end with depth maps or voxel grids as supervision!</p>\n",
      "\n",
      "<p>As LSMs can predict 3D from a variable number of images (even just a single image), they can choose to either rely heavily on multi-view stereopsis cues or single-view semantic cues depending on the instance and number of views at hand. LSMs can produce both coarse full 3D voxel grids as well as dense depth maps thus unifying the two major paradigms in 3D prediction using deep neural networks.</p>\n",
      "\n",
      "<p><img src=\"http://bair.berkeley.edu/blog/assets/unified-3d/voxel_results.png\" alt=\"voxel\" /></p>\n",
      "\n",
      "<p>In our report, we showed drastic improvements on voxel based multi-view 3D object reconstruction when compared to the <a href=\"http://3d-r2n2.stanford.edu/\">previous state-of-the-art</a> which integrates multiple views using a recurrent neural network. We also demonstrated out-of-category generalization, i.e. LSMs can reconstruct cars even if they are only trained on images of aeroplanes and chairs. This is only possible due to our geometric treatment of the task.\n",
      "We also show dense reconstructions from a few views - much fewer than what is required by classical MVS systems.</p>\n",
      "\n",
      "<p><img src=\"http://bair.berkeley.edu/blog/assets/unified-3d/depth_results.png\" alt=\"voxel\" /></p>\n",
      "\n",
      "<h2 id=\"whats-next\">What’s Next?</h2>\n",
      "<p>LSMs are a step towards unifying a number of paradigms in 3D reconstruction - single and multi-view, semantic and geometric reconstruction, coarse and dense predictions. A joint treatment of these problems helps us learn models that are more robust and accurate while also being simpler to deploy than pipelined solutions.</p>\n",
      "\n",
      "<p>These are exciting times in 3D computer vision. Predicting <a href=\"http://bair.berkeley.edu/blog/2017/08/23/high-quality-3d-obj-reconstruction/\">high resolution geometry</a> with deep networks is now possible. We can even train for 3D prediction <a href=\"http://bair.berkeley.edu/blog/2017/07/11/confluence-of-geometry-and-learning/\">without explicit 3D</a> supervision. We can’t wait to use these techniques/ideas within LSMs. It remains to be seen how lifting images from 2D to 3D and reasoning about them in metric world space would help other downstream tasks such as navigation and grasping but it sure will be an interesting journey! We will release the code for LSMs soon for easy experimentation and reproducibility. Feel free to use it and leave comments!</p>\n",
      "\n",
      "<hr />\n",
      "\n",
      "<p>We would like to thank Saurabh Gupta, Shubham Tulsiani and David Fouhey.</p>\n",
      "\n",
      "<p><strong>This blog post is based on the following report</strong></p>\n",
      "\n",
      "<ul>\n",
      "  <li><a href=\"https://arxiv.org/abs/1708.05375\"><em>Learning a Multi-view Stereo Machine</em></a><br />\n",
      "<a href=\"https://people.eecs.berkeley.edu/~akar/\">Abhishek Kar</a>, <a href=\"https://people.eecs.berkeley.edu/~chaene/\">Christian Häne</a>, <a href=\"https://people.eecs.berkeley.edu/~malik/\">Jitendra Malik</a>, NIPS, 2017</li>\n",
      "</ul>\n",
      "\n",
      "  Link: http://bair.berkeley.edu/blog/2017/09/05/unified-3d/\n",
      "  PubDate: Tue, 05 Sep 2017 02:00:00 -0700\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20171011143554/http://bair.berkeley.edu/blog/feed.xml\n",
      "- Source: https://bair.berkeley.edu/blog/feed.xml\n",
      "  Channel Title: The Berkeley Artificial Intelligence Research Blog\n",
      "  Channel Description: The BAIR Blog\n",
      "- Title: Learning Diverse Skills via Maximum Entropy Deep Reinforcement Learning\n",
      "  Description: <p>Deep reinforcement learning (deep RL) has achieved success in many tasks, such as playing video games from raw pixels (Mnih et al., 2015), playing the game of Go (Silver et al., 2016), and simulated robotic locomotion (e.g. Schulman et al., 2015). Standard deep RL algorithms aim to master a single way to solve a given task, typically the first way that seems to work well. Therefore, training is sensitive to randomness in the environment, initialization of the policy, and the algorithm implementation. This phenomenon is illustrated in Figure 1, which shows two policies trained to optimize a reward function that encourages forward motion: while both policies have converged to a high-performing gait, these gaits are substantially different from each other.</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/blog/assets/softq/figure_1_walker_two_gaits_v2.gif\" alt=\"Figure 1: Trained simulated walking robots.\" /><br />\n",
      "<i>\n",
      "Figure 1: Trained simulated walking robots.<br />\n",
      "[credit: John Schulman and Patrick Coady (<a href=\"https://gym.openai.com/envs/Walker2d-v1/\">OpenAI Gym)</a>]\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<!--more-->\n",
      "\n",
      "<p>Why might finding only a single solution be undesirable? Knowing only one way to act makes agents vulnerable to environmental changes that are common in the real-world. For example, consider a robot (Figure 2) navigating its way to the goal (blue cross) in a simple maze. At training time (Figure 2a), there are two passages that lead to the goal. The agent will likely commit to the solution via the upper passage as it is slightly shorter. However, if we change the environment by blocking the upper passage with a wall (Figure 2b), the solution the agent has found becomes infeasible. Since the agent focused entirely on the upper passage during learning, it has almost no knowledge of the lower passage. Therefore, adapting to the new situation in Figure 2b requires the agent to relearn the entire task from scratch.</p>\n",
      "\n",
      "<table class=\"col-2\">\n",
      "  <tr>\n",
      "    <td style=\"text-align:center;\">\n",
      "\t\t\t<img src=\"http://bair.berkeley.edu/blog/assets/softq/figure_2a_maze_one_path.png\" alt=\"maze_one_path\" width=\"300\" /><p class=\"center\">2a</p>\n",
      "\t\t</td>\n",
      "    <td style=\"text-align:center;\">\n",
      "\t\t\t<img src=\"http://bair.berkeley.edu/blog/assets/softq/figure_2b_maze-two-paths.png\" alt=\"maze-two-paths\" width=\"300\" /><p class=\"center\">2b</p>\n",
      "\t\t</td>\n",
      "  </tr>\n",
      "</table>\n",
      "<p style=\"text-align:center;\">\n",
      "<i>\n",
      "Figure 2: A robot navigating a maze.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<h3 id=\"maximum-entropy-policies-and-their-energy-forms\">Maximum Entropy Policies and Their Energy Forms</h3>\n",
      "<p>Let us begin with a review of RL: an agent interacts with an environment by iteratively observing the current <em>state</em> ($\\mathbf{s}$), taking an <em>action</em> ($\\mathbf{a}$), and receiving a <em>reward</em> ($r$). It employs a (stochastic) policy ($\\pi$) to select actions, and finds the best policy that maximizes the cumulative reward it collects throughout an episode of length $T$:</p>\n",
      "\n",
      "<script type=\"math/tex; mode=display\">\\pi^* = \\arg\\!\\max_{\\pi} \\mathbb{E}_{\\pi}\\left[ \\sum_{t=0}^T r_t \\right]</script>\n",
      "\n",
      "<p>We define the Q-function, $Q(\\mathbf{s},\\mathbf{a})$, as the expected cumulative reward after taking action a at state s. Consider the robot in Figure 2a again. When the robot is in the initial state, the Q-function may look like the one depicted in Figure 3a (grey curve), with two distinct modes corresponding to the two passages. A conventional RL approach is to specify a unimodal policy distribution, centered at the maximal Q-value and extending to the neighbouring actions to provide noise for exploration (red distribution). Since the exploration is biased towards the upper passage, the agent refines its policy there and ignores the lower passage completely.</p>\n",
      "\n",
      "<table class=\"col-2\">\n",
      "  <tr>\n",
      "    <td style=\"text-align:center;\">\n",
      "\t\t\t<img src=\"http://bair.berkeley.edu/blog/assets/softq/figure_3a_unimodal-policy.png\" alt=\"unimodal-policy\" width=\"300\" /><p class=\"center\">3a</p>\n",
      "\t\t</td>\n",
      "    <td style=\"text-align:center;\">\n",
      "\t\t\t<img src=\"http://bair.berkeley.edu/blog/assets/softq/figure_3b_multimodal_policy.png\" alt=\"multimodal_policy\" width=\"300\" /><p class=\"center\">3b</p>\n",
      "\t\t</td>\n",
      "  </tr>\n",
      "</table>\n",
      "<p style=\"text-align:center;\">\n",
      "<i>\n",
      "Figure 3: A multimodal Q-function.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>An obvious solution, at the high level, is to ensure the agent explores all promising states while prioritizing the more promising ones. One way to formalize this idea is to define the policy directly in terms of exponentiated Q-values (Figure 3b, green distribution):</p>\n",
      "\n",
      "<script type=\"math/tex; mode=display\">\\pi(\\mathbf{a}|\\mathbf{s}) \\propto \\exp Q(\\mathbf{s}, \\mathbf{a})</script>\n",
      "\n",
      "<p>This density has the form of the Boltzmann distribution, where the Q-function serves as the negative energy, which assigns a non-zero likelihood to all actions. As a consequence, the agent will become aware of all behaviours that lead to solving the task, which can help the agent adapt to changing situations in which some of the solutions might have become infeasible. In fact, we can show that the policy defined through the energy form is an optimal solution for the maximum-entropy RL objective</p>\n",
      "\n",
      "<script type=\"math/tex; mode=display\">\\pi_{\\mathrm{MaxEnt}}^* = \\arg\\!\\max_{\\pi} \\mathbb{E}_{\\pi}\\left[ \\sum_{t=0}^T r_t + \\mathcal{H}(\\pi(\\cdot | \\mathbf{s}_t)) \\right]</script>\n",
      "\n",
      "<p>which simply augments the conventional RL objective with the entropy of the policy (Ziebart 2010).</p>\n",
      "\n",
      "<p>The idea of learning such <a href=\"https://en.wikipedia.org/wiki/Principle_of_maximum_entropy\">maximum entropy models</a> has its origin in statistical modeling, in which the goal is to find the probability distribution that has the highest entropy while still satisfying the observed statistics. For example, if the distribution is on the Euclidean space and the observed statistics are the mean and the covariance, then the maximum entropy distribution is a Gaussian with the corresponding mean and covariance. In practice, we prefer maximum-entropy models as they assume the least about the unknowns while matching the observed information.</p>\n",
      "\n",
      "<p>A number of prior works have employed the maximum-entropy principle in the context of reinforcement learning and optimal control. Ziebart (2008) used the maximum entropy principle to resolve ambiguities in inverse reinforcement learning, where several reward functions can explain the observed demonstrations. Several works (Todorov 2008; Toussaint, 2009]) have studied the connection between inference and control via the maximum entropy formulation. Todorov (2007, 2009) also showed how the maximum entropy principle can be employed to make MDPs linearly solvable, and Fox et al. (2016) utilized the principle as a means to incorporate prior knowledge into a reinforcement learning policy.</p>\n",
      "\n",
      "<h2 id=\"soft-bellman-equation-and-soft-q-learning\">Soft Bellman Equation and Soft Q-Learning</h2>\n",
      "<p>We can obtain the optimal solution of the maximum entropy objective by employing the <em>soft Bellman equation</em></p>\n",
      "\n",
      "<script type=\"math/tex; mode=display\">Q(\\mathbf{s}_t, \\mathbf{a}_t) = \\mathbb{E}\\left[r_t + \\gamma\\ \\mathrm{softmax}_{\\mathbf{a}} Q(\\mathbf{s}_{t+1}, \\mathbf{a})\\right]</script>\n",
      "\n",
      "<p>where</p>\n",
      "\n",
      "<script type=\"math/tex; mode=display\">\\mathrm{softmax}_{\\mathbf{a}} f(\\mathbf{a}) := \\log \\int \\exp f(\\mathbf{a}) \\, d\\mathbf{a}</script>\n",
      "\n",
      "<p>The soft Bellman equation can be shown to hold for the optimal Q-function of the entropy augmented reward function (e.g. Ziebart 2010). Note the similarity to the conventional Bellman equation, which instead has the hard max of the Q-function over the actions instead of the softmax. Like the hard version, the soft Bellman equation is a contraction, which allows solving for the Q-function using dynamic programming or model-free TD learning in tabular state and action spaces (e.g. Ziebart, 2008; Rawlik, 2012; Fox, 2016).</p>\n",
      "\n",
      "<p>However, in continuous domains, there are two major challenges. First, exact dynamic programming is infeasible, since the soft Bellman equation needs to hold for every state and action, and the softmax involves integrating over the entire action space. Second, the optimal policy is defined by an intractable energy-based distribution, which is difficult to sample from. To address the first challenge, we can employ expressive neural network function approximators, which can be trained with stochastic gradient descent on sampled states and actions and then generalize effectively to new state-action tuples. To address the second challenge, we can employ approximate inference techniques, such as Markov chain Monte Carlo, which has been explored in prior works for energy-based policies (Heess, 2012). To accelerate inference, we use the amortized Stein variational gradient descent (Wang and Liu, 2016) to train an inference network to generate approximate samples. The resulting algorithm, termed <em>soft Q-learning</em>, combines deep Q-learning and the amortized Stein variational gradient descent.</p>\n",
      "\n",
      "<h2 id=\"application-to-reinforcement-learning\">Application to Reinforcement Learning</h2>\n",
      "<p>Now that we can learn maximum entropy policies via soft Q-learning, we might wonder: what are the practical uses of this approach? In the following sections, we illustrate with experiments that soft Q-learning allows for better exploration, enables policy transfer between similar tasks, allows new policies to be easily composed from existing policies, and improves robustness through extensive exploration at training time.</p>\n",
      "\n",
      "<h3 id=\"better-exploration\">Better Exploration</h3>\n",
      "<p>Soft Q-learning (SQL) provides us with an implicit exploration strategy by assigning each action a non-zero probability, shaped by the current belief about its value, effectively combining exploration and exploitation in a natural way. To see this, let us consider a two-passage maze (Figure 4) similar to the one discussed in the introduction. The task is to find a way to the goal state, denoted by a blue square. Suppose that the reward is proportional to the distance to the goal.  Since the maze is almost symmetric, such a reward results in a bimodal objective, but only one of the modes corresponds to an actual solution to the task. Thus, exploring both passages at training time is crucial to discover which of the two is really best. A unimodal policy can only solve this task if it is lucky enough to commit to the lower passage from the start. On the other hand, a multimodal soft Q-learning policy can solve the task consistently by following both passages randomly until the agent finds the goal (Figure 4).</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/blog/assets/softq/figure_4_ant_maze.gif\" alt=\"A policy trained with soft Q-learning.\" /><br />\n",
      "<i>\n",
      "Figure 4: A policy trained with soft Q-learning can explore both passages during training.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<h3 id=\"fine-tuning-maximum-entropy-policies\">Fine-Tuning Maximum Entropy Policies</h3>\n",
      "<p>The standard practice in RL is to train an agent from scratch for each new task. This can be slow because the agent throws away knowledge acquired from previous tasks. Instead, the agent can transfer skills from similar previous tasks, allowing it to learn new tasks more quickly. One way to transfer skills is to pre-train policies for general purpose tasks, and then use them as templates or initializations for more specific tasks. For example, the skill of walking subsumes the skill of navigating through a maze, and therefore the walking skill can serve as an efficient initialization for learning the navigation skill. To illustrate this idea, we trained a maximum entropy policy by rewarding the agent for walking at a high speed, regardless of the direction. The resulting policy learns to walk, but does not commit to any single direction due to the maximum entropy objective (Figure 5a). Next, we specialized the walking skill to a range of navigation skills, such as the one in Figure 5b. In the new task, the agent only needs to choose which walking behavior will move itself closer to the goal, which is substantially easier than learning the same skill from scratch.  A conventional policy would converge to a specific behaviour when trained for the general task. For example, it may only learn to walk in a single direction. Consequently, it cannot directly transfer the walking skill to the maze environment, which requires movement in multiple directions.</p>\n",
      "\n",
      "<table class=\"col-2\">\n",
      "  <tr>\n",
      "    <td style=\"text-align:center;\">\n",
      "\t\t\t<img src=\"http://bair.berkeley.edu/blog/assets/softq/figure_5a_pretrain_softql_small.gif\" alt=\"pretrain_softql_small\" width=\"200\" /><p class=\"center\">5a</p>\n",
      "\t\t</td>\n",
      "    <td style=\"text-align:center;\">\n",
      "\t\t\t<img src=\"http://bair.berkeley.edu/blog/assets/softq/figure_5b_finetune_ushape_2.gif\" alt=\"finetune_ushape_2\" width=\"350\" /><p class=\"center\">5b</p>\n",
      "\t\t</td>\n",
      "  </tr>\n",
      "</table>\n",
      "<p style=\"text-align:center;\">\n",
      "<i>\n",
      "Figure 5: Maximum entropy pretraining allows agents to learn more quickly in new environments. Videos of the same pretrained policy fine-tuned for other target tasks can be found at <a href=\"https://www.youtube.com/watch?v=7Nm1N6sUoVs&amp;feature=youtu.be\">this</a> link.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<h3 id=\"compositionality\">Compositionality</h3>\n",
      "<p>In a similar vein to general-to-specific transfer, we can compose new skills from existing policies—even without any fine-tuning—by intersecting different skills. The idea is simple: take two soft policies, each corresponding to a different set of behaviors, and combine them by adding together their Q-functions. In fact, it is possible to show that the combined policy is approximately optimal for the combined task, obtained by simply adding the reward functions of the constituent tasks, up to a bounded error. Consider a planar manipulator as the one pictured below. The two agents on the left are trained to move the cylindrical object to a target location illustrated with red stripes. Note, how the solution space of the two tasks overlap: by moving the cylinder to the intersection of the stripes, both tasks can be solved simultaneously. Indeed, the policy on the right, which is obtained by simply summing together the two Q-functions, moves the cylinder to the intersection, without the need to train a policy explicitly for the combined task. Conventional policies do not exhibit the same compositionality property as they can only represent specific, disjoint solutions.</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/blog/assets/softq/figure_6_composition_small.gif\" alt=\"Combining two skills into a new one\" /><br />\n",
      "<i>\n",
      "Figure 6: Combining two skills into a new one.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<h3 id=\"robustness\">Robustness</h3>\n",
      "<p>Because the maximum entropy formulation encourages agents to try all possible solutions, the agents learn to explore a large portion of the state space. Thus they learn to act in various situations, and are more robust against perturbations in the environment. To illustrate this, we trained a Sawyer robot to stack Lego blocks together by specifying a target end-effector pose. Figure 7 shows some snapshots during training.</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/blog/assets/softq/figure_7_sawyer_training_white_bg.gif\" alt=\"Training to stack Lego blocks with soft Q-learning.\" /><br />\n",
      "<i>\n",
      "Figure 7: Training to stack Lego blocks with soft Q-learning.<br />\n",
      "[credit: Aurick Zhou]\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>The robot succeeded for the first time after 30 minutes; after an hour, it was able to stack the blocks consistently; and after two hours, the policy had fully converged. The converged policy is also robust to perturbations as shown in the video below, in which the arm is perturbed into configurations that are very different from what it encounters during normal execution, and it is able to successfully recover every time.</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/blog/assets/softq/figure_8_sawyer_bully_policy.gif\" alt=\"The trained policy is robust to perturbations\" /><br />\n",
      "<i>\n",
      "Figure 8: The trained policy is robust to perturbations.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<h2 id=\"related-work\">Related Work</h2>\n",
      "<p>Soft optimality has also been studied in recent papers in the context of learning from multi-step transitions (Nachum et al., 2017) and its connection to policy gradient methods (Schulman et al., 2017). A related concept is discussed by O’Donoghue et al. (2016), who also consider entropy regularization and Boltzmann exploration. This version of entropy regularization only considers the entropy of the current state, and does not take into account the entropy for the future states.</p>\n",
      "\n",
      "<p>To our knowledge, only a few prior works have demonstrated successful model-free reinforcement learning directly on real-world robots. Gu et al. (2016) showed that NAF could learn door opening tasks, using about 2.5 hours of experience parallelized across two robots. Rusu et al. (2016) used RL to train a robot arm to reach a red square, with pretraining in simulation. Večerı́k et al. (2017) showed that, if initialized from demonstration, a Sawyer robot could perform a peg-insertion style task with about 30 minutes of experience. It is worth noting that our soft Q-learning results, shown above, used only a single robot for training, and did not use any simulation or demonstrations.</p>\n",
      "\n",
      "<hr />\n",
      "\n",
      "<p>We would like to thank Sergey Levine, Pieter Abbeel, and Gregory Kahn for their valuable feedback when preparing this blog post.</p>\n",
      "\n",
      "<p>This post is based on the following paper:  <br />\n",
      "<strong>Reinforcement Learning with Deep Energy-Based Policies</strong>  <br />\n",
      "Haarnoja T., Tang H., Abbeel P., Levine S. <em>ICML 2017</em>.<br />\n",
      "<a href=\"https://arxiv.org/abs/1702.08165\">paper</a>, <a href=\"https://github.com/haarnoja/softqlearning\">code</a>, <a href=\"https://sites.google.com/view/softqlearning/home\">videos</a></p>\n",
      "\n",
      "<h2 id=\"references\">References</h2>\n",
      "<p><strong>Related concurrent papers</strong></p>\n",
      "<ul>\n",
      "  <li>Schulman, J., Abbeel, P. and Chen, X. Equivalence Between Policy Gradients and Soft Q-Learning. <em>arXiv preprint arXiv:1704.06440</em>, 2017.</li>\n",
      "  <li>Nachum, O., Norouzi, M., Xu, K. and Schuurmans, D. Bridging the Gap Between Value and Policy Based Reinforcement Learning. <em>NIPS 2017</em>.</li>\n",
      "</ul>\n",
      "\n",
      "<p><strong>Papers leveraging the maximum entropy principle</strong></p>\n",
      "<ul>\n",
      "  <li>Kappen, H. J. Path integrals and symmetry breaking for optimal control theory. <em>Journal of Statistical Mechanics: Theory And Experiment</em>, 2005(11): P11011, 2005.</li>\n",
      "  <li>Todorov, E. Linearly-solvable Markov decision problems. In <em>Advances in Neural Information Processing Systems</em>, pp. 1369–1376. MIT Press, 2007.</li>\n",
      "  <li>Todorov, E. General duality between optimal control and estimation. In IEEE Conf. on Decision and Control, pp. 4286–4292. IEEE, 2008.</li>\n",
      "  <li>Todorov, E. (2009). Compositionality of optimal control laws. In <em>Advances in Neural Information Processing Systems</em> (pp. 1856-1864).</li>\n",
      "  <li>Ziebart, B. D., Maas, A. L., Bagnell, J. A., and Dey, A. K. Maximum entropy inverse reinforcement learning. In AAAI Conference on Artificial Intelligence, pp. 1433–1438, 2008.</li>\n",
      "  <li>Toussaint, M. Robot trajectory optimization using approximate inference. In <em>Int. Conf. on Machine Learning</em>, pp. 1049–1056. ACM, 2009.</li>\n",
      "  <li>Ziebart, B. D. Modeling purposeful adaptive behavior with the principle of maximum causal entropy. PhD thesis, 2010.</li>\n",
      "  <li>Rawlik, K., Toussaint, M., and Vijayakumar, S. On stochastic optimal control and reinforcement learning by approximate inference. <em>Proceedings of Robotics: Science and Systems VIII</em>, 2012.</li>\n",
      "  <li>Fox, R., Pakman, A., and Tishby, N. Taming the noise in reinforcement learning via soft updates. In <em>Conf. on Uncertainty in Artificial Intelligence</em>, 2016.</li>\n",
      "</ul>\n",
      "\n",
      "<p><strong>Model-free RL in the real-world</strong></p>\n",
      "<ul>\n",
      "  <li>Gu, S., Lillicrap, T., Sutskever, I., and Levine, S. Continuous deep Q-learning with model-based acceleration. In Int. Conf. on Machine Learning, pp. 2829–2838, 2016.</li>\n",
      "  <li>M. Večerı́k, T. Hester, J. Scholz, F. Wang, O. Pietquin, B. Piot, N. Heess, T. Rothörl, T. Lampe, and M. Riedmiller, “Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards,” <em>arXiv preprint arXiv:1707.08817</em>, 2017.</li>\n",
      "</ul>\n",
      "\n",
      "<p><strong>Other references</strong><br />\n",
      "Heess, N., Silver, D., and Teh, Y.W. Actor-critic reinforcement learning with energy-based policies. In Workshop on Reinforcement Learning, pp. 43. Citeseer, 2012.</p>\n",
      "\n",
      "<p>Jaynes, E.T. Prior probabilities. IEEE Transactions on systems science and cybernetics, 4(3), pp.227-241, 1968.</p>\n",
      "\n",
      "<p>Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. Continuous control with deep reinforcement learning. ICLR 2016.</p>\n",
      "\n",
      "<p>Liu, Q. and Wang, D. Stein variational gradient descent: A general purpose bayesian inference algorithm. In <em>Advances In Neural Information Processing Systems</em>, pp. 2370–2378, 2016.</p>\n",
      "\n",
      "<p>Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A, Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. Human-level control through deep reinforcement learning. <em>Nature</em>, 518 (7540):529–533, 2015.</p>\n",
      "\n",
      "<p>Mnih, V., Badia, A.P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D. and Kavukcuoglu, K. Asynchronous methods for deep reinforcement learning. In <em>International Conference on Machine Learning</em> (pp. 1928-1937), 2016.</p>\n",
      "\n",
      "<p>O’Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V. PGQ: Combining policy gradient and Q-learning. <em>arXiv preprint arXiv:1611.01626</em>, 2016.</p>\n",
      "\n",
      "<p>Rusu, A. A., Vecerik, M., Rothörl, T., Heess, N., Pascanu, R. and Hadsell, R., Sim-to-real robot learning from pixels with progressive nets. <em>arXiv preprint arXiv:1610.04286</em>, 2016.</p>\n",
      "\n",
      "<p>Schulman, J., Levine, S., Abbeel, P., Jordan, M., &amp; Moritz, P. Trust region policy optimization. Proceedings of the 32nd International Conference on Machine Learning (ICML-15), 2015.</p>\n",
      "\n",
      "<p>Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M. and Dieleman, S. Mastering the game of Go with deep neural networks and tree search. <em>Nature</em>, 529(7587), 484-489, 2016.</p>\n",
      "\n",
      "<p>Sutton, R. S. and Barto, A. G. <em>Reinforcement learning: An introduction</em>, volume 1. MIT press Cambridge, 1998.</p>\n",
      "\n",
      "<p>Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W. and Abbeel, P. Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World. <em>arXiv preprint arXiv:1703.06907</em>, 2017.</p>\n",
      "\n",
      "<p>Wang, D., and Liu, Q. Learning to draw samples: With application to amortized MLE for generative adversarial learning. <em>arXiv preprint arXiv:1611.01722</em>, 2016.</p>\n",
      "\n",
      "<!-- [Mnih2015]:\n",
      "[Silver2016]:\n",
      "[Schulman2015]:\n",
      "[ZieBart2010]: -->\n",
      "<!-- [Ziebart2008]:\n",
      "[Todorov2008]:\n",
      "[Toussaint2009]:\n",
      "[Todorov2007]:\n",
      "[Todorov2009]:\n",
      "[Fox2016]:\n",
      "[Rawlik2012]:\n",
      "[Heess2012]:\n",
      "[WangLiu2016]:\n",
      "[Nachum2017]:\n",
      "[Schulman2017]:\n",
      "[ODonoghue2016]:\n",
      "[Gu2016]:\n",
      "[Rusu2016]:\n",
      "[Vecerik2017]: -->\n",
      "\n",
      "  Link: http://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/\n",
      "  PubDate: Fri, 06 Oct 2017 02:00:00 -0700\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20171019234522/http://bair.berkeley.edu:80/blog/feed.xml\n",
      "- Source: https://bair.berkeley.edu/blog/feed.xml\n",
      "  Channel Title: The Berkeley Artificial Intelligence Research Blog\n",
      "  Channel Description: The BAIR Blog\n",
      "- Title: Learning Long Duration Sequential Task Structure From Demonstrations with Application in Surgical Robotics\n",
      "  Description: <p style=\"text-align:center;\">\n",
      "<!--@Daniel arrange this however you want-->\n",
      "<img src=\"http://bair.berkeley.edu/blog/assets/surgical_robots/cutting-gif.gif\" height=\"180\" style=\"margin: 10px;\" />\n",
      "<img src=\"http://bair.berkeley.edu/blog/assets/surgical_robots/binpicking-gif.gif\" height=\"180\" style=\"margin: 10px;\" />\n",
      "<img src=\"http://bair.berkeley.edu/blog/assets/surgical_robots/debridement-gif.gif\" height=\"180\" style=\"margin: 10px;\" />\n",
      "<br />\n",
      "</p>\n",
      "\n",
      "<p>Deep imitation learning and deep reinforcement learning have potential to learn\n",
      "robot control policies that map high-dimensional sensor inputs to controls.\n",
      "While these approaches have been very successful at learning short duration tasks, such\n",
      "as grasping (Pinto and Gupta 2016, Levine et al. 2016) and peg insertion (Levine\n",
      "et al. 2016), scaling learning to longer time horizons can require a prohibitive\n",
      "amount of demonstration data—whether acquired from experts or self-supervised.\n",
      "Long-duration sequential tasks suffer from the classic problem of “temporal\n",
      "credit assignment”, namely, the difficulty in assigning credit (or blame) to\n",
      "actions under uncertainty of the time when their consequences are observed\n",
      "(Sutton 1984). However, long-term behaviors are often composed of short-term\n",
      "skills that solve decoupled subtasks. Consider designing a controller for\n",
      "parallel parking where the overall task can be decomposed into three phases:\n",
      "pulling up, reversing, and adjusting. Similarly, assembly tasks can often be\n",
      "decomposed into individual steps based on which parts need to be manipulated.\n",
      "These short-term skills can be parametrized more concisely—as an analogy,\n",
      "consider locally linear approximations to an overall nonlinear function—and\n",
      "this reduced parametrization can be substantially easier to learn.</p>\n",
      "\n",
      "<p>This post summarizes results from three recent papers that propose algorithms\n",
      "that learn to decompose a longer task into shorter subtasks.  We report\n",
      "experiments in the context of autonomous surgical subtasks and we believe the\n",
      "results apply to a variety of applications from manufacturing to home robotics.\n",
      "We present three algorithms: Transition State Clustering (TSC), Sequential\n",
      "Windowed Inverse Reinforcement Learning (SWIRL), and Deep Discovery of\n",
      "Continuous Options (DDCO).  TSC considers robustly learning important switching\n",
      "events (significant changes in motion) that occur across all demonstrations.\n",
      "SWIRL proposes an algorithm that approximates a value function by a sequence of\n",
      "shorter term quadratic rewards. DDCO is a general framework for imitation\n",
      "learning with a hierarchical representation of the action space. In retrospect,\n",
      "all three algorithms are special cases of the same general framework, where the\n",
      "demonstrator’s behavior is generatively modeled as a sequential composition of\n",
      "unknown closed-loop policies that switch when reaching parameterized “transition\n",
      "states”.</p>\n",
      "\n",
      "<!--more-->\n",
      "\n",
      "<h1 id=\"application-to-surgical-robotics\">Application to Surgical Robotics</h1>\n",
      "\n",
      "<p>Robots such as Intuitive Surgical’s da Vinci have facilitated millions of\n",
      "surgical procedures worldwide using local teleoperation.  Automation of surgical\n",
      "sub-tasks has the potential to reduce surgeon tedium and fatigue, operating\n",
      "time, and enable supervised tele-surgery over higher-latency networks. Designing\n",
      "surgical robot controllers is particularly difficult due to a limited field of\n",
      "view and imprecise actuation.</p>\n",
      "\n",
      "<p>As a concrete task, pattern cutting is one of the Fundamentals of Laparoscopic\n",
      "Surgery, a training suite required of surgical residents.   In this standard\n",
      "surgical training task, the  surgeon must cut and remove a pattern printed on a\n",
      "sheet of gauze, and is scored on time and accuracy:</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/blog/assets/surgical_robots/pattern-cutting-task.png\" alt=\"Figure 1: Pattern Cutting Task, from the Fundamentals of Laparoscopic Surgery.\" /><br />\n",
      "<i>\n",
      "Pattern cutting task from the Fundamentals of Laparoscopic Surgery.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>In (Murali 2015), we manually coded this task using hand-crafted Deterministic\n",
      "Finite Automaton on the da Vinci surgical robot. The DFA integrated 10 different\n",
      "manipulation primitives and two computer vision based checks:</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/blog/assets/surgical_robots/dfa-pattern-cutting.png\" alt=\"Figure 2: DFA for Pattern Cutting.\" width=\"600\" /><br />\n",
      "<i>\n",
      "Deterministic finite automaton from Murali et al. 2016 to automate pattern\n",
      "cutting.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>Designing this DFA required painstaking trial-and-error, and perceptual checks\n",
      "required constant tuning to account for lighting and registration changes. This\n",
      "motivated us to consider the extent to which we could learn such structure from\n",
      "demonstration data. This blog post describes our efforts over the last three\n",
      "years at learning hierarchical representations from demonstrations. This\n",
      "research has helped us automate several surgical robotic tasks with minimal\n",
      "expert design of the DFA, as shown in the three GIFs at the top of the post.</p>\n",
      "\n",
      "<h1 id=\"learning-transition-conditions\">Learning Transition Conditions</h1>\n",
      "\n",
      "<p>The first paper, Transition State Clustering (Krishnan et al. 2015), explores\n",
      "the problem of learning transition conditions from demonstrations, i.e.,\n",
      "conditions that trigger a switch or a transition between manipulation behaviors\n",
      "in a task.  In many important tasks, while the actual motions may vary and be\n",
      "noisy, each demonstration contains roughly the same sequence of primitive\n",
      "motions. This consistent, repeated structure can be exploited to infer global\n",
      "transition criteria by identifying state-space conditions correlated with\n",
      "significant changes in motion. By assuming a known sequential order of\n",
      "primitives, the problem reduces to segmenting each trajectory and corresponding\n",
      "those segments across trajectories. This involves finding a common set of\n",
      "segment-to-segment transition events.</p>\n",
      "\n",
      "<p>We formalized this intuition in an algorithm called Transition State Clustering\n",
      "(TSC). Let <script type=\"math/tex\">D=\\{d_i\\}</script> be a set of demonstrations of a robotic task. Each\n",
      "demonstration of a task $d$ is a discrete-time sequence of $T$ state vectors in\n",
      "a feature-space $\\mathcal{X}$. The feature space is a concatenation of kinematic\n",
      "features $X$ (e.g., robot position) and sensory features $V$. These were\n",
      "low-dimensional visual features from the environment calculated by hard-coded\n",
      "image processing and manual annotation.</p>\n",
      "\n",
      "<p>A segmentation of a task is defined as a function $\\mathcal{S}$ that maps each\n",
      "trajectory to a non-decreasing sequence of integers in ${1,2,…,k}$.  This\n",
      "function tells us more than just the endpoints of segments, since it also labels\n",
      "each segment according to its sub-task. By contrast, a transition indicator\n",
      "function $\\mathcal{T}$ is maps each demonstration $d$ to a sequence of\n",
      "indicators in ${0,1}$:</p>\n",
      "\n",
      "<script type=\"math/tex; mode=display\">\\mathbf{T}: d \\mapsto ( a_t )_{1,...,|d|}, a_t \\in {0,1}.</script>\n",
      "\n",
      "<p>such that <script type=\"math/tex\">\\mathcal{T}(d)_t</script> indicates whether the demonstration switched from\n",
      "one sub-task to another after time $t$. For a demonstration $d_i$, let $o_{i,t}$\n",
      "denote the kinematic state, visual state, and time $(x,v,t)$ at time $t$.\n",
      "Transition States are the set of state-time tuples where the indicator is 1:</p>\n",
      "\n",
      "<script type=\"math/tex; mode=display\">\\Gamma = \\bigcup_{i}^N ~\\{o_{i,t} \\in d_i ~: \\mathbf{T}(d_i)_t = 1\\}.</script>\n",
      "\n",
      "<p>In TSC, we model the probability distribution that generates $\\Gamma$ as a\n",
      "Gaussian Mixture Model and identify the mixture components.  These components\n",
      "identify regions of the state space correlated with candidate transitions. We\n",
      "can take any motion-based model for detecting changes in behavior and generate\n",
      "candidates. Then, we probabilistically ground these candidate transitions in\n",
      "state-space and perceptual conditions that are consistent across demonstrations.\n",
      "Intuitively, this algorithm consists of two steps: first segmentation, and then\n",
      "clustering the segment end-points.</p>\n",
      "\n",
      "<p>There are a number of important implementation details to make this model work\n",
      "in practice on real noisy data. Since the kinematic and visual features often\n",
      "have very different scales and topological properties, we often have to model\n",
      "them separately during the clustering step. We hierarchically apply a GMM model\n",
      "by first performing a hard clustering on the kinematic features, and then within\n",
      "each cluster fitting the probabilistic model over the perceptual features. This\n",
      "allows us to prune out clusters that are not representative (i.e., do not have\n",
      "transitions from all demonstrations). Furthermore, Hyper-parameter selection is\n",
      "a known problem in mixture models. Recent results in Bayesian statistics can\n",
      "mitigate some of these problems by defining a soft prior of the number of\n",
      "mixtures. The Dirichlet Process (DP) defines a distribution over the parameters\n",
      "of discrete distributions, in our case, the probabilities of a categorical\n",
      "distribution, as well as the size of its support $m$ (Kulis 2011). The\n",
      "hyper-parameters of the DP can be inferred with variational Expectation\n",
      "Maximization.</p>\n",
      "\n",
      "<p>In the pattern cutting task, TSC found the following transition conditions:</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/blog/assets/surgical_robots/pattern-cutting-concept.png\" height=\"175\" style=\"margin: 30px;\" alt=\"Figure 3: Conceptual diagram of pattern cutting.\" />\n",
      "<img src=\"http://bair.berkeley.edu/blog/assets/surgical_robots/pattern-cutting.png\" height=\"200\" alt=\"Figure 3: Conceptual diagram of pattern cutting.\" /><br />\n",
      "<i>\n",
      "Surgical pattern cutting task.  Left: manually identified transitions. Right:\n",
      "automatically discovered transition states (a) and transition state clusters\n",
      "(b).\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>We marked 6 manually identified primitive motions from (Murali et al. 2015): (1)\n",
      "start, (2) notch, (3) finish 1st cut, (4) cross-over, (5) finish 2nd cut, and\n",
      "(6) connect the two cuts. TSC automatically identifies 7 segments, which\n",
      "correspond well to our prior work. It is worth noting that there is one extra\n",
      "cluster (marked 2’), that does not correspond to a transition in the manual\n",
      "segmentation.</p>\n",
      "\n",
      "<p>At 2’, the operator finishes a notch and begins to cut. While at a logical\n",
      "level, notching and cutting are both penetration actions, they correspond to two\n",
      "different motion regimes due to the positioning of the end-effector. TSC\n",
      "separates them into different clusters even though the human annotators\n",
      "overlooked this important transition.</p>\n",
      "\n",
      "<h1 id=\"connection-to-inverse-reinforcement-learning\">Connection to Inverse Reinforcement Learning</h1>\n",
      "\n",
      "<p>We next explored how the transitions learned by TSC can be used to shape rewards\n",
      "in long horizon tasks. Sequential Windowed Inverse Reinforcement Learning\n",
      "(Krishnan et al. 2016),  models a task as a sequence of quadratic reward\n",
      "functions</p>\n",
      "\n",
      "<script type=\"math/tex; mode=display\">\\mathbf{R}_{seq} = [R_1, \\ldots ,R_k ]</script>\n",
      "\n",
      "<p>and transition regions</p>\n",
      "\n",
      "<script type=\"math/tex; mode=display\">G = [ \\rho_1, \\ldots,\\rho_k ]</script>\n",
      "\n",
      "<p>such that $R_1$ is the reward function until $\\rho_1$ is reached, after which\n",
      "$R_2$ becomes the reward and so on.</p>\n",
      "\n",
      "<p>We assume that we have access to a supervisor that provides demonstrations which\n",
      "are optimal w.r.t an unknown reward function $\\mathbf{R}^*$ (not necessarily\n",
      "quadratic), and which reach each $\\rho \\in G$ (also unknown) in the same order.\n",
      "SWIRL is an algorithm to recover <script type=\"math/tex\">\\mathbf{R}_{seq}</script> and $G$ from the\n",
      "demonstration trajectories. SWIRL applies to tasks with a discrete or continuous\n",
      "state-space and a discrete action-space. The state space can represent spatial,\n",
      "kinematic, or sensory states (e.g., visual features), as long as the\n",
      "trajectories are smooth and not very high-dimensional.  Finally,\n",
      "$\\mathbf{R}_{seq}$ and $G$ can be used in an RL algorithm to find an optimal\n",
      "policy for a task.</p>\n",
      "\n",
      "<p>TSC can be interpreted as inferring the subtask transition regions $G$. Once the\n",
      "transitions are found, SWIRL applies Maximum Entropy Inverse Reinforcement\n",
      "Learning to find a local quadratic reward function that guides the robot to the\n",
      "transition condition. Segmentation further simplifies the estimation of dynamics\n",
      "models, which are required for inference in MaxEnt-IRL, since many complex\n",
      "systems can be locallyapproximated linearly in a short time horizon.  The goal\n",
      "of MaxEnt-IRL is to find a reward function such that an optimal policy w.r.t\n",
      "that reward function is close to the expert demonstration. The agent is modeled\n",
      "as nosily optimal, where it takes actions from a policy $\\pi$:</p>\n",
      "\n",
      "<script type=\"math/tex; mode=display\">\\pi(a \\mid s, \\theta) \\propto \\exp\\{A_\\theta(s,a)\\}.</script>\n",
      "\n",
      "<p>$A_\\theta$ is the advantage function (gap between the values of action $a$ and\n",
      "of the optimal action in state $s$) for the reward parametrized by $\\theta$.\n",
      "The objective is to maximize the log-likelihood that the demonstration\n",
      "trajectories were generated by  $\\theta$. In MaxEnt-IRL, this objective can be\n",
      "estimated reliably in two cases, discrete and linear-gaussian systems, since it\n",
      "requires an efficient forward search of the policy given a particular reward\n",
      "parametrized by $\\theta$. Thus, we assume that our demonstrations can be modeled\n",
      "either discretely or with linear dynamics.</p>\n",
      "\n",
      "<p>Learning a policy from $\\mathbf{R}_{seq}$ and $G$ is nontrivial because solving\n",
      "$k$ independent problems neglects any shared structure in the value function\n",
      "during the policy learning phase (e.g., a common failure state). Jointly\n",
      "learning over all segments introduces a dependence on history, namely, any\n",
      "policy must complete step $i$ before step $i+1$. Learning a memory-dependent\n",
      "policy could lead to an exponential overhead of additional states. SWIRL\n",
      "exploits the fact that TSC is in a sense a Markov process, and shows that the\n",
      "problem can be posed as a proper MDP in a lifted state-space that includes an\n",
      "indicator variable of the highest-index ${1,…,k}$ transition region that has\n",
      "been reached so far.</p>\n",
      "\n",
      "<p>SWIRL applies a variant of Q-Learning to optimize the policy over the sequential\n",
      "rewards. The basic change to the algorithm is to augment the state-space with an\n",
      "indicator vector that indicates the transition regions that have been reached.\n",
      "Each of the rollouts now records a tuple</p>\n",
      "\n",
      "<script type=\"math/tex; mode=display\">(s, i \\in {0,...,k-1},a,r, s', i' \\in {0,...,k-1})</script>\n",
      "\n",
      "<p>that additionally stores this information. The Q function is now defined over\n",
      "states, actions, and segment index–which also selects the appropriate local\n",
      "reward function:</p>\n",
      "\n",
      "<script type=\"math/tex; mode=display\">Q(s,a,v) = R_k(s,a) + \\arg \\max_{a} Q(s',a, k')</script>\n",
      "\n",
      "<p>We also need to define an exploration policy, i.e., a stochastic policy with\n",
      "which we will collect rollouts. To initialize the Q-Learning, we apply\n",
      "Behavioral Cloning locally for each of the segments to get a policy $\\pi_i$. We\n",
      "apply an $\\epsilon$-greedy version of these policies to collect rollouts.</p>\n",
      "\n",
      "<p>We evaluated SWIRL on a deformable sheet tensioning task. A sheet of surgical\n",
      "gauze is fixtured at the two far corners using a pair of clips. The unclipped\n",
      "part of the gauze is allowed to rest on soft silicone padding. The robot’s  task\n",
      "is to reach for the unclipped part, grasp it, lift the gauze, and tension the\n",
      "sheet to be as planar as possible. An open-loop policy, one that does not react\n",
      "to unexpected changes, typically fails on this task because it requires some\n",
      "feedback of whether gauze is properly grasped, how the gauze has deformed after\n",
      "grasping, and visual feedback of whether the gauze is planar. The task is\n",
      "sequential, as some grasps pick up more or less of the material and the\n",
      "flattening procedure has to be accordingly modified.</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/blog/assets/surgical_robots/tensioning-task.png\" alt=\"Figure 4: Deformable Sheet Tensioning Setup.\" /><br />\n",
      "<i>\n",
      "Deformable sheet tensioning setup.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>We provided 15 demonstrations through a keyboard-based tele-operation interface.\n",
      "The average length of the demonstrations was 48.4 actions (although we sampled\n",
      "observations at a higher frequency, about 10 observations for every action).\n",
      "From these 15 demonstrations, SWIRL identifies four segments. One of the\n",
      "segments corresponds to moving to the correct grasping position, one corresponds\n",
      "to making the grasp, one lifting the gauze up again, and one corresponds to\n",
      "straightening the gauze. One of the interesting aspects of this task is that the\n",
      "segmentation requires multiple features, and segmenting any single signal may\n",
      "miss an important feature.</p>\n",
      "\n",
      "<p>Then, we tried to learn a policy from the rewards constructed by SWIRL. We\n",
      "define a Q-Network with a single-layer Multi-Layer Perceptron with 32 hidden\n",
      "units and sigmoid activation. For each of the segments, we apply Behavioral\n",
      "Cloning locally with the same architecture as the Q-network (with an additional\n",
      "softmax over the output layer) to get an initial policy. We roll out 100 trials\n",
      "with an $\\epsilon=0.1$ greedy version of these segmented policies. The results\n",
      "are depicted below:</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/blog/assets/surgical_robots/swirl-tensioning.png\" alt=\"Figure 5: Deformable Sheet Tensioning Demonstration.\" /><br />\n",
      "<i>\n",
      "A representative demonstration of the deformable sheet tensioning task with\n",
      "relevant features plotted over time. SWIRL identifies 4 segments which\n",
      "correspond to reaching, grasping, lifting, and tensioning.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>SWIRL achieves more than a 4 times higher reward than ab initio RL, 3  time\n",
      "higher than pure behavioral cloning, and a 56% higher reward than naively\n",
      "applying behavioral cloning with TSC segments.</p>\n",
      "\n",
      "<h1 id=\"hierarchical-representations\">Hierarchical Representations</h1>\n",
      "\n",
      "<p>We are now exploring a generalization of TSC and  SWIRL with a new algorithm:\n",
      "Deep Discovery of Continuous Options (DDCO Krishnan et al. 2017, to be presented\n",
      "at the 1st Conference on Robot Learning in November).</p>\n",
      "\n",
      "<p>An option represents a low-level policy that can be invoked by a high-level\n",
      "policy to perform a certain sub-task. Formally, an option $h$ in an options set\n",
      "$\\mathcal H$ is specified by a control policy $\\pi_h(a_t | s_t)$ and a\n",
      "stochastic termination condition $\\psi_h(s_t)\\in[0,1]$. The high-level policy\n",
      "$\\eta(h_t | s_t)$ defines the distribution over options given the state. Once an\n",
      "option $h$ is invoked, physical controls are selected by the option’s policy\n",
      "$\\pi_h$ until it terminates. After each physical control is applied and the next\n",
      "state $s’$ is reached, the option $h$ terminates with probability $\\psi_h(s’)$,\n",
      "and if it does then the high-level policy selects a new option $h’$ with\n",
      "distribution $\\eta(h’ | s’)$. Thus the interaction of the hierarchical control\n",
      "policy $\\langle\\eta,(\\pi_h,\\psi_h)_{h\\in\\mathcal H}\\rangle$ with the system\n",
      "induces a stochastic process over the states $s_t$, the options $h_t$, the\n",
      "controls $a_t$, and the binary termination indicators $b_t$.</p>\n",
      "\n",
      "<p>DDCO is a policy-gradient algorithm that discovers parametrized options by\n",
      "fitting their parameters to maximize the likelihood of a set of demonstration\n",
      "trajectories. We denote by $\\theta$ the vector of all trainable parameters used\n",
      "for $\\eta$ and for $\\pi_h$ and $\\psi_h$ of each option $h\\in\\mathcal H$. For\n",
      "example, $\\theta$ can be the weights and biases of a feed-forward network that\n",
      "computes these probabilities. We wish to find the $\\theta\\in\\Theta$ that\n",
      "maximizes the log-likelihood of generating each demonstration trajectory\n",
      "$\\xi=(s_0,a_0,s_1,\\ldots,s_T)$. The challenge is that this log-likelihood\n",
      "depends on the latent variables in the stochastic process, the options and the\n",
      "termination indicators $\\zeta = (b_0,h_0,b_1,h_1,\\ldots,h_{T-1})$. DDCO\n",
      "optimizes this objective with an Expectation-Gradient algorithm:</p>\n",
      "\n",
      "<script type=\"math/tex; mode=display\">\\nabla_\\theta L[\\theta;\\xi] = \\mathbb{E}_\\theta[\\nabla_\\theta \\log \\mathbb{P}_\\theta(\\zeta,\\xi) | \\xi],</script>\n",
      "\n",
      "<p>where $\\mathbb{P}_\\theta(\\zeta,\\xi)$ is the joint probability of the latent and\n",
      "observable variables, given by</p>\n",
      "\n",
      "<script type=\"math/tex; mode=display\">\\mathbb{P}_\\theta(\\zeta,\\xi) = p_0(s_0) \\delta_{b_0=1}\\eta(h_0 | s_0)\n",
      "    \\prod_{t=1}^{T-1} \\mathbb{P}_\\theta(b_t, h_t | h_{t-1}, s_t) \\prod_{t=0}^{T-1}\n",
      "    \\pi_{h_t}(a_t | s_t) p(s_{t+1} |s_t, a_t) ,</script>\n",
      "\n",
      "<p>where in the latent transition <script type=\"math/tex\">\\mathbb{P}_\\theta(b_t, h_t | h_{t-1}, s_t)</script> we have\n",
      "with probability $\\psi_{h_{t-1}}(s_t)$ that $b_t=1$ and $h_t$ is drawn from\n",
      "$\\eta(\\cdot|s_t)$, and otherwise that $b_t=0$ and $h_t$ is unchanged, i.e.</p>\n",
      "\n",
      "<script type=\"math/tex; mode=display\">% <![CDATA[\n",
      "\\begin{align}\n",
      "    \\mathbb{P}_\\theta(b_t {=} 1, h_t | h_{t-1}, s_t) &= \\psi_{h_{t-1}}(s_t) \\eta(h_t | s_t) \\\\\n",
      "    \\mathbb{P}_\\theta(b_t {=} 0, h_t | h_{t-1}, s_t) &= (1 - \\psi_{h_{t-1}}(s_t)) \\delta_{h_t = h_{t-1}}.\n",
      "\\end{align} %]]></script>\n",
      "\n",
      "<p>The log-likelihood gradient can be computed in two steps, an E-step where the\n",
      "marginal posteriors</p>\n",
      "\n",
      "<script type=\"math/tex; mode=display\">u_t(h) = \\mathbb{P}_\\theta(h_t {=} h | \\xi); \\quad v_t(h) = \\mathbb{P}_\\theta(b_t {=} 1,\n",
      "h_t {=} h | \\xi); \\quad w_t(h) = \\mathbb{P}_\\theta(h_t {=} h, b_{t+1} {=} 0 | \\xi)</script>\n",
      "\n",
      "<p>are computed using a forward-backward algorithm similar to Baum-Welch, and a\n",
      "G-step:</p>\n",
      "\n",
      "<script type=\"math/tex; mode=display\">% <![CDATA[\n",
      "\\begin{align}\n",
      "\\nabla_\\theta L[\\theta;\\xi] &= \\sum_{h\\in\\mathcal{H}} \\Biggl( \\sum_{t=0}^{T-1}\n",
      "    \\Biggl(v_t(h) \\nabla_\\theta \\log \\eta(h | s_t) +  u_t(h)\\nabla_\\theta\n",
      "    \\log \\pi_h(a_t | s_t)\\Biggr) \\\\\n",
      "    & + \\sum_{t=0}^{T-2} \\Biggl((u_t(h)-w_t(h)) \\nabla_\\theta \\log\n",
      "    \\psi_h(s_{t+1}) + w_t(h) \\nabla_\\theta \\log (1 - \\psi_h(s_{t+1}))\n",
      "    \\Biggr)\\Biggr).\n",
      "\\end{align} %]]></script>\n",
      "\n",
      "<p>The gradient computed above can then be used in any stochastic gradient descent\n",
      "algorithm. In our experiments we use Adam and Momentum.</p>\n",
      "\n",
      "<p>We evaluated DDCO in an Imitation Learning setting with surgical robotic tasks.\n",
      "In one task, the robot is given a foam bin with a pile of 5–8 needles of three\n",
      "different types, each 1–3mm in diameter. The robot must extract needles of a\n",
      "specified type and place them in an “accept” cup, while placing all other\n",
      "needles in a “reject” cup. The task is successful if the entire foam bin is\n",
      "cleared into the correct cups. To define the state space for this task, we first\n",
      "generate binary images from overhead stereo images, and apply a color-based\n",
      "segmentation to identify the needles (the “image” input). Then, we use a\n",
      "classifier trained in advance on 40 hand-labeled images to identify and provide\n",
      "a candidate grasp point, specified by position and direction in image space (the\n",
      "“grasp” input).  Additionally, the 6 DoF robot gripper pose and the open-closed\n",
      "state of the gripper are observed (the “kin” input). The state space of the\n",
      "robot is (“image”, “grasp”, “kin”), and the control space is the 6 joint angles\n",
      "and the gripper angle.</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/blog/assets/surgical_robots/dvrk-bin-picking.png\" alt=\"Figure 6: Needle Pick and Place Task.\" /><br />\n",
      "<i>\n",
      "Needle pick and place task on the surgical robot.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>In 10 trials, 7/10 were successful. The main failure mode was unsuccessful\n",
      "grasping due to picking either no needles or multiple needles. As the piles were\n",
      "cleared and became sparser, the robot’s grasping policy became somewhat brittle.\n",
      "The grasp success rate was 66% on 99 attempted grasps. In contrast, we rarely\n",
      "observed failures at the other aspects of the task, reaching 97% successful\n",
      "recovery on 34 failed grasps.</p>\n",
      "\n",
      "<p>The learned options are interpretable on intuitive task boundaries. For each of\n",
      "the 4 options, we plot how heavily the different inputs are weighted (image,\n",
      "grasp, or kin) in computing the option’s action. Nonzero values of the ReLU\n",
      "units are marked in white and indicate input relevance:</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/blog/assets/surgical_robots/ddco-activations.png\" alt=\"Figure 7: DDCO Options.\" /><br />\n",
      "<i>\n",
      "We plot the average activations of the feature layer for of each option,\n",
      "indicating which inputs (image, gripper angle, or kinematics ) are relevant to\n",
      "the policy and termination.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>We see that the options are clearly specialized. The first option has a strong\n",
      "dependence only on the grasp candidate, the second option attends almost\n",
      "exclusively to the image, while the last two options rely mostly on the\n",
      "kinematics and grasp features.</p>\n",
      "\n",
      "<h1 id=\"conclusion\">Conclusion</h1>\n",
      "\n",
      "<p>To summarize, learning sequential task structure from demonstrations has many\n",
      "applications in robotics such as automating surgical sub-tasks and can be\n",
      "facilitated by segmenting to learning task structure. We see several avenues for\n",
      "future work: (1) representations that better model rotational geometry and\n",
      "configuration spaces, (2) hybrid schemes that consider both parametrized\n",
      "primitives and those derived from analytic formulae, and (3) consideration of\n",
      "state-space segmentation as well as temporal segmentation.</p>\n",
      "\n",
      "<hr />\n",
      "\n",
      "<h2 id=\"references\">References</h2>\n",
      "\n",
      "<p>(For links to papers, see the homepages of <a href=\"https://www.ocf.berkeley.edu/~sanjayk/\">Sanjay\n",
      "Krishnan</a> or <a href=\"http://goldberg.berkeley.edu/pubs/\">Ken\n",
      "Goldberg</a>.)</p>\n",
      "\n",
      "<p>Sanjay Krishnan*, Roy Fox*, Ion Stoica, Ken Goldberg. DDCO: Discovery of Deep\n",
      "Continuous Options for Robot Learning from Demonstrations. Conference on Robot\n",
      "Learning (CoRL). 2017.</p>\n",
      "\n",
      "<p>Sanjay Krishnan, Animesh Garg, Richard Liaw, Brijen Thananjeyan, Lauren Miller,\n",
      "Florian T. Pokorny, Ken Goldberg. SWIRL: A Sequential Windowed Inverse\n",
      "Reinforcement Learning Algorithm for Robot Tasks With Delayed Rewards. Workshop\n",
      "on Algorithmic Foundations of Robotics (WAFR) 2016.</p>\n",
      "\n",
      "<p>Sanjay Krishnan*, Animesh Garg*, Sachin Patil, Colin Lea, Gregory Hager,\n",
      "Pieter Abbeel, Ken Goldberg. Transition State Clustering: Unsupervised Surgical\n",
      "Task Segmentation For Robot Learning. International Symposium on Robotics\n",
      "Research (ISRR). 2015.</p>\n",
      "\n",
      "<p>Adithyavairavan Murali*, Siddarth Sen*, Ben Kehoe, Animesh Garg, Seth McFarland,\n",
      "Sachin Patil, W. Douglas Boyd, Susan Lim, Pieter Abbeel, Ken Goldberg. Learning\n",
      "by Observation for Surgical Subtasks: Multilateral Cutting of 3D Viscoelastic\n",
      "and 2D Orthotropic Tissue Phantoms. International Conference on Robotics and\n",
      "Automation (ICRA). May 2015.</p>\n",
      "\n",
      "<h2 id=\"external-references\">External References</h2>\n",
      "\n",
      "<p>Richard Sutton. Temporal credit assignment in reinforcement learning. 1984.</p>\n",
      "\n",
      "<p>Richard Sutton, Doina Precup, and Satinder Singh. Between MDPs and semi-MDPs: A\n",
      "framework for temporal abstraction in reinforcement learning. Artificial\n",
      "intelligence. 1999.</p>\n",
      "\n",
      "<p>Lerrel Pinto, and Abhinav Gupta. Supersizing self-supervision: Learning to grasp\n",
      "from 50k tries and 700 robot hours. International Conference on Robotics and\n",
      "Automation (ICRA). 2016.</p>\n",
      "\n",
      "<p>Sergey Levine, Peter Pastor, Alex Krizhevsky, Julian Ibarz, Deirdre Quillen.\n",
      "Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and\n",
      "Large-Scale Data Collection (International Journal of Robotics Research). 2017.</p>\n",
      "\n",
      "<p>Sergey Levine*, Chelsea Finn*, Trevor Darrell, and Pieter Abbeel. End-to-end\n",
      "training of deep visuomotor policies. Journal of Machine Learning Research\n",
      "(JMLR). 2016.</p>\n",
      "\n",
      "  Link: http://bair.berkeley.edu/blog/2017/10/17/lfd-surgical-robots/\n",
      "  PubDate: Tue, 17 Oct 2017 02:00:00 -0700\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20171120150744/http://bair.berkeley.edu:80/blog/feed.xml\n",
      "- Source: https://bair.berkeley.edu/blog/feed.xml\n",
      "  Channel Title: The Berkeley Artificial Intelligence Research Blog\n",
      "  Channel Description: The BAIR Blog\n",
      "- Title: The Emergence of a Fovea while Learning to Attend\n",
      "  Description: <h2 id=\"why-we-need-attention\">Why we need Attention</h2>\n",
      "\n",
      "<p>What we see through our eyes is only a very small part of the world around us.  At any given time our eyes are sampling only a fraction of the surrounding light field. Even within this fraction, most of the resolution is dedicated to the center of gaze which has the highest concentration of <em>ganglion cells</em>. These cells are responsible for conveying a retinal image from our eyes to our brain. Unlike a camera, the spatial distribution of ganglion cells is highly non-uniform. As a result, our brain receives a <em>foveated</em> image:</p>\n",
      "\n",
      "<table class=\"col-2\">\n",
      "  <tr>\n",
      "    <td style=\"text-align:center;\">\n",
      "\t\t\t<img src=\"http://bair.berkeley.edu/blog/assets/fovea/bee.png\" width=\"500\" />\n",
      "\t\t</td>\n",
      "    <td style=\"text-align:center;\">\n",
      "\t\t\t<img src=\"http://bair.berkeley.edu/blog/assets/fovea/butterfly.png\" width=\"500\" />\n",
      "\t\t</td>\n",
      "  </tr>\n",
      "</table>\n",
      "<p style=\"text-align:center;\">\n",
      "<i>\n",
      "A foveated image with a center of gaze on the bee (left) and butterfly (right)\n",
      "(<a href=\"https://en.wikipedia.org/wiki/Foveated_imaging\">source</a>).\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>Despite the fact that these cells cover only a fraction of our visual field, roughly 30% of our cortex is still dedicated to processing the signal that they provide. You can imagine our brain would have to be impractically large to handle the full visual field at high resolution. Suffice it to say, the amount of neural processing dedicated to vision is rather large and it would be beneficial to survival if it were used efficiently.</p>\n",
      "\n",
      "<p><em>Attention</em> is a fundamental property of many intelligent systems. Since the resources of any physical system are limited, it is important to allocate them in an effective manner. Attention involves the dynamic allocation of information processing resources to best accomplish a specific task. In nature, we find this very apparent in the design of animal visual systems. By moving gaze rapidly within the scene, limited neural resources are effectively spread over the entire visual scene.</p>\n",
      "\n",
      "<!--more-->\n",
      "\n",
      "<h2 id=\"overt-attention\">Overt Attention</h2>\n",
      "\n",
      "<p>In this work, we study <em>overt</em> attention mechanisms which involve the explicit movement of the sensory organ. An example of this form of attention can be seen in the adolescent jumping spider:</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/blog/assets/fovea/spider.gif\" /><br />\n",
      "<i>\n",
      "An adolescent jumping spider using overt attention.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>We can see the spider is attending to different parts of its environment by making careful, deliberate movements of its body. When peering through its translucent head, you can even see the spider moving its eye stalks in a similar manner to how humans move their own eyes. These eye movements are called <em>saccades</em>.</p>\n",
      "\n",
      "<p>In this work, we build a model visual system that must make saccades over a scene in order to find and recognize an object. This model allows us to study the properties of an attentional system by exploring the design parameters that optimize performance. One parameter of interest in visual neuroscience is the <em>retinal sampling lattice</em> which defines the relative positions of the array of ganglion cells in our eyes.</p>\n",
      "\n",
      "<table class=\"col-2\">\n",
      "  <tr>\n",
      "    <td style=\"text-align:center;\">\n",
      "\t\t\t<img src=\"http://bair.berkeley.edu/blog/assets/fovea/translate.gif\" width=\"500\" />\n",
      "\t\t</td>\n",
      "    <td style=\"text-align:center;\">\n",
      "\t\t\t<img src=\"http://bair.berkeley.edu/blog/assets/fovea/nnmodel.png\" width=\"500\" />\n",
      "\t\t</td>\n",
      "  </tr>\n",
      "</table>\n",
      "<p style=\"text-align:center;\">\n",
      "<i>\n",
      "(Left) Our model retinal sampling lattice attending to different parts of a simple scene. (Right) Our neural network model which controls the window of attention.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<h2 id=\"approximating-evolution-through-gradient-descent\">Approximating Evolution through Gradient Descent</h2>\n",
      "\n",
      "<p>Evolutionary pressure has presumably tuned the retinal sampling lattice in the primate retina to be optimal for visual search tasks faced by the animal. In lieu of simulating evolution, we utilize a more efficient <em>stochastic gradient descent</em> procedure for our in-silico model by constructing a fully differentiable dynamic model of attention.</p>\n",
      "\n",
      "<p>Most neural networks are composed of learnable feature extractors which transform a fixed input to a more abstract representation such as a category. While the internal features (i.e. weight matrices and kernel filters) are learned during training, the geometry of the input remains fixed. We extend the deep learning framework to create learnable <em>structural features</em>. We learn the geometry of the neural sampling lattice in the retina.</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/blog/assets/fovea/structure.png\" /><br />\n",
      "<i>\n",
      "Structural features of one cell in the lattice.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>The retinal sampling lattice of our model is learned via backpropagation. Similar to the way weights are adjusted in a neural network, we adjust the parameters of the retinal tiling to optimize a loss function. We initialize the retinal sampling lattice to a regular square grid and update the parameterization of this layout using gradient descent.</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/blog/assets/fovea/learning.png\" width=\"500\" /><br />\n",
      "<i>\n",
      "Learning structural features from initialization using gradient descent.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>Over time, this layout will converge to a configuration which is locally optimal to minimize the task loss. In our case, we classify of the MNIST digit in a larger visual scene. Below we see how the retinal layout changes during training:</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/blog/assets/fovea/by4.png\" /><br />\n",
      "<i>\n",
      "Retinal sampling lattice during training at initialization, 1, 10, 100 epochs respectively.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>Surprisingly, the cells change in a very structured manner, smoothly transforming from a uniform grid to an eccentricity dependent lattice. We notice a concentration of high acuity cells appear near the center of the sampling array.  Furthermore, the cells spread their individual centers to create a sampling lattice which covers the full image.</p>\n",
      "\n",
      "<h2 id=\"controlling-the-emergence-of-a-fovea\">Controlling the Emergence of a Fovea</h2>\n",
      "\n",
      "<p>Since our model is in-silico, we can endow our model with properties not found in nature to see what other layouts will emerge. For example, we can give our model the ability to zoom in and out of an image by rescaling the entire grid to cover a smaller or larger area:</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/blog/assets/fovea/rescale.gif\" width=\"300\" /><br />\n",
      "<i>\n",
      "Retinal sampling lattice which also has the ability to rescale itself.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>We show the difference in the learned retinal layout below. For comparison, the left image is the retinal layout when our model does not have the ability to zoom while the right image is the layout learned when zooming is possible.</p>\n",
      "\n",
      "<table class=\"col-2\">\n",
      "  <tr>\n",
      "    <td style=\"text-align:center;\">\n",
      "\t\t\t<img src=\"http://bair.berkeley.edu/blog/assets/fovea/translate_only.png\" width=\"200\" />\n",
      "\t\t</td>\n",
      "    <td style=\"text-align:center;\">\n",
      "\t\t\t<img src=\"http://bair.berkeley.edu/blog/assets/fovea/translate_and_zoom.png\" width=\"200\" />\n",
      "\t\t</td>\n",
      "  </tr>\n",
      "</table>\n",
      "<p style=\"text-align:center;\">\n",
      "<i>\n",
      "(Left) Retinal lattice of a model only able to translate. (Right) Retinal lattice of a model able to translate and zoom.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>When our attention model is able to zoom, a very different layout emerges. Notice there is much less diversity in the retinal ganglion cells. They cells keep many of the properties they were initialized with.</p>\n",
      "\n",
      "<p>To get a better idea of the utility of our learned retinal layout, we compared the performance of a retina with a fixed (unlearnable) lattice, a learnable lattice without zoom and a learnable lattice with zoom:</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/blog/assets/fovea/comparison.png\" width=\"500\" /><br />\n",
      "<i>\n",
      "Performance on two versions (Dataset 1 and Dataset 2) of the Cluttered MNIST dataset. Dataset 2 contains randomly resized MNIST digits making it more difficult than Dataset 1.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>Perhaps not surprisingly, having zoom/learnable lattice significantly outperform a fixed lattice which can only translate. But what is interesting is the performance between a learnable lattice only with the ability to translate performs about as well as a model which can also zoom. This is further evidence that zooming and a foveal layout of the retinal lattice could be serving the same functional purpose.</p>\n",
      "\n",
      "<h2 id=\"interpretability-of-attention\">Interpretability of Attention</h2>\n",
      "\n",
      "<p>Earlier, we described the utility of attention in efficiently utilizing limited resources. Attention also provides insight into how the complex systems we build function internally. When our vision model attends over specific parts of an image during its processing, we get an idea of what the model deems relevant to perform a task. In our case, the model solves the recognition task by learning to place its fovea over the digit indicating its utility in classifying the digit. We also see the model in the bottom row utilizes its ability to zoom for the same purpose.</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/blog/assets/fovea/3by4.png\" width=\"600\" /><br />\n",
      "<i>\n",
      "The attention movements our model takes unrolled in time. Model with fixed lattice (top), learnable lattice (center), learnable lattice with zoom ability (bottom).\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<h2 id=\"conclusion\">Conclusion</h2>\n",
      "\n",
      "<p>Often we find loose inspiration from biology to motivate our machine learning models. The work by Hubel and Wiesel <sup id=\"fnref:3\"><a href=\"#fn:3\" class=\"footnote\">1</a></sup> inspired the Neocognitron model <sup id=\"fnref:4\"><a href=\"#fn:4\" class=\"footnote\">2</a></sup> which in turn inspired the Convolutional Neural Network <sup id=\"fnref:5\"><a href=\"#fn:5\" class=\"footnote\">3</a></sup> as we know it today. In this work, we go in the other direction where we try to explain a physical feature we observe in biology using the computational models developed in deep learning<sup id=\"fnref:2\"><a href=\"#fn:2\" class=\"footnote\">4</a></sup>. In the future, these results may lead us to think about new ways of designing the front end of active vision systems, modeled after the foveated sampling lattice of the primate retina.  We hope this virtuous cycle of inspiration continues in the future.</p>\n",
      "\n",
      "<p>If you want to learn more, check out our paper published in ICLR 2017:</p>\n",
      "\n",
      "<p><a href=\"https://arxiv.org/abs/1611.09430\">Emergence of foveal image sampling from learning to attend in visual scenes</a><br />\n",
      "(https://arxiv.org/abs/1611.09430)</p>\n",
      "\n",
      "<hr />\n",
      "\n",
      "<div class=\"footnotes\">\n",
      "  <ol>\n",
      "    <li id=\"fn:3\">\n",
      "      <p>Hubel, David H., and Torsten N. Wiesel. “Receptive fields, binocular interaction and functional architecture in the cat’s visual cortex.” The Journal of physiology 160.1 (1962): 106-154.&nbsp;<a href=\"#fnref:3\" class=\"reversefootnote\">&#8617;</a></p>\n",
      "    </li>\n",
      "    <li id=\"fn:4\">\n",
      "      <p>Fukushima, Kunihiko, and Sei Miyake. “Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition.” Competition and cooperation in neural nets. Springer, Berlin, Heidelberg, 1982. 267-285.&nbsp;<a href=\"#fnref:4\" class=\"reversefootnote\">&#8617;</a></p>\n",
      "    </li>\n",
      "    <li id=\"fn:5\">\n",
      "      <p>LeCun, Yann, et al. “Handwritten digit recognition with a back-propagation network.” Advances in neural information processing systems. 1990.&nbsp;<a href=\"#fnref:5\" class=\"reversefootnote\">&#8617;</a></p>\n",
      "    </li>\n",
      "    <li id=\"fn:2\">\n",
      "      <p>Gregor, Karol, et al. “DRAW: A Recurrent Neural Network For Image Generation.” International Conference on Machine Learning. 2015.&nbsp;<a href=\"#fnref:2\" class=\"reversefootnote\">&#8617;</a></p>\n",
      "    </li>\n",
      "  </ol>\n",
      "</div>\n",
      "\n",
      "  Link: http://bair.berkeley.edu/blog/2017/11/09/learn-to-attend-fovea/\n",
      "  PubDate: Thu, 09 Nov 2017 01:00:00 -0800\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20171221202139/http://bair.berkeley.edu:80/blog/feed.xml\n",
      "- Source: https://bair.berkeley.edu/blog/feed.xml\n",
      "  Channel Title: The Berkeley Artificial Intelligence Research Blog\n",
      "  Channel Description: The BAIR Blog\n",
      "- Title: Reverse Curriculum Generation for Reinforcement Learning Agents\n",
      "  Description: <p>Reinforcement Learning (RL) is a powerful technique capable of solving complex tasks such as <a href=\"https://arxiv.org/abs/1506.02438\">locomotion</a>, <a href=\"https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf\">Atari games</a>, <a href=\"https://arxiv.org/abs/1509.02971\">racing games</a>, and <a href=\"https://arxiv.org/abs/1504.00702\">robotic manipulation tasks</a>, all through training an agent to optimize behaviors over a reward function. There are many tasks, however, for which it is <strong>hard to design a reward function that is both easy to train and that yields the desired behavior once optimized</strong>. Suppose we want a robotic arm to learn how to place a ring onto a peg. The most natural reward function would be for an agent to receive a reward of 1 at the desired end configuration and 0 everywhere else.  However, the required motion for this task–to align the ring at the top of the peg and then slide it to the bottom–is impractical to learn under such a binary reward, because the usual random exploration of our initial policy is unlikely to ever reach the goal, as seen in Video 1a. Alternatively, one can try to <a href=\"https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/NgHaradaRussell-shaping-ICML1999.pdf\">shape the reward function</a> to potentially alleviate this problem, but finding a good shaping <a href=\"https://arxiv.org/abs/1704.03073\">requires considerable expertise and experimentation</a>. For example, directly minimizing the distance between the center of the ring and the bottom of the peg leads to an unsuccessful policy that smashes the ring against the peg, as in Video 1b. We propose a method to learn efficiently without modifying the reward function, by automatically generating a curriculum over start positions.</p>\n",
      "\n",
      "<table class=\"col-2\">\n",
      "  <tr>\n",
      "    <td style=\"text-align:center;\">\n",
      "\t\t\t<img src=\"http://bair.berkeley.edu/static/blog/reverse_curriculum/ring_fail_cross.gif\" alt=\"ring_fail_cross\" />\n",
      "\t\t</td>\n",
      "    <td style=\"text-align:center;\">\n",
      "\t\t\t<img src=\"http://bair.berkeley.edu/static/blog/reverse_curriculum/ring_shapping_cross.gif\" alt=\"ring_shapping_cross\" />\n",
      "\t\t</td>\n",
      "  </tr>\n",
      "  <tr>\n",
      "    <td><p>\n",
      "      <i>Video 1a: A randomly initialized policy is unable to reach the goal from most start positions, hence being unable to learn.</i>\n",
      "\t\t</p></td>\n",
      "    <td><p>\n",
      "      <i>Video 1b: Shaping the reward with a penalty on the distance from the ring center to the peg bottom yields an undesired behavior.\n",
      "</i>\n",
      "\t\t</p></td>\n",
      "  </tr>\n",
      "</table>\n",
      "\n",
      "<!--more-->\n",
      "\n",
      "<h2 id=\"curriculum-instead-of-reward-shaping\">Curriculum instead of Reward Shaping</h2>\n",
      "\n",
      "<p>We would like to train an agent to reach the goal from any starting position, without requiring an expert to shape the reward. Clearly, not all starting positions are equally difficult. In particular, even a random agent that is placed near to the goal will be able to reach the goal some of the time, receive a reward, and hence start learning! This acquired knowledge can then be bootstrapped to solve the task starting from further away from the goal. By <strong>choosing the ordering of the starting positions that we use in training</strong>, we can exploit this underlying structure of the problem and improve learning efficiency. A key advantage of this technique is that the <strong>reward function is not modified</strong>, and optimizing the sparse reward directly is less prone to yielding undesired behaviors. Ordering a set of related tasks to be learned is referred to as <strong>curriculum learning</strong>, and a central question for us is how to choose this task ordering. Our method, which we explain in more detail below, uses the performance of the learning agent to automatically generate a curriculum of tasks which start from the goal and expand outwards.</p>\n",
      "\n",
      "<h3 id=\"reverse-curriculum-intuition\">Reverse Curriculum Intuition</h3>\n",
      "<p>In <em>goal-oriented</em> tasks the aim is to reach a desired configuration from any start state. For example, in the ring-on-peg task introduced above, we desire to place the ring on the peg starting from any configuration. From most start positions, the random exploration of our initial policy never reaches the goal and hence perceives no reward. Nevertheless, it can be seen in Video 2a how a random policy is likely to reach the bottom of the peg if it is initialized from a nearby position. Then, once we have learned how to reach the goal from around the goal, learning from further away is easier since the agent already knows how to proceed if exploratory actions drive its state nearby the goal, as in Video 2b. Eventually, the agent successfully learns to reach  the goal from a wide range of starting positions, as in Video 2c.</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img height=\"180\" src=\"http://bair.berkeley.edu/static/blog/reverse_curriculum/ring_curr1.gif\" title=\"ring_curr1\" />\n",
      "<img height=\"180\" src=\"http://bair.berkeley.edu/static/blog/reverse_curriculum/ring_curr2.gif\" title=\"ring_curr2\" />\n",
      "<img height=\"200\" src=\"http://bair.berkeley.edu/static/blog/reverse_curriculum/ring_curr3.gif\" title=\"ring_curr3\" />\n",
      "<br />\n",
      "<i>\n",
      "Video 2a-c: Our method strives to learn first starting nearby the goal, and then progressively expands in reverse the positions from where it starts.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>This method of learning in reverse, or growing outwards from the goal, draws inspiration from Dynamic Programming methods, where the solutions to easier sub-problems are used to compute the solution to harder problems.</p>\n",
      "\n",
      "<h3 id=\"starts-of-intermediate-difficulty-soid\">Starts of Intermediate Difficulty (SoID)</h3>\n",
      "\n",
      "<p>To implement this reverse curriculum, we need to ensure that this outwards expansion happens at the right pace for the learning agent. In other words, we want to mathematically describe a set of starts that tracks the current agent performance and provides a good learning signal to our Reinforcement Learning algorithm. In particular we focus on Policy Gradient algorithms, which improve a parameterized policy by taking steps in the direction of an estimated gradient of the total expected reward . This gradient estimation is usually a variation of the original <a href=\"https://link.springer.com/article/10.1007/BF00992696\">REINFORCE</a>, which is estimated by collecting $N$ on-policy trajectories <script type=\"math/tex\">\\{\\tau^i\\}_{i=1..N}</script> starting from states <script type=\"math/tex\">\\{s^i_0\\}_{i=1..N}</script>.</p>\n",
      "\n",
      "<script type=\"math/tex; mode=display\">\\nabla_\\theta\\eta=\\frac{1}{N}\\sum_{i=1}^N\\nabla_\\theta\\log\\pi_\\theta(\\tau^i)[R(\\tau^i, s^i_0)-R(\\pi_i, s^i_0)] \\quad (1)</script>\n",
      "\n",
      "<p>In <em>goal-oriented</em> tasks, the trajectory reward $R(\\tau^i, s^i_0)$ is binary, indicating whether the agent reached the goal. Therefore, the usual baseline $R(\\pi_i, s^i_0)$ estimates the probability of reaching the goal if the current policy $\\pi_\\theta$ is executed starting from $s^i_0$. Hence, we see from Eq. (1) that the terms of the sum corresponding to trajectories collected from starts $s^i_0$ that have success probability 0 or 1 will vanish. These are “wasted” trajectories as they do not contribute to the estimation of the gradient – they are either too hard or too easy. A similar analysis was already introduced in our prior <a href=\"https://arxiv.org/abs/1705.06366\">work on multi-task RL</a>. In this case, to avoid training from starts from where our current policy either never gets to the goal or already masters it, we introduce the concept of “Start of Intermediate Difficulty” (SoID), which are start states $s_0$ that satisfy:</p>\n",
      "\n",
      "<script type=\"math/tex; mode=display\">% <![CDATA[\n",
      "s_0:R_{min} < R(\\pi_i, s_0^i) < R_{max} \\quad (2) %]]></script>\n",
      "\n",
      "<p>The values of <script type=\"math/tex\">R_{min}</script> and <script type=\"math/tex\">R_{max}</script> have the straightforward interpretation of minimum success probability acceptable for training from that start and maximum success probability above which we prefer to focus on training from other starts. In all our experiments we used 10% and 90%.</p>\n",
      "\n",
      "<h2 id=\"automatic-generation-of-the-reverse-curriculum\">Automatic Generation of the Reverse Curriculum</h2>\n",
      "\n",
      "<p>From the above intuition and derivation, we would like to train our policy with trajectories starting from SoID states. Unfortunately, finding all starts that exactly satisfy Eq. (2) at every policy update is intractable, and hence we introduce an efficient approximation to automatically generate this reverse curriculum: we sample states nearby the starts that were estimated to be SoID during the previous iteration. To do that, we propose a way to <em>filter out non-SoID starts using the trajectories collected during the last training iteration and then sample nearby states</em>. The full algorithm is illustrated in Video 3 and details are given below.</p>\n",
      "\n",
      "<div class=\"videoWrapper\">\n",
      "  <iframe src=\"https://www.youtube.com/embed/ANcJ3Hqk7sY?rel=0\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\n",
      "</div>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<i>\n",
      "Video 3: Animation illustrating the main steps of our algorithm, and how it automatically produces a curriculum adapted to the current agent performance.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<h3 id=\"filtering-out-non-soid\">Filtering out Non-SoID</h3>\n",
      "\n",
      "<p>At every policy gradient training iteration, we collect <script type=\"math/tex\">N</script> trajectories from some start positions <script type=\"math/tex\">\\{s^i_0\\}_{i=1..N}</script>. For most start states, we collect at least three trajectories starting from there, and hence we can compute a Monte Carlo estimate of the success probability of our policy from those starts <script type=\"math/tex\">R_\\theta(s^i_0)</script>. For every <script type=\"math/tex\">s^i_0</script> that the estimate is not within the fixed bounds <script type=\"math/tex\">R_{min}</script> and <script type=\"math/tex\">R_{max}</script>, we discard this start so that we don’t train from it during the next iteration. Starts that were SoID for a previous policy might not be SoID for the current policy because they are now mastered or because the updated policy got worse at them, so it is important to keep filtering out the non-SoID starts to maintain a curriculum adapted to the current agent performance.</p>\n",
      "\n",
      "<h3 id=\"sampling-nearby\">Sampling Nearby</h3>\n",
      "\n",
      "<p>After filtering the non-SoID we need to obtain new SoIDs to keep expanding the starts from where we train. We do that by sampling states nearby the remaining SoID because those have a similar level of difficulty for the current policy – and hence might also be SoID. But what is a good way to sample nearby a certain state <script type=\"math/tex\">s^i_0</script>? We propose to take random exploratory actions from that <script type=\"math/tex\">s^i_0</script>, and record the visited states. This technique is preferable to applying noise in state space directly because that might yield states that are not even feasible or that cannot be reached by executing actions from the original <script type=\"math/tex\">s^i_0</script>.</p>\n",
      "\n",
      "<h3 id=\"assumptions\">Assumptions</h3>\n",
      "\n",
      "<p>To initialize the algorithm, we need to <strong>seed it with one start at the goal</strong> <script type=\"math/tex\">s^g</script> and then run brownian motion from it, train from the collected starts, filter out the non-SoID and iterate. This is usually easy to provide when specifying the problem, and is a milder assumption than requiring a full demonstration of how to get to that point.</p>\n",
      "\n",
      "<p>Our algorithm exploits the capability of <strong>choosing the start distribution</strong> from where the collected trajectories start. This is the case in many systems, like all simulated ones. <a href=\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.7.7601\">Kakade and Langford</a> also build upon this assumption, and propose theoretical evidence of the usefulness of modifying the start distribution.</p>\n",
      "\n",
      "<h2 id=\"application-to-robotics\">Application to Robotics</h2>\n",
      "\n",
      "<p><em>Navigation</em> to a fixed goal and <em>fine-grained manipulation</em> to a desired configuration are two examples of goal-oriented robotics tasks. We analyze how the proposed algorithm automatically generates a reverse curriculum for the following tasks: Point-mass Maze (Fig. 1a), Ant Maze (Fig. 1b), Ring-on-Peg (Fig. 1c) and Key insertion (Fig. 1d).</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img height=\"175\" src=\"http://bair.berkeley.edu/static/blog/reverse_curriculum/point_mass.png\" title=\"point_mass\" />\n",
      "<img height=\"175\" src=\"http://bair.berkeley.edu/static/blog/reverse_curriculum/ant_maze.png\" title=\"ant_maze\" />\n",
      "<img height=\"175\" src=\"http://bair.berkeley.edu/static/blog/reverse_curriculum/ring_on_peg.png\" title=\"ring_on_peg\" />\n",
      "<img height=\"175\" src=\"http://bair.berkeley.edu/static/blog/reverse_curriculum/key_insertion.png\" title=\"key_insertion\" />\n",
      "<br />\n",
      "<i>\n",
      "Fig. 1a-d: Tasks where we illustrate the performance of our method (from left to right): Point-mass Maze, Ant Maze, Ring-on-Peg, Key insertion.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<h3 id=\"point-mass-maze\">Point-mass Maze</h3>\n",
      "\n",
      "<p>In this task we want to learn how to reach the end of the red area in the upper-right corner of Fig. 1a from any start point within the maze. We see in Fig. 2 that a randomly initialized policy – as we have at iteration <script type=\"math/tex\">i=1</script>, has a success probability of zero from everywhere but around the goal. The second row of Fig. 2 shows how our algorithm proposes start positions nearby the goal at <script type=\"math/tex\">i=1</script>. We see in the subsequent columns that the starts generated by our method keep tracking the area where the training policy succeeds sometimes but not always, hence giving a good learning signal for any policy gradient learning method.</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img width=\"90%\" src=\"http://bair.berkeley.edu/static/blog/reverse_curriculum/performance.png\" title=\"performance\" />\n",
      "<br />\n",
      "<i>\n",
      "Fig. 2: Snapshots of the policy performance and the starts generated by our reverse curriculum (replay buffer not depicted for clarity), always tracking the regions at an intermediate level of difficulty.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>To avoid forgetting how to reach the goal from some areas, we keep a replay buffer of all starts that were SoID for any previous policy. In every training iteration we sample a fraction of the trajectories starting from states in this replay.</p>\n",
      "\n",
      "<h3 id=\"ant-maze-navigation\">Ant Maze Navigation</h3>\n",
      "\n",
      "<p>In robotics, a complex coordinated motion is often required to reach the desired configuration. For example, a quadruped like the one in Fig. 1b needs to know how to coordinate all its torques to move and advance towards the goal. As seen in a final policy reported in Video 4, our algorithm is able to learn this behavior even when only a success/failure reward is provided when reaching the goal! The reward function was not modified to include any distance-to-goal, Center of Mass speed, or exploration bonus.</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img height=\"250\" src=\"http://bair.berkeley.edu/static/blog/reverse_curriculum/ant_maze.gif\" title=\"ant_maze_gif\" />\n",
      "<br />\n",
      "<i>\n",
      "Video 4: Emergence of complex coordination and use of environment contacts when training with our reverse curriculum method - even under sparse rewards.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<h3 id=\"fine-grained-manipulation\">Fine-grained Manipulation</h3>\n",
      "\n",
      "<p>Our method can also tackle complex robotic manipulation problems like the ones depicted in Fig. 1c and 1d. Both task have a seven Degrees of Freedom arm and have complex contact constraints. The first task requires the robot to insert a ring down to the bottom of the peg, and the second task seeks to insert a key in a lock, rotate 90 degrees clockwise, insert it further and rotate 90 degrees counterclockwise. In both cases, a reward is only granted when the desired end configuration is reached. State-of-the-art RL algorithms without curriculum are unable to learn how to solve the task, but with our reverse curriculum generation we can obtain a successful policy from a wide range of start positions, as observed in Videos 5a and 5b.</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img width=\"30%\" src=\"http://bair.berkeley.edu/static/blog/reverse_curriculum/ring_success.gif\" title=\"ring_success\" />\n",
      "<img width=\"30%\" src=\"http://bair.berkeley.edu/static/blog/reverse_curriculum/key_success.gif\" title=\"key_success\" />\n",
      "<br />\n",
      "<i>\n",
      "Video 5a (left) and Video 5b (right): Final policies obtained with our reverse curriculum approach in the Ring-on-Peg and the Key Insertion tasks. The agent succeeds from a wide range of initial positions and is able to leverage the contacts to guide itself.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<h2 id=\"conclusions-and-future-directions\">Conclusions and Future Directions</h2>\n",
      "\n",
      "<p>Recently RL methods have been moving away from the single-task paradigm to tackle sets of tasks. This is an effort to get closer to real-world scenarios, where every time a tasks needs to be executed there are always variations in the starting configuration, goal or other parameters. Therefore it is of utmost importance to advance the field of curriculum learning to exploit the underlying structure of these sets of tasks. Our Reverse Curriculum strategy is a step in this direction, yielding impressive results in locomotion and complex manipulation tasks that cannot be solved without a curriculum.</p>\n",
      "\n",
      "<p>Furthermore, it can be observed in the videos of our final policy for the manipulation tasks that the agent has learned to exploit the contacts in the environment instead of avoiding them. Therefore, the learning based aspect of the presented method has a great potential to tackle problems that classical motion planning algorithms could struggle with, such as environments with non-rigid objects or with uncertainties in the task geometric parameters. We also leave as future work to combine our curriculum-generation approach with <a href=\"https://arxiv.org/abs/1703.06907\">domain randomization methods</a> to obtain policies that are transferable to the real world.</p>\n",
      "\n",
      "<p>If you want to learn more, check out our paper published in the Conference on Robot Learning:</p>\n",
      "\n",
      "<p><em>Carlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, Pieter Abbeel. <a href=\"http://proceedings.mlr.press/v78/florensa17a/florensa17a.pdf\">Reverse Curriculum Generation for Reinforcement Learning</a>. In CoRL 2017.</em></p>\n",
      "\n",
      "<p><em>We have also open-sourced the code in the <a href=\"https://sites.google.com/view/reversecurriculum\">project website</a>.</em></p>\n",
      "\n",
      "<hr />\n",
      "<p>We would like to thank the co-authors of this work, who also provided very valuable feedback for this blog post: David Held, Markus Wulfmeier, Michael R. Zhang and Pieter Abbeel.</p>\n",
      "\n",
      "<h2 id=\"further-readings\">Further Readings:</h2>\n",
      "\n",
      "<p>A. Graves, M. G. Bellemare, J. Menick, R. Munos, and K. Kavukcuoglu. Automated Curriculum Learning for Neural Networks. arXiv preprint, arXiv:1704.03003, 2017.</p>\n",
      "\n",
      "<p>M. Asada, S. Noda, S. Tawaratsumida, and K. Hosoda. Purposive behavior acquisition for a real robot by Vision-Based reinforcement learning. Machine Learning, 1996.</p>\n",
      "\n",
      "<p>A. Karpathy and M. Van De Panne. Curriculum learning for motor skills. In Canadian Conference on Artificial Intelligence, pages 325–330. Springer, 2012.</p>\n",
      "\n",
      "<p>J. Schmidhuber. POWER PLAY : Training an Increasingly General Problem Solver by Continually Searching for the Simplest Still Unsolvable Problem. Frontiers in Psychology, 2013.</p>\n",
      "\n",
      "<p>A. Baranes and P.-Y. Oudeyer. Active learning of inverse models with intrinsically motivated goal exploration in robots. Robotics and Autonomous Systems, 61(1), 2013.</p>\n",
      "\n",
      "<p>S. Sharma and B. Ravindran. Online Multi-Task Learning Using Biased Sampling. arXiv preprint arXiv: 1702.06053, 2017. 9</p>\n",
      "\n",
      "<p>S. Sukhbaatar, I. Kostrikov, A. Szlam, and R. Fergus. Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play. arXiv preprint, arXiv: 1703.05407, 2017.</p>\n",
      "\n",
      "<p>J. A. Bagnell, S. Kakade, A. Y. Ng, and J. Schneider. Policy search by dynamic programming. Advances in Neural Information Processing Systems, 16:79, 2003.</p>\n",
      "\n",
      "<p>S. Kakade and J. Langford. Approximately Optimal Approximate Reinforcement Learning. International Conference in Machine Learning, 2002.</p>\n",
      "\n",
      "\n",
      "  Link: http://bair.berkeley.edu/blog/2017/12/20/reverse-curriculum/\n",
      "  PubDate: Wed, 20 Dec 2017 01:00:00 -0800\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20180122203228/http://bair.berkeley.edu:80/blog/feed.xml\n",
      "- Source: https://bair.berkeley.edu/blog/feed.xml\n",
      "  Channel Title: The Berkeley Artificial Intelligence Research Blog\n",
      "  Channel Description: The BAIR Blog\n",
      "- Title: Ray: A Distributed System for AI\n",
      "  Description: <p>As machine learning algorithms and techniques have advanced, more and more machine learning applications require multiple machines and must exploit parallelism.\n",
      "However, the infrastructure for doing machine learning on clusters remains ad-hoc. While good solutions for specific use cases (e.g., parameter servers or hyperparameter search) and high-quality distributed systems outside of AI do exist (e.g., Hadoop or Spark), practitioners developing algorithms at the frontier often build their own systems infrastructure from scratch. This amounts to a lot of redundant effort.</p>\n",
      "\n",
      "<p>As an example, take a conceptually simple algorithm like <a href=\"https://arxiv.org/abs/1703.03864\">Evolution Strategies for reinforcement learning</a>. The algorithm is about a dozen lines of pseudocode, and its Python implementation doesn’t take much more than that. However, running the algorithm efficiently on a larger machine or cluster requires significantly more software engineering. The authors’ implementation involves thousands of lines of code and must define communication protocols, message serialization and deserialization strategies, and various data handling strategies.</p>\n",
      "\n",
      "<p>One of <a href=\"https://github.com/ray-project/ray\">Ray</a>’s goals is to enable practitioners to turn a prototype algorithm that runs on a laptop into a high-performance distributed application that runs efficiently on a cluster (or on a single multi-core machine) with relatively few additional lines of code. Such a framework should include the performance benefits of a hand-optimized system without requiring the user to reason about scheduling, data transfers, and machine failures.</p>\n",
      "\n",
      "<!--more-->\n",
      "\n",
      "<h1 id=\"an-open-source-framework-for-ai\">An Open-Source Framework for AI</h1>\n",
      "\n",
      "<p><strong>Relation to deep learning frameworks:</strong> Ray is fully compatible with deep learning frameworks like TensorFlow, PyTorch, and MXNet, and it is natural to use one or more deep learning frameworks along with Ray in many applications (for example, our reinforcement learning libraries use TensorFlow and PyTorch heavily).</p>\n",
      "\n",
      "<p><strong>Relation to other distributed systems:</strong> Many popular distributed systems are used today, but most of them were not built with AI applications in mind and lack the required performance for supporting and the APIs for expressing AI applications. The following features are missing (in various combinations) from today’s distributed systems:</p>\n",
      "\n",
      "<ul>\n",
      "  <li>Support for millisecond level tasks and millions of tasks per second</li>\n",
      "  <li>Nested parallelism (parallelizing tasks within tasks, e.g., parallel simulations inside of a hyperparameter search) (see the figure below)</li>\n",
      "  <li>Arbitrary task dependencies determined dynamically at runtime (e.g., to avoid waiting for slow workers)</li>\n",
      "  <li>Tasks operating on shared mutable state (e.g., neural net weights or a simulator)</li>\n",
      "  <li>Support for heterogeneous resources (CPUs, GPUs, etc)</li>\n",
      "</ul>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/ray/task_hier.png\" alt=\"nested_parallelism\" width=\"600\" /><br />\n",
      "<i>\n",
      "A simple example of nested parallelism. One application runs two parallel experiments (each of which is a long-running task), and each experiment runs a number of parallel simulations (each of which is also a task).\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>There are two main ways of using Ray: through its lower-level APIs and higher-level libraries.\n",
      "The higher-level libraries are built on top of the lower-level APIs.\n",
      "Currently these include <a href=\"http://ray.readthedocs.io/en/latest/rllib.html\">Ray RLlib</a>, a scalable reinforcement learning library and <a href=\"http://ray.readthedocs.io/en/latest/tune.html\">Ray.tune</a>, an efficient distributed hyperparameter search library.</p>\n",
      "\n",
      "<h1 id=\"ray-lower-level-apis\">Ray Lower-Level APIs</h1>\n",
      "<p>The goal of the Ray API is to make it natural to express very general computational patterns and applications without being restricted to fixed patterns like MapReduce.</p>\n",
      "\n",
      "<h2 id=\"dynamic-task-graphs\">Dynamic Task Graphs</h2>\n",
      "<p>The underlying primitive in a Ray application or job is a <em>dynamic task graph</em>. This is very different from the computation graph in TensorFlow. Whereas in TensorFlow, a computation graph represents a neural network and is executed many times in a single application, in Ray, the task graph represents the entire application and is only executed a single time. The task graph is not known up front. It is constructed dynamically as an application runs, and the execution of one task may trigger the creation of more tasks.</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/ray/task_graph.png\" alt=\"dynamic_task_graph\" width=\"300\" /><br />\n",
      "<i>\n",
      "An example computation graph. The white ovals refer to tasks, and the blue boxes refer to objects. Arrows indicate that a task depends on an object or that a task creates an object.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>Arbitrary Python functions can be executed as tasks, and they can depend arbitrarily on the outputs of other tasks.\n",
      "This is illustrated in the example below.</p>\n",
      "<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\"># Define two remote functions. Invocations of these functions create tasks</span>\n",
      "<span class=\"c\"># that are executed remotely.</span>\n",
      "\n",
      "<span class=\"nd\">@ray.remote</span>\n",
      "<span class=\"k\">def</span> <span class=\"nf\">multiply</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">):</span>\n",
      "    <span class=\"k\">return</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n",
      "\n",
      "<span class=\"nd\">@ray.remote</span>\n",
      "<span class=\"k\">def</span> <span class=\"nf\">zeros</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"p\">):</span>\n",
      "    <span class=\"k\">return</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"p\">)</span>\n",
      "\n",
      "<span class=\"c\"># Start two tasks in parallel. These immediately return futures and the</span>\n",
      "<span class=\"c\"># tasks are executed in the background.</span>\n",
      "<span class=\"n\">x_id</span> <span class=\"o\">=</span> <span class=\"n\">zeros</span><span class=\"o\">.</span><span class=\"n\">remote</span><span class=\"p\">((</span><span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">))</span>\n",
      "<span class=\"n\">y_id</span> <span class=\"o\">=</span> <span class=\"n\">zeros</span><span class=\"o\">.</span><span class=\"n\">remote</span><span class=\"p\">((</span><span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">))</span>\n",
      "\n",
      "<span class=\"c\"># Start a third task. This will not be scheduled until the first two</span>\n",
      "<span class=\"c\"># tasks have completed.</span>\n",
      "<span class=\"n\">z_id</span> <span class=\"o\">=</span> <span class=\"n\">multiply</span><span class=\"o\">.</span><span class=\"n\">remote</span><span class=\"p\">(</span><span class=\"n\">x_id</span><span class=\"p\">,</span> <span class=\"n\">y_id</span><span class=\"p\">)</span>\n",
      "\n",
      "<span class=\"c\"># Get the result. This will block until the third task completes.</span>\n",
      "<span class=\"n\">z</span> <span class=\"o\">=</span> <span class=\"n\">ray</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"n\">z_id</span><span class=\"p\">)</span>\n",
      "</code></pre></div></div>\n",
      "\n",
      "<h2 id=\"actors\">Actors</h2>\n",
      "<p>One thing that can’t be done with just the remote functions and tasks described above is to have multiple tasks operating on the same shared mutable state.\n",
      "This comes up in multiple contexts in machine learning where the shared state may be the state of a simulator, the weights of a neural network, or something else entirely.\n",
      "Ray uses an actor abstraction to encapsulate mutable state shared between multiple tasks.\n",
      "Here’s a toy example of how to do this with an Atari simulator.</p>\n",
      "\n",
      "<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">import</span> <span class=\"nn\">gym</span>\n",
      "\n",
      "<span class=\"nd\">@ray.remote</span>\n",
      "<span class=\"k\">class</span> <span class=\"nc\">Simulator</span><span class=\"p\">(</span><span class=\"nb\">object</span><span class=\"p\">):</span>\n",
      "    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n",
      "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">env</span> <span class=\"o\">=</span> <span class=\"n\">gym</span><span class=\"o\">.</span><span class=\"n\">make</span><span class=\"p\">(</span><span class=\"s\">\"Pong-v0\"</span><span class=\"p\">)</span>\n",
      "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">reset</span><span class=\"p\">()</span>\n",
      "\n",
      "    <span class=\"k\">def</span> <span class=\"nf\">step</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">action</span><span class=\"p\">):</span>\n",
      "        <span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">(</span><span class=\"n\">action</span><span class=\"p\">)</span>\n",
      "\n",
      "<span class=\"c\"># Create a simulator, this will start a remote process that will run</span>\n",
      "<span class=\"c\"># all methods for this actor.</span>\n",
      "<span class=\"n\">simulator</span> <span class=\"o\">=</span> <span class=\"n\">Simulator</span><span class=\"o\">.</span><span class=\"n\">remote</span><span class=\"p\">()</span>\n",
      "\n",
      "<span class=\"n\">observations</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n",
      "<span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">):</span>\n",
      "    <span class=\"c\"># Take action 0 in the simulator. This call does not block and</span>\n",
      "    <span class=\"c\"># it returns a future.</span>\n",
      "    <span class=\"n\">observations</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">simulator</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"o\">.</span><span class=\"n\">remote</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">))</span>\n",
      "</code></pre></div></div>\n",
      "\n",
      "<p>Though simple, an actor can be used in very flexible ways.\n",
      "For example, and actor can encapsulate a simulator or a neural network policy, and it can be used for distributed training (as with a parameter server) or for policy serving in a live application.</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/ray/policy_actor.png\" alt=\"policy_actor\" width=\"200\" style=\"max-width:28%;margin-right:20px;\" />\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/ray/param_actor.png\" alt=\"param_actor\" width=\"400\" style=\"max-width:58%;margin-left:20px;\" /><br />\n",
      "<i>\n",
      "<b>Left:</b> An actor serving predictions/actions to a number of client processes.\n",
      "<b>Right:</b> Multiple parameter server actors performing distributed training with multiple worker processes.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<h2 id=\"parameter-server-example\">Parameter server example</h2>\n",
      "<p>A parameter server can be implemented as a Ray actor as follows:</p>\n",
      "<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nd\">@ray.remote</span>\n",
      "<span class=\"k\">class</span> <span class=\"nc\">ParameterServer</span><span class=\"p\">(</span><span class=\"nb\">object</span><span class=\"p\">):</span>\n",
      "    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">keys</span><span class=\"p\">,</span> <span class=\"n\">values</span><span class=\"p\">):</span>\n",
      "        <span class=\"c\"># These values will be mutated, so we must create a local copy.</span>\n",
      "        <span class=\"n\">values</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">value</span><span class=\"o\">.</span><span class=\"n\">copy</span><span class=\"p\">()</span> <span class=\"k\">for</span> <span class=\"n\">value</span> <span class=\"ow\">in</span> <span class=\"n\">values</span><span class=\"p\">]</span>\n",
      "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">keys</span><span class=\"p\">,</span> <span class=\"n\">values</span><span class=\"p\">))</span>\n",
      "\n",
      "    <span class=\"k\">def</span> <span class=\"nf\">get</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">keys</span><span class=\"p\">):</span>\n",
      "        <span class=\"k\">return</span> <span class=\"p\">[</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"n\">key</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">key</span> <span class=\"ow\">in</span> <span class=\"n\">keys</span><span class=\"p\">]</span>\n",
      "\n",
      "    <span class=\"k\">def</span> <span class=\"nf\">update</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">keys</span><span class=\"p\">,</span> <span class=\"n\">values</span><span class=\"p\">):</span>\n",
      "        <span class=\"c\"># This update function adds to the existing values, but the update</span>\n",
      "        <span class=\"c\"># function can be defined arbitrarily.</span>\n",
      "        <span class=\"k\">for</span> <span class=\"n\">key</span><span class=\"p\">,</span> <span class=\"n\">value</span> <span class=\"ow\">in</span> <span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">keys</span><span class=\"p\">,</span> <span class=\"n\">values</span><span class=\"p\">):</span>\n",
      "            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"n\">key</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"n\">value</span>\n",
      "</code></pre></div></div>\n",
      "\n",
      "<p>See a <a href=\"http://ray.readthedocs.io/en/latest/example-parameter-server.html\">more complete example</a>.</p>\n",
      "\n",
      "<p>To instantiate the parameter server, do the following.</p>\n",
      "<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">parameter_server</span> <span class=\"o\">=</span> <span class=\"n\">ParameterServer</span><span class=\"o\">.</span><span class=\"n\">remote</span><span class=\"p\">(</span><span class=\"n\">initial_keys</span><span class=\"p\">,</span> <span class=\"n\">initial_values</span><span class=\"p\">)</span>\n",
      "</code></pre></div></div>\n",
      "<p>To create four long-running workers that continuously retrieve and update the parameters, do the following.</p>\n",
      "\n",
      "<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nd\">@ray.remote</span>\n",
      "<span class=\"k\">def</span> <span class=\"nf\">worker_task</span><span class=\"p\">(</span><span class=\"n\">parameter_server</span><span class=\"p\">):</span>\n",
      "    <span class=\"k\">while</span> <span class=\"bp\">True</span><span class=\"p\">:</span>\n",
      "        <span class=\"n\">keys</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s\">'key1'</span><span class=\"p\">,</span> <span class=\"s\">'key2'</span><span class=\"p\">,</span> <span class=\"s\">'key3'</span><span class=\"p\">]</span>\n",
      "        <span class=\"c\"># Get the latest parameters.</span>\n",
      "        <span class=\"n\">values</span> <span class=\"o\">=</span> <span class=\"n\">ray</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"n\">parameter_server</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"o\">.</span><span class=\"n\">remote</span><span class=\"p\">(</span><span class=\"n\">keys</span><span class=\"p\">))</span>\n",
      "        <span class=\"c\"># Compute some parameter updates.</span>\n",
      "        <span class=\"n\">updates</span> <span class=\"o\">=</span> <span class=\"err\">…</span>\n",
      "        <span class=\"c\"># Update the parameters.</span>\n",
      "        <span class=\"n\">parameter_server</span><span class=\"o\">.</span><span class=\"n\">update</span><span class=\"o\">.</span><span class=\"n\">remote</span><span class=\"p\">(</span><span class=\"n\">keys</span><span class=\"p\">,</span> <span class=\"n\">updates</span><span class=\"p\">)</span>\n",
      "\n",
      "<span class=\"c\"># Start 4 long-running tasks.</span>\n",
      "<span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">):</span>\n",
      "    <span class=\"n\">worker_task</span><span class=\"o\">.</span><span class=\"n\">remote</span><span class=\"p\">(</span><span class=\"n\">parameter_server</span><span class=\"p\">)</span>\n",
      "</code></pre></div></div>\n",
      "\n",
      "<h1 id=\"ray-high-level-libraries\">Ray High-Level Libraries</h1>\n",
      "<p><a href=\"http://ray.readthedocs.io/en/latest/rllib.html\">Ray RLlib</a> is a scalable reinforcement learning library built to run on many machines.\n",
      "It can be used through example training scripts as well as through a Python API.\n",
      "It currently includes implementations of:</p>\n",
      "<ul>\n",
      "  <li>A3C</li>\n",
      "  <li>DQN</li>\n",
      "  <li>Evolution Strategies</li>\n",
      "  <li>PPO</li>\n",
      "</ul>\n",
      "\n",
      "<p>We are working on adding more algorithms.\n",
      "RLlib is fully compatible with the <a href=\"https://gym.openai.com/docs/\">OpenAI gym</a>.</p>\n",
      "\n",
      "<p><a href=\"http://ray.readthedocs.io/en/latest/tune.html\">Ray.tune</a> is an efficient distributed hyperparameter search library.\n",
      "It provides a Python API for use with deep learning, reinforcement learning, and other compute-intensive tasks.\n",
      "Here is a toy example illustrating usage:</p>\n",
      "\n",
      "<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">from</span> <span class=\"nn\">ray.tune</span> <span class=\"kn\">import</span> <span class=\"n\">register_trainable</span><span class=\"p\">,</span> <span class=\"n\">grid_search</span><span class=\"p\">,</span> <span class=\"n\">run_experiments</span>\n",
      "\n",
      "<span class=\"c\"># The function to optimize. The hyperparameters are in the config</span>\n",
      "<span class=\"c\"># argument.</span>\n",
      "<span class=\"k\">def</span> <span class=\"nf\">my_func</span><span class=\"p\">(</span><span class=\"n\">config</span><span class=\"p\">,</span> <span class=\"n\">reporter</span><span class=\"p\">):</span>\n",
      "    <span class=\"kn\">import</span> <span class=\"nn\">time</span><span class=\"p\">,</span> <span class=\"n\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span>\n",
      "    <span class=\"n\">i</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n",
      "    <span class=\"k\">while</span> <span class=\"bp\">True</span><span class=\"p\">:</span>\n",
      "        <span class=\"n\">reporter</span><span class=\"p\">(</span><span class=\"n\">timesteps_total</span><span class=\"o\">=</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">mean_accuracy</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">i</span> <span class=\"o\">**</span> <span class=\"n\">config</span><span class=\"p\">[</span><span class=\"s\">'alpha'</span><span class=\"p\">]))</span>\n",
      "        <span class=\"n\">i</span> <span class=\"o\">+=</span> <span class=\"n\">config</span><span class=\"p\">[</span><span class=\"s\">'beta'</span><span class=\"p\">]</span>\n",
      "        <span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">sleep</span><span class=\"p\">(</span><span class=\"mf\">0.01</span><span class=\"p\">)</span>\n",
      "\n",
      "<span class=\"n\">register_trainable</span><span class=\"p\">(</span><span class=\"s\">'my_func'</span><span class=\"p\">,</span> <span class=\"n\">my_func</span><span class=\"p\">)</span>\n",
      "\n",
      "<span class=\"n\">run_experiments</span><span class=\"p\">({</span>\n",
      "    <span class=\"s\">'my_experiment'</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n",
      "        <span class=\"s\">'run'</span><span class=\"p\">:</span> <span class=\"s\">'my_func'</span><span class=\"p\">,</span>\n",
      "        <span class=\"s\">'resources'</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s\">'cpu'</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"s\">'gpu'</span><span class=\"p\">:</span> <span class=\"mi\">0</span><span class=\"p\">},</span>\n",
      "        <span class=\"s\">'stop'</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"s\">'mean_accuracy'</span><span class=\"p\">:</span> <span class=\"mi\">100</span><span class=\"p\">},</span>\n",
      "        <span class=\"s\">'config'</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n",
      "            <span class=\"s\">'alpha'</span><span class=\"p\">:</span> <span class=\"n\">grid_search</span><span class=\"p\">([</span><span class=\"mf\">0.2</span><span class=\"p\">,</span> <span class=\"mf\">0.4</span><span class=\"p\">,</span> <span class=\"mf\">0.6</span><span class=\"p\">]),</span>\n",
      "            <span class=\"s\">'beta'</span><span class=\"p\">:</span> <span class=\"n\">grid_search</span><span class=\"p\">([</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">]),</span>\n",
      "        <span class=\"p\">},</span>\n",
      "    <span class=\"p\">}</span>\n",
      "<span class=\"p\">})</span>\n",
      "</code></pre></div></div>\n",
      "\n",
      "<p>In-progress results can be visualized live using tools such as Tensorboard and rllab’s VisKit (or you can read the JSON logs directly).\n",
      "Ray.tune supports grid search, random search, and more sophisticated early stopping algorithms like HyperBand.</p>\n",
      "\n",
      "<h1 id=\"more-information\">More Information</h1>\n",
      "\n",
      "<p>For more information about Ray, please take a look at the following links.</p>\n",
      "<ul>\n",
      "  <li><a href=\"https://arxiv.org/abs/1712.05889\">Our paper</a></li>\n",
      "  <li><a href=\"https://github.com/ray-project/ray\">Our codebase</a></li>\n",
      "  <li><a href=\"http://ray.readthedocs.io/en/latest/index.html\">Our documentation</a></li>\n",
      "</ul>\n",
      "\n",
      "<p>Ray can be installed with <code class=\"highlighter-rouge\">pip install ray</code>.\n",
      "We encourage you to try out Ray and see what you think.\n",
      "If you have any feedback for us, please let us know, e.g., through our mailing list <a href=\"mailto:ray-dev@googlegroups.com\">ray-dev@googlegroups.com</a>.</p>\n",
      "\n",
      "\n",
      "  Link: http://bair.berkeley.edu/blog/2018/01/09/ray/\n",
      "  PubDate: Tue, 09 Jan 2018 01:00:00 -0800\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20180130192846/http://bair.berkeley.edu/blog/feed.xml\n",
      "- Source: https://bair.berkeley.edu/blog/feed.xml\n",
      "  Channel Title: The Berkeley Artificial Intelligence Research Blog\n",
      "  Channel Description: The BAIR Blog\n",
      "- Title: Kernel Feature Selection via Conditional Covariance Minimization\n",
      "  Description: <p>Feature selection is a common method for dimensionality reduction that\n",
      "encourages model interpretability. With large data sets becoming ever more\n",
      "prevalent, feature selection has seen widespread usage across a variety of\n",
      "real-world tasks in recent years, including text classification, gene selection\n",
      "from microarray data, and face recognition. We study the problem of supervised\n",
      "feature selection, which entails finding a subset of the input features that\n",
      "explains the output well. This practice can reduce the computational expense of\n",
      "downstream learning by removing features that are redundant or noisy, while\n",
      "simultaneously providing insight into the data through the features that remain.</p>\n",
      "\n",
      "<p>Feature selection algorithms can generally be divided into three main\n",
      "categories: filter methods, wrapper methods, and embedded methods. Filter\n",
      "methods select features based on intrinsic properties of the data, independent\n",
      "of the learning algorithm to be used. For example, we may compute the\n",
      "correlation between each feature and the response variable, and select the\n",
      "variables with the highest correlation. Wrapper methods are more specialized in\n",
      "contrast, aiming to find features that optimize the performance of a specific\n",
      "predictor. For example, we may train multiple SVMs, each with a different subset\n",
      "of features, and choose the subset of features with the lowest loss on the\n",
      "training data. Because there are exponentially many subsets of features, wrapper\n",
      "methods often employ greedy algorithms. Finally, embedded methods are\n",
      "multipurpose techniques that incorporate feature selection and prediction into a\n",
      "single problem, often by optimizing an objective combining a goodness-of-fit\n",
      "term with a penalty on the number of parameters. One example is the LASSO method\n",
      "for constructing a linear model, which penalizes the coefficients with an\n",
      "$\\ell_1$ penalty.</p>\n",
      "\n",
      "<p>In this post, we propose conditional covariance minimization (CCM), a feature\n",
      "selection method that aims to unify the first two perspectives. We first\n",
      "describe our approach in the sections that follow. We then demonstrate through\n",
      "several synthetic experiments that our method is capable of capturing joint\n",
      "nonlinear relationships between collections of features. Finally, we show that\n",
      "our algorithm has performance comparable to or better than several other popular\n",
      "feature selection algorithms on a variety of real-world tasks.</p>\n",
      "\n",
      "<!--more-->\n",
      "\n",
      "<h1 id=\"formulating-feature-selection\">Formulating feature selection</h1>\n",
      "\n",
      "<p>One way to view the problem of feature selection is from the lens of dependence.\n",
      "Ideally, we would like to identify a subset of features\n",
      "$\\mathcal{T}$ of a pre-selected size $m$ such that the remaining features are\n",
      "conditionally independent of the responses given $\\mathcal{T}$. However, this\n",
      "may not be achievable when $m$ is small. We therefore quantify the extent of the\n",
      "remaining conditional dependence using some metric, and aim to minimize it over\n",
      "all subsets $\\mathcal{T}$ of the appropriate size.</p>\n",
      "\n",
      "<p>Alternatively, we might like to find the subset of features $\\mathcal{T}$ that\n",
      "can most effectively predict the output $Y$ within the context of a specific learning\n",
      "problem. The prediction error in our framework is defined as the mean square\n",
      "error between the labels and the predictions made by the best classifier\n",
      "selected from a class of functions.</p>\n",
      "\n",
      "<h1 id=\"our-method\">Our method</h1>\n",
      "\n",
      "<p>We propose a criterion that can simultaneously characterize dependence and\n",
      "prediction error in regression. Roughly, we first introduce two function spaces\n",
      "on the domain of a subset of features $X_\\mathcal{T}$ and the domain of the\n",
      "response variable $Y$ respectively. Each function space is a complete inner\n",
      "product space (Hilbert space) equipped with a kernel function which spans the\n",
      "whole space and has the ‘‘reproducing property’’. Such a function space is\n",
      "called a <em>Reproducing Kernel Hilbert Space</em> (RKHS). Then we define an operator\n",
      "for the RKHS over the domain of the response variable that characterizes the\n",
      "conditional dependence of the response variable on the input data given the\n",
      "selected features. Such an operator is called the <em>conditional covariance\n",
      "operator</em>. We use the trace of the operator computed with respect to the\n",
      "empirical distribution as our optimization criterion, which is also the\n",
      "estimated regression error of the best predictor within the given RKHS over the\n",
      "domain of the input data.  Directly minimizing this criterion over subsets of\n",
      "features is computationally intractable. Instead, we formulate a relaxed problem\n",
      "by weighting each feature with a real-valued scalar between 0 and 1, and add an\n",
      "$\\ell_1$-penalty over the weights. The objective of the relaxed problem can be\n",
      "represented in terms of kernel matrices, and is readily optimized using\n",
      "gradient-based approaches.</p>\n",
      "\n",
      "<h1 id=\"results\">Results</h1>\n",
      "\n",
      "<p>We evaluate our approach on both synthetic and real-world data sets. We compare\n",
      "with several strong existing algorithms, including recursive feature elimination\n",
      "(RFE), Minimum Redundancy Maximum Relevance (mRMR), BAHSIC, and filter methods\n",
      "using mutual information (MI) and Pearson’s correlation (PC). RFE is a very\n",
      "popular wrapper method that greedily selects features based on the scores they\n",
      "receive from a classifier. mRMR selects features that capture different\n",
      "information from one another but each correlate well with the response variable.\n",
      "BAHSIC is a kernel method that greedily optimizes the dependence between\n",
      "selected features and the response variable. Lastly, filter methods employing MI\n",
      "or PC greedily optimize the respective metrics between selected subsets of\n",
      "features and the response.</p>\n",
      "\n",
      "<h2 id=\"synthetic-data\">Synthetic data</h2>\n",
      "\n",
      "<p>We use the following synthetic data sets:</p>\n",
      "\n",
      "<ul>\n",
      "  <li>\n",
      "    <p>Orange Skin. Given $Y=-1$, ten features $(X_1,\\dots,X_{10})$ are independent\n",
      "standard normal random variables. Given $Y=1$, the first four features are\n",
      "standard normal random variables conditioned on $9 \\leq \\sum_{j=1}^4 X_j^2\n",
      "\\leq 16$, and the remaining six features $(X_5,\\dots,X_{10})$ are independent\n",
      "standard normal random variables.</p>\n",
      "  </li>\n",
      "  <li>\n",
      "    <p>3-dimensional XOR as 4-way classification. Consider the 8 corners of the\n",
      "3-dimensional hypercube $(v_1, v_2, v_3) \\in \\{-1,1\\}^3$, and group them by\n",
      "the tuples $(v_1 v_3, v_2 v_3)$, leaving 4 sets of vectors paired with their\n",
      "negations $\\{v^{(i)}, -v^{(i)}\\}$. Given a class $i$, a sample is generated\n",
      "by selecting $v^{(i)}$ or $-v^{(i)}$ with equal probability and adding some\n",
      "noise.  Each sample additionally has 7 standard normal noise features for a\n",
      "total of 10 dimensions.</p>\n",
      "  </li>\n",
      "  <li>\n",
      "    <p>Additive nonlinear regression. Consider the following additive model:</p>\n",
      "\n",
      "    <script type=\"math/tex; mode=display\">Y=-2\\sin(2X_1)+\\max(X_2,0)+X_3+\\exp(-X_4)+\\varepsilon.</script>\n",
      "\n",
      "    <p>Each sample additionally has 6 noise features for a total of 10 dimensions.\n",
      "All features and the noise $\\varepsilon$ are generated from standard normal\n",
      "distributions.</p>\n",
      "  </li>\n",
      "</ul>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/kernels/synthetic_data.png\" alt=\"orange_skin\" /><br />\n",
      "<i>\n",
      "Left: Orange Skin in 2d. Right: XOR in 2d.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>The first data set represents a standard nonlinear binary classification task.\n",
      "The second data set is a multi-class classification task where each feature is\n",
      "independent of $Y$ by itself but a combination of three features has a joint\n",
      "effect on $Y$. The third data set arises from an additive model for nonlinear\n",
      "regression.</p>\n",
      "\n",
      "<p>Each data set has $d=10$ dimensions in total, but only $m=3$ or $4$ true\n",
      "features. Since the identity of these features is known, we can evaluate the\n",
      "performance of a given feature selection algorithm by computing the median rank\n",
      "it assigns to the real features, with lower median ranks indicating better\n",
      "performance.</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/kernels/synthetic_results.png\" alt=\"synthetic_results\" /><br />\n",
      "<i>\n",
      "The above plots show the median rank (y-axis) of the true features as\n",
      "a function of sample size (x-axis) for the simulated data sets. Lower median\n",
      "ranks are better. The dotted line indicates the optimal median rank.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>On the binary and 4-way classification tasks, our method outperforms all other\n",
      "algorithms, succeeding in identifying the true features using fewer than 50\n",
      "samples where others require close to 100 or even fail to converge. On the\n",
      "additive nonlinear model, several algorithms perform well, and our method is on\n",
      "par with the best of them across all sample sizes.</p>\n",
      "\n",
      "<h2 id=\"real-world-data\">Real-world data</h2>\n",
      "\n",
      "<p>We now turn our attention to a collection of real-word tasks, studying the\n",
      "performance of our method and other nonlinear approaches (mRMR, BAHSIC, MI) when\n",
      "used in conjunction with a kernel SVM for downstream classification.</p>\n",
      "\n",
      "<p>We carry out experiments on 12 standard benchmark tasks from the ASU feature\n",
      "selection website and the UCI repository. A summary of our data sets is provided\n",
      "in the following table.</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/kernels/table.png\" alt=\"table\" width=\"600\" /><br />\n",
      "</p>\n",
      "\n",
      "<p>The data sets are drawn from several domains including gene data, image data,\n",
      "and voice data, and span both the low-dimensional and high-dimensional regimes.</p>\n",
      "\n",
      "<p>For every task, we run each algorithm being evaluated to obtain ranks for all\n",
      "features. Performance is then measured by training a kernel SVM on the top $m$\n",
      "features and computing the resulting accuracy. Our results are shown in the\n",
      "following figures.</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/kernels/real_results.png\" alt=\"real_results\" /><br />\n",
      "<i>\n",
      "The above plots show classification accuracy (y-axis) versus number of\n",
      "selected features (x-axis) for our real-world benchmark data sets. Higher\n",
      "accuracies are better.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>Compared with three other popular methods for nonlinear feature selection, we\n",
      "find that our method is the strongest performer in the large majority of cases,\n",
      "sometimes by a substantial margin as in the case of TOX-171. While our method is\n",
      "occasionally outperformed in the beginning when the number of selected features\n",
      "is small, it either ties or overtakes the leading method by the end in all but\n",
      "one instance.</p>\n",
      "\n",
      "<h1 id=\"conclusion\">Conclusion</h1>\n",
      "\n",
      "<p>In this post, we propose conditional covariance minimization (CCM), an approach\n",
      "to feature selection based on minimizing the trace of the conditional covariance\n",
      "operator. The idea is to select the features that maximally account for the\n",
      "dependence of the response on the covariates. We accomplish this by relaxing an\n",
      "intractable discrete formulation of the problem to obtain a continuous\n",
      "approximation suitable for gradient-based optimization. We demonstrate the\n",
      "effectiveness of our approach on multiple synthetic and real-world experiments,\n",
      "finding that it often outperforms other state-of-the-art approaches, including\n",
      "another competitive kernel feature selection method based on the Hilbert-Schmidt\n",
      "independence criterion.</p>\n",
      "\n",
      "<h1 id=\"more-information\">More Information</h1>\n",
      "\n",
      "<p>For more information about our algorithm, please take a look at the following links:</p>\n",
      "\n",
      "<ul>\n",
      "  <li><a href=\"https://papers.nips.cc/paper/7270-kernel-feature-selection-via-conditional-covariance-minimization\">Our paper</a></li>\n",
      "  <li><a href=\"https://github.com/Jianbo-Lab/CCM\">Our code</a></li>\n",
      "</ul>\n",
      "\n",
      "<p>Please let us know if you have any questions or suggestions.</p>\n",
      "\n",
      "\n",
      "  Link: http://bair.berkeley.edu/blog/2018/01/23/kernels/\n",
      "  PubDate: Tue, 23 Jan 2018 09:00:00 +0000\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20180223130148/http://bair.berkeley.edu:80/blog/feed.xml\n",
      "- Source: https://bair.berkeley.edu/blog/feed.xml\n",
      "  Channel Title: The Berkeley Artificial Intelligence Research Blog\n",
      "  Channel Description: The BAIR Blog\n",
      "- Title: Learning Robot Objectives from Physical Human Interaction\n",
      "  Description: <p>Humans physically interact with each other every day – from grabbing someone’s hand when they are about to spill their drink, to giving your friend a nudge to steer them in the right direction, physical interaction is an intuitive way to convey information about personal preferences and how to perform a task correctly.</p>\n",
      "\n",
      "<p>So why aren’t we physically interacting with current robots the way we do with each other? Seamless physical interaction between a human and a robot requires a lot: lightweight robot designs, reliable torque or force sensors, safe and reactive control schemes, the ability to predict the intentions of human collaborators, and more! Luckily, robotics has made many advances in the design of <a href=\"http://www.roboticgizmos.com/wp-content/uploads/2016/10/20/jaco2.gif\">personal robots</a> specifically developed with humans in mind.</p>\n",
      "\n",
      "<p>However, consider the example from the beginning where you grab your friend’s hand as they are about to spill their drink. Instead of your friend who is spilling, imagine it was a robot. Because state-of-the-art robot planning and control algorithms typically assume human physical interventions are disturbances, once you let go of the robot, it will resume its erroneous trajectory and continue spilling the drink. The key to this gap comes from how robots reason about physical interaction: instead of thinking about <em>why</em> the human physically intervened and replanning in accordance with what the human wants, most robots simply resume their original behavior after the interaction ends.</p>\n",
      "\n",
      "<p>We argue that <strong>robots should treat physical human interaction as useful information about how they should be doing the task</strong>. We formalize reacting to physical interaction as an objective (or reward) learning problem and propose a solution that enables robots to change their behaviors <em>while they are performing a task</em> according to the information gained during these interactions.</p>\n",
      "\n",
      "<!--more-->\n",
      "\n",
      "<h2 id=\"reasoning-about-physical-interaction-unknown-disturbance-versus-intentional-information\">Reasoning About Physical Interaction: Unknown Disturbance versus Intentional Information</h2>\n",
      "\n",
      "<p>The field of <em>physical human-robot interaction</em> (pHRI) studies the design, control, and planning problems that arise from close physical interaction between a human and a robot in a shared workspace. Prior research in pHRI has developed safe and responsive control methods to react to a physical interaction that happens while the robot is performing a task. Proposed by <a href=\"http://summerschool.stiff-project.org/fileadmin/pdf/Hog1985.pdf\">Hogan et. al.</a>, impedance control is one of the most commonly used methods to move a robot along a desired trajectory when there are people in the workspace. With this control method, the <strong>robot acts like a spring</strong>: it allows the person to push it, but moves back to an original desired position after the human stops applying forces. While this strategy is very fast and enables the robot to safely adapt to the human’s forces, the robot does not leverage these interventions to update its understanding of the task. Left alone, the robot would continue to perform the task in the same way as it had planned before any human interactions.</p>\n",
      "\n",
      "<p><img style=\"float:right; margin:20px\" src=\"http://bair.berkeley.edu/static/blog/phri/impedance_control.gif\" alt=\"impedance_control\" width=\"50%\" /></p>\n",
      "\n",
      "<p>Why is this the case? It boils down to what assumptions the robot makes about its knowledge of the task and the meaning of the forces it senses. Typically, a robot is given a notion of its task in the form of an <em>objective function</em>. This objective function encodes rewards for different aspects of the task like  “reach a goal at location X”  or  “move close to the table while staying far away from people”. The robot uses its objective function to produce a motion that best satisfies all the aspects of the task: for example, the robot would move toward goal X while choosing a path that is far from a human and close to the table. If the robot’s original objective function was correct, then any physical interaction is simply a disturbance from its correct path. Thus, the robot should allow the physical interaction to perturb it for safety purposes, but it will return to the original path it planned since it stubbornly believes it is correct.</p>\n",
      "\n",
      "<p>In contrast, we argue that human interventions are often intentional and occur because the robot is doing something wrong. While the robot’s original behavior may have been optimal with respect to its pre-defined objective function, the fact that a human intervention was necessary implies that <strong>the original objective function was not quite right</strong>. Thus, physical human interactions are no longer disturbances but rather informative observations about what the robot’s true objective should be. With this in mind, we take inspiration from <a href=\"http://ai.stanford.edu/~ang/papers/icml00-irl.pdf\">inverse reinforcement learning</a> (IRL), where the robot observes some behavior (e.g., being pushed away from the table) and tries to infer an unknown objective function (e.g., “stay farther away from the table”). Note that while many IRL methods focus on the robot doing better <em>the next time</em> it performs the task, we focus on the robot completing its <em>current</em> task correctly.</p>\n",
      "\n",
      "<h2 id=\"formalizing-reacting-to-phri\">Formalizing Reacting to pHRI</h2>\n",
      "\n",
      "<p>With our insight on physical human-robot interactions, we can formalize pHRI as a dynamical system, where the robot is unsure about the correct objective function and the human’s interactions provide it with information. This formalism defines a broad class of pHRI algorithms, which includes existing methods such as impedance control, and enables us to derive a novel online learning method.</p>\n",
      "\n",
      "<p>We will focus on two parts of the formalism: (1) the structure of the objective function and (2) the observation model that lets the robot reason about the objective given a human physical interaction. Let <script type=\"math/tex\">x</script> be the robot’s state (e.g., position and velocity) and <script type=\"math/tex\">u_R</script> be the robot’s action (e.g., the torque it applies to its joints). The human can physically interact with the robot by applying an external torque, called <script type=\"math/tex\">u_H</script>, and the robot moves to the next state via its dynamics, <script type=\"math/tex\">\\dot{x} = f(x,u_R+u_H)</script>.</p>\n",
      "\n",
      "<h3 id=\"the-robot-objective-doing-the-task-right-with-minimal-human-interaction\">The Robot Objective: Doing the Task Right with Minimal Human Interaction</h3>\n",
      "\n",
      "<p>In pHRI, we want the robot to learn from the human, but at the same time we do not want to overburden the human with constant physical intervention. Hence, we can write down an objective for the robot that optimizes both completing the task and minimizing the amount of interaction required, ultimately trading off between the two.</p>\n",
      "\n",
      "<script type=\"math/tex; mode=display\">r(x,u_R,u_H;\\theta) = \\theta^{\\top} \\phi(x,u_R,u_H) - ||u_H||^2</script>\n",
      "\n",
      "<p>Here, <script type=\"math/tex\">\\phi(x,u_R,u_H)</script> encodes the task-related features (e.g., “distance to table”, “distance to human”, “distance to goal”) and <script type=\"math/tex\">\\theta</script> determines the relative weight of each of these features. In the function, <script type=\"math/tex\">\\theta</script> encapsulates the true objective – if the robot knew exactly how to weight all the aspects of its task, then it could compute how to perform the task optimally. However, this parameter is not known by the robot! Robots will not always know the right way to perform a task, and certainly not the human-preferred way.</p>\n",
      "\n",
      "<h3 id=\"the-observation-model-inferring-the-right-objective-from-human-interaction\">The Observation Model: Inferring the Right Objective from Human Interaction</h3>\n",
      "\n",
      "<p>As we have argued, the robot should observe the human’s actions to infer the unknown task objective. To link the direct human forces that the robot measures with the objective function, the robot uses an <em>observation model</em>. Building on prior work in  <a href=\"https://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf\">maximum entropy IRL</a> as well as the Bolzmann distributions used in <a href=\"http://web.mit.edu/clbaker/www/papers/cogsci2007.pdf\">cognitive science models</a> of human behavior, we model the human’s interventions as corrections which approximately maximize the robot’s expected reward at state <script type=\"math/tex\">x</script> while taking action <script type=\"math/tex\">u_R+u_H</script>. This expected reward emcompasses the immediate and future rewards and is captured by the <script type=\"math/tex\">Q</script>-value:</p>\n",
      "\n",
      "<script type=\"math/tex; mode=display\">P(u_H \\mid x, u_R; \\theta) \\propto e^{Q(x,u_R+u_H;\\theta)}</script>\n",
      "\n",
      "<p>Intuitively, this model says that a human is more likely to choose a physical correction that, when combined with the robot’s action, leads to a desirable (i.e., high-reward) behavior.</p>\n",
      "\n",
      "<h2 id=\"learning-from-physical-human-robot-interactions-in-real-time\">Learning from Physical Human-Robot Interactions in Real-Time</h2>\n",
      "\n",
      "<p>Much like teaching another human, we expect that the robot will continuously learn while we interact with it. However, the learning framework that we have introduced requires that the robot solve a Partially Observable Markov Decision Process (POMDP); unfortunately, it is well known that solving POMDPs exactly is at best computationally expensive, and at worst intractable. Nonetheless, we can derive approximations from this formalism that can enable the robot to learn and act while humans are interacting.</p>\n",
      "\n",
      "<p>To achieve such in-task learning, we make three approximations summarized below:</p>\n",
      "\n",
      "<p><strong>1) Separate estimating the true objective from solving for the optimal control policy.</strong> This means at every timestep, the robot updates its belief over possible <script type=\"math/tex\">\\theta</script> values, and then re-plans an optimal control policy with the new distribution.</p>\n",
      "\n",
      "<p><strong>2) Separate planning from control</strong>. Computing an optimal control policy means computing the optimal action to take at every state in a continuous state, action, and belief space. Although re-computing a full optimal <em>policy</em> after every interaction is not tractable in real-time, we can re-compute an optimal <em>trajectory</em> from the current state in real-time. This means that the robot first plans a trajectory that best satisfies the current estimate of the objective, and then uses an impedance controller to track this trajectory.  The use of impedance control here gives us the nice properties described earlier, where people can physically modify the robot’s state while still being safe during interaction.</p>\n",
      "\n",
      "<p>Looking back at our estimation step, we will make a similar shift to trajectory space and modify our observation model to reflect this:</p>\n",
      "\n",
      "<script type=\"math/tex; mode=display\">P(u_H \\mid x, u_R; \\theta) \\propto e^{Q(x,u_R+u_H;\\theta)} \\rightarrow P(\\xi_H \\mid \\xi_R; \\theta) \\propto e^{R(\\xi_H, \\xi_R;\\theta)}</script>\n",
      "\n",
      "<p>Now, our observation model depends only on the cumulative reward <script type=\"math/tex\">R</script> along a trajectory, which is easily computed by summing up the reward at each timestep. With this approximation, when reasoning about the true objective, the robot only has to consider the likelihood of a human’s preferred trajectory, <script type=\"math/tex\">\\xi_H</script>, given the current trajectory it is executing, <script type=\"math/tex\">\\xi_R</script>.</p>\n",
      "\n",
      "<p>But what is the human’s preferred trajectory, <script type=\"math/tex\">\\xi_H</script>? The robot only gets to directly measure the human’s force $u_H$. One way to infer what is the human’s preferred trajectory is by propagating the human’s force throughout the robot’s current trajectory, <script type=\"math/tex\">\\xi_R</script>. Figure 1. builds up the trajectory deformation based on prior work from <a href=\"http://dylanlosey.com/wp-content/uploads/2016/07/TRO_2017.pdf\">Losey and O’Malley</a>, starting from the robot’s original trajectory, then the force application, and then the deformation to produce <script type=\"math/tex\">\\xi_H</script>.</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img width=\"50%\" src=\"http://bair.berkeley.edu/static/blog/phri/deformation_process.png\" alt=\"deformation_process\" /><br />\n",
      "<i>\n",
      "Fig 1. To infer the human’s prefered trajectory given the current planned trajectory, the robot first measures the human’s interaction force, $u_H$, and then smoothly deforms the waypoints near interaction point to get the human’s preferred trajectory, $\\xi_H$.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p><strong>3) Plan with maximum a posteriori (MAP) estimate of <script type=\"math/tex\">\\theta</script></strong>. Finally, because <script type=\"math/tex\">\\theta</script> is a continuous variable and potentially high-dimensional, and since our observation model is not Gaussian, rather than planning with the full belief over <script type=\"math/tex\">\\theta</script>, we will plan only with the MAP estimate. We find that the MAP estimate under a 2nd order Taylor Series Expansion about the robot’s current trajectory with a Gaussian prior is equivalent to running online gradient descent:</p>\n",
      "\n",
      "<script type=\"math/tex; mode=display\">\\theta^{t+1} = \\theta^{t} + \\alpha(\\Phi(\\xi^t_H) - \\Phi(\\xi^t_R))</script>\n",
      "\n",
      "<p>At every timestep, the robot updates its estimate of <script type=\"math/tex\">\\theta</script> in the direction of the cumulative feature difference, <script type=\"math/tex\">\\Phi(\\xi) = \\sum_{x^t \\in \\xi} \\phi(x^t)</script>, between its current optimal trajectory and the human’s preferred trajectory. In the Learning from Demonstration literature, this update rule is analogous to online <a href=\"https://www.ri.cmu.edu/pub_files/pub4/ratliff_nathan_2006_1/ratliff_nathan_2006_1.pdf\">Max Margin Planning</a>; it is also analogous to <a href=\"https://arxiv.org/pdf/1601.00741.pdf\">coactive learning</a>, where the user modifies waypoints for the current task to teach a reward function for future tasks.</p>\n",
      "\n",
      "<p>Ultimately, putting these three steps together leads us to an elegant approximate solution to the original POMDP. At every timestep, the robot plans a trajectory <script type=\"math/tex\">\\xi_R</script> and begins to move. The human can physically interact, enabling the robot to sense their force $u_H$. The robot uses the human’s force to deform its original trajectory and produce the human’s desired trajectory, <script type=\"math/tex\">\\xi_H</script>. Then the robot reasons about what aspects of the task are different between its original and the human’s preferred trajectory, and updates <script type=\"math/tex\">\\theta</script> in the direction of that difference. Using the new feature weights, the robot replans a trajectory that better aligns with the human’s preferences.</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/phri/algorithm.gif\" alt=\"algorithm\" />\n",
      "</p>\n",
      "\n",
      "<p>For a more thorough description of our formalism and approximations, please see <a href=\"http://proceedings.mlr.press/v78/bajcsy17a/bajcsy17a.pdf\">our recent paper from the 2017 Conference on Robot Learning</a>.</p>\n",
      "\n",
      "<h2 id=\"learning-from-humans-in-the-real-world\">Learning from Humans in the Real World</h2>\n",
      "\n",
      "<p>To evaluate the benefits of in-task learning on a real personal robot, we recruited 10 participants for a user study. Each participant interacted with the robot running our proposed online learning method as well as a baseline where the robot did not learn from physical interaction and simply ran impedance control.</p>\n",
      "\n",
      "<p>Fig 2. shows the three experimental household manipulation tasks, in each of which the robot started with an initially incorrect objective that participants had to correct. For example, the robot would move a cup from the shelf to the table, but without worrying about tilting the cup (perhaps not noticing that there is liquid inside).</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img width=\"30%\" src=\"http://bair.berkeley.edu/static/blog/phri/task1.png\" title=\"cup\" />\n",
      "<img width=\"30%\" src=\"http://bair.berkeley.edu/static/blog/phri/task2.png\" title=\"table\" />\n",
      "<img width=\"30%\" src=\"http://bair.berkeley.edu/static/blog/phri/task3.png\" title=\"laptop\" />\n",
      "<br />\n",
      "<i>\n",
      "Fig 2. Trajectory generated with initial objective marked in black, and the desired trajectory from true objective in blue. Participants need to correct the robot to teach it to hold the cup upright (left), move closer to the table (center), and avoid going over the laptop (right).  </i>\n",
      "</p>\n",
      "\n",
      "<p>We measured the robot’s performance with respect to the true objective, the total effort the participant exerted, the total amount of interaction time, and the responses of a 7-point Likert scale survey.</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/phri/task1.gif\" alt=\"cup gif\" /><br />\n",
      "<i>\n",
      "In Task 1, participants have to physically intervene when they see the robot tilting the cup and teach the robot to keep the cup upright.  \n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/phri/task2.gif\" alt=\"table gif\" /><br />\n",
      "<i>\n",
      "Task 2 had participants teaching the robot to move closer to the table.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/phri/task3.gif\" alt=\"laptop gif\" /><br />\n",
      "<i>\n",
      "For Task 3, the robot’s original trajectory goes over a laptop. Participants have to physically teach the robot to move around the laptop instead of over it.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>The results of our user studies suggest that learning from physical interaction leads to better robot task performance with less human effort. Participants were able to <strong>get the robot to execute the correct behavior faster with less effort and interaction time</strong> when the robot was actively learning from their interactions during the task. Additionally, <strong>participants believed the robot understood their preferences more, took less effort to interact with, and was a more collaborative partner</strong>.</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img width=\"30%\" src=\"http://bair.berkeley.edu/static/blog/phri/taskCost_cameraready.png\" title=\"task cost\" />\n",
      "<img width=\"30%\" src=\"http://bair.berkeley.edu/static/blog/phri/taskEffort_cameraready.png\" title=\"task effort\" />\n",
      "<img width=\"30%\" src=\"http://bair.berkeley.edu/static/blog/phri/taskTime_cameraready.png\" title=\"task time\" />\n",
      "<br />\n",
      "<i>\n",
      "Fig 3. Learning from interaction significantly outperformed not learning for each of our objective measures, including task cost, human effort, interaction time.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>Ultimately, we propose that robots should not treat human interactions as disturbances, but rather as informative actions. We showed that robots imbued with this sort of reasoning are capable of updating their understanding of the task they are performing and completing it correctly, rather than relying on people to guide them until the task is done.</p>\n",
      "\n",
      "<p>This work is merely a step in exploring learning robot objectives from pHRI. Many open questions remain including developing solutions that can handle dynamical aspects (like preferences about the timing of the motion) and how and when to generalize learned objectives to new tasks. Additionally, robot reward functions will often have many task-related features and human interactions may only give information about a certain subset of relevant weights. Our recent work in HRI 2018 studied how a robot can disambiguate what the person is trying to correct by learning about only a single feature weight at a time. Overall, not only do we need algorithms that can learn from physical interaction with humans, but these methods must also reason about the inherent difficulties humans experience when trying to kinesthetically teach a complex – and possibly unfamiliar – robotic system.</p>\n",
      "\n",
      "<hr />\n",
      "\n",
      "<p>Thank you to Dylan Losey and Anca Dragan for their helpful feedback in writing this blog post.</p>\n",
      "\n",
      "<hr />\n",
      "\n",
      "<p>This post is based on the following papers:</p>\n",
      "\n",
      "<ul>\n",
      "  <li>\n",
      "    <p>A. Bajcsy* , D.P. Losey*, M.K. O’Malley, and A.D. Dragan. <strong>Learning Robot Objectives from Physical Human Robot Interaction</strong>. Conference on Robot Learning (CoRL), 2017.</p>\n",
      "  </li>\n",
      "  <li>\n",
      "    <p>A. Bajcsy , D.P. Losey, M.K. O’Malley, and A.D. Dragan. <strong>Learning from Physical Human Corrections, One Feature at a Time</strong>. International Conference on Human-Robot Interaction (HRI), 2018.</p>\n",
      "  </li>\n",
      "</ul>\n",
      "\n",
      "\n",
      "  Link: http://bair.berkeley.edu/blog/2018/02/06/phri/\n",
      "  PubDate: Tue, 06 Feb 2018 01:00:00 -0800\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20180311165200/http://bair.berkeley.edu/blog/feed.xml\n",
      "- Source: https://bair.berkeley.edu/blog/feed.xml\n",
      "  Channel Title: The Berkeley Artificial Intelligence Research Blog\n",
      "  Channel Description: The BAIR Blog\n",
      "- Title: Learning Robot Objectives from Physical Human Interaction\n",
      "  Description: <p>Humans physically interact with each other every day – from grabbing someone’s hand when they are about to spill their drink, to giving your friend a nudge to steer them in the right direction, physical interaction is an intuitive way to convey information about personal preferences and how to perform a task correctly.</p>\n",
      "\n",
      "<p>So why aren’t we physically interacting with current robots the way we do with each other? Seamless physical interaction between a human and a robot requires a lot: lightweight robot designs, reliable torque or force sensors, safe and reactive control schemes, the ability to predict the intentions of human collaborators, and more! Luckily, robotics has made many advances in the design of <a href=\"http://www.roboticgizmos.com/wp-content/uploads/2016/10/20/jaco2.gif\">personal robots</a> specifically developed with humans in mind.</p>\n",
      "\n",
      "<p>However, consider the example from the beginning where you grab your friend’s hand as they are about to spill their drink. Instead of your friend who is spilling, imagine it was a robot. Because state-of-the-art robot planning and control algorithms typically assume human physical interventions are disturbances, once you let go of the robot, it will resume its erroneous trajectory and continue spilling the drink. The key to this gap comes from how robots reason about physical interaction: instead of thinking about <em>why</em> the human physically intervened and replanning in accordance with what the human wants, most robots simply resume their original behavior after the interaction ends.</p>\n",
      "\n",
      "<p>We argue that <strong>robots should treat physical human interaction as useful information about how they should be doing the task</strong>. We formalize reacting to physical interaction as an objective (or reward) learning problem and propose a solution that enables robots to change their behaviors <em>while they are performing a task</em> according to the information gained during these interactions.</p>\n",
      "\n",
      "<!--more-->\n",
      "\n",
      "<h2 id=\"reasoning-about-physical-interaction-unknown-disturbance-versus-intentional-information\">Reasoning About Physical Interaction: Unknown Disturbance versus Intentional Information</h2>\n",
      "\n",
      "<p>The field of <em>physical human-robot interaction</em> (pHRI) studies the design, control, and planning problems that arise from close physical interaction between a human and a robot in a shared workspace. Prior research in pHRI has developed safe and responsive control methods to react to a physical interaction that happens while the robot is performing a task. Proposed by <a href=\"http://summerschool.stiff-project.org/fileadmin/pdf/Hog1985.pdf\">Hogan et. al.</a>, impedance control is one of the most commonly used methods to move a robot along a desired trajectory when there are people in the workspace. With this control method, the <strong>robot acts like a spring</strong>: it allows the person to push it, but moves back to an original desired position after the human stops applying forces. While this strategy is very fast and enables the robot to safely adapt to the human’s forces, the robot does not leverage these interventions to update its understanding of the task. Left alone, the robot would continue to perform the task in the same way as it had planned before any human interactions.</p>\n",
      "\n",
      "<p><img style=\"float:right; margin:20px\" src=\"http://bair.berkeley.edu/static/blog/phri/impedance_control.gif\" alt=\"impedance_control\" width=\"50%\" /></p>\n",
      "\n",
      "<p>Why is this the case? It boils down to what assumptions the robot makes about its knowledge of the task and the meaning of the forces it senses. Typically, a robot is given a notion of its task in the form of an <em>objective function</em>. This objective function encodes rewards for different aspects of the task like  “reach a goal at location X”  or  “move close to the table while staying far away from people”. The robot uses its objective function to produce a motion that best satisfies all the aspects of the task: for example, the robot would move toward goal X while choosing a path that is far from a human and close to the table. If the robot’s original objective function was correct, then any physical interaction is simply a disturbance from its correct path. Thus, the robot should allow the physical interaction to perturb it for safety purposes, but it will return to the original path it planned since it stubbornly believes it is correct.</p>\n",
      "\n",
      "<p>In contrast, we argue that human interventions are often intentional and occur because the robot is doing something wrong. While the robot’s original behavior may have been optimal with respect to its pre-defined objective function, the fact that a human intervention was necessary implies that <strong>the original objective function was not quite right</strong>. Thus, physical human interactions are no longer disturbances but rather informative observations about what the robot’s true objective should be. With this in mind, we take inspiration from <a href=\"http://ai.stanford.edu/~ang/papers/icml00-irl.pdf\">inverse reinforcement learning</a> (IRL), where the robot observes some behavior (e.g., being pushed away from the table) and tries to infer an unknown objective function (e.g., “stay farther away from the table”). Note that while many IRL methods focus on the robot doing better <em>the next time</em> it performs the task, we focus on the robot completing its <em>current</em> task correctly.</p>\n",
      "\n",
      "<h2 id=\"formalizing-reacting-to-phri\">Formalizing Reacting to pHRI</h2>\n",
      "\n",
      "<p>With our insight on physical human-robot interactions, we can formalize pHRI as a dynamical system, where the robot is unsure about the correct objective function and the human’s interactions provide it with information. This formalism defines a broad class of pHRI algorithms, which includes existing methods such as impedance control, and enables us to derive a novel online learning method.</p>\n",
      "\n",
      "<p>We will focus on two parts of the formalism: (1) the structure of the objective function and (2) the observation model that lets the robot reason about the objective given a human physical interaction. Let <script type=\"math/tex\">x</script> be the robot’s state (e.g., position and velocity) and <script type=\"math/tex\">u_R</script> be the robot’s action (e.g., the torque it applies to its joints). The human can physically interact with the robot by applying an external torque, called <script type=\"math/tex\">u_H</script>, and the robot moves to the next state via its dynamics, <script type=\"math/tex\">\\dot{x} = f(x,u_R+u_H)</script>.</p>\n",
      "\n",
      "<h3 id=\"the-robot-objective-doing-the-task-right-with-minimal-human-interaction\">The Robot Objective: Doing the Task Right with Minimal Human Interaction</h3>\n",
      "\n",
      "<p>In pHRI, we want the robot to learn from the human, but at the same time we do not want to overburden the human with constant physical intervention. Hence, we can write down an objective for the robot that optimizes both completing the task and minimizing the amount of interaction required, ultimately trading off between the two.</p>\n",
      "\n",
      "<script type=\"math/tex; mode=display\">r(x,u_R,u_H;\\theta) = \\theta^{\\top} \\phi(x,u_R,u_H) - ||u_H||^2</script>\n",
      "\n",
      "<p>Here, <script type=\"math/tex\">\\phi(x,u_R,u_H)</script> encodes the task-related features (e.g., “distance to table”, “distance to human”, “distance to goal”) and <script type=\"math/tex\">\\theta</script> determines the relative weight of each of these features. In the function, <script type=\"math/tex\">\\theta</script> encapsulates the true objective – if the robot knew exactly how to weight all the aspects of its task, then it could compute how to perform the task optimally. However, this parameter is not known by the robot! Robots will not always know the right way to perform a task, and certainly not the human-preferred way.</p>\n",
      "\n",
      "<h3 id=\"the-observation-model-inferring-the-right-objective-from-human-interaction\">The Observation Model: Inferring the Right Objective from Human Interaction</h3>\n",
      "\n",
      "<p>As we have argued, the robot should observe the human’s actions to infer the unknown task objective. To link the direct human forces that the robot measures with the objective function, the robot uses an <em>observation model</em>. Building on prior work in  <a href=\"https://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf\">maximum entropy IRL</a> as well as the Bolzmann distributions used in <a href=\"http://web.mit.edu/clbaker/www/papers/cogsci2007.pdf\">cognitive science models</a> of human behavior, we model the human’s interventions as corrections which approximately maximize the robot’s expected reward at state <script type=\"math/tex\">x</script> while taking action <script type=\"math/tex\">u_R+u_H</script>. This expected reward emcompasses the immediate and future rewards and is captured by the <script type=\"math/tex\">Q</script>-value:</p>\n",
      "\n",
      "<script type=\"math/tex; mode=display\">P(u_H \\mid x, u_R; \\theta) \\propto e^{Q(x,u_R+u_H;\\theta)}</script>\n",
      "\n",
      "<p>Intuitively, this model says that a human is more likely to choose a physical correction that, when combined with the robot’s action, leads to a desirable (i.e., high-reward) behavior.</p>\n",
      "\n",
      "<h2 id=\"learning-from-physical-human-robot-interactions-in-real-time\">Learning from Physical Human-Robot Interactions in Real-Time</h2>\n",
      "\n",
      "<p>Much like teaching another human, we expect that the robot will continuously learn while we interact with it. However, the learning framework that we have introduced requires that the robot solve a Partially Observable Markov Decision Process (POMDP); unfortunately, it is well known that solving POMDPs exactly is at best computationally expensive, and at worst intractable. Nonetheless, we can derive approximations from this formalism that can enable the robot to learn and act while humans are interacting.</p>\n",
      "\n",
      "<p>To achieve such in-task learning, we make three approximations summarized below:</p>\n",
      "\n",
      "<p><strong>1) Separate estimating the true objective from solving for the optimal control policy.</strong> This means at every timestep, the robot updates its belief over possible <script type=\"math/tex\">\\theta</script> values, and then re-plans an optimal control policy with the new distribution.</p>\n",
      "\n",
      "<p><strong>2) Separate planning from control</strong>. Computing an optimal control policy means computing the optimal action to take at every state in a continuous state, action, and belief space. Although re-computing a full optimal <em>policy</em> after every interaction is not tractable in real-time, we can re-compute an optimal <em>trajectory</em> from the current state in real-time. This means that the robot first plans a trajectory that best satisfies the current estimate of the objective, and then uses an impedance controller to track this trajectory.  The use of impedance control here gives us the nice properties described earlier, where people can physically modify the robot’s state while still being safe during interaction.</p>\n",
      "\n",
      "<p>Looking back at our estimation step, we will make a similar shift to trajectory space and modify our observation model to reflect this:</p>\n",
      "\n",
      "<script type=\"math/tex; mode=display\">P(u_H \\mid x, u_R; \\theta) \\propto e^{Q(x,u_R+u_H;\\theta)} \\rightarrow P(\\xi_H \\mid \\xi_R; \\theta) \\propto e^{R(\\xi_H, \\xi_R;\\theta)}</script>\n",
      "\n",
      "<p>Now, our observation model depends only on the cumulative reward <script type=\"math/tex\">R</script> along a trajectory, which is easily computed by summing up the reward at each timestep. With this approximation, when reasoning about the true objective, the robot only has to consider the likelihood of a human’s preferred trajectory, <script type=\"math/tex\">\\xi_H</script>, given the current trajectory it is executing, <script type=\"math/tex\">\\xi_R</script>.</p>\n",
      "\n",
      "<p>But what is the human’s preferred trajectory, <script type=\"math/tex\">\\xi_H</script>? The robot only gets to directly measure the human’s force $u_H$. One way to infer what is the human’s preferred trajectory is by propagating the human’s force throughout the robot’s current trajectory, <script type=\"math/tex\">\\xi_R</script>. Figure 1. builds up the trajectory deformation based on prior work from <a href=\"http://dylanlosey.com/wp-content/uploads/2016/07/TRO_2017.pdf\">Losey and O’Malley</a>, starting from the robot’s original trajectory, then the force application, and then the deformation to produce <script type=\"math/tex\">\\xi_H</script>.</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img width=\"50%\" src=\"http://bair.berkeley.edu/static/blog/phri/deformation_process.png\" alt=\"deformation_process\" /><br />\n",
      "<i>\n",
      "Fig 1. To infer the human’s prefered trajectory given the current planned trajectory, the robot first measures the human’s interaction force, $u_H$, and then smoothly deforms the waypoints near interaction point to get the human’s preferred trajectory, $\\xi_H$.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p><strong>3) Plan with maximum a posteriori (MAP) estimate of <script type=\"math/tex\">\\theta</script></strong>. Finally, because <script type=\"math/tex\">\\theta</script> is a continuous variable and potentially high-dimensional, and since our observation model is not Gaussian, rather than planning with the full belief over <script type=\"math/tex\">\\theta</script>, we will plan only with the MAP estimate. We find that the MAP estimate under a 2nd order Taylor Series Expansion about the robot’s current trajectory with a Gaussian prior is equivalent to running online gradient descent:</p>\n",
      "\n",
      "<script type=\"math/tex; mode=display\">\\theta^{t+1} = \\theta^{t} + \\alpha(\\Phi(\\xi^t_H) - \\Phi(\\xi^t_R))</script>\n",
      "\n",
      "<p>At every timestep, the robot updates its estimate of <script type=\"math/tex\">\\theta</script> in the direction of the cumulative feature difference, <script type=\"math/tex\">\\Phi(\\xi) = \\sum_{x^t \\in \\xi} \\phi(x^t)</script>, between its current optimal trajectory and the human’s preferred trajectory. In the Learning from Demonstration literature, this update rule is analogous to online <a href=\"https://www.ri.cmu.edu/pub_files/pub4/ratliff_nathan_2006_1/ratliff_nathan_2006_1.pdf\">Max Margin Planning</a>; it is also analogous to <a href=\"https://arxiv.org/pdf/1601.00741.pdf\">coactive learning</a>, where the user modifies waypoints for the current task to teach a reward function for future tasks.</p>\n",
      "\n",
      "<p>Ultimately, putting these three steps together leads us to an elegant approximate solution to the original POMDP. At every timestep, the robot plans a trajectory <script type=\"math/tex\">\\xi_R</script> and begins to move. The human can physically interact, enabling the robot to sense their force $u_H$. The robot uses the human’s force to deform its original trajectory and produce the human’s desired trajectory, <script type=\"math/tex\">\\xi_H</script>. Then the robot reasons about what aspects of the task are different between its original and the human’s preferred trajectory, and updates <script type=\"math/tex\">\\theta</script> in the direction of that difference. Using the new feature weights, the robot replans a trajectory that better aligns with the human’s preferences.</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/phri/algorithm.gif\" alt=\"algorithm\" />\n",
      "</p>\n",
      "\n",
      "<p>For a more thorough description of our formalism and approximations, please see <a href=\"http://proceedings.mlr.press/v78/bajcsy17a/bajcsy17a.pdf\">our recent paper from the 2017 Conference on Robot Learning</a>.</p>\n",
      "\n",
      "<h2 id=\"learning-from-humans-in-the-real-world\">Learning from Humans in the Real World</h2>\n",
      "\n",
      "<p>To evaluate the benefits of in-task learning on a real personal robot, we recruited 10 participants for a user study. Each participant interacted with the robot running our proposed online learning method as well as a baseline where the robot did not learn from physical interaction and simply ran impedance control.</p>\n",
      "\n",
      "<p>Fig 2. shows the three experimental household manipulation tasks, in each of which the robot started with an initially incorrect objective that participants had to correct. For example, the robot would move a cup from the shelf to the table, but without worrying about tilting the cup (perhaps not noticing that there is liquid inside).</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img width=\"30%\" src=\"http://bair.berkeley.edu/static/blog/phri/task1.png\" title=\"cup\" />\n",
      "<img width=\"30%\" src=\"http://bair.berkeley.edu/static/blog/phri/task2.png\" title=\"table\" />\n",
      "<img width=\"30%\" src=\"http://bair.berkeley.edu/static/blog/phri/task3.png\" title=\"laptop\" />\n",
      "<br />\n",
      "<i>\n",
      "Fig 2. Trajectory generated with initial objective marked in black, and the desired trajectory from true objective in blue. Participants need to correct the robot to teach it to hold the cup upright (left), move closer to the table (center), and avoid going over the laptop (right).  </i>\n",
      "</p>\n",
      "\n",
      "<p>We measured the robot’s performance with respect to the true objective, the total effort the participant exerted, the total amount of interaction time, and the responses of a 7-point Likert scale survey.</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/phri/task1.gif\" alt=\"cup gif\" /><br />\n",
      "<i>\n",
      "In Task 1, participants have to physically intervene when they see the robot tilting the cup and teach the robot to keep the cup upright.  \n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/phri/task2.gif\" alt=\"table gif\" /><br />\n",
      "<i>\n",
      "Task 2 had participants teaching the robot to move closer to the table.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/phri/task3.gif\" alt=\"laptop gif\" /><br />\n",
      "<i>\n",
      "For Task 3, the robot’s original trajectory goes over a laptop. Participants have to physically teach the robot to move around the laptop instead of over it.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>The results of our user studies suggest that learning from physical interaction leads to better robot task performance with less human effort. Participants were able to <strong>get the robot to execute the correct behavior faster with less effort and interaction time</strong> when the robot was actively learning from their interactions during the task. Additionally, <strong>participants believed the robot understood their preferences more, took less effort to interact with, and was a more collaborative partner</strong>.</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img width=\"30%\" src=\"http://bair.berkeley.edu/static/blog/phri/taskCost_cameraready.png\" title=\"task cost\" />\n",
      "<img width=\"30%\" src=\"http://bair.berkeley.edu/static/blog/phri/taskEffort_cameraready.png\" title=\"task effort\" />\n",
      "<img width=\"30%\" src=\"http://bair.berkeley.edu/static/blog/phri/taskTime_cameraready.png\" title=\"task time\" />\n",
      "<br />\n",
      "<i>\n",
      "Fig 3. Learning from interaction significantly outperformed not learning for each of our objective measures, including task cost, human effort, interaction time.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>Ultimately, we propose that robots should not treat human interactions as disturbances, but rather as informative actions. We showed that robots imbued with this sort of reasoning are capable of updating their understanding of the task they are performing and completing it correctly, rather than relying on people to guide them until the task is done.</p>\n",
      "\n",
      "<p>This work is merely a step in exploring learning robot objectives from pHRI. Many open questions remain including developing solutions that can handle dynamical aspects (like preferences about the timing of the motion) and how and when to generalize learned objectives to new tasks. Additionally, robot reward functions will often have many task-related features and human interactions may only give information about a certain subset of relevant weights. Our recent work in HRI 2018 studied how a robot can disambiguate what the person is trying to correct by learning about only a single feature weight at a time. Overall, not only do we need algorithms that can learn from physical interaction with humans, but these methods must also reason about the inherent difficulties humans experience when trying to kinesthetically teach a complex – and possibly unfamiliar – robotic system.</p>\n",
      "\n",
      "<hr />\n",
      "\n",
      "<p>Thank you to Dylan Losey and Anca Dragan for their helpful feedback in writing this blog post.</p>\n",
      "\n",
      "<hr />\n",
      "\n",
      "<p>This post is based on the following papers:</p>\n",
      "\n",
      "<ul>\n",
      "  <li>\n",
      "    <p>A. Bajcsy* , D.P. Losey*, M.K. O’Malley, and A.D. Dragan. <strong>Learning Robot Objectives from Physical Human Robot Interaction</strong>. Conference on Robot Learning (CoRL), 2017.</p>\n",
      "  </li>\n",
      "  <li>\n",
      "    <p>A. Bajcsy , D.P. Losey, M.K. O’Malley, and A.D. Dragan. <strong>Learning from Physical Human Corrections, One Feature at a Time</strong>. International Conference on Human-Robot Interaction (HRI), 2018.</p>\n",
      "  </li>\n",
      "</ul>\n",
      "\n",
      "\n",
      "  Link: http://bair.berkeley.edu/blog/2018/02/06/phri/\n",
      "  PubDate: Tue, 06 Feb 2018 01:00:00 -0800\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20180327002420/http://bair.berkeley.edu:80/blog/feed.xml\n",
      "- Source: https://bair.berkeley.edu/blog/feed.xml\n",
      "  Channel Title: The Berkeley Artificial Intelligence Research Blog\n",
      "  Channel Description: The BAIR Blog\n",
      "- Title: Transfer Your Font Style with GANs\n",
      "  Description: <p style=\"text-align:center;\">\n",
      "<img height=\"300\" src=\"http://bair.berkeley.edu/static/blog/mcgan/given-new-titles.jpg\" />\n",
      "<br />\n",
      "<i>\n",
      "Left: Given movie poster, Right: New movie title generated by MC-GAN.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>Text is a prominent visual element of 2D design. Artists invest significant time\n",
      "into designing glyphs that are visually compatible with other elements in their\n",
      "shape and texture. This process is labor intensive and artists often design only\n",
      "the subset of glyphs that are necessary for a title or an annotation, which\n",
      "makes it difficult to alter the text after the design is created, or to transfer\n",
      "an observed instance of a font to your own project.</p>\n",
      "\n",
      "<p>Early research on glyph synthesis focused on geometric modeling of outlines,\n",
      "which is limited to particular glyph topology (e.g., cannot be applied to\n",
      "decorative or hand-written glyphs) and cannot be used with image input.\n",
      "With the rise of deep neural networks, researchers have looked at modeling\n",
      "glyphs from images. On the other hand, synthesizing data consistent with \n",
      "partial observations is an interesting problem in computer vision and graphics\n",
      "such as multi-view image generation, completing missing regions in images, \n",
      "and generating 3D shapes. Font data is an example that provides a clean factorization\n",
      "of style and content.</p>\n",
      "\n",
      "<p>Recent advances in conditional generative adversarial networks (cGANS) [1] have\n",
      "been successful in many generative applications. However, they do best only with\n",
      "fairly specialized domains and not with general or multi-domain style transfer.\n",
      "Similarly, when directly used to generate fonts, cGAN models produce significant\n",
      "artifacts. For instance, given the following five letters,</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/mcgan/given_tower.png\" width=\"20%\" /><br />\n",
      "</p>\n",
      "\n",
      "<p>a conditional GAN model is not successful in generating all 26 letters with the same style:</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/mcgan/pix2pix_tower.png\" width=\"100%\" /><br />\n",
      "</p>\n",
      "\n",
      "<!--more-->\n",
      "\n",
      "<h1 id=\"multi-content-gan-for-few-shot-font-style-transfer\">Multi-Content GAN for Few Shot Font Style Transfer</h1>\n",
      "\n",
      "<p>Instead of training a single network for all possible typeface ornamentations,\n",
      "we designed the multi-content GAN architecture [2] to retrain a customized\n",
      "magical network for each observed character set with only a handful of observed\n",
      "glyphs.  This model considers content (i.e., A-Z glyphs) along channels and\n",
      "style (i.e., glyph ornamentations) along network layers to transfer the style of\n",
      "given glyphs to the contents of unseen ones.</p>\n",
      "\n",
      "<p>The multi-content GAN model consists of a stacked cGAN architecture to predict\n",
      "the coarse glyph shapes and an ornamentation network to predict color and\n",
      "texture of the final glyphs. The first network, called GlyphNet, predicts glyph\n",
      "masks while the second network, called OrnaNet, fine-tunes color and\n",
      "ornamentation of the generated glyphs from the first network. Each sub-network\n",
      "follows the conditional generative adversarial network (cGAN) architecture\n",
      "modified for its specific purpose of stylizing glyphs or ornamentation\n",
      "prediction.</p>\n",
      "\n",
      "<h2 id=\"network-architecture\">Network Architecture</h2>\n",
      "\n",
      "<p>Here is the schematic of GlyphNet to learn the general shape of the font\n",
      "manifold from a set of training fonts. Input and output of the GlyphNet are\n",
      "stacks of the glyphs where a channel is assigned for each letter.  In each\n",
      "training iteration, $x_1$ includes a randomly chosen subset of $y_1$ glyphs with\n",
      "the remaining input channels being zeroed out.</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/mcgan/GlyphNet-simple.jpg\" width=\"90%\" /><br />\n",
      "</p>\n",
      "\n",
      "<p>With this novel glyph stack design, correlations between different glyphs are\n",
      "learned across network channels in order to transfer their style automatically.\n",
      "The following plots represent such correlations through the structural\n",
      "similarity (SSIM) metric on a random set of 1500 font examples. Computing the\n",
      "structural similarity between each generated glyph and its ground truth, 25\n",
      "distributions are found when a single letter has been observed at a time. These\n",
      "plots show the distributions $\\alpha| \\beta$ of generating letter $\\alpha$ when\n",
      "letter $\\beta$ is observed (in blue) vs when any other letter rather than\n",
      "$\\beta$ is given (in red). Distributions for the two most informative given\n",
      "letters and the two least informative ones in generating each of the 26 letters\n",
      "are shown in this figure. For instance, looking at the fifth row of the figure,\n",
      "letters F and B are the most constructive in generating letter E compared with\n",
      "other letters while I and W are the least informative ones. As other examples, O\n",
      "and C are the most guiding letters for constructing G as well as R and B for\n",
      "generating P.</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/mcgan/corr_.png\" width=\"80%\" /><br />\n",
      "</p>\n",
      "\n",
      "<p>Therefore, for any desirable font with only a few observed letters, the\n",
      "pre-trained GlyphNet generates all 26 A-Z glyphs. But how should we transfer\n",
      "ornamentation? The second network, OrnaNet, takes these generated glyphs and\n",
      "after a simple reshape transformation and gray-scale channel repetition,\n",
      "represented by $\\mathcal{T}$ in the next figure, generates outputs enriched with\n",
      "desirable color and ornamentation using a conditional GAN architecture. Inputs\n",
      "and outputs of the OrnaNet are batches of RGB images instead of stacks where the\n",
      "RGB channels for each letter, as an image, are repeats of its corresponding\n",
      "gray-scale glyph generated by the GlyphNet. Multiple regularizers in the OrnaNet\n",
      "penalize deviation of the masks of stylized letters from their corresponding\n",
      "glyph shapes.</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/mcgan/End-to-End-Simple.jpg\" width=\"100%\" /><br />\n",
      "</p>\n",
      "\n",
      "<h2 id=\"results\">Results</h2>\n",
      "\n",
      "<p>Below, we demonstrate exemplar sentences using the font style given in a single\n",
      "word.</p>\n",
      "\n",
      "<form>\n",
      "<p style=\"text-align:center;\">\n",
      "<font size=\"5\"> Given: </font>\n",
      "<input type=\"radio\" name=\"option_font1\" onclick=\"update_im_font1(this.value)\" value=\"generate_ft51_1\" />\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/mcgan/input_generate_ft51_1.png\" width=\"14%\" />\n",
      "<input type=\"radio\" name=\"option_font1\" onclick=\"update_im_font1(this.value)\" value=\"content_ft51_1\" />\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/mcgan/input_content_ft51_1.png\" width=\"14%\" />\n",
      "<input type=\"radio\" name=\"option_font1\" onclick=\"update_im_font1(this.value)\" value=\"transfer_ft51_1\" checked=\"\" />\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/mcgan/input_transfer_ft51_1.png\" width=\"14%\" />\n",
      "<input type=\"radio\" name=\"option_font1\" onclick=\"update_im_font1(this.value)\" value=\"texture_ft51_1\" />\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/mcgan/input_texture_ft51_1.png\" width=\"14%\" />\n",
      "</p>\n",
      "</form>\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/mcgan/transfer_ft51_1.png\" name=\"im_font1\" id=\"im_font1\" width=\"90%\" />\n",
      "</p>\n",
      "<p><br /></p>\n",
      "\n",
      "<form>\n",
      "<p style=\"text-align:center;\">\n",
      "<font size=\"5\"> Given: </font>\n",
      "<input type=\"radio\" name=\"option_font2\" onclick=\"update_im_font2(this.value)\" value=\"generate_ft37_1\" />\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/mcgan/input_generate_ft37_1.png\" width=\"14%\" />\n",
      "<input type=\"radio\" name=\"option_font2\" onclick=\"update_im_font2(this.value)\" value=\"content_ft37_1\" />\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/mcgan/input_content_ft37_1.png\" width=\"14%\" />\n",
      "<input type=\"radio\" name=\"option_font2\" onclick=\"update_im_font2(this.value)\" value=\"transfer_ft37_1\" checked=\"\" />\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/mcgan/input_transfer_ft37_1.png\" width=\"14%\" />\n",
      "<input type=\"radio\" name=\"option_font2\" onclick=\"update_im_font2(this.value)\" value=\"texture_ft37_1\" />\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/mcgan/input_texture_ft37_1.png\" width=\"14%\" />\n",
      "</p>\n",
      "</form>\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/mcgan/transfer_ft37_1.png\" name=\"im_font2\" id=\"im_font2\" width=\"90%\" />\n",
      "</p>\n",
      "<p><br /></p>\n",
      "\n",
      "<form>\n",
      "<p style=\"text-align:center;\">\n",
      "<font size=\"5\"> Given: </font>\n",
      "<input type=\"radio\" name=\"option_font3\" onclick=\"update_im_font3(this.value)\" value=\"generate_ft55_1\" />\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/mcgan/input_generate_ft55_1.png\" width=\"13%\" />\n",
      "<input type=\"radio\" name=\"option_font3\" onclick=\"update_im_font3(this.value)\" value=\"content_ft55_1\" />\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/mcgan/input_content_ft55_1.png\" width=\"13%\" />\n",
      "<input type=\"radio\" name=\"option_font3\" onclick=\"update_im_font3(this.value)\" value=\"transfer_ft55_1\" checked=\"\" />\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/mcgan/input_transfer_ft55_1.png\" width=\"13%\" />\n",
      "<input type=\"radio\" name=\"option_font3\" onclick=\"update_im_font3(this.value)\" value=\"texture_ft55_1\" />\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/mcgan/input_texture_ft55_1.png\" width=\"13%\" />\n",
      "</p>\n",
      "</form>\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/mcgan/transfer_ft55_1.png\" name=\"im_font3\" id=\"im_font3\" width=\"90%\" />\n",
      "</p>\n",
      "<p><br /></p>\n",
      "\n",
      "<form>\n",
      "<p style=\"text-align:center;\">\n",
      "<font size=\"5\"> Given: </font>\n",
      "<input type=\"radio\" name=\"option_font4\" onclick=\"update_im_font4(this.value)\" value=\"generate_SHREK\" />\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/mcgan/input_generate_SHREK.png\" width=\"14.5%\" />\n",
      "<input type=\"radio\" name=\"option_font4\" onclick=\"update_im_font4(this.value)\" value=\"content_SHREK\" />\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/mcgan/input_content_SHREK.png\" width=\"14.5%\" /> \n",
      "<input type=\"radio\" name=\"option_font4\" onclick=\"update_im_font4(this.value)\" value=\"transfer_SHREK\" checked=\"\" />\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/mcgan/input_transfer_SHREK.png\" width=\"14.5%\" />\n",
      "<input type=\"radio\" name=\"option_font4\" onclick=\"update_im_font4(this.value)\" value=\"texture_SHREK\" />\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/mcgan/input_texture_SHREK.png\" width=\"14.5%\" />\n",
      "</p>\n",
      "</form>\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/mcgan/transfer_SHREK.png\" name=\"im_font4\" id=\"im_font4\" width=\"90%\" />\n",
      "</p>\n",
      "<p><br /></p>\n",
      "\n",
      "<p>Also, here is the gradual improvement in the OrnaNet prediction:</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/mcgan/ft51_1_fake_B.gif\" width=\"100%\" /><br />\n",
      "</p>\n",
      "\n",
      "<h1 id=\"references\">References</h1>\n",
      "\n",
      "<p> <font size=\"3\"> [1] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. \"Image-to-Image Translation with Conditional Adversarial Networks.\" CVPR 2017.</font></p>\n",
      "\n",
      "<p> <font size=\"3\"> [2] Samaneh Azadi, Matthew Fisher, Vladimir Kim, Zhaowen Wang, Eli Shechtman, and Trevor Darrell. \"Multi-Content GAN for Few-Shot Font Style Transfer.\" CVPR 2018.</font> </p>\n",
      "\n",
      "<h1 id=\"more-information\">More Information</h1>\n",
      "\n",
      "<p>For more information about Multi-Content GAN, please take a look at the following links:</p>\n",
      "\n",
      "<ul>\n",
      "  <li><a href=\"https://arxiv.org/abs/1712.00516\">Arxiv preprint</a></li>\n",
      "  <li><a href=\"https://github.com/azadis/MC-GAN\">MC-GAN code</a></li>\n",
      "</ul>\n",
      "\n",
      "<p>Please let us know if you have any questions or suggestions.</p>\n",
      "\n",
      "  Link: http://bair.berkeley.edu/blog/2018/03/13/mcgan/\n",
      "  PubDate: Tue, 13 Mar 2018 09:00:00 +0000\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20180426221707/http://bair.berkeley.edu/blog/feed.xml\n",
      "- Source: https://bair.berkeley.edu/blog/feed.xml\n",
      "  Channel Title: The Berkeley Artificial Intelligence Research Blog\n",
      "  Channel Description: The BAIR Blog\n",
      "- Title: TDM: From Model-Free to Model-Based Deep Reinforcement Learning\n",
      "  Description: <p>\n",
      "You’ve decided that you want to bike from your house by UC Berkeley to the\n",
      "Golden Gate Bridge. It’s a nice 20 mile ride, but there’s a problem: you’ve\n",
      "never ridden a bike before! To make matters worse, you are new to the Bay Area,\n",
      "and all you have is a good ol’ fashion map to guide you. How do you get started?\n",
      "</p>\n",
      "\n",
      "<p>\n",
      "Let’s first figure out how to ride a bike. One strategy would be to do a lot of\n",
      "studying and planning. Read books on how to ride bicycles. Study physics and\n",
      "anatomy. Plan out all the different muscle movements that you’ll make in\n",
      "response to each perturbation. This approach is noble, but for anyone who’s ever\n",
      "learned to ride a bike, they know that this strategy is doomed to fail. There’s\n",
      "only one way to learn how to ride a bike: trial and error. Some tasks like\n",
      "riding a bike are just too complicated to plan out in your head.  </p>\n",
      "\n",
      "<p>\n",
      "Once you’ve learned how to ride your bike, how would you get to the Golden Gate\n",
      "Bridge? You could reuse your trial-and-error strategy. Take a few random turns\n",
      "and see if you end up at the Golden Gate Bridge. Unfortunately, this strategy\n",
      "would take a very, very long time. For this sort of problem, planning is a much\n",
      "faster strategy, and requires considerably less real-world experience and\n",
      "trial-and-error. In reinforcement learning terms, it is more\n",
      "<i>sample-efficient</i>.\n",
      "</p>\n",
      "\n",
      "<!--\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/tdm/riding-bike-small.png\" alt=\"Some skills you learn by trial and error.\" /><br />\n",
      "<i>\n",
      "Some skills you learn by trial and error.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/tdm/hitchhiker-small.png\" alt=\"Other times, planning ahead is better.\" /><br />\n",
      "<i>\n",
      "Other times, planning ahead is better.\n",
      "</i>\n",
      "</p>\n",
      "-->\n",
      "\n",
      "<p>\n",
      "<table class=\"col-2\">\n",
      "  <tr>\n",
      "    <td style=\"text-align:center;\">\n",
      "      <img src=\"http://bair.berkeley.edu/static/blog/tdm/riding-bike-small.png\" height=\"260\" />\n",
      "    </td>\n",
      "    <td style=\"text-align:center;\">\n",
      "      <img src=\"http://bair.berkeley.edu/static/blog/tdm/hitchhiker-small.png\" height=\"260\" />\n",
      "    </td>\n",
      "  </tr>\n",
      "</table>\n",
      "<p style=\"text-align:center;\">\n",
      "<i>\n",
      "Left: some skills you learn by trial and error. Right: other times, planning\n",
      "ahead is better.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>\n",
      "While simple, this thought experiment highlights some important aspects of human\n",
      "intelligence. For some tasks, we use a trial-and-error approach, and for others\n",
      "we use a planning approach. A similar phenomena seems to have emerged in\n",
      "reinforcement learning (RL). In the parlance of RL, empirical results show that\n",
      "some tasks are better suited for model-free (trial-and-error) approaches, and\n",
      "others are better suited for model-based (planning) approaches.  </p>\n",
      "\n",
      "<p>\n",
      "However, the biking analogy also highlights that the two systems are not\n",
      "completely independent. In particularly, to say that learning to ride a bike is\n",
      "<i>just</i> trial-and-error is an oversimplification. In fact, when learning to\n",
      "bike by trial-and-error, you’ll employ a bit of planning. Perhaps your plan will\n",
      "initially be, “Don’t fall over.” As you improve, you’ll make more ambitious\n",
      "plans, such as, “Bike forwards for two meters without falling over.” Eventually,\n",
      "your bike-riding skills will be so proficient that you can start to plan in very\n",
      "abstract terms (“Bike to the end of the road.”) to the point that all there is\n",
      "left to do is planning and you no longer need to worry about the nitty-gritty\n",
      "details of riding a bike. We see that there is a gradual transition from the\n",
      "model-free (trial-and-error) strategy to a model-based (planning) strategy. If\n",
      "we could develop artificial intelligence algorithms--and specifically RL\n",
      "algorithms--that mimic this behavior, it could result in an algorithm that both\n",
      "performs well (by using trial-and-error methods early on) and is sample\n",
      "efficient (by later switching to a planning approach to achieve more abstract\n",
      "goals).\n",
      "</p>\n",
      "\n",
      "<p>\n",
      "This post covers temporal difference model (TDM), which is a RL algorithm that\n",
      "captures this smooth transition between model-free and model-based RL. Before\n",
      "describing TDMs, we start by first describing how a typical model-based RL\n",
      "algorithm works.\n",
      "</p>\n",
      "\n",
      "<!--more-->\n",
      "\n",
      "<h2 id=\"model-based-reinforcement-learning\">Model-Based Reinforcement Learning</h2>\n",
      "\n",
      "<p>\n",
      "In reinforcement learning, we have some some state space $\\mathcal{S}$ and\n",
      "action space $\\mathcal{A}$. If at time $t$ we are in state $s_t \\in \\mathcal{S}$\n",
      "and take action $a_t\\in \\mathcal{A}$, we transition to a new state $s_{t+1} =\n",
      "f(s_t, a_t)$ according to a dynamics model $f: \\mathcal{S} \\times \\mathcal{A}\n",
      "\\mapsto \\mathcal{S}$. The goal is to maximize rewards summed over the visited\n",
      "state: $\\sum_{t=1}^{T-1} r(s_t, a_, s_{t+1})$. Model-based RL algorithms assume\n",
      "you are given (or learn) the dynamics model $f$. Given this dynamics model,\n",
      "there are a variety of model-based algorithms. For this post, we consider\n",
      "methods that perform the following optimization to choose a sequence of actions\n",
      "and states to maximize rewards: \n",
      "</p>\n",
      "\n",
      "<script type=\"math/tex; mode=display\">\n",
      "  \\qquad \\text{max}_{a_{1:T-1}, s_{1:T}} \\sum_{t=1}^{T-1} r(s_t, a_t, s_{t+1}) \\text{ subject to }f(s_t, a_t) = s_{t+1}\n",
      "</script>\n",
      "\n",
      "<p>\n",
      "  The optimization says to choose a sequence of states and actions that you maximize the rewards, while ensuring that the trajectory is feasible. Here, feasible means that each state-action-next-state transition is valid. For example, in the image below if you start in state $s_t$ and take action $a_t$, only the top $s_{t+1}$ results in a feasible transition.\n",
      "</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/tdm/bike-feasibility-compressed.png\" alt=\"Planning a trip to the Golden Gate Bridge would be much easier if you could defy physics. However, the constraint in the model-based optimization problem ensures that only trajectories like the top row will be outputted. The bottom two trajectories may have high reward, but they’re not feasible.\" width=\"80%\" /><br />\n",
      "  <i>\n",
      "Planning a trip to the Golden Gate Bridge would be much easier if you could defy\n",
      "physics. However, the constraint in the model-based optimization problem ensures\n",
      "that only trajectories like the top row will be outputted. The bottom two\n",
      "trajectories may have high reward, but they’re not feasible.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>\n",
      "In our biking problem, the optimization might result in a biking plan from\n",
      "Berkeley (top right) to the Golden Gate Bridge (middle left) that looks like\n",
      "this:\n",
      "</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/tdm/mb-bike-plan-small.png\" alt=\"An example of a plan (states and actions) outputted the optimization problem.\" width=\"80%\" /><br />\n",
      "<i>\n",
      "An example of a plan (states and actions) outputted the optimization problem.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>\n",
      "While conceptually nice, this plan is not very realistic. Model-based approaches\n",
      "use a model $f(s, a)$ that predict the state at the very next time step. In\n",
      "robotics, a time step usually corresponds to a tenth or a hundredth of a second.\n",
      "So perhaps a more realistic depiction of the resulting plan might look like:\n",
      "</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/tdm/mb-short-time-small.png\" alt=\"A more realistic plan.\" width=\"80%\" /><br />\n",
      "<i>\n",
      "A more realistic plan.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>\n",
      "If we think about how we plan in everyday life, we realize that we plan at much\n",
      "more temporally abstract terms. Rather than planning the position that our bike\n",
      "will be at the next tenth of a second, we make longer-term plan things like, “I\n",
      "will go to the end of the road.” Furthermore, we can only make these temporally\n",
      "abstract plans once we’ve learned how to ride a bike in the first place. As\n",
      "discussed earlier, we need some way to (1) start the learning using a\n",
      "trial-and-error approach and (2) provide a mechanism to gradually increase the\n",
      "level of abstraction that we use to plan. For this, we introduce temporal\n",
      "difference models.\n",
      "</p>\n",
      "\n",
      "<h2 id=\"temporal-difference-models\">Temporal Difference Models</h2>\n",
      "<p>\n",
      "  A temporal difference model (TDM)$^\\dagger$, which we will write as $Q(s, a, s_g, \\tau)$, is a function that, given a state $s \\in \\mathcal{S}$, action $a \\in \\mathcal{A}$, and goal state $s_g \\in \\mathcal{S}$, predicts how close an agent can get to the goal within $\\tau$ time steps. Intuitively, a TDM answers the question, “If I try to bike to San Francisco in 30 minutes, how close will I get?” For robotics, a natural way to measure closeness is use Euclidean distance.\n",
      "</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/tdm/tdm-visualization-small.png\" alt=\"A TDM predicts how close you will get to the goal (Golden Gate Bridge) after a fixed amount of time. After 30 minutes of biking, maybe you only reach the grey biker in the image above. In this case, the grey line represents the distance that the TDM should predict.\" width=\"80%\" /><br />\n",
      "<i>\n",
      "A TDM predicts how close you will get to the goal (Golden Gate Bridge) after a\n",
      "fixed amount of time. After 30 minutes of biking, maybe you only reach the grey\n",
      "biker in the image above. In this case, the grey line represents the distance\n",
      "that the TDM should predict.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>\n",
      "For those familiar with reinforcement learning, it turns out that a TDM can be\n",
      "viewed as a goal-conditioned Q function in a finite-horizon MDP. Because a TDM\n",
      "is just another Q function, we can train it with model-free (trial-and-error)\n",
      "algorithms. We use <a href=\"https://arxiv.org/abs/1509.02971\">deep deterministic\n",
      "policy gradient</a> (DDPG) to train a TDM and retroactively relabel the goal and\n",
      "time horizon to increase the sample efficiency of our learning algorithm. In\n",
      "theory, any Q-learning algorithm could be used to train the TDM, but we found\n",
      "this to be effective. We encourage readers to check out the paper for more\n",
      "details.\n",
      "</p>\n",
      "\n",
      "<h3 id=\"planning-with-a-tdm\">Planning with a TDM</h3>\n",
      "<p>\n",
      "Once we train a TDM, how can we use it to plan? It turns out that we can plan with the following optimization:\n",
      "</p>\n",
      "\n",
      "<script type=\"math/tex; mode=display\">\n",
      "  \\qquad \\text{max}_{a_1, a_K, a_{2K}, s_1, s_K, s_{2K}, ..} \\sum_{t=1, K, 2K, ...} r(s_t) \\text{ subject to } Q(s_t, a_t, s_{t+K}, K) = 0\n",
      "</script>\n",
      "\n",
      "<p>\n",
      "The intuition is similar to the model-based formulation. Choose a sequence of\n",
      "actions and states that maximize rewards and that are feasible. A key difference\n",
      "is that we only plan <i>every $K$ time steps</i>, rather than every time step.\n",
      "The constraint that $Q(s_t, a_t, s_{t+K}, K) = 0$ enforces the feasibility of\n",
      "the trajectory. Visually, rather than explicitly planning $K$ steps and actions\n",
      "like so \n",
      "</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/tdm/comp-mb.jpeg\" alt=\"Model based planning many steps.\" width=\"80%\" /><br />\n",
      "</p>\n",
      "\n",
      "<p>\n",
      "We can instead directly plan over $K$ time steps as shown below:\n",
      "</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/tdm/comp-tdm.jpeg\" alt=\"TDM planning one step\" width=\"80%\" /><br />\n",
      "</p>\n",
      "\n",
      "<p>\n",
      "As we increase $K$, we get temporally more and more abstract plans. In between\n",
      "the $K$ time steps, we use a model-free approach to take actions, thereby\n",
      "allowing the model-free policy “abstract away” the details of how the goal is\n",
      "actually reached. For the biking problem and for large enough values of $K$, the\n",
      "optimization could result in a plan like:\n",
      "</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/tdm/tdm-plan-small.png\" alt=\"A model-based planner can be used to choose temporally abstract goals. A model-free algorithm can be used to reach those goals.\" width=\"80%\" /><br />\n",
      "<i>\n",
      "A model-based planner can be used to choose temporally abstract goals. A model-free algorithm can be used to reach those goals.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>\n",
      "One caveat is that this formulation can only optimize the reward at every $K$\n",
      "steps. However, many tasks only care about some states, such as the final state\n",
      "(e.g. “reach the Golden Gate Bridge”) and so this still captures a variety of\n",
      "interesting tasks.\n",
      "</p>\n",
      "\n",
      "<h3 id=\"related-work\">Related Work</h3>\n",
      "<p>\n",
      "  We’re not the first to look at the connection between model-based and model-free reinforcement. <a href=\"https://users.cs.duke.edu/~parr/icml08.pdf\">Parr ‘08</a> and <a href=\"https://pdfs.semanticscholar.org/61d4/897dbf7ced83a0eb830a8de0dd64abb58ebd.pdf\">Boyan ‘99</a>, are particularly related, though they focus mainly on tabular and linear function approximators. The idea of training a goal condition Q function was also explored in <a href=\"http://www.incompleteideas.net/papers/horde-aamas-11.pdf\">Sutton ‘11</a> and <a href=\"http://proceedings.mlr.press/v37/schaul15.pdf\">Schaul ‘15</a>, in the context of robot navigation and Atari games. Lastly, the relabelling scheme that we use is inspired by the work of <a href=\"https://arxiv.org/abs/1707.01495\">Andrychowicz ‘17</a>.\n",
      "</p>\n",
      "\n",
      "\n",
      "<h2 id=\"experiments\">Experiments</h2>\n",
      "<p>\n",
      "  We tested TDMs on five simulated continuous control tasks and one real-world robotics task. One of the simulated tasks is to train a robot arm to push a cylinder to a target position. An example of the final pushing TDM policy and the associate learning curves are shown below:\n",
      "</p>\n",
      "\n",
      "<table class=\"col-2\">\n",
      "  <tr>\n",
      "    <td style=\"text-align:center;\">\n",
      "      <img src=\"http://bair.berkeley.edu/static/blog/tdm/pusher-video-small.gif\" alt=\"Pusher video\" height=\"300\" />\n",
      "    </td>\n",
      "    <td style=\"text-align:center;\">\n",
      "      <img src=\"http://bair.berkeley.edu/static/blog/tdm/pusher-learning-curve.jpg\" alt=\"Pusher learning curve\" height=\"300\" />\n",
      "    </td>\n",
      "  </tr>\n",
      "</table>\n",
      "<p style=\"text-align:center;\">\n",
      "<i>\n",
      "Left: TDM policy for reaching task. Right: Learning curves. TDM is blue (lower is better).\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>\n",
      "In the learning curve to the right, we plot the final distance to goal versus\n",
      "the number of environment samples (lower is better). Our simulation controls the\n",
      "robots at 20 Hz, meaning that 1000 steps corresponds to 50 seconds in the real\n",
      "world. The dynamics of this environment are relatively easy to learn, meaning\n",
      "that a model-based approach should excel. As expected, the model-based\n",
      "approaches (purple curve) learns quickly--roughly 3000 steps, or 25 minutes--and\n",
      "performs well. The TDM approach (blue curve) also learn quickly--roughly 2000\n",
      "steps, or 17 minutes. The model-free DDPG (without TDMs) baseline eventually\n",
      "solves the task, but requires many more training samples. One reason the TDM\n",
      "approach learns so quickly is that it effective is a model-based methods in\n",
      "disguise.\n",
      "</p>\n",
      "\n",
      "<p>\n",
      "The story looks much better for model-free approaches when we move to locomotion\n",
      "tasks, which have substantially harder dynamics. One of the locomotion tasks\n",
      "involves training a quadruped robot to move to a certain position. The resulting\n",
      "TDM policy is shown below on the left, along with the accompanying learning\n",
      "curve on the right.\n",
      "</p>\n",
      "\n",
      "<p>\n",
      "<table class=\"col-2\">\n",
      "  <tr>\n",
      "    <td style=\"text-align:center;\">\n",
      "      <img src=\"http://bair.berkeley.edu/static/blog/tdm/ant-video.gif\" alt=\"Pusher video\" height=\"300\" />\n",
      "    </td>\n",
      "    <td style=\"text-align:center;\">\n",
      "      <img src=\"http://bair.berkeley.edu/static/blog/tdm/ant-learning-curve.jpg\" alt=\"Pusher learning curve\" height=\"300\" />\n",
      "    </td>\n",
      "  </tr>\n",
      "</table>\n",
      "<p style=\"text-align:center;\">\n",
      "<i>\n",
      "Left: TDM policy for locomotion task. Right: Learning curves. TDM is blue (lower is better).\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>\n",
      "  Just as we use trial-and-error rather than planning to master riding a bicycle, we expect model-free methods to perform better than model-based methods on these locomotion tasks. This is precisely what we see in the learning curve on the right: the model-based method plateaus in performance. The model-free DDPG method learns more slowly, but eventually outperforms the model-based approach. TDM manages to both learn quickly and achieve good final performance. There are more experiments in the paper, including training a real-world 7 degree-of-freedom Sawyer to reach positions. We encourage the readers to check them out!\n",
      "</p>\n",
      "\n",
      "<h2 id=\"future-directions\">Future Directions</h2>\n",
      "<p>\n",
      "  Temporal difference models provide a formalism and practical algorithm for interpolating from model-free to model-based control. However, there’s a lot of future work to be done. For one, the derivation assumes that the environment and policies are deterministic. In practice, most environments are stochastic. Even if they were deterministic, there are compelling reasons to use a stochastic policy in practice (see <a href=\"http://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/\">this blog post</a> for one example). Extending TDMs to this setting would help move TDMs to more realistic environments. Another idea would be to combine TDMs with alternative model-based planning optimization algorithms that the ones we used in the paper.  Lastly, we’d like to apply TDMs to more challenging tasks with real-world robots, like locomotion, manipulation, and, of course, bicycling to the Golden Gate Bridge.\n",
      "</p>\n",
      "<p>\n",
      "  This work will be presented at ICLR 2018. For more information about TDMs, check out the following links and come see us at our poster presentation at ICLR in Vancouver:\n",
      "</p>\n",
      "<ul>\n",
      "<li><a href=\"https://arxiv.org/abs/1802.09081\">ArXiv Preprint</a></li>\n",
      "<li><a href=\"https://github.com/vitchyr/rlkit\">Code</a></li>\n",
      "</ul>\n",
      "\n",
      "<p>\n",
      "  Let us know if you have any questions or comments!\n",
      "</p>\n",
      "\n",
      "<p>\n",
      "  $^\\dagger$ We call it a temporal difference model because we train $Q$ with temporal difference learning and use $Q$ as a model.\n",
      "</p>\n",
      "<hr />\n",
      "\n",
      "<p>I would like to thank Sergey Levine and Shane Gu for their valuable feedback when preparing this blog post.</p>\n",
      "\n",
      "<!--\n",
      "<h2 id=\"references\">References</h2>\n",
      "<ul>\n",
      "  <li>Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob\n",
      "McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. arXiv\n",
      "preprint arXiv:1707.01495, 2017.</li>\n",
      "  <li>Justin A Boyan. Least-squares temporal difference learning. In Proceedings of the 16th International\n",
      "Conference on Machine Learning, pp. 49–56, 1999.</li>\n",
      "  <li>Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,\n",
      "David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv\n",
      "preprint arXiv:1509.02971, 2015.</li>\n",
      "  <li>Ronald Parr, Lihong Li, Gavin Taylor, Christopher Painter-Wakefield, and Michael L Littman. An\n",
      "analysis of linear models, linear value-function approximation, and feature selection for reinforcement\n",
      "learning. In International Conference on Machine learning, 2008.</li>\n",
      "  <li>Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators.\n",
      "In Proceedings of the 32nd International Conference on Machine Learning, pp. 1312–1320, 2015.</li>\n",
      "  <li>Richard S Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick M Pilarski, Adam White,\n",
      "and Doina Precup. Horde: A scalable real-time architecture for learning knowledge from unsupervised\n",
      "sensorimotor interaction. In The 10th International Conference on Autonomous Agents and\n",
      "Multiagent Systems-Volume 2, pp. 761–768. International Foundation for Autonomous Agents\n",
      "and Multiagent Systems, 2011.</li>\n",
      "</ul>\n",
      "-->\n",
      "</p></p>\n",
      "\n",
      "  Link: http://bair.berkeley.edu/blog/2018/04/26/tdm/\n",
      "  PubDate: Thu, 26 Apr 2018 09:00:00 +0000\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20180427084414/http://bair.berkeley.edu:80/blog/feed.xml\n",
      "- Source: https://bair.berkeley.edu/blog/feed.xml\n",
      "  Channel Title: The Berkeley Artificial Intelligence Research Blog\n",
      "  Channel Description: The BAIR Blog\n",
      "- Title: TDM: From Model-Free to Model-Based Deep Reinforcement Learning\n",
      "  Description: <p>\n",
      "You’ve decided that you want to bike from your house by UC Berkeley to the\n",
      "Golden Gate Bridge. It’s a nice 20 mile ride, but there’s a problem: you’ve\n",
      "never ridden a bike before! To make matters worse, you are new to the Bay Area,\n",
      "and all you have is a good ol’ fashion map to guide you. How do you get started?\n",
      "</p>\n",
      "\n",
      "<p>\n",
      "Let’s first figure out how to ride a bike. One strategy would be to do a lot of\n",
      "studying and planning. Read books on how to ride bicycles. Study physics and\n",
      "anatomy. Plan out all the different muscle movements that you’ll make in\n",
      "response to each perturbation. This approach is noble, but for anyone who’s ever\n",
      "learned to ride a bike, they know that this strategy is doomed to fail. There’s\n",
      "only one way to learn how to ride a bike: trial and error. Some tasks like\n",
      "riding a bike are just too complicated to plan out in your head.  </p>\n",
      "\n",
      "<p>\n",
      "Once you’ve learned how to ride your bike, how would you get to the Golden Gate\n",
      "Bridge? You could reuse your trial-and-error strategy. Take a few random turns\n",
      "and see if you end up at the Golden Gate Bridge. Unfortunately, this strategy\n",
      "would take a very, very long time. For this sort of problem, planning is a much\n",
      "faster strategy, and requires considerably less real-world experience and\n",
      "trial-and-error. In reinforcement learning terms, it is more\n",
      "<i>sample-efficient</i>.\n",
      "</p>\n",
      "\n",
      "<!--\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/tdm/riding-bike-small.png\" alt=\"Some skills you learn by trial and error.\" /><br />\n",
      "<i>\n",
      "Some skills you learn by trial and error.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/tdm/hitchhiker-small.png\" alt=\"Other times, planning ahead is better.\" /><br />\n",
      "<i>\n",
      "Other times, planning ahead is better.\n",
      "</i>\n",
      "</p>\n",
      "-->\n",
      "\n",
      "<p>\n",
      "<table class=\"col-2\">\n",
      "  <tr>\n",
      "    <td style=\"text-align:center;\">\n",
      "      <img src=\"http://bair.berkeley.edu/static/blog/tdm/riding-bike-small.png\" height=\"260\" />\n",
      "    </td>\n",
      "    <td style=\"text-align:center;\">\n",
      "      <img src=\"http://bair.berkeley.edu/static/blog/tdm/hitchhiker-small.png\" height=\"260\" />\n",
      "    </td>\n",
      "  </tr>\n",
      "</table>\n",
      "<p style=\"text-align:center;\">\n",
      "<i>\n",
      "Left: some skills you learn by trial and error. Right: other times, planning\n",
      "ahead is better.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>\n",
      "While simple, this thought experiment highlights some important aspects of human\n",
      "intelligence. For some tasks, we use a trial-and-error approach, and for others\n",
      "we use a planning approach. A similar phenomena seems to have emerged in\n",
      "reinforcement learning (RL). In the parlance of RL, empirical results show that\n",
      "some tasks are better suited for model-free (trial-and-error) approaches, and\n",
      "others are better suited for model-based (planning) approaches.  </p>\n",
      "\n",
      "<p>\n",
      "However, the biking analogy also highlights that the two systems are not\n",
      "completely independent. In particularly, to say that learning to ride a bike is\n",
      "<i>just</i> trial-and-error is an oversimplification. In fact, when learning to\n",
      "bike by trial-and-error, you’ll employ a bit of planning. Perhaps your plan will\n",
      "initially be, “Don’t fall over.” As you improve, you’ll make more ambitious\n",
      "plans, such as, “Bike forwards for two meters without falling over.” Eventually,\n",
      "your bike-riding skills will be so proficient that you can start to plan in very\n",
      "abstract terms (“Bike to the end of the road.”) to the point that all there is\n",
      "left to do is planning and you no longer need to worry about the nitty-gritty\n",
      "details of riding a bike. We see that there is a gradual transition from the\n",
      "model-free (trial-and-error) strategy to a model-based (planning) strategy. If\n",
      "we could develop artificial intelligence algorithms--and specifically RL\n",
      "algorithms--that mimic this behavior, it could result in an algorithm that both\n",
      "performs well (by using trial-and-error methods early on) and is sample\n",
      "efficient (by later switching to a planning approach to achieve more abstract\n",
      "goals).\n",
      "</p>\n",
      "\n",
      "<p>\n",
      "This post covers temporal difference model (TDM), which is a RL algorithm that\n",
      "captures this smooth transition between model-free and model-based RL. Before\n",
      "describing TDMs, we start by first describing how a typical model-based RL\n",
      "algorithm works.\n",
      "</p>\n",
      "\n",
      "<!--more-->\n",
      "\n",
      "<h2 id=\"model-based-reinforcement-learning\">Model-Based Reinforcement Learning</h2>\n",
      "\n",
      "<p>\n",
      "In reinforcement learning, we have some some state space $\\mathcal{S}$ and\n",
      "action space $\\mathcal{A}$. If at time $t$ we are in state $s_t \\in \\mathcal{S}$\n",
      "and take action $a_t\\in \\mathcal{A}$, we transition to a new state $s_{t+1} =\n",
      "f(s_t, a_t)$ according to a dynamics model $f: \\mathcal{S} \\times \\mathcal{A}\n",
      "\\mapsto \\mathcal{S}$. The goal is to maximize rewards summed over the visited\n",
      "state: $\\sum_{t=1}^{T-1} r(s_t, a_, s_{t+1})$. Model-based RL algorithms assume\n",
      "you are given (or learn) the dynamics model $f$. Given this dynamics model,\n",
      "there are a variety of model-based algorithms. For this post, we consider\n",
      "methods that perform the following optimization to choose a sequence of actions\n",
      "and states to maximize rewards: \n",
      "</p>\n",
      "\n",
      "<script type=\"math/tex; mode=display\">\n",
      "  \\qquad \\text{max}_{a_{1:T-1}, s_{1:T}} \\sum_{t=1}^{T-1} r(s_t, a_t, s_{t+1}) \\text{ subject to }f(s_t, a_t) = s_{t+1}\n",
      "</script>\n",
      "\n",
      "<p>\n",
      "  The optimization says to choose a sequence of states and actions that you maximize the rewards, while ensuring that the trajectory is feasible. Here, feasible means that each state-action-next-state transition is valid. For example, in the image below if you start in state $s_t$ and take action $a_t$, only the top $s_{t+1}$ results in a feasible transition.\n",
      "</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/tdm/bike-feasibility-compressed.png\" alt=\"Planning a trip to the Golden Gate Bridge would be much easier if you could defy physics. However, the constraint in the model-based optimization problem ensures that only trajectories like the top row will be outputted. The bottom two trajectories may have high reward, but they’re not feasible.\" width=\"80%\" /><br />\n",
      "  <i>\n",
      "Planning a trip to the Golden Gate Bridge would be much easier if you could defy\n",
      "physics. However, the constraint in the model-based optimization problem ensures\n",
      "that only trajectories like the top row will be outputted. The bottom two\n",
      "trajectories may have high reward, but they’re not feasible.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>\n",
      "In our biking problem, the optimization might result in a biking plan from\n",
      "Berkeley (top right) to the Golden Gate Bridge (middle left) that looks like\n",
      "this:\n",
      "</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/tdm/mb-bike-plan-small.png\" alt=\"An example of a plan (states and actions) outputted the optimization problem.\" width=\"80%\" /><br />\n",
      "<i>\n",
      "An example of a plan (states and actions) outputted the optimization problem.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>\n",
      "While conceptually nice, this plan is not very realistic. Model-based approaches\n",
      "use a model $f(s, a)$ that predict the state at the very next time step. In\n",
      "robotics, a time step usually corresponds to a tenth or a hundredth of a second.\n",
      "So perhaps a more realistic depiction of the resulting plan might look like:\n",
      "</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/tdm/mb-short-time-small.png\" alt=\"A more realistic plan.\" width=\"80%\" /><br />\n",
      "<i>\n",
      "A more realistic plan.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>\n",
      "If we think about how we plan in everyday life, we realize that we plan at much\n",
      "more temporally abstract terms. Rather than planning the position that our bike\n",
      "will be at the next tenth of a second, we make longer-term plan things like, “I\n",
      "will go to the end of the road.” Furthermore, we can only make these temporally\n",
      "abstract plans once we’ve learned how to ride a bike in the first place. As\n",
      "discussed earlier, we need some way to (1) start the learning using a\n",
      "trial-and-error approach and (2) provide a mechanism to gradually increase the\n",
      "level of abstraction that we use to plan. For this, we introduce temporal\n",
      "difference models.\n",
      "</p>\n",
      "\n",
      "<h2 id=\"temporal-difference-models\">Temporal Difference Models</h2>\n",
      "<p>\n",
      "  A temporal difference model (TDM)$^\\dagger$, which we will write as $Q(s, a, s_g, \\tau)$, is a function that, given a state $s \\in \\mathcal{S}$, action $a \\in \\mathcal{A}$, and goal state $s_g \\in \\mathcal{S}$, predicts how close an agent can get to the goal within $\\tau$ time steps. Intuitively, a TDM answers the question, “If I try to bike to San Francisco in 30 minutes, how close will I get?” For robotics, a natural way to measure closeness is use Euclidean distance.\n",
      "</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/tdm/tdm-visualization-small.png\" alt=\"A TDM predicts how close you will get to the goal (Golden Gate Bridge) after a fixed amount of time. After 30 minutes of biking, maybe you only reach the grey biker in the image above. In this case, the grey line represents the distance that the TDM should predict.\" width=\"80%\" /><br />\n",
      "<i>\n",
      "A TDM predicts how close you will get to the goal (Golden Gate Bridge) after a\n",
      "fixed amount of time. After 30 minutes of biking, maybe you only reach the grey\n",
      "biker in the image above. In this case, the grey line represents the distance\n",
      "that the TDM should predict.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>\n",
      "For those familiar with reinforcement learning, it turns out that a TDM can be\n",
      "viewed as a goal-conditioned Q function in a finite-horizon MDP. Because a TDM\n",
      "is just another Q function, we can train it with model-free (trial-and-error)\n",
      "algorithms. We use <a href=\"https://arxiv.org/abs/1509.02971\">deep deterministic\n",
      "policy gradient</a> (DDPG) to train a TDM and retroactively relabel the goal and\n",
      "time horizon to increase the sample efficiency of our learning algorithm. In\n",
      "theory, any Q-learning algorithm could be used to train the TDM, but we found\n",
      "this to be effective. We encourage readers to check out the paper for more\n",
      "details.\n",
      "</p>\n",
      "\n",
      "<h3 id=\"planning-with-a-tdm\">Planning with a TDM</h3>\n",
      "<p>\n",
      "Once we train a TDM, how can we use it to plan? It turns out that we can plan with the following optimization:\n",
      "</p>\n",
      "\n",
      "<script type=\"math/tex; mode=display\">\n",
      "  \\qquad \\text{max}_{a_1, a_K, a_{2K}, s_1, s_K, s_{2K}, ..} \\sum_{t=1, K, 2K, ...} r(s_t) \\text{ subject to } Q(s_t, a_t, s_{t+K}, K) = 0\n",
      "</script>\n",
      "\n",
      "<p>\n",
      "The intuition is similar to the model-based formulation. Choose a sequence of\n",
      "actions and states that maximize rewards and that are feasible. A key difference\n",
      "is that we only plan <i>every $K$ time steps</i>, rather than every time step.\n",
      "The constraint that $Q(s_t, a_t, s_{t+K}, K) = 0$ enforces the feasibility of\n",
      "the trajectory. Visually, rather than explicitly planning $K$ steps and actions\n",
      "like so \n",
      "</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/tdm/comp-mb.jpeg\" alt=\"Model based planning many steps.\" width=\"80%\" /><br />\n",
      "</p>\n",
      "\n",
      "<p>\n",
      "We can instead directly plan over $K$ time steps as shown below:\n",
      "</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/tdm/comp-tdm.jpeg\" alt=\"TDM planning one step\" width=\"80%\" /><br />\n",
      "</p>\n",
      "\n",
      "<p>\n",
      "As we increase $K$, we get temporally more and more abstract plans. In between\n",
      "the $K$ time steps, we use a model-free approach to take actions, thereby\n",
      "allowing the model-free policy “abstract away” the details of how the goal is\n",
      "actually reached. For the biking problem and for large enough values of $K$, the\n",
      "optimization could result in a plan like:\n",
      "</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img src=\"http://bair.berkeley.edu/static/blog/tdm/tdm-plan-small.png\" alt=\"A model-based planner can be used to choose temporally abstract goals. A model-free algorithm can be used to reach those goals.\" width=\"80%\" /><br />\n",
      "<i>\n",
      "A model-based planner can be used to choose temporally abstract goals. A model-free algorithm can be used to reach those goals.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>\n",
      "One caveat is that this formulation can only optimize the reward at every $K$\n",
      "steps. However, many tasks only care about some states, such as the final state\n",
      "(e.g. “reach the Golden Gate Bridge”) and so this still captures a variety of\n",
      "interesting tasks.\n",
      "</p>\n",
      "\n",
      "<h3 id=\"related-work\">Related Work</h3>\n",
      "<p>\n",
      "  We’re not the first to look at the connection between model-based and model-free reinforcement. <a href=\"https://users.cs.duke.edu/~parr/icml08.pdf\">Parr ‘08</a> and <a href=\"https://pdfs.semanticscholar.org/61d4/897dbf7ced83a0eb830a8de0dd64abb58ebd.pdf\">Boyan ‘99</a>, are particularly related, though they focus mainly on tabular and linear function approximators. The idea of training a goal condition Q function was also explored in <a href=\"http://www.incompleteideas.net/papers/horde-aamas-11.pdf\">Sutton ‘11</a> and <a href=\"http://proceedings.mlr.press/v37/schaul15.pdf\">Schaul ‘15</a>, in the context of robot navigation and Atari games. Lastly, the relabelling scheme that we use is inspired by the work of <a href=\"https://arxiv.org/abs/1707.01495\">Andrychowicz ‘17</a>.\n",
      "</p>\n",
      "\n",
      "\n",
      "<h2 id=\"experiments\">Experiments</h2>\n",
      "<p>\n",
      "  We tested TDMs on five simulated continuous control tasks and one real-world robotics task. One of the simulated tasks is to train a robot arm to push a cylinder to a target position. An example of the final pushing TDM policy and the associate learning curves are shown below:\n",
      "</p>\n",
      "\n",
      "<table class=\"col-2\">\n",
      "  <tr>\n",
      "    <td style=\"text-align:center;\">\n",
      "      <img src=\"http://bair.berkeley.edu/static/blog/tdm/pusher-video-small.gif\" alt=\"Pusher video\" height=\"300\" />\n",
      "    </td>\n",
      "    <td style=\"text-align:center;\">\n",
      "      <img src=\"http://bair.berkeley.edu/static/blog/tdm/pusher-learning-curve.jpg\" alt=\"Pusher learning curve\" height=\"300\" />\n",
      "    </td>\n",
      "  </tr>\n",
      "</table>\n",
      "<p style=\"text-align:center;\">\n",
      "<i>\n",
      "Left: TDM policy for reaching task. Right: Learning curves. TDM is blue (lower is better).\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>\n",
      "In the learning curve to the right, we plot the final distance to goal versus\n",
      "the number of environment samples (lower is better). Our simulation controls the\n",
      "robots at 20 Hz, meaning that 1000 steps corresponds to 50 seconds in the real\n",
      "world. The dynamics of this environment are relatively easy to learn, meaning\n",
      "that a model-based approach should excel. As expected, the model-based\n",
      "approaches (purple curve) learns quickly--roughly 3000 steps, or 25 minutes--and\n",
      "performs well. The TDM approach (blue curve) also learn quickly--roughly 2000\n",
      "steps, or 17 minutes. The model-free DDPG (without TDMs) baseline eventually\n",
      "solves the task, but requires many more training samples. One reason the TDM\n",
      "approach learns so quickly is that it effective is a model-based methods in\n",
      "disguise.\n",
      "</p>\n",
      "\n",
      "<p>\n",
      "The story looks much better for model-free approaches when we move to locomotion\n",
      "tasks, which have substantially harder dynamics. One of the locomotion tasks\n",
      "involves training a quadruped robot to move to a certain position. The resulting\n",
      "TDM policy is shown below on the left, along with the accompanying learning\n",
      "curve on the right.\n",
      "</p>\n",
      "\n",
      "<p>\n",
      "<table class=\"col-2\">\n",
      "  <tr>\n",
      "    <td style=\"text-align:center;\">\n",
      "      <img src=\"http://bair.berkeley.edu/static/blog/tdm/ant-video.gif\" alt=\"Pusher video\" height=\"300\" />\n",
      "    </td>\n",
      "    <td style=\"text-align:center;\">\n",
      "      <img src=\"http://bair.berkeley.edu/static/blog/tdm/ant-learning-curve.jpg\" alt=\"Pusher learning curve\" height=\"300\" />\n",
      "    </td>\n",
      "  </tr>\n",
      "</table>\n",
      "<p style=\"text-align:center;\">\n",
      "<i>\n",
      "Left: TDM policy for locomotion task. Right: Learning curves. TDM is blue (lower is better).\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>\n",
      "  Just as we use trial-and-error rather than planning to master riding a bicycle, we expect model-free methods to perform better than model-based methods on these locomotion tasks. This is precisely what we see in the learning curve on the right: the model-based method plateaus in performance. The model-free DDPG method learns more slowly, but eventually outperforms the model-based approach. TDM manages to both learn quickly and achieve good final performance. There are more experiments in the paper, including training a real-world 7 degree-of-freedom Sawyer to reach positions. We encourage the readers to check them out!\n",
      "</p>\n",
      "\n",
      "<h2 id=\"future-directions\">Future Directions</h2>\n",
      "<p>\n",
      "  Temporal difference models provide a formalism and practical algorithm for interpolating from model-free to model-based control. However, there’s a lot of future work to be done. For one, the derivation assumes that the environment and policies are deterministic. In practice, most environments are stochastic. Even if they were deterministic, there are compelling reasons to use a stochastic policy in practice (see <a href=\"http://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/\">this blog post</a> for one example). Extending TDMs to this setting would help move TDMs to more realistic environments. Another idea would be to combine TDMs with alternative model-based planning optimization algorithms that the ones we used in the paper.  Lastly, we’d like to apply TDMs to more challenging tasks with real-world robots, like locomotion, manipulation, and, of course, bicycling to the Golden Gate Bridge.\n",
      "</p>\n",
      "<p>\n",
      "  This work will be presented at ICLR 2018. For more information about TDMs, check out the following links and come see us at our poster presentation at ICLR in Vancouver:\n",
      "</p>\n",
      "<ul>\n",
      "<li><a href=\"https://arxiv.org/abs/1802.09081\">ArXiv Preprint</a></li>\n",
      "<li><a href=\"https://github.com/vitchyr/rlkit\">Code</a></li>\n",
      "</ul>\n",
      "\n",
      "<p>\n",
      "  Let us know if you have any questions or comments!\n",
      "</p>\n",
      "\n",
      "<p>\n",
      "  $^\\dagger$ We call it a temporal difference model because we train $Q$ with temporal difference learning and use $Q$ as a model.\n",
      "</p>\n",
      "<hr />\n",
      "\n",
      "<p>I would like to thank Sergey Levine and Shane Gu for their valuable feedback when preparing this blog post.</p>\n",
      "\n",
      "<!--\n",
      "<h2 id=\"references\">References</h2>\n",
      "<ul>\n",
      "  <li>Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob\n",
      "McGrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. arXiv\n",
      "preprint arXiv:1707.01495, 2017.</li>\n",
      "  <li>Justin A Boyan. Least-squares temporal difference learning. In Proceedings of the 16th International\n",
      "Conference on Machine Learning, pp. 49–56, 1999.</li>\n",
      "  <li>Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,\n",
      "David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv\n",
      "preprint arXiv:1509.02971, 2015.</li>\n",
      "  <li>Ronald Parr, Lihong Li, Gavin Taylor, Christopher Painter-Wakefield, and Michael L Littman. An\n",
      "analysis of linear models, linear value-function approximation, and feature selection for reinforcement\n",
      "learning. In International Conference on Machine learning, 2008.</li>\n",
      "  <li>Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators.\n",
      "In Proceedings of the 32nd International Conference on Machine Learning, pp. 1312–1320, 2015.</li>\n",
      "  <li>Richard S Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick M Pilarski, Adam White,\n",
      "and Doina Precup. Horde: A scalable real-time architecture for learning knowledge from unsupervised\n",
      "sensorimotor interaction. In The 10th International Conference on Autonomous Agents and\n",
      "Multiagent Systems-Volume 2, pp. 761–768. International Foundation for Autonomous Agents\n",
      "and Multiagent Systems, 2011.</li>\n",
      "</ul>\n",
      "-->\n",
      "</p></p>\n",
      "\n",
      "  Link: http://bair.berkeley.edu/blog/2018/04/26/tdm/\n",
      "  PubDate: Thu, 26 Apr 2018 09:00:00 +0000\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20180528201355/http://bair.berkeley.edu:80/blog/feed.xml\n",
      "- Source: https://bair.berkeley.edu/blog/feed.xml\n",
      "  Channel Title: The Berkeley Artificial Intelligence Research Blog\n",
      "  Channel Description: The BAIR Blog\n",
      "- Title: Delayed Impact of Fair Machine Learning\n",
      "  Description: <style type=\"text/css\">\n",
      "  td {\n",
      "    font-size: 10pt;\n",
      "    font-family: 'Roboto', sans-serif;\n",
      "    border: none !important;\n",
      "    padding: 0 !important;\n",
      "  }\n",
      "  ul {\n",
      "    line-height: 180%;\n",
      "    font-family: 'Roboto', sans-serif;\n",
      "  }\n",
      "  .thin {\n",
      "    width: 170px;\n",
      "  }\n",
      "  .annotation {\n",
      "    color: #a00;\n",
      "    font-size: 10pt;\n",
      "    visibility: hidden;\n",
      "    stroke: #d00;\n",
      "    stroke-width: 5;\n",
      "    fill:none;\n",
      "    font-family: 'Roboto', sans-serif;\n",
      "  }\n",
      "  .demo {\n",
      "    font: 10pt;\n",
      "    color: #fff;\n",
      "    padding: 6px;\n",
      "    border: 0;\n",
      "    border-radius: 4px;\n",
      "    box-shadow: none;\n",
      "    margin-bottom: 6px;\n",
      "    width: 100%;\n",
      "    background: #555;\n",
      "    opacity: .5;\n",
      "    font-family: 'Roboto', sans-serif;\n",
      "  }\n",
      "  .broken {\n",
      "    color: #f00;\n",
      "  }\n",
      "  .readout {\n",
      "    font-weight: 700;\n",
      "  }\n",
      "  .title {\n",
      "    font-weight: 700;\n",
      "    font-family: 'Roboto', sans-serif;\n",
      "  }\n",
      "  .big-label {\n",
      "    font-size: 16pt;\n",
      "    font-family: 'Roboto', sans-serif;\n",
      "  }\n",
      "  .medium-label {\n",
      "    font-size: 12pt;\n",
      "    font-family: 'Roboto', sans-serif;\n",
      "  }\n",
      "  .figure-title {\n",
      "    font-size: 24px;\n",
      "    font-weight: 400;\n",
      "    font-family: 'Roboto', sans-serif;\n",
      "  }\n",
      "  .figure-caption {\n",
      "    font-weight:100;\n",
      "    font-size: 12pt;\n",
      "    margin-bottom: 10px;\n",
      "    font-family: 'Roboto', sans-serif;\n",
      "  }\n",
      "  .histogram-axis text {\n",
      "    font: 9pt 'Roboto', sans-serif;\n",
      "    font-weight: 100;\n",
      "    color: #000;\n",
      "  }\n",
      "  .histogram-legend {\n",
      "    margin-top: 16px;\n",
      "    font-family: 'Roboto', sans-serif;\n",
      "  }\n",
      "  .instructions {\n",
      "    font-weight: 700;\n",
      "    font-family: 'Roboto', sans-serif;\n",
      "  }\n",
      "  .correctness-label {\n",
      "    font-size: 9pt;\n",
      "    font-weight: 700;\n",
      "    color: #000;\n",
      "    font-family: 'Roboto', sans-serif;\n",
      "  }\n",
      "  .explanation {\n",
      "    font-size: 9pt;\n",
      "    font-weight: 100;\n",
      "    color: #ccc;\n",
      "    font-family: 'Roboto', sans-serif;\n",
      "  }\n",
      "  .pie-label {\n",
      "    font-size: 9pt;\n",
      "    font-weight: 700;\n",
      "    color: #000;\n",
      "    font-family: 'Roboto', sans-serif;\n",
      "  }\n",
      "  .pie-label1 {\n",
      "    font-size: 12pt;\n",
      "    font-weight: 700;\n",
      "    color: #000;\n",
      "    font-family: 'Roboto', sans-serif;\n",
      "  }\n",
      "  .pie-number {\n",
      "    font-size: 9pt;\n",
      "    font-weight: 300;\n",
      "    color: #000;\n",
      "    font-family: 'Roboto', sans-serif;\n",
      "  }\n",
      "  .line {\n",
      "    fill: none;\n",
      "    stroke: darkgrey;\n",
      "    stroke-width: 2px;\n",
      "  }\n",
      "  .line_maxprof {\n",
      "    fill: none;\n",
      "    stroke: orange;\n",
      "    stroke-width: 2px;\n",
      "  }\n",
      "  .line_dempar {\n",
      "    fill: none;\n",
      "    stroke: teal;\n",
      "    stroke-width: 2px;\n",
      "  }\n",
      "  .line_eqop {\n",
      "    fill: none;\n",
      "    stroke: magenta;\n",
      "    stroke-width: 2px;\n",
      "  }\n",
      "  .tick line{\n",
      "    stroke: lightgrey;\n",
      "    stroke-opacity: 0.7;\n",
      "    shape-rendering: crispEdges;\n",
      "  }\n",
      "  .legend-label {\n",
      "    font-size: 8pt;\n",
      "    font-weight: 300;\n",
      "    color: #666;\n",
      "    font-family: 'Roboto', sans-serif;\n",
      "  }\n",
      "  .bold-label {\n",
      "    font-size: 10pt;\n",
      "    font-weight: 700;\n",
      "    font-family: 'Roboto', sans-serif;\n",
      "  }\n",
      "  .margin-text {\n",
      "    font-size: 9pt;\n",
      "    font-weight: 300;\n",
      "    color: #666;\n",
      "    font-family: 'Roboto', sans-serif;\n",
      "  }\n",
      "  .margin-bold {\n",
      "    font-size: 9pt;\n",
      "    font-weight: 700;\n",
      "    font-family: 'Roboto', sans-serif;\n",
      "  }\n",
      "  .domain {\n",
      "    display: none;\n",
      "  }\n",
      "  .profit-readout {\n",
      "    margin-left: 10px;\n",
      "    font-family: 'Roboto', sans-serif;\n",
      "  }\n",
      "  #profit-title {\n",
      "    font-size: 18pt;\n",
      "    font-family: 'Roboto', sans-serif;\n",
      "  }\n",
      "  #total-profit {\n",
      "    font-size: 18pt;\n",
      "    font-weight: 700;\n",
      "    font-family: 'Roboto', sans-serif;\n",
      "  }\n",
      "  #top-sidebar {\n",
      "    font-size: 10pt;\n",
      "    color: #555;\n",
      "    font-family: 'Roboto', sans-serif;\n",
      "  }\n",
      "  #single-histogram-table {\n",
      "    font-family: 'Roboto', sans-serif;\n",
      "  }\n",
      "</style>\n",
      "\n",
      "<p>Machine learning systems trained to minimize prediction error may often exhibit\n",
      "discriminatory behavior based on sensitive characteristics such as race and\n",
      "gender. One reason could be due to historical bias in the data. In various\n",
      "application domains including lending, hiring, criminal justice, and\n",
      "advertising, machine learning has been criticized for its potential to <em>harm</em>\n",
      "historically underrepresented or disadvantaged groups.</p>\n",
      "\n",
      "<p>In this post, we talk about our recent work on aligning decisions made by\n",
      "machine learning with long term social welfare goals. Commonly, machine learning\n",
      "models produce a <strong>score</strong> that summarizes information about an individual in\n",
      "order to make decisions about them. For example, a <em>credit score</em> summarizes an\n",
      "individual’s credit history and financial activities in a way that informs the\n",
      "bank about their creditworthiness. Let us continue to use the lending setting as\n",
      "a running example.</p>\n",
      "\n",
      "<!--more-->\n",
      "\n",
      "<p>Any group of individuals has a particular distribution of credit scores, as\n",
      "visualized below.</p>\n",
      "\n",
      "<div id=\"single-histogram-table\">\n",
      "</div>\n",
      "\n",
      "<p>Scores can be turned into decisions by defining a threshold. For example,\n",
      "individuals above the threshold score are granted loans, and individuals below\n",
      "the score threshold are denied loans. Such a decision rule is called a\n",
      "<em>threshold policy</em>.</p>\n",
      "\n",
      "<p>Scores can be interpreted as encoding estimated probabilities of defaulting on a\n",
      "loan. For example, 90% of people with a credit score of 650 may be expected to\n",
      "repay a loan granted to them. This allows the bank to forecast the profit they\n",
      "would expect to make by providing identical loans to individuals with credit\n",
      "score 650. In the same fashion, the bank can predict the profit they would\n",
      "expect by loaning to all individuals with a credit above 650, or indeed above\n",
      "any given threshold.</p>\n",
      "\n",
      "<div id=\"single-histogram-interactive-table\">\n",
      "</div>\n",
      "\n",
      "<p>Without other considerations, a bank will attempt to maximize its total profit.\n",
      "The profit depends on the ratio of the amount the bank gains from a repaid loan\n",
      "to the amount the bank loses from a defaulted loan. In the above interactive\n",
      "figure, this ratio of gain to loss is 1 to -4. As losses become more costly\n",
      "relative to gains, the bank will issue loans more conservatively, and raise its\n",
      "loaning threshold. We call the fraction of the population above this threshold\n",
      "the <em>selection rate</em>.</p>\n",
      "\n",
      "<h2 id=\"the-outcome-curve\">The outcome curve</h2>\n",
      "\n",
      "<p>Lending decisions not only affect the institution, but the individuals as well.\n",
      "A default event (a borrower’s failure to repay a loan) not only diminishes\n",
      "profit for the bank; it also worsens the credit score of the borrower. A\n",
      "successful lending outcome leads to profit for the bank and also to an increase\n",
      "in credit score for the borrower. In our running example, the ratio of the\n",
      "change in credit scores for a borrower is 1 (repaid) to -2 (defaulted).</p>\n",
      "\n",
      "<p>For threshold policies, the outcome, defined as expected change in score for a\n",
      "population, can be parametrized as a function of the selection rate; we call\n",
      "this function the <strong>outcome curve</strong>. As the selection rate in one group varies, the\n",
      "outcome experienced by the group also varies. These population-level outcomes\n",
      "depend both on the probability of repayment (as encoded by the score), and cost\n",
      "and benefit of the loaning decision to the individual.</p>\n",
      "\n",
      "<center>\n",
      "  <img src=\"http://bair.berkeley.edu/static/blog/delayed_impact/outcome_curve_blog.png\" alt=\"drawing\" style=\"width: 500px;\" />\n",
      "  <p style=\"text-align:center;\">\n",
      "  <i>\n",
      "  Figure 3\n",
      "  </i>\n",
      "  </p>\n",
      "</center>\n",
      "\n",
      "<p>The above figure shows the outcome curve for a typical population. When enough\n",
      "individuals in a group are granted loans and successfully repay them, the\n",
      "average credit score in that group is likely to increase. Unconstrained\n",
      "profit-maximization results in a positive average score change in the population\n",
      "in this case. As we deviate from profit maximization to give out loans to more\n",
      "people, the average score change increases up to a certain point where it is\n",
      "maximized. We may call this the <em>altruistic optimum</em>. We can also increase the\n",
      "selection rate up to a point where the average score change is lower than under\n",
      "unconstrained profit-maximization but still positive, as illustrated by the\n",
      "dotted yellow regions. We say that selection rates in the dotted yellow regions\n",
      "are causing <em>relative harm</em>. However, if too many individuals are unable to\n",
      "repay their loans, the average credit score for the group will decrease, as is\n",
      "the case in the red striped region.</p>\n",
      "\n",
      "<div id=\"single-curves-table\">\n",
      "</div>\n",
      "\n",
      "<h2 id=\"multiple-groups\">Multiple groups</h2>\n",
      "\n",
      "<p>How does a given threshold policy affect individuals across different groups?\n",
      "Two groups with different distributions of credit scores will experience\n",
      "different outcomes.</p>\n",
      "\n",
      "<p>Suppose the second group has a distribution of credit scores that is different\n",
      "from the first group, and also has fewer people. We can think of this group as a\n",
      "historically disadvantaged minority. Let’s denote this group as the <em>blue group</em>;\n",
      "we would like to ensure that the bank’s lending policy doesn’t hurt or\n",
      "shortchange them disproportionately.</p>\n",
      "\n",
      "<p>We imagine the bank can choose different thresholds for each group. While\n",
      "group-dependent thresholds may face legal challenges, they are inevitable in\n",
      "avoiding the differential outcomes that a fixed-threshold decision could invoke.</p>\n",
      "\n",
      "<div id=\"comparison-histogram-table\">\n",
      "</div>\n",
      "\n",
      "<p>It makes sense to ask what choices of thresholds lead to an expected improvement\n",
      "in the score distribution within the blue group. As we mentioned before, an\n",
      "unconstrained bank policy would maximize profit, choosing thresholds that meet a\n",
      "break-even point above which it is profitable to give out loans. In fact, the\n",
      "profit-maximizing threshold (a credit score of 580) is the same for\n",
      "both groups.</p>\n",
      "\n",
      "<h2 id=\"fairness-criteria\">Fairness Criteria</h2>\n",
      "\n",
      "<p>Groups with different distributions over scores will have differently shaped\n",
      "outcome curves (see the top half of Figure 6, which shows outcome curves\n",
      "resulting from actual credit score data and a simple outcome model).  As an\n",
      "alternative to unconstrained profit maximization, one might consider <em>fairness\n",
      "constraints</em>, which equalize decisions between groups with respect to some\n",
      "objective function. A variety of fairness criteria have been proposed to protect\n",
      "the disadvantaged group by an appeal to intuition. With a model of outcomes, we\n",
      "are now equipped to answer concretely whether fairness constraints actually\n",
      "encourage more positive outcomes.</p>\n",
      "\n",
      "<p>One frequently proposed fairness criterion, <em>demographic parity</em>, requires the\n",
      "bank to lend to both groups at an equal rate. Subject to this requirement, the\n",
      "bank would continue to maximize profit to the extent possible.  Another\n",
      "criterion, <em>equality of opportunity</em>, equalizes the <em>true positive rates</em>\n",
      "between the two groups, requiring the bank to lend in both groups at an equal\n",
      "rate among individuals who will repay their loan.</p>\n",
      "\n",
      "<p>While these fairness criteria are a natural way to think about equalizing static\n",
      "decisions, they often ignore the future effects these policies have on\n",
      "population outcomes. Figure 6 illustrates this point by contrasting the policies\n",
      "resulting from max profit, demographic parity and equal opportunity. Try\n",
      "selecting each loan strategy to see the bank profit and credit score change they\n",
      "result in! Both demographic parity and equal opportunity reduce the bank’s\n",
      "profit compared to max profit. But do they improve the outcome of the blue\n",
      "population over max profit? While the max profit strategy underloans to the blue\n",
      "population relative to the altruistic optimum, equal opportunity overloans\n",
      "relative to the altruistic optimum, and demographic parity overloans to the\n",
      "point of causing relative harm in the blue population.  Can you find the\n",
      "thresholds that lead to the altruistic optimum? Try it out in the interactive\n",
      "visualization!</p>\n",
      "\n",
      "<div id=\"comparison-curves-table\">\n",
      "</div>\n",
      "\n",
      "<p>If the goal of employing fairness criteria is to increase or equalize long term\n",
      "well-being over all populations, we’ve just shown that there are scenarios where\n",
      "fairness criteria actually act against this objective. In other words, fairness\n",
      "constraints can also reduce welfare in already disadvantaged populations.\n",
      "Constructing an accurate model to forecast the effects that\n",
      "decisions have on population outcomes may help to mitigate possible\n",
      "unanticipated harms of applying fairness constraints.</p>\n",
      "\n",
      "<h2 id=\"considering-outcomes-for-fair-machine-learning\">Considering outcomes for “fair” machine learning</h2>\n",
      "\n",
      "<p>We advocate for a view toward long-term outcomes in the discussion of “fair’’\n",
      "machine learning. Without a careful model of delayed outcomes, one cannot\n",
      "foresee the impact a fairness criterion would have if enforced as a constraint\n",
      "on a classification system.  However, if an accurate outcome model is available,\n",
      "there are more direct ways to optimize for positive outcomes than via existing\n",
      "fairness criteria. Specifically, the outcome curve gives us a way to deviate\n",
      "from the maximum profit strategy in a way that most directly improves outcomes.</p>\n",
      "\n",
      "<p>An outcome model is a concrete way to incorporate domain knowledge into the\n",
      "classification process. This aligns well with much scholarship that points to\n",
      "the context-sensitive nature of fairness in machine learning. The outcome curve\n",
      "provides an interpretable visual device to highlight application-specific\n",
      "tradeoffs.</p>\n",
      "\n",
      "<p>For more details, please check out <a href=\"https://arxiv.org/abs/1803.04383\">the full version of our paper</a>, which will \n",
      "also appear at the 35th International Conference on Machine learning in Stockholm, Sweden. Our work\n",
      "is only a start in exploring how outcome models can mitigate undesirable societal\n",
      "impacts of machine learning algorithms. We believe there is much more work to be\n",
      "done in order to ensure the long-term fairness of machine learning, as\n",
      "algorithms impact the lives of more people.</p>\n",
      "\n",
      "<p><em>Acknowledgements</em>. The authors would like to thank Martin Wattenberg and\n",
      "Fernanda Viégas for the <a href=\"https://research.google.com/bigpicture/attacking-discrimination-in-ml/\">interactive visualizations</a> that inspired us and were used as reference\n",
      "for those in this post.</p>\n",
      "\n",
      "\n",
      "  Link: http://bair.berkeley.edu/blog/2018/05/17/delayed-impact/\n",
      "  PubDate: Thu, 17 May 2018 09:00:00 +0000\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20180610114353/http://bair.berkeley.edu/blog/feed.xml\n",
      "- Source: https://bair.berkeley.edu/blog/feed.xml\n",
      "  Channel Title: The Berkeley Artificial Intelligence Research Blog\n",
      "  Channel Description: The BAIR Blog\n",
      "- Title: BDD100K: A Large-scale Diverse Driving Video Database\n",
      "  Description: <p>TL;DR, we released the largest and most diverse driving video dataset with rich\n",
      "annotations called BDD100K. You can access the data for research now at <a href=\"http://bdd-data.berkeley.edu/\">http://bdd-data.berkeley.edu</a>.  We  have\n",
      "recently released <a href=\"https://arxiv.org/abs/1805.04687\">an arXiv\n",
      "report</a> on it. And there is still time to participate in <a href=\"http://bdd-data.berkeley.edu/wad-2018.html\">our CVPR 2018 challenges</a>!</p>\n",
      "\n",
      "<div class=\"videoWrapper\">\n",
      "  <iframe src=\"https://www.youtube.com/embed/IGi9K9FY35Y?rel=0\" frameborder=\"0\" allowfullscreen=\"\"></iframe>\n",
      "</div>\n",
      "\n",
      "<p><br /></p>\n",
      "\n",
      "<!--\n",
      "<iframe width=\"100%\" height=\"50%\"\n",
      "src=\"https://www.youtube.com/embed/IGi9K9FY35Y?autoplay=1&rel=0&amp;controls=0&amp;showinfo=0\"\n",
      "frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>\n",
      "-->\n",
      "\n",
      "<!--more-->\n",
      "\n",
      "<h2 id=\"large-scale-diverse-driving-video-pick-four\">Large-scale, Diverse, Driving, Video: Pick Four</h2>\n",
      "\n",
      "<p>Autonomous driving is poised to change the life in every community. However,\n",
      "recent events show that it is not clear yet how a man-made perception system can\n",
      "avoid even seemingly obvious mistakes when a driving system is deployed in the\n",
      "real world. As computer vision researchers, we are interested in exploring the\n",
      "frontiers of perception algorithms for self-driving to make it safer. To design\n",
      "and test potential algorithms, we would like to make use of all the information\n",
      "from the data collected by a real driving platform. Such data has four major\n",
      "properties: it is large-scale, diverse, captured on the street, and with\n",
      "temporal information. Data diversity is especially important to test the\n",
      "robustness of perception algorithms. However, current open datasets can only\n",
      "cover a subset of the properties described above. Therefore, with the help of <a href=\"https://www.getnexar.com/\">Nexar</a>, we are releasing the BDD100K\n",
      "database, which is the largest and most diverse open driving video dataset so\n",
      "far for computer vision research. This project is organized and sponsored by <a href=\"https://deepdrive.berkeley.edu/\">Berkeley DeepDrive</a> Industry\n",
      "Consortium, which investigates state-of-the-art technologies in computer vision\n",
      "and machine learning for automotive applications.</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img width=\"750\" src=\"http://bair.berkeley.edu/static/blog/bdd/geo_distribution.jpg\" />\n",
      "<br />\n",
      "<i>\n",
      "Locations of a random video subset.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>As suggested in the name, our dataset consists of 100,000 videos. Each video is\n",
      "about 40 seconds long, 720p, and 30 fps. The videos also come with GPS/IMU\n",
      "information recorded by cell-phones to show rough driving trajectories. Our\n",
      "videos were collected from diverse locations in the United States, as shown in\n",
      "the figure above. Our database covers different weather conditions, including\n",
      "sunny, overcast, and rainy, as well as  different times of day including daytime\n",
      "and nighttime. The table below summarizes comparisons with previous datasets,\n",
      "which shows our dataset is much larger and more diverse.</p>\n",
      "\n",
      "<!--\n",
      "<table>\n",
      "     <tr>\n",
      "         <th></th>\n",
      "         <th style=\"text-align: center\"> <a href=\"http://www.cvlibs.net/publications/Geiger2012CVPR.pdf\"> KITTI</th>\n",
      "         <th style=\"text-align: center\"> <a href=\"https://arxiv.org/abs/1604.01685\"> Cityscapes</th>\n",
      "         <th style=\"text-align: center\"> <a href=\"https://arxiv.org/pdf/1803.06184v1.pdf\"> ApolloScape</th>\n",
      "         <th style=\"text-align: center\"> <a href=\"https://research.mapillary.com/img/publications/ICCV17a.pdf\"> Mapillary</th>\n",
      "         <th style=\"text-align: center\"> <a href=\"https://arxiv.org/abs/1805.04687\"> BDD100K </a> </th>\n",
      "     </tr>\n",
      "     <tr>\n",
      "         <td align=\"center\"># Sequences</td>\n",
      "         <td align=\"center\">22</td>\n",
      "         <td align=\"center\">~50</td>\n",
      "         <td align=\"center\">4</td>\n",
      "         <td align=\"center\">N/A</td>\n",
      "         <td align=\"center\">100,000</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "         <td align=\"center\"># Images</td>\n",
      "         <td align=\"center\">14,999</td>\n",
      "         <td align=\"center\">5000 (+2000)</td>\n",
      "         <td align=\"center\">143,906</td>\n",
      "         <td align=\"center\">25,000</td>\n",
      "         <td align=\"center\">120,000,000</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "         <td align=\"center\">Multiple Cities</td>\n",
      "         <td align=\"center\" style=\"color:red\">No</td>\n",
      "         <td align=\"center\" style=\"color:green\">Yes</td>\n",
      "         <td align=\"center\" style=\"color:red\">No</td>\n",
      "         <td align=\"center\" style=\"color:green\">Yes</td>\n",
      "         <td align=\"center\" style=\"color:green\">Yes</td>\n",
      "    </tr>\n",
      "         <tr>\n",
      "         <td align=\"center\">Multiple Weathers</td>\n",
      "         <td align=\"center\" style=\"color:red\">No</td>\n",
      "         <td align=\"center\" style=\"color:red\">No</td>\n",
      "         <td align=\"center\" style=\"color:red\">No</td>\n",
      "         <td align=\"center\" style=\"color:green\">Yes</td>\n",
      "         <td align=\"center\" style=\"color:green\">Yes</td>\n",
      "    </tr>\n",
      "         <tr>\n",
      "         <td align=\"center\">Multiple Times of Day</td>\n",
      "         <td align=\"center\" style=\"color:red\">No</td>\n",
      "         <td align=\"center\" style=\"color:red\">No</td>\n",
      "         <td align=\"center\" style=\"color:red\">No</td>\n",
      "         <td align=\"center\" style=\"color:green\">Yes</td>\n",
      "         <td align=\"center\" style=\"color:green\">Yes</td>\n",
      "    </tr>\n",
      "         <tr>\n",
      "         <td align=\"center\">Multiple Scene types</td>\n",
      "         <td align=\"center\" style=\"color:green\">Yes</td>\n",
      "         <td align=\"center\" style=\"color:red\">No</td>\n",
      "         <td align=\"center\" style=\"color:red\">No</td>\n",
      "         <td align=\"center\" style=\"color:green\">Yes</td>\n",
      "         <td align=\"center\" style=\"color:green\">Yes</td>\n",
      "    </tr>\n",
      "</table>\n",
      "-->\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img width=\"750\" src=\"http://bair.berkeley.edu/static/blog/bdd/table0.png\" />\n",
      "<br />\n",
      "<i>\n",
      "Comparisons with some other street scene datasets. It is hard to fairly compare\n",
      "# images between datasets, but we list them here as a rough reference.\n",
      "# Sequences are lists as a reference for diversity, but different datasets have different sequence lengths.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>The videos and their trajectories can be useful for imitation learning of\n",
      "driving policies, as in our <a href=\"https://arxiv.org/abs/1612.01079\">CVPR 2017\n",
      "paper</a>. To facilitate computer vision research on our large-scale dataset, we\n",
      "also provide basic annotations on the video keyframes, as detailed in the next\n",
      "section. You can download the data and annotations now at <a href=\"http://bdd-data.berkeley.edu\">http://bdd-data.berkeley.edu</a>.</p>\n",
      "\n",
      "<h2 id=\"annotations\">Annotations</h2>\n",
      "\n",
      "<p>We sample a keyframe at the 10th second from each video and provide annotations\n",
      "for those keyframes. They are labeled at several levels: image tagging, road\n",
      "object bounding boxes, drivable areas, lane markings, and full-frame instance\n",
      "segmentation. These annotations will help us understand the diversity of the\n",
      "data and object statistics in different types of scenes. We will discuss the\n",
      "labeling process in a different blog post. More information about the\n",
      "annotations can be found in our <a href=\"https://arxiv.org/abs/1805.04687\">arXiv\n",
      "report</a>.</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img width=\"750\" src=\"http://bair.berkeley.edu/static/blog/bdd/annotation_examples.png\" />\n",
      "<br />\n",
      "<i>\n",
      "Overview of our annotations.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<h3 id=\"road-object-detection\">Road Object Detection</h3>\n",
      "\n",
      "<p>We label object bounding boxes for objects that commonly appear on the road on\n",
      "all of the 100,000 keyframes to understand the distribution of the objects and\n",
      "their locations. The bar chart below shows the object counts. There are also\n",
      "other ways to play with the statistics in our annotations. For example, we can\n",
      "compare the object counts under different weather conditions or in different\n",
      "types of scenes. This chart also shows the diverse set of objects that appear in\n",
      "our dataset, and the scale of our dataset –  more than 1 million cars. The\n",
      "reader should be reminded here that those are distinct objects with distinct\n",
      "appearances and contexts.</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img width=\"750\" src=\"http://bair.berkeley.edu/static/blog/bdd/bbox_instance.png\" />\n",
      "<br />\n",
      "<i>\n",
      "Statistics of different types of objects.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<p>Our dataset is also suitable for studying some particular domains. For example,\n",
      "if you are interested in detecting and avoiding pedestrians on the streets, you\n",
      "also have a reason to study our dataset since it contains more pedestrian\n",
      "instances than previous specialized datasets as shown in the table below.</p>\n",
      "\n",
      "<!--\n",
      "<table>\n",
      "     <tr>\n",
      "         <th></th>\n",
      "         <th style=\"text-align: center\"> <a href='https://core.ac.uk/download/pdf/4875878.pdf'> Caltech</th>\n",
      "         <th style=\"text-align: center\"> <a href='http://www.cvlibs.net/publications/Geiger2012CVPR.pdf'> KITTI</th>\n",
      "         <th style=\"text-align: center\"> <a href='https://arxiv.org/abs/1702.05693'> CityPerson</th>\n",
      "         <th style=\"text-align: center\"> <a href='https://arxiv.org/abs/1805.04687'> BDD100K </a> </th>\n",
      "     </tr>\n",
      "     <tr>\n",
      "         <td align=\"center\"># persons</td>\n",
      "         <td align=\"center\">1,273</td>\n",
      "         <td align=\"center\">6,336</td>\n",
      "         <td align=\"center\">19,654</td>\n",
      "         <td align=\"center\">86,047</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "         <td align=\"center\"># per image</td>\n",
      "         <td align=\"center\">1.4</td>\n",
      "         <td align=\"center\">0.8</td>\n",
      "         <td align=\"center\">7.0</td>\n",
      "         <td align=\"center\">1.2</td>\n",
      "    </tr>\n",
      "</table>\n",
      "-->\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img width=\"600\" src=\"http://bair.berkeley.edu/static/blog/bdd/table1.png\" />\n",
      "<br />\n",
      "<i>\n",
      "Comparisons with other pedestrian datasets regarding training set size.\n",
      "</i>\n",
      "</p>\n",
      "\n",
      "<h3 id=\"lane-markings\">Lane Markings</h3>\n",
      "\n",
      "<p>Lane markings are important road instructions for human drivers. They are also\n",
      "critical cues of driving direction and localization for the autonomous driving\n",
      "systems when GPS or maps does not have accurate global coverage. We divide the\n",
      "lane markings into two types based on how they instruct the vehicles in the\n",
      "lanes. Vertical lane markings (marked in red in the figures below) indicate\n",
      "markings that are  along the driving direction of their lanes. Parallel lane\n",
      "markings (marked in blue in the figures below) indicate those that are  for the\n",
      "vehicles in the lanes to stop. We also provide attributes for the markings such\n",
      "as solid vs. dashed and double vs. single.</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img width=\"750\" src=\"http://bair.berkeley.edu/static/blog/bdd/lane_markings.png\" />\n",
      "</p>\n",
      "\n",
      "<p>If you are ready to try out your lane marking prediction algorithms, please look\n",
      "no further. Here is the comparison with existing lane marking datasets.</p>\n",
      "\n",
      "<!--\n",
      "<table>\n",
      "     <tr>\n",
      "         <th></th>\n",
      "         <th style=\"text-align: center\"> Training </th>\n",
      "         <th style=\"text-align: center\"> Total </th>\n",
      "         <th style=\"text-align: center\"> Sequences </th>\n",
      "         <th style=\"text-align: center\"> Weather </th>\n",
      "         <th style=\"text-align: center\"> Time </a> </th>\n",
      "         <th style=\"text-align: center\"> Attributes </a> </th>\n",
      "     </tr>\n",
      "     <tr>\n",
      "         <td><a href='https://arxiv.org/abs/1411.7113'> Caltech Lanes Dataset</a></td>\n",
      "         <td align=\"center\">-</td>\n",
      "         <td align=\"center\">1,224</td>\n",
      "         <td align=\"center\">4</td>\n",
      "         <td align=\"center\">1</td>\n",
      "         <td align=\"center\">1</td>\n",
      "         <td align=\"center\">2</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "       <td><a href='https://ieeexplore.ieee.org/document/6232144/'> Road Marking Dataset</a></td>\n",
      "       <td align=\"center\">-</td>\n",
      "       <td align=\"center\">1,443</td>\n",
      "       <td align=\"center\">29</td>\n",
      "       <td align=\"center\">2</td>\n",
      "       <td align=\"center\">3</td>\n",
      "       <td align=\"center\">10</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "       <td><a href='http://www.cvlibs.net/projects/autonomous_vision_survey/literature/Fritsch2013ITSC.pdf'> KITTI Road</a></td>\n",
      "       <td align=\"center\">289</td>\n",
      "       <td align=\"center\">579</td>\n",
      "       <td align=\"center\">-</td>\n",
      "       <td align=\"center\">1</td>\n",
      "       <td align=\"center\">1</td>\n",
      "       <td align=\"center\">2</td>\n",
      "   </tr>\n",
      "   <tr>\n",
      "       <td><a href='https://arxiv.org/abs/1710.06288'> VPGNet</a></td>\n",
      "       <td align=\"center\">14,783</td>\n",
      "       <td align=\"center\">21,097</td>\n",
      "       <td align=\"center\">-</td>\n",
      "       <td align=\"center\">4</td>\n",
      "       <td align=\"center\">2</td>\n",
      "       <td align=\"center\">17</td>\n",
      "   </tr>\n",
      "   <tr>\n",
      "       <td><a href='https://arxiv.org/abs/1805.04687'> BDD100K</a></td>\n",
      "       <td align=\"center\">70,000</td>\n",
      "       <td align=\"center\">100,000</td>\n",
      "       <td align=\"center\">100,000</td>\n",
      "       <td align=\"center\">6</td>\n",
      "       <td align=\"center\">3</td>\n",
      "       <td align=\"center\">11</td>\n",
      "   </tr>\n",
      "</table>\n",
      "-->\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img width=\"750\" src=\"http://bair.berkeley.edu/static/blog/bdd/table2.png\" />\n",
      "</p>\n",
      "\n",
      "<h3 id=\"drivable-areas\">Drivable Areas</h3>\n",
      "\n",
      "<p>Whether we can drive on a road does not only depend on lane markings and traffic\n",
      "devices. It also depends on the complicated interactions with other objects\n",
      "sharing the road. In the end, it  is important to understand which area can be\n",
      "driven on. To investigate this problem, we also provide segmentation annotations\n",
      "of drivable areas as shown below. We divide  the drivable areas into two\n",
      "categories based on the trajectories of the ego vehicle: direct drivable, and\n",
      "alternative drivable. Direct drivable, marked in  red, means the ego vehicle has\n",
      "the road priority and can keep driving in that area. Alternative drivable,\n",
      "marked in  blue, means the ego vehicle can drive in the area, but has to be\n",
      "cautious since the road priority  potentially belongs to other vehicles.</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img width=\"750\" src=\"http://bair.berkeley.edu/static/blog/bdd/drivable_area.png\" />\n",
      "</p>\n",
      "\n",
      "<h3 id=\"full-frame-segmentation\">Full-frame Segmentation</h3>\n",
      "\n",
      "<p>It has been shown on Cityscapes dataset that full-frame fine instance\n",
      "segmentation can greatly bolster research in dense prediction and object\n",
      "detection, which are pillars of a wide range of computer vision applications. As\n",
      "our videos are in a different domain, we provide instance segmentation\n",
      "annotations as well to compare the domain shift relative by different datasets.\n",
      "It can be expensive and laborious to obtain full pixel-level segmentation.\n",
      "Fortunately, with our own labeling tool, the labeling cost could be reduced by\n",
      "50%. In the end, we label a subset of 10K images with full-frame instance\n",
      "segmentation. Our label set is compatible with the training annotations in\n",
      "Cityscapes to make it easier to study domain shift between the datasets.</p>\n",
      "\n",
      "<p style=\"text-align:center;\">\n",
      "<img width=\"750\" src=\"http://bair.berkeley.edu/static/blog/bdd/segmentation.jpg\" />\n",
      "</p>\n",
      "\n",
      "<h2 id=\"driving-challenges\">Driving Challenges</h2>\n",
      "\n",
      "<p>We are hosting <a href=\"http://bdd-data.berkeley.edu/wad-2018.html\">three\n",
      "challenges</a> in CVPR 2018 Workshop on Autonomous Driving based on our data:\n",
      "road object detection, drivable area prediction, and domain adaptation of\n",
      "semantic segmentation. The detection task requires your algorithm to find all of\n",
      "the target objects in our testing images and drivable area prediction requires\n",
      "segmenting the areas a car can drive in. In domain adaptation, the testing data\n",
      "is collected in China. Systems are thus challenged to get models learned in the\n",
      "US to work in the crowded streets in Beijing, China. You can submit your results\n",
      "now after <a href=\"http://bdd-data.berkeley.edu/login.html\">logging in our\n",
      "online submission portal</a>. Make sure to check out <a href=\"https://github.com/ucbdrive/bdd-data\">our toolkit</a> to jump start your\n",
      "participation.</p>\n",
      "\n",
      "<p>Join our CVPR workshop challenges to claim your cash prizes!!!</p>\n",
      "\n",
      "<h2 id=\"future-work\">Future Work</h2>\n",
      "\n",
      "<p>The perception system for self-driving is by no means only about monocular\n",
      "videos. It may also include panorama and stereo videos as well as  other types\n",
      "of sensors like LiDAR and radar. We hope to provide and study those\n",
      "multi-modality sensor data as well in the near future.</p>\n",
      "\n",
      "<h2 id=\"reference-links\">Reference Links</h2>\n",
      "\n",
      "<p><a href=\"https://core.ac.uk/download/pdf/4875878.pdf\"> Caltech,\n",
      "<a href=\"http://www.cvlibs.net/publications/Geiger2012CVPR.pdf\"> KITTI,\n",
      "<a href=\"https://arxiv.org/abs/1702.05693\"> CityPerson,\n",
      "<a href=\"https://arxiv.org/abs/1604.01685\"> Cityscapes,\n",
      "<a href=\"https://arxiv.org/pdf/1803.06184v1.pdf\"> ApolloScape,\n",
      "<a href=\"https://research.mapillary.com/img/publications/ICCV17a.pdf\"> Mapillary,\n",
      "<a href=\"https://arxiv.org/abs/1411.7113\"> Caltech Lanes Dataset,\n",
      "<a href=\"https://ieeexplore.ieee.org/document/6232144/\"> Road Marking Dataset,\n",
      "<a href=\"http://www.cvlibs.net/projects/autonomous_vision_survey/literature/Fritsch2013ITSC.pdf\"> KITTI Road,\n",
      "<a href=\"https://arxiv.org/abs/1710.06288\"> VPGNet</a></a></a></a></a></a></a></a></a></a></p>\n",
      "\n",
      "  Link: http://bair.berkeley.edu/blog/2018/05/30/bdd/\n",
      "  PubDate: Wed, 30 May 2018 09:00:00 +0000\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/20180625153007/http://bair.berkeley.edu/blog/feed.xml\n",
      "- Source: https://bair.berkeley.edu/blog/feed.xml\n",
      "  Channel Title: The Berkeley Artificial Intelligence Research Blog\n",
      "  Channel Description: The BAIR Blog\n",
      "- Title: BDD100K Blog Update\n",
      "  Description: <p>We are excited by the interest and excitement generated by our BDD100K dataset.\n",
      "<a href=\"http://bair.berkeley.edu/blog/2018/05/30/bdd/\">Our data release and blog post</a> were covered in an unsolicited article by\n",
      "the UC Berkeley newspaper, the Daily Cal, which was then picked up by other news\n",
      "services without our prompting or intervention.  The paper describing this\n",
      "dataset is under review at the ECCV 2018 conference, and we followed the rules\n",
      "of that conference (as communicated to us by the Program Chairs in prompt email\n",
      "response when we asked for clarification following the reporter’s request; the\n",
      "ECCV PC’s replied that ECCV follows <a href=\"http://cvpr2017.thecvf.com/submission/main_conference/author_guidelines#policies\">CVPR’s long-standing policy</a>).  We thus\n",
      "declined to speak to the reporters after they reached out to us. We did not, and\n",
      "have not, communicated with any media outlets regarding this story.</p>\n",
      "\n",
      "<p>While the Daily Cal article was accurate; unfortunately, other media outlets who\n",
      "followed in reporting the story made claims that were attributed to us\n",
      "incorrectly, and which do not represent our view.  In particular, several media\n",
      "outlets attributed to us a claim that the BDD100K dataset was “800 times” bigger\n",
      "than other industrial datasets, specifically mentioning Baidu’s ApolloScape.\n",
      "While it is true our dataset does contain more raw images than other datasets,\n",
      "including Baidu’s, the stated claim is misleading and we did not put that line\n",
      "or anything like it in a paper, blog post, or spoken comment to anyone. It\n",
      "appears that some reporters(s) viewed the data in tables in our paper and came\n",
      "up with this conclusory comment themselves as it made an exciting headline, yet\n",
      "attributed it to us.  In fact, it is inappropriate in our view to summarize the\n",
      "difference between our dataset and Baidu’s in a single comment that ours is 800x\n",
      "larger.  Comparing the number of raw images directly is not the most appropriate\n",
      "way to compare these types of datasets.</p>\n",
      "\n",
      "<p>Importantly, different datasets focus on different aspects of the autonomous\n",
      "driving challenge. Our dataset is crowd-sourced, and covers a very large area\n",
      "and diverse visual phenomena (indeed significantly more diverse than previous\n",
      "efforts, in our view), but it is very clearly limited to monocular RGB image\n",
      "data and associated mobile device metadata. Other dataset collection efforts are\n",
      "complementary in our view. Baidu’s, KITTI, and CityScapes each contain important\n",
      "additional sensing modalities and are collected with fully calibrated apparatus\n",
      "including actuation channels. (The dataset from Mapillary is also notable, and\n",
      "similar to ours in being diverse, crowd-sourced, and densely annotated, but\n",
      "differs in that we include video and dynamic metadata relevant to driving\n",
      "control.) We look forward to projects at Berkeley and elsewhere that leverage\n",
      "both BDD100K and these other datasets as the research community brings the\n",
      "potential of autonomous driving to reality.</p>\n",
      "\n",
      "<!--more-->\n",
      "\n",
      "\n",
      "  Link: http://bair.berkeley.edu/blog/2018/06/18/bdd-update/\n",
      "  PubDate: Mon, 18 Jun 2018 09:00:00 +0000\n",
      "  Author: None\n",
      "\n",
      "Memento: https://web.archive.org/web/https://becominghuman.ai/feed\n",
      "- Source: https://becominghuman.ai/feed\n",
      "  Channel Title: Becoming Human: Artificial Intelligence Magazine - Medium\n",
      "  Channel Description: Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity. - Medium\n",
      "- Title: AGI in 2025 |Do you think what matters today will still matter in the coming months? TL;DR: No!\n",
      "  Description: None\n",
      "  Link: https://becominghuman.ai/agi-in-2025-do-you-think-what-matters-today-will-still-matter-in-the-coming-months-tl-dr-no-5f22fc93c221?source=rss----5e5bef33608a---4\n",
      "  PubDate: Mon, 03 Feb 2025 16:58:24 GMT\n",
      "  Author: M. Pajuhaan\n",
      "\n",
      "Memento: https://web.archive.org/web/20170526193329/https://becominghuman.ai/feed\n",
      "- Source: https://becominghuman.ai/feed\n",
      "  Channel Title: Becoming Human - Medium\n",
      "  Channel Description: Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity. - Medium\n",
      "- Title: Advanced Lane-Detection for Self-Driving Cars\n",
      "  Description: None\n",
      "  Link: https://becominghuman.ai/advanced-lane-detection-for-self-driving-cars-9579e1f057ef?source=rss----5e5bef33608a---4\n",
      "  PubDate: Wed, 24 May 2017 21:17:30 GMT\n",
      "  Author: David Clark\n",
      "\n",
      "Memento: https://web.archive.org/web/20170630013818/https://becominghuman.ai/feed\n",
      "- Source: https://becominghuman.ai/feed\n",
      "  Channel Title: Becoming Human - Medium\n",
      "  Channel Description: Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity. - Medium\n",
      "- Title: Understanding Evolutionary Algorithms\n",
      "  Description: None\n",
      "  Link: https://becominghuman.ai/understanding-evolutionary-algorithms-58f7a2845537?source=rss----5e5bef33608a---4\n",
      "  PubDate: Mon, 19 Jun 2017 21:40:19 GMT\n",
      "  Author: Egor Dezhic\n",
      "\n",
      "Memento: https://web.archive.org/web/20180815093604/https://becominghuman.ai/feed\n",
      "- Source: https://becominghuman.ai/feed\n",
      "  Channel Title: Becoming Human: Artificial Intelligence Magazine - Medium\n",
      "  Channel Description: Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity. - Medium\n",
      "- Title: What is Artificial Intelligence and why now?\n",
      "  Description: None\n",
      "  Link: https://becominghuman.ai/what-is-ai-and-why-now-79d94f77dc91?source=rss----5e5bef33608a---4\n",
      "  PubDate: Wed, 08 Aug 2018 21:04:14 GMT\n",
      "  Author: Rodrigo Beceiro\n",
      "\n",
      "Memento: https://web.archive.org/web/20180821080415/https://becominghuman.ai/feed\n",
      "- Source: https://becominghuman.ai/feed\n",
      "  Channel Title: Becoming Human: Artificial Intelligence Magazine - Medium\n",
      "  Channel Description: Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity. - Medium\n",
      "- Title: Deep Learning — An ELI5 Intro to Neural Networks\n",
      "  Description: None\n",
      "  Link: https://becominghuman.ai/deep-learning-an-eli5-intro-to-neural-networks-baf7b02c1ae5?source=rss----5e5bef33608a---4\n",
      "  PubDate: Thu, 16 Aug 2018 11:05:56 GMT\n",
      "  Author: Hammad Asad\n",
      "\n",
      "Memento: https://web.archive.org/web/20180828110554/https://becominghuman.ai/feed\n",
      "- Source: https://becominghuman.ai/feed\n",
      "  Channel Title: Becoming Human: Artificial Intelligence Magazine - Medium\n",
      "  Channel Description: Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity. - Medium\n",
      "- Title: Product to Platform — Inside Amazon’s Dominance\n",
      "  Description: None\n",
      "  Link: https://becominghuman.ai/product-to-platform-inside-amazons-dominance-8d051a24d2c2?source=rss----5e5bef33608a---4\n",
      "  PubDate: Sun, 26 Aug 2018 12:01:01 GMT\n",
      "  Author: Matt Ward\n",
      "\n",
      "Memento: https://web.archive.org/web/20180917135230/https://becominghuman.ai/feed\n",
      "- Source: https://becominghuman.ai/feed\n",
      "  Channel Title: Becoming Human: Artificial Intelligence Magazine - Medium\n",
      "  Channel Description: Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity. - Medium\n",
      "- Title: Neural organizations — the future of organizational intelligence?\n",
      "  Description: None\n",
      "  Link: https://becominghuman.ai/neural-organizations-4017b546f561?source=rss----5e5bef33608a---4\n",
      "  PubDate: Fri, 17 Aug 2018 08:25:03 GMT\n",
      "  Author: Mike Bullock\n",
      "\n",
      "Memento: https://web.archive.org/web/20180920065220/https://becominghuman.ai/feed\n",
      "- Source: https://becominghuman.ai/feed\n",
      "  Channel Title: Becoming Human: Artificial Intelligence Magazine - Medium\n",
      "  Channel Description: Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity. - Medium\n",
      "- Title: How Do We Close the Funding Gap for Women Entrepreneurs?\n",
      "  Description: None\n",
      "  Link: https://becominghuman.ai/how-do-we-close-the-funding-gap-for-women-entrepreneurs-62461864b111?source=rss----5e5bef33608a---4\n",
      "  PubDate: Thu, 13 Sep 2018 12:48:34 GMT\n",
      "  Author: Flavia Richardson\n",
      "\n",
      "Memento: https://web.archive.org/web/20180921101042/https://becominghuman.ai/feed\n",
      "- Source: https://becominghuman.ai/feed\n",
      "  Channel Title: Becoming Human: Artificial Intelligence Magazine - Medium\n",
      "  Channel Description: Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity. - Medium\n",
      "- Title: Next-gen AI: techniques\n",
      "  Description: None\n",
      "  Link: https://becominghuman.ai/next-gen-ai-techniques-e1f7033ce25c?source=rss----5e5bef33608a---4\n",
      "  PubDate: Thu, 20 Sep 2018 13:07:06 GMT\n",
      "  Author: Axo Sal\n",
      "\n",
      "Memento: https://web.archive.org/web/20181002080400/https://becominghuman.ai/feed\n",
      "- Source: https://becominghuman.ai/feed\n",
      "  Channel Title: Becoming Human: Artificial Intelligence Magazine - Medium\n",
      "  Channel Description: Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity. - Medium\n",
      "- Title: 7 Most Trusted Bitcoin Wallets To use In 2019\n",
      "  Description: None\n",
      "  Link: https://becominghuman.ai/top-7-trusted-bitcoin-wallets-to-use-in-2019-37dbd3aa1b14?source=rss----5e5bef33608a---4\n",
      "  PubDate: Thu, 20 Sep 2018 13:18:17 GMT\n",
      "  Author: Mantra Malhotra\n",
      "\n",
      "Memento: https://web.archive.org/web/20181005101352/https://becominghuman.ai/feed\n",
      "- Source: https://becominghuman.ai/feed\n",
      "  Channel Title: Becoming Human: Artificial Intelligence Magazine - Medium\n",
      "  Channel Description: Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity. - Medium\n",
      "- Title: Google: The God of the Internet?\n",
      "  Description: None\n",
      "  Link: https://becominghuman.ai/amazon-google-facebook-and-apples-strategies-for-world-domination-740a25191bcc?source=rss----5e5bef33608a---4\n",
      "  PubDate: Fri, 31 Aug 2018 12:01:02 GMT\n",
      "  Author: Matt Ward\n",
      "\n",
      "Memento: https://web.archive.org/web/20181023093623/https://becominghuman.ai/feed\n",
      "- Source: https://becominghuman.ai/feed\n",
      "  Channel Title: Becoming Human: Artificial Intelligence Magazine - Medium\n",
      "  Channel Description: Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity. - Medium\n",
      "- Title: Reinforcement Learning | The Very Basics\n",
      "  Description: None\n",
      "  Link: https://becominghuman.ai/reinforcement-learning-the-very-basics-53098b446002?source=rss----5e5bef33608a---4\n",
      "  PubDate: Sun, 14 Oct 2018 21:32:16 GMT\n",
      "  Author: Rahul Bhatia\n",
      "\n",
      "Memento: https://web.archive.org/web/20181028125440/https://becominghuman.ai/feed\n",
      "- Source: https://becominghuman.ai/feed\n",
      "  Channel Title: Becoming Human: Artificial Intelligence Magazine - Medium\n",
      "  Channel Description: Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity. - Medium\n",
      "- Title: Effective Data Preprocessing and Feature Engineering\n",
      "  Description: None\n",
      "  Link: https://becominghuman.ai/effective-data-preprocessing-and-feature-engineering-452d3a948262?source=rss----5e5bef33608a---4\n",
      "  PubDate: Mon, 22 Oct 2018 11:32:45 GMT\n",
      "  Author: Ali Hamza\n",
      "\n",
      "Memento: https://web.archive.org/web/20181029154826/https://becominghuman.ai/feed\n",
      "- Source: https://becominghuman.ai/feed\n",
      "  Channel Title: Becoming Human: Artificial Intelligence Magazine - Medium\n",
      "  Channel Description: Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity. - Medium\n",
      "- Title: Top 10 Artificial Intelligence (AI) Technologies In 2019\n",
      "  Description: None\n",
      "  Link: https://becominghuman.ai/top-10-artificial-intelligence-ai-technologies-in-2019-f7151d3ce370?source=rss----5e5bef33608a---4\n",
      "  PubDate: Wed, 24 Oct 2018 06:37:20 GMT\n",
      "  Author: Techiezlounge\n",
      "\n",
      "Memento: https://web.archive.org/web/20181031120424/https://becominghuman.ai/feed\n",
      "- Source: https://becominghuman.ai/feed\n",
      "  Channel Title: Becoming Human: Artificial Intelligence Magazine - Medium\n",
      "  Channel Description: Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity. - Medium\n",
      "- Title: The Age of Evocative Machines\n",
      "  Description: None\n",
      "  Link: https://becominghuman.ai/https-medium-com-timoth3y-the-age-of-evocative-machines-10a07eff4070?source=rss----5e5bef33608a---4\n",
      "  PubDate: Tue, 23 Oct 2018 00:23:12 GMT\n",
      "  Author: Tim Romero\n",
      "\n",
      "Memento: https://web.archive.org/web/20181102113012/https://becominghuman.ai/feed\n",
      "- Source: https://becominghuman.ai/feed\n",
      "  Channel Title: Becoming Human: Artificial Intelligence Magazine - Medium\n",
      "  Channel Description: Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity. - Medium\n",
      "- Title: Practical Artificial Intelligence For Game Development\n",
      "  Description: None\n",
      "  Link: https://becominghuman.ai/practical-artificial-intelligence-for-game-development-5b0ebf35993b?source=rss----5e5bef33608a---4\n",
      "  PubDate: Fri, 26 Oct 2018 21:25:04 GMT\n",
      "  Author: Daniel Jooryabi\n",
      "\n",
      "Memento: https://web.archive.org/web/20181104161413/https://becominghuman.ai/feed\n",
      "- Source: https://becominghuman.ai/feed\n",
      "  Channel Title: Becoming Human: Artificial Intelligence Magazine - Medium\n",
      "  Channel Description: Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity. - Medium\n",
      "- Title: Artificial Intelligence and Reasons Behind Its Certifications\n",
      "  Description: None\n",
      "  Link: https://becominghuman.ai/artificial-intelligence-and-reasons-behind-its-certifications-c425684cad16?source=rss----5e5bef33608a---4\n",
      "  PubDate: Thu, 18 Oct 2018 00:00:00 GMT\n",
      "  Author: Michael Warne\n",
      "\n",
      "Memento: https://web.archive.org/web/20181113043414/https://becominghuman.ai/feed\n",
      "- Source: https://becominghuman.ai/feed\n",
      "  Channel Title: Becoming Human: Artificial Intelligence Magazine - Medium\n",
      "  Channel Description: Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity. - Medium\n",
      "- Title: Building a product? Make sure AI is part of the product strategy.\n",
      "  Description: None\n",
      "  Link: https://becominghuman.ai/building-a-product-make-sure-ai-is-part-of-the-product-strategy-1ccac31c58bb?source=rss----5e5bef33608a---4\n",
      "  PubDate: Mon, 05 Nov 2018 14:35:55 GMT\n",
      "  Author: Yuriy Mikitchenko\n",
      "\n",
      "Memento: https://web.archive.org/web/20181114072220/https://becominghuman.ai/feed\n",
      "- Source: https://becominghuman.ai/feed\n",
      "  Channel Title: Becoming Human: Artificial Intelligence Magazine - Medium\n",
      "  Channel Description: Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity. - Medium\n",
      "- Title: Conscious Artifical Intelligence C.A.I. Foundations.\n",
      "  Description: None\n",
      "  Link: https://becominghuman.ai/conscious-artifical-intelligence-c-a-i-foundations-7118f0d2ad64?source=rss----5e5bef33608a---4\n",
      "  PubDate: Wed, 07 Nov 2018 20:39:14 GMT\n",
      "  Author: Keno Leon\n",
      "\n",
      "Memento: https://web.archive.org/web/20181205094333/https://becominghuman.ai/feed\n",
      "- Source: https://becominghuman.ai/feed\n",
      "  Channel Title: Becoming Human: Artificial Intelligence Magazine - Medium\n",
      "  Channel Description: Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity. - Medium\n",
      "- Title: NeurIPS: to make the most of it and get to the top\n",
      "  Description: None\n",
      "  Link: https://becominghuman.ai/neurips-to-make-the-most-of-it-and-get-to-the-top-cb103d5cdf00?source=rss----5e5bef33608a---4\n",
      "  PubDate: Mon, 03 Dec 2018 12:16:35 GMT\n",
      "  Author: Dbrain\n",
      "\n",
      "Memento: https://web.archive.org/web/https://www.microsoft.com/en-us/research/feed\n",
      "- Source: https://www.microsoft.com/en-us/research/feed\n",
      "  Channel Title: Microsoft Research\n",
      "  Channel Description: None\n",
      "- Title: Research Focus: Week of March 24, 2025\n",
      "  Description: <p>In this issue, we examine a new conversation segmentation method that delivers more coherent and personalized agent conversation, and we review efforts to improve MLLMs’ understanding of geologic maps. Check out the latest research and other updates.</p>\n",
      "<p>The post <a href=\"https://www.microsoft.com/en-us/research/blog/research-focus-week-of-march-24-2025/\">Research Focus: Week of March 24, 2025</a> appeared first on <a href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "\n",
      "  Link: https://www.microsoft.com/en-us/research/blog/research-focus-week-of-march-24-2025/\n",
      "  PubDate: Wed, 26 Mar 2025 16:00:00 +0000\n",
      "  Author: Microsoft Research Team\n",
      "\n",
      "Memento: https://web.archive.org/web/20160702215923/https://www.microsoft.com/en-us/research/feed\n",
      "- Source: https://www.microsoft.com/en-us/research/feed\n",
      "  Channel Title: Microsoft Research\n",
      "  Channel Description: None\n",
      "- Title: Preliminary KDD Cup 2016 third phase and overall results announced\n",
      "  Description: <p>By Alex Wade, Principal Program Manager, Microsoft Research The preliminary results are in for the third and final round of the $10,000 KDD Cup 2016, and it was a squeaker!  The top ten finalists, out of a field of 3,773 submissions from 554 teams, were separated by only .01 points. Their task? To rank research [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research/preliminary-kdd-cup-2016-third-phase-and-overall-results-announced/\">Preliminary KDD Cup 2016 third phase and overall results announced</a> appeared first on <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "\n",
      "  Link: https://www.microsoft.com/en-us/research/preliminary-kdd-cup-2016-third-phase-and-overall-results-announced/\n",
      "  PubDate: Thu, 07 Jul 2016 17:45:10 +0000\n",
      "  Author: Krissi Thomas\n",
      "\n",
      "Memento: https://web.archive.org/web/20160708091441/https://www.microsoft.com/en-us/research/feed/\n",
      "- Source: https://www.microsoft.com/en-us/research/feed\n",
      "  Channel Title: Microsoft Research\n",
      "  Channel Description: None\n",
      "- Title: Preliminary KDD Cup 2016 third phase and overall results announced\n",
      "  Description: <p>By Alex Wade, Principal Program Manager, Microsoft Research The preliminary results are in for the third and final round of the $10,000 KDD Cup 2016, and it was a squeaker!  The top ten finalists, out of a field of 3,773 submissions from 554 teams, were separated by only .01 points. Their task? To rank research [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research/preliminary-kdd-cup-2016-third-phase-and-overall-results-announced/\">Preliminary KDD Cup 2016 third phase and overall results announced</a> appeared first on <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "\n",
      "  Link: https://www.microsoft.com/en-us/research/preliminary-kdd-cup-2016-third-phase-and-overall-results-announced/\n",
      "  PubDate: Thu, 07 Jul 2016 17:45:10 +0000\n",
      "  Author: Krissi Thomas\n",
      "\n",
      "Memento: https://web.archive.org/web/20160723065431/https://www.microsoft.com/en-us/research/feed/\n",
      "- Source: https://www.microsoft.com/en-us/research/feed\n",
      "  Channel Title: Microsoft Research\n",
      "  Channel Description: None\n",
      "- Title: The next 25 years of research: Disruption, invention and an element of surprise\n",
      "  Description: <p>By Allison Linn, Senior Writer, Microsoft Over the next 25 years, research scientists will use technology to better humanity, to make more sense of the world and to use our time more efficiently. We’ll disrupt some industries and invent others. We’ll produce technology that we didn’t even know we wanted – or needed. At a [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research/the-next-25-years-of-research-disruption-invention-and-an-element-of-surprise/\">The next 25 years of research: Disruption, invention and an element of surprise</a> appeared first on <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "\n",
      "  Link: https://www.microsoft.com/en-us/research/the-next-25-years-of-research-disruption-invention-and-an-element-of-surprise/\n",
      "  PubDate: Fri, 22 Jul 2016 16:00:46 +0000\n",
      "  Author: Alyssa Hughes\n",
      "\n",
      "Memento: https://web.archive.org/web/20160812095616/https://www.microsoft.com/en-us/research/feed/\n",
      "- Source: https://www.microsoft.com/en-us/research/feed\n",
      "  Channel Title: Microsoft Research\n",
      "  Channel Description: None\n",
      "- Title: Long-term collaboration takes aim at mobile browsing\n",
      "  Description: <p>By Lily Sun, Research Program Manager, Microsoft Research Asia As mobile browsing continues to consume an ever larger share of Internet services, the stakes of improving the mobile user experience have never been greater. That’s one of the reasons that Microsoft Research Asia (MSRA) and Peking University are embarking on a joint project to raise the [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research/long-term-collaboration-takes-aim-mobile-browsing/\">Long-term collaboration takes aim at mobile browsing</a> appeared first on <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "\n",
      "  Link: https://www.microsoft.com/en-us/research/long-term-collaboration-takes-aim-mobile-browsing/\n",
      "  PubDate: Thu, 11 Aug 2016 16:00:50 +0000\n",
      "  Author: Alyssa Hughes\n",
      "\n",
      "Memento: https://web.archive.org/web/20160812142457/https://www.microsoft.com/en-us/research/feed/\n",
      "- Source: https://www.microsoft.com/en-us/research/feed\n",
      "  Channel Title: Microsoft Research\n",
      "  Channel Description: None\n",
      "- Title: Long-term collaboration takes aim at mobile browsing\n",
      "  Description: <p>By Lily Sun, Research Program Manager, Microsoft Research Asia As mobile browsing continues to consume an ever larger share of Internet services, the stakes of improving the mobile user experience have never been greater. That’s one of the reasons that Microsoft Research Asia (MSRA) and Peking University are embarking on a joint project to raise the [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research/long-term-collaboration-takes-aim-mobile-browsing/\">Long-term collaboration takes aim at mobile browsing</a> appeared first on <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "\n",
      "  Link: https://www.microsoft.com/en-us/research/long-term-collaboration-takes-aim-mobile-browsing/\n",
      "  PubDate: Thu, 11 Aug 2016 16:00:50 +0000\n",
      "  Author: Alyssa Hughes\n",
      "\n",
      "Memento: https://web.archive.org/web/20160819010738/https://www.microsoft.com/en-us/research/feed/\n",
      "- Source: https://www.microsoft.com/en-us/research/feed\n",
      "  Channel Title: Microsoft Research\n",
      "  Channel Description: None\n",
      "- Title: Long-term collaboration takes aim at mobile browsing\n",
      "  Description: <p>By Lily Sun, Research Program Manager, Microsoft Research Asia As mobile browsing continues to consume an ever larger share of Internet services, the stakes of improving the mobile user experience have never been greater. That’s one of the reasons that Microsoft Research Asia (MSRA) and Peking University are embarking on a joint project to raise the [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research/long-term-collaboration-takes-aim-mobile-browsing/\">Long-term collaboration takes aim at mobile browsing</a> appeared first on <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "\n",
      "  Link: https://www.microsoft.com/en-us/research/long-term-collaboration-takes-aim-mobile-browsing/\n",
      "  PubDate: Thu, 11 Aug 2016 16:00:50 +0000\n",
      "  Author: Alyssa Hughes\n",
      "\n",
      "Memento: https://web.archive.org/web/20160819102315/https://www.microsoft.com/en-us/research/feed/\n",
      "- Source: https://www.microsoft.com/en-us/research/feed\n",
      "  Channel Title: Microsoft Research\n",
      "  Channel Description: None\n",
      "- Title: Long-term collaboration takes aim at mobile browsing\n",
      "  Description: <p>By Lily Sun, Research Program Manager, Microsoft Research Asia As mobile browsing continues to consume an ever larger share of Internet services, the stakes of improving the mobile user experience have never been greater. That’s one of the reasons that Microsoft Research Asia (MSRA) and Peking University are embarking on a joint project to raise the [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research/long-term-collaboration-takes-aim-mobile-browsing/\">Long-term collaboration takes aim at mobile browsing</a> appeared first on <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "\n",
      "  Link: https://www.microsoft.com/en-us/research/long-term-collaboration-takes-aim-mobile-browsing/\n",
      "  PubDate: Thu, 11 Aug 2016 16:00:50 +0000\n",
      "  Author: Alyssa Hughes\n",
      "\n",
      "Memento: https://web.archive.org/web/20160826022848/https://www.microsoft.com/en-us/research/feed\n",
      "- Source: https://www.microsoft.com/en-us/research/feed\n",
      "  Channel Title: Microsoft Research\n",
      "  Channel Description: None\n",
      "- Title: Nanotechnology comes to life with needle-based human interface devices\n",
      "  Description: <p>By Noboru Kuno, Research Program Manager, Microsoft Research Researchers at Microsoft and Tokyo’s Keio University have developed systems that could allow people to use tiny, painless needles to do things like monitor medical conditions or receive information without looking at a screen. The research project, which explores the convergence of micro- and nanotechnology, wearable sensors [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research/nanotechnology-comes-life-needle-based-human-interface-devices/\">Nanotechnology comes to life with needle-based human interface devices</a> appeared first on <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "\n",
      "  Link: https://www.microsoft.com/en-us/research/nanotechnology-comes-life-needle-based-human-interface-devices/\n",
      "  PubDate: Wed, 24 Aug 2016 17:40:52 +0000\n",
      "  Author: Alyssa Hughes\n",
      "\n",
      "Memento: https://web.archive.org/web/20160826125452/https://www.microsoft.com/en-us/research/feed/\n",
      "- Source: https://www.microsoft.com/en-us/research/feed\n",
      "  Channel Title: Microsoft Research\n",
      "  Channel Description: None\n",
      "- Title: Nanotechnology comes to life with needle-based human interface devices\n",
      "  Description: <p>By Noboru Kuno, Research Program Manager, Microsoft Research Researchers at Microsoft and Tokyo’s Keio University have developed systems that could allow people to use tiny, painless needles to do things like monitor medical conditions or receive information without looking at a screen. The research project, which explores the convergence of micro- and nanotechnology, wearable sensors [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research/nanotechnology-comes-life-needle-based-human-interface-devices/\">Nanotechnology comes to life with needle-based human interface devices</a> appeared first on <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "\n",
      "  Link: https://www.microsoft.com/en-us/research/nanotechnology-comes-life-needle-based-human-interface-devices/\n",
      "  PubDate: Wed, 24 Aug 2016 17:40:52 +0000\n",
      "  Author: Alyssa Hughes\n",
      "\n",
      "Memento: https://web.archive.org/web/20160827032655/http://www.microsoft.com/en-us/research/feed\n",
      "- Source: https://www.microsoft.com/en-us/research/feed\n",
      "  Channel Title: Microsoft Research\n",
      "  Channel Description: None\n",
      "- Title: Nanotechnology comes to life with needle-based human interface devices\n",
      "  Description: <p>By Noboru Kuno, Research Program Manager, Microsoft Research Researchers at Microsoft and Tokyo’s Keio University have developed systems that could allow people to use tiny, painless needles to do things like monitor medical conditions or receive information without looking at a screen. The research project, which explores the convergence of micro- and nanotechnology, wearable sensors [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research/nanotechnology-comes-life-needle-based-human-interface-devices/\">Nanotechnology comes to life with needle-based human interface devices</a> appeared first on <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "\n",
      "  Link: https://www.microsoft.com/en-us/research/nanotechnology-comes-life-needle-based-human-interface-devices/\n",
      "  PubDate: Wed, 24 Aug 2016 17:40:52 +0000\n",
      "  Author: Alyssa Hughes\n",
      "\n",
      "Memento: https://web.archive.org/web/20160828190453/https://www.microsoft.com/en-us/research/feed/\n",
      "- Source: https://www.microsoft.com/en-us/research/feed\n",
      "  Channel Title: Microsoft Research\n",
      "  Channel Description: None\n",
      "- Title: Summer school data science research could trigger real world changes\n",
      "  Description: <p>By John Kaiser, Writer, Microsoft Research Microsoft Research hosted its third annual Data Science Summer School in New York City as a diverse group of undergraduate students deployed some of the latest data crunching techniques on millions of rows of anonymized data in an effort to uncover useful information. “We’re really hoping to give them [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research/summer-school-data-science-research-trigger-real-world-changes/\">Summer school data science research could trigger real world changes</a> appeared first on <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "\n",
      "  Link: https://www.microsoft.com/en-us/research/summer-school-data-science-research-trigger-real-world-changes/\n",
      "  PubDate: Fri, 26 Aug 2016 16:00:31 +0000\n",
      "  Author: Alyssa Hughes\n",
      "\n",
      "Memento: https://web.archive.org/web/20160829023815/http://www.microsoft.com/en-us/research/feed/\n",
      "- Source: https://www.microsoft.com/en-us/research/feed\n",
      "  Channel Title: Microsoft Research\n",
      "  Channel Description: None\n",
      "- Title: Summer school data science research could trigger real world changes\n",
      "  Description: <p>By John Kaiser, Writer, Microsoft Research Microsoft Research hosted its third annual Data Science Summer School in New York City as a diverse group of undergraduate students deployed some of the latest data crunching techniques on millions of rows of anonymized data in an effort to uncover useful information. “We’re really hoping to give them [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research/summer-school-data-science-research-trigger-real-world-changes/\">Summer school data science research could trigger real world changes</a> appeared first on <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "\n",
      "  Link: https://www.microsoft.com/en-us/research/summer-school-data-science-research-trigger-real-world-changes/\n",
      "  PubDate: Fri, 26 Aug 2016 16:00:31 +0000\n",
      "  Author: Alyssa Hughes\n",
      "\n",
      "Memento: https://web.archive.org/web/20160902135114/https://www.microsoft.com/en-us/research/feed/\n",
      "- Source: https://www.microsoft.com/en-us/research/feed\n",
      "  Channel Title: Microsoft Research\n",
      "  Channel Description: None\n",
      "- Title: Microsoft Research brings summer school to Russia’s emergent tech hub\n",
      "  Description: <p>By John Kaiser, Writer, Microsoft Microsoft Research recently concluded  its eighth annual summer school in Kazan, Russia, challenging students to conceptualize and build new applications from the sensors and devices emerging from the Internet of Things (IoT). “Our summer schools are an opportunity to expose students to the latest technologies in computer science,” said Judith [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research/microsoft-research-brings-summer-school-russias-emergent-tech-hub/\">Microsoft Research brings summer school to Russia’s emergent tech hub</a> appeared first on <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "\n",
      "  Link: https://www.microsoft.com/en-us/research/microsoft-research-brings-summer-school-russias-emergent-tech-hub/\n",
      "  PubDate: Thu, 01 Sep 2016 20:44:25 +0000\n",
      "  Author: Alyssa Hughes\n",
      "\n",
      "Memento: https://web.archive.org/web/20160909125701/https://www.microsoft.com/en-us/research/feed/\n",
      "- Source: https://www.microsoft.com/en-us/research/feed\n",
      "  Channel Title: Microsoft Research\n",
      "  Channel Description: None\n",
      "- Title: Microsoft Research brings summer school to Russia’s emergent tech hub\n",
      "  Description: <p>By John Kaiser, Writer, Microsoft Microsoft Research recently concluded  its eighth annual summer school in Kazan, Russia, challenging students to conceptualize and build new applications from the sensors and devices emerging from the Internet of Things (IoT). “Our summer schools are an opportunity to expose students to the latest technologies in computer science,” said Judith [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research/microsoft-research-brings-summer-school-russias-emergent-tech-hub/\">Microsoft Research brings summer school to Russia’s emergent tech hub</a> appeared first on <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "\n",
      "  Link: https://www.microsoft.com/en-us/research/microsoft-research-brings-summer-school-russias-emergent-tech-hub/\n",
      "  PubDate: Thu, 01 Sep 2016 20:44:25 +0000\n",
      "  Author: Alyssa Hughes\n",
      "\n",
      "Memento: https://web.archive.org/web/20160911170623/https://www.microsoft.com/en-us/research/feed\n",
      "- Source: https://www.microsoft.com/en-us/research/feed\n",
      "  Channel Title: Microsoft Research\n",
      "  Channel Description: None\n",
      "- Title: Microsoft Research brings summer school to Russia’s emergent tech hub\n",
      "  Description: <p>By John Kaiser, Writer, Microsoft Microsoft Research recently concluded  its eighth annual summer school in Kazan, Russia, challenging students to conceptualize and build new applications from the sensors and devices emerging from the Internet of Things (IoT). “Our summer schools are an opportunity to expose students to the latest technologies in computer science,” said Judith [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research/microsoft-research-brings-summer-school-russias-emergent-tech-hub/\">Microsoft Research brings summer school to Russia’s emergent tech hub</a> appeared first on <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "\n",
      "  Link: https://www.microsoft.com/en-us/research/microsoft-research-brings-summer-school-russias-emergent-tech-hub/\n",
      "  PubDate: Thu, 01 Sep 2016 20:44:25 +0000\n",
      "  Author: Alyssa Hughes\n",
      "\n",
      "Memento: https://web.archive.org/web/20160911170626/https://www.microsoft.com/en-us/research/feed/\n",
      "- Source: https://www.microsoft.com/en-us/research/feed\n",
      "  Channel Title: Microsoft Research\n",
      "  Channel Description: None\n",
      "- Title: Microsoft Research brings summer school to Russia’s emergent tech hub\n",
      "  Description: <p>By John Kaiser, Writer, Microsoft Microsoft Research recently concluded  its eighth annual summer school in Kazan, Russia, challenging students to conceptualize and build new applications from the sensors and devices emerging from the Internet of Things (IoT). “Our summer schools are an opportunity to expose students to the latest technologies in computer science,” said Judith [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research/microsoft-research-brings-summer-school-russias-emergent-tech-hub/\">Microsoft Research brings summer school to Russia’s emergent tech hub</a> appeared first on <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "\n",
      "  Link: https://www.microsoft.com/en-us/research/microsoft-research-brings-summer-school-russias-emergent-tech-hub/\n",
      "  PubDate: Thu, 01 Sep 2016 20:44:25 +0000\n",
      "  Author: Alyssa Hughes\n",
      "\n",
      "Memento: https://web.archive.org/web/20160911170629/https://www.microsoft.com/en-us/research/feed/\n",
      "- Source: https://www.microsoft.com/en-us/research/feed\n",
      "  Channel Title: Microsoft Research\n",
      "  Channel Description: None\n",
      "- Title: Microsoft Research brings summer school to Russia’s emergent tech hub\n",
      "  Description: <p>By John Kaiser, Writer, Microsoft Microsoft Research recently concluded  its eighth annual summer school in Kazan, Russia, challenging students to conceptualize and build new applications from the sensors and devices emerging from the Internet of Things (IoT). “Our summer schools are an opportunity to expose students to the latest technologies in computer science,” said Judith [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research/microsoft-research-brings-summer-school-russias-emergent-tech-hub/\">Microsoft Research brings summer school to Russia’s emergent tech hub</a> appeared first on <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "\n",
      "  Link: https://www.microsoft.com/en-us/research/microsoft-research-brings-summer-school-russias-emergent-tech-hub/\n",
      "  PubDate: Thu, 01 Sep 2016 20:44:25 +0000\n",
      "  Author: Alyssa Hughes\n",
      "\n",
      "Memento: https://web.archive.org/web/20160916063521/https://www.microsoft.com/en-us/research/feed/\n",
      "- Source: https://www.microsoft.com/en-us/research/feed\n",
      "  Channel Title: Microsoft Research\n",
      "  Channel Description: None\n",
      "- Title: Microsoft Research brings summer school to Russia’s emergent tech hub\n",
      "  Description: <p>By John Kaiser, Writer, Microsoft Microsoft Research recently concluded  its eighth annual summer school in Kazan, Russia, challenging students to conceptualize and build new applications from the sensors and devices emerging from the Internet of Things (IoT). “Our summer schools are an opportunity to expose students to the latest technologies in computer science,” said Judith [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research/microsoft-research-brings-summer-school-russias-emergent-tech-hub/\">Microsoft Research brings summer school to Russia’s emergent tech hub</a> appeared first on <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "\n",
      "  Link: https://www.microsoft.com/en-us/research/microsoft-research-brings-summer-school-russias-emergent-tech-hub/\n",
      "  PubDate: Thu, 01 Sep 2016 20:44:25 +0000\n",
      "  Author: Alyssa Hughes\n",
      "\n",
      "Memento: https://web.archive.org/web/20160916212954/https://www.microsoft.com/en-us/research/feed/\n",
      "- Source: https://www.microsoft.com/en-us/research/feed\n",
      "  Channel Title: Microsoft Research\n",
      "  Channel Description: None\n",
      "- Title: Boosting fitness and workplace performance looks to machine learning for answers\n",
      "  Description: <p>By Miran Lee, Principal Research Program Manager, Microsoft Research A field study exploring the usability of integrating fitness equipment into a workstation environment has researchers looking to tap next-generation machine learning innovations to address the seemingly elusive challenge of burning calories without ever leaving the desk. Elliptical trainers small enough to fit under a desk [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research/boosting-fitness-and-workplace-performance-looks-to-machine-learning-for-answers/\">Boosting fitness and workplace performance looks to machine learning for answers</a> appeared first on <a rel=\"nofollow\" href=\"https://www.microsoft.com/en-us/research\">Microsoft Research</a>.</p>\n",
      "\n",
      "  Link: https://www.microsoft.com/en-us/research/boosting-fitness-and-workplace-performance-looks-to-machine-learning-for-answers/\n",
      "  PubDate: Fri, 16 Sep 2016 18:32:57 +0000\n",
      "  Author: Alyssa Hughes\n",
      "\n",
      "Memento: https://web.archive.org/web/20220129214838/https://machinelearningmastery.com/feed/\n",
      "- Source: https://machinelearningmastery.com/feed\n",
      "  Channel Title: Machine Learning Mastery\n",
      "  Channel Description: Making developers awesome at machine learning\n",
      "- Title: Setting Breakpoints and Exception Hooks in Python\n",
      "  Description: <p>Last Updated on January 22, 2022 There are different ways of debugging code in Python, one of which is to [&#8230;]</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://machinelearningmastery.com/setting-breakpoints-and-exception-hooks-in-python/\">Setting Breakpoints and Exception Hooks in Python</a> appeared first on <a rel=\"nofollow\" href=\"https://machinelearningmastery.com\">Machine Learning Mastery</a>.</p>\n",
      "\n",
      "  Link: https://machinelearningmastery.com/setting-breakpoints-and-exception-hooks-in-python/\n",
      "  PubDate: Fri, 21 Jan 2022 15:51:41 +0000\n",
      "  Author: Stefania Cristina\n",
      "\n",
      "Memento: https://web.archive.org/web/20200427003837/https://eng.uber.com/tag/machine-learning/feed/\n",
      "- Source: https://eng.uber.com/tag/machine-learning/feed\n",
      "  Channel Title: Machine Learning – Uber Engineering Blog\n",
      "  Channel Description: Software engineering and technologies that set the world in motion\n",
      "- Title: Under the Hood of Uber ATG’s Machine Learning Infrastructure and Versioning Control Platform for Self-Driving Vehicles\n",
      "  Description: <p><span style=\"font-weight: 400;\">As Uber experienced exponential growth over the last few years, now supporting 14 million trips each day, our engineers proved they could build for scale. That value extends to other areas, including </span><a href=\"https://www.uber.com/us/en/atg/\" target=\"_blank\" rel=\"noopener noreferrer\"><span style=\"font-weight: 400;\">Uber ATG</span></a><span style=\"font-weight: 400;\"> (Advanced Technologies Group) and its quest </span>&#8230;</p>\n",
      "<p>The post <a rel=\"nofollow\" href=\"https://eng.uber.com/machine-learning-model-life-cycle-version-control/\">Under the Hood of Uber ATG’s Machine Learning Infrastructure and Versioning Control Platform for Self-Driving Vehicles</a> appeared first on <a rel=\"nofollow\" href=\"https://eng.uber.com\">Uber Engineering Blog</a>.</p>\n",
      "\n",
      "  Link: https://eng.uber.com/machine-learning-model-life-cycle-version-control/\n",
      "  PubDate: Wed, 04 Mar 2020 17:00:38 +0000\n",
      "  Author: Yu Guo\n",
      "\n",
      "Memento: https://web.archive.org/web/20180529034430/https://aws.amazon.com/blogs/machine-learning/feed/\n",
      "- Source: https://aws.amazon.com/blogs/machine-learning/feed\n",
      "  Channel Title: AWS Machine Learning Blog\n",
      "  Channel Description: Official Machine Learning Blog of Amazon Web Services\n",
      "- Title: Amazon Translate is now supported in AWS Mobile SDK for Android and iOS\n",
      "  Description: Amazon Translate is a neural machine translation service that delivers fast, high-quality, and affordable language translation. Support for Amazon Translate API is now available in the AWS Mobile SDK for Android and iOS. Now, you can use the AWS Mobile SDK to develop and publish multilingual mobile apps quickly and easily with Amazon Translate. By […]\n",
      "  Link: https://aws.amazon.com/blogs/machine-learning/amazon-translate-is-now-supported-in-aws-mobile-sdk-for-android-and-ios/\n",
      "  PubDate: Fri, 25 May 2018 20:51:08 +0000\n",
      "  Author: Woo Kim\n"
     ]
    }
   ],
   "source": [
    "# Print the first memento data\n",
    "for memento, data in list(feed_datas.items()):\n",
    "    print(f\"\\nMemento: {memento}\")\n",
    "    for item in data[:1]:\n",
    "        print(f\"- Source: {item['source']}\")\n",
    "        print(f\"  Channel Title: {item['channel_title']}\")\n",
    "        print(f\"  Channel Description: {item['channel_description']}\")\n",
    "        print(f\"- Title: {item['title']}\")\n",
    "        print(f\"  Description: {item['description']}\")\n",
    "        print(f\"  Link: {item['link']}\")\n",
    "        print(f\"  PubDate: {item['pubDate']}\")\n",
    "        print(f\"  Author: {item['author']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468a9b228e6b481790b7b402f4572c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing memento data:   0%|          | 0/257 [00:00<?, ?item/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] Failed to parse XML from memento https://proceedings.mlr.press//v263/assets/rss/feed.xml: not well-formed (invalid token): line 36, column 19\n",
      "[ERROR] Failed to parse XML from memento https://proceedings.mlr.press//v250/assets/rss/feed.xml: not well-formed (invalid token): line 901, column 291\n"
     ]
    }
   ],
   "source": [
    "# Special code to retrive link for http://proceedings.mlr.press//feed.xml\n",
    "feed = \"http://proceedings.mlr.press//feed.xml\"\n",
    "content = fetch_xml_from_memento(feed)\n",
    "memento_data = extract_memento_data(feed, content)\n",
    "for item in tqdm(memento_data, desc=\"Processing memento data\", unit=\"item\"):\n",
    "    # Example link: https://proceedings.mlr.press//v1/assets/rss/feed.xml\n",
    "    memento = f\"https://proceedings.mlr.press//{item['title']}/assets/rss/feed.xml\"\n",
    "    content = fetch_xml_from_memento(memento)\n",
    "    if content is None:\n",
    "        continue\n",
    "    data = extract_memento_data(feed, content)\n",
    "    if data is None:\n",
    "        continue\n",
    "    feed_datas[memento] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten all items from all mementos into one list\n",
    "all_items = []\n",
    "seen_keys = set()\n",
    "\n",
    "for memento_data in feed_datas.values():\n",
    "    for item in memento_data:\n",
    "        key = (item[\"title\"], item[\"link\"], item[\"pubDate\"], item[\"author\"])\n",
    "        if key not in seen_keys:\n",
    "            seen_keys.add(key)\n",
    "            all_items.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total items fetched: 36040\n",
      "\n",
      "Total unique items: 26650\n"
     ]
    }
   ],
   "source": [
    "# Compare the number of unique items with the original list\n",
    "print(f\"\\nTotal items fetched: {sum(len(data) for data in feed_datas.values())}\")\n",
    "print(f\"\\nTotal unique items: {len(all_items)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved unique items to memento_data\\memento_data_20250419_004034.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the unique items to a CSV file\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "output_dir = \"memento_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, f\"memento_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\")\n",
    "df = pd.DataFrame(all_items)\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"\\nSaved unique items to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
